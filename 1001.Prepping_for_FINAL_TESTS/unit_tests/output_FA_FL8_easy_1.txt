dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 25}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
res_block_channels:[32, 64, 64]
res_block_ds:[False, False, False]
reward_support:[-1, 1, 41]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64, 64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 41]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:2
max_workers:32
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
resampling:False
resampling_use_max:False
resampling_assess_best_child:False
rs_start:1000
ep_to_batch_ratio:[11, 12]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
env:<class 'game_play.frozen_lakeGym_Image.gridWorld'>
same_env_each_time:True
channels:3
env_size:[8, 8]
observable_size:[8, 8]
game_modes:1
env_map:[['S' 'H' 'F' 'F' 'F' 'H' 'F' 'F']
 ['F' 'F' 'F' 'H' 'F' 'H' 'H' 'F']
 ['H' 'F' 'F' 'H' 'F' 'F' 'F' 'H']
 ['H' 'F' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'H' 'H' 'F' 'H' 'F']
 ['H' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'G']]
max_steps:100
actions_size:5
optimal_score:1
total_frames:205000
exp_gamma:0.95
atari_env:False
reward_clipping:False
memory_size:100
image_size:[48, 48]
timesteps_in_obs:2
store_prev_actions:True
running_reward_in_obs:False
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 1, 6]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
state_size:[6, 6]
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 64)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:25
detach_expV_calc:True
use_new_episode_expV:True
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['S' 'H' 'F' 'F' 'F' 'H' 'F' 'F']
 ['F' 'F' 'F' 'H' 'F' 'H' 'H' 'F']
 ['H' 'F' 'F' 'H' 'F' 'F' 'F' 'H']
 ['H' 'F' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'H' 'H' 'F' 'H' 'F']
 ['H' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'G']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1 36
1 40
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1 65
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1 84
1 105
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.00352154755753211
maxi score, test score, baseline:  0.0001 0.0 0.0001
1 201
in main func line 156:  2
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
deleting a thread, now have 2 threads
Frames:  1161 train batches done:  34 episodes:  206
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.31552497
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.02110405101798505
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.3567637
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
2 246
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.36308357
2 272
2 276
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.37946305
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.2596764733559718
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
3 323
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
3 337
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
3 348
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
4 431
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
4 443
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.2500077609047562
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
5 522
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.20230639170413273
siam score:  -0.2681184
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.28385672
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.4250267
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
8 598
8 624
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.38520703
first move QE:  -0.20402941643320097
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
9 673
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
11 770
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
12 781
12 787
12 795
first move QE:  -0.21188057717186645
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
12 832
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.2099358
siam score:  -0.20179456
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus -0.021853206399828195
first move QE:  -0.21433358321387144
16 978
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
17 991
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.18134655
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.16322409
siam score:  -0.1625497
rdn beta is 0 so we're just using the maxi policy
18 1054
line 256 mcts: sample exp_bonus -0.01079908774942473
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [-0.293]
 [-0.002]
 [-0.293]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.008]
 [-0.026]
 [-0.021]
 [-0.035]
 [-0.005]] [[0.935]
 [0.899]
 [0.909]
 [0.88 ]
 [0.94 ]]
using explorer policy with actor:  1
siam score:  -0.2273819
20 1112
20 1114
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.058]
 [-0.08 ]
 [-0.046]
 [-0.057]
 [-0.069]] [[0.366]
 [0.335]
 [0.382]
 [0.366]
 [0.351]]
siam score:  -0.23899612
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
20 1141
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.159]
 [0.47 ]
 [0.301]
 [0.062]
 [0.159]] [[0.106]
 [0.313]
 [0.201]
 [0.042]
 [0.106]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.23376065
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.116]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]] [[0.077]
 [0.077]
 [0.077]
 [0.077]
 [0.077]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
21 1188
21 1195
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
21 1233
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.147]
 [ 0.   ]
 [-0.059]
 [ 0.   ]
 [ 0.   ]] [[0.   ]
 [0.098]
 [0.059]
 [0.098]
 [0.098]]
23 1294
23 1300
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[-0.]
 [-0.]
 [-0.]
 [ 0.]
 [-0.]] [[0.211]
 [0.211]
 [0.211]
 [0.54 ]
 [0.211]] [[0.01]
 [0.01]
 [0.01]
 [0.23]
 [0.01]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.615]
 [0.615]
 [0.253]
 [0.615]
 [0.615]] [[0.564]
 [0.564]
 [0.203]
 [0.564]
 [0.564]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
24 1364
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
29 1436
29 1445
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
29 1467
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.346]
 [ 0.346]
 [-0.022]
 [ 0.346]
 [ 0.346]] [[0.833]
 [0.833]
 [0.097]
 [0.833]
 [0.833]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
29 1498
29 1500
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.23624916
30 1536
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
30 1543
30 1551
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.02]
 [-0.02]
 [-0.02]
 [-0.02]
 [-0.02]] [[0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.631]]
siam score:  -0.26784804
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.007]
 [ 0.003]
 [ 0.048]
 [ 0.124]
 [ 0.114]] [[0.02 ]
 [0.024]
 [0.039]
 [0.064]
 [0.061]]
siam score:  -0.2558846
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.03 ]
 [-0.025]
 [ 0.   ]
 [ 0.024]
 [-0.03 ]] [[0.174]
 [0.179]
 [0.204]
 [0.228]
 [0.174]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.025]
 [-0.054]
 [-0.045]
 [-0.025]
 [-0.025]] [[0.082]
 [0.053]
 [0.062]
 [0.082]
 [0.082]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.009]
 [-0.01 ]
 [-0.012]
 [-0.009]
 [-0.009]] [[0.115]
 [0.115]
 [0.112]
 [0.115]
 [0.115]]
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.061]
 [-0.07 ]
 [-0.057]
 [-0.049]
 [-0.048]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.26555642
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]] [[0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.12452501921558372
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.3649671
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.092]
 [-0.1  ]
 [-0.101]
 [-0.096]
 [-0.094]] [[0.024]
 [0.007]
 [0.005]
 [0.015]
 [0.019]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
33 1743
33 1744
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.011]
 [-0.011]
 [-0.011]
 [-0.079]
 [-0.011]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
38 1844
UNIT TEST: sample policy line 217 mcts : [0.208 0.167 0.167 0.375 0.083]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
40 1903
40 1907
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.11458938168331645
siam score:  -0.20743747
first move QE:  -0.11458938168331645
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.021]
 [-0.021]
 [-0.017]
 [-0.021]
 [-0.021]] [[0.116]
 [0.116]
 [0.123]
 [0.116]
 [0.116]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
41 1948
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [-0.023]
 [ 0.   ]
 [-0.027]
 [-0.025]] [[0.009]
 [0.002]
 [0.009]
 [0.   ]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.019]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]] [[0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]]
using explorer policy with actor:  1
siam score:  -0.35080445
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
43 2046
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.34180954
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
44 2079
44 2098
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
in main func line 156:  45
45 2121
siam score:  -0.30665958
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.10476433202451353
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0]
 [0]
 [0]
 [0]
 [0]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.001]
 [-0.048]
 [-0.001]
 [ 0.   ]
 [ 0.002]] [[0.164]
 [0.086]
 [0.165]
 [0.167]
 [0.17 ]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.28681052
48 2246
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.09985329852556747
50 2284
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.083 0.208 0.083 0.333 0.292]
using explorer policy with actor:  1
50 2297
50 2301
50 2323
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
50 2344
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.46894416
siam score:  -0.4665078
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
50 2393
first move QE:  -0.09528933172405771
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.009]
 [-0.035]
 [-0.075]
 [-0.006]
 [ 0.002]] [[0.056]
 [0.039]
 [0.012]
 [0.058]
 [0.063]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
50 2415
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.052]
 [-0.053]
 [-0.058]
 [-0.07 ]
 [-0.014]] [[0.083]
 [0.08 ]
 [0.072]
 [0.047]
 [0.159]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.067]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus -0.01930851283179882
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.33225006
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.4454338
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.002]
 [ 0.001]
 [-0.002]
 [ 0.   ]] [[0.097]
 [0.1  ]
 [0.098]
 [0.094]
 [0.097]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.44607422
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.01 ]
 [-0.029]
 [-0.   ]
 [-0.001]
 [-0.006]] [[0.084]
 [0.052]
 [0.1  ]
 [0.099]
 [0.091]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.018]
 [-0.012]
 [-0.047]
 [-0.014]
 [-0.009]] [[0.091]
 [0.1  ]
 [0.053]
 [0.096]
 [0.103]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
58 2646
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus -0.05279045855117105
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.033]
 [-0.038]
 [-0.049]
 [-0.052]
 [-0.051]] [[0.052]
 [0.041]
 [0.019]
 [0.013]
 [0.016]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.33833882
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0]
 [0]
 [0]
 [0]
 [0]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.36764434
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.065]]
siam score:  -0.3626128
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
63 2788
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.01 ]
 [-0.023]
 [-0.032]
 [-0.008]
 [-0.015]] [[0.092]
 [0.069]
 [0.054]
 [0.094]
 [0.083]]
64 2810
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
65 2838
siam score:  -0.38294005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.167 0.167 0.083 0.417 0.167]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[0.077]
 [0.077]
 [0.077]
 [0.077]
 [0.077]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
67 2958
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
70 2989
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.30233985
using explorer policy with actor:  1
71 3008
siam score:  -0.36745322
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.005]
 [ 0.001]
 [-0.036]
 [ 0.001]
 [ 0.004]] [[0.108]
 [0.103]
 [0.054]
 [0.103]
 [0.107]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.018]
 [-0.01 ]
 [-0.024]
 [-0.022]
 [-0.001]] [[0.03 ]
 [0.043]
 [0.019]
 [0.022]
 [0.058]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
71 3053
71 3055
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]] [[0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
71 3062
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [-0.005]
 [ 0.   ]
 [ 0.   ]
 [-0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.010912431000242667
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
71 3112
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.008]
 [-0.058]
 [-0.02 ]
 [-0.008]
 [-0.008]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
71 3122
71 3126
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.012]
 [-0.012]
 [-0.012]
 [-0.009]
 [-0.007]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.002]
 [0.004]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
72 3191
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
72 3230
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
STARTED EXPV TRAINING ON FRAME NO.  20047
siam score:  -0.44883156
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.439]
 [-0.172]
 [ 0.439]
 [-0.005]
 [ 0.439]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.338]
 [-0.048]
 [ 0.338]
 [ 0.338]
 [ 0.338]] [[0.908]
 [0.394]
 [0.908]
 [0.908]
 [0.908]]
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.121]
 [0.121]
 [0.121]
 [0.702]
 [0.334]] [[0.617]
 [0.617]
 [0.617]
 [1.585]
 [0.972]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.006525559765724425
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus -0.05371726510476375
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
77 3291
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.3798805
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20047
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.04 ]
 [ 0.04 ]
 [ 0.04 ]
 [-0.052]
 [ 0.04 ]] [[0.817]
 [0.817]
 [0.817]
 [0.726]
 [0.817]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.3912545
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.3937296
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.058]
 [ 0.058]
 [ 0.058]
 [-0.061]
 [ 0.058]] [[0.4  ]
 [0.4  ]
 [0.4  ]
 [0.321]
 [0.4  ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.015]
 [-0.015]
 [-0.015]
 [ 0.001]
 [-0.015]] [[0.921]
 [0.921]
 [0.921]
 [0.946]
 [0.921]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20047
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.414]
 [-0.051]
 [ 0.063]
 [ 0.838]
 [ 1.035]] [[0.062]
 [0.442]
 [0.561]
 [1.372]
 [1.578]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.323]
 [-0.055]
 [-0.005]
 [ 0.124]
 [ 0.137]] [[0.175]
 [0.532]
 [0.599]
 [0.771]
 [0.789]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.42489293
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]] [[0.8]
 [0.8]
 [0.8]
 [0.8]
 [0.8]]
96 3485
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.44504273
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
99 3517
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.40675682
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
105 3560
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.117]
 [1.117]
 [0.693]
 [1.595]
 [1.687]] [[1.   ]
 [1.   ]
 [0.669]
 [1.372]
 [1.444]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.35835066
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
115 3664
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
in main func line 156:  116
first move QE:  -0.0528973182138384
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.3910051
117 3706
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.143]
 [-0.143]
 [-0.143]
 [-0.135]
 [ 0.097]] [[1.531]
 [1.531]
 [1.531]
 [1.546]
 [2.   ]]
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
120 3769
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]] [[1.322]
 [1.322]
 [1.322]
 [1.322]
 [1.322]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.19066379580844683
120 3775
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
122 3788
123 3793
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
siam score:  -0.48673743
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.48046014
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.099]
 [0.37 ]
 [0.262]
 [0.272]
 [0.287]] [[0.125]
 [0.306]
 [0.234]
 [0.241]
 [0.251]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
132 3859
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.527]
 [0.527]
 [0.324]
 [0.489]
 [0.527]] [[0.841]
 [0.841]
 [0.579]
 [0.793]
 [0.841]]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.5012022
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20047
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20047
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
139 3991
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 2.150574565361036
siam score:  -0.46373284
139 4002
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
139 4024
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.663]
 [ 0.531]
 [ 0.458]
 [-0.194]
 [ 0.526]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.018]
 [1.537]
 [2.018]
 [2.018]
 [1.713]] [[2.   ]
 [1.322]
 [2.   ]
 [2.   ]
 [1.57 ]]
in main func line 156:  140
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.04270455269839428
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.44363394
line 256 mcts: sample exp_bonus 0.15670056807888738
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
146 4071
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.443]
 [4.443]
 [4.443]
 [6.827]
 [7.753]] [[0.601]
 [0.601]
 [0.601]
 [1.269]
 [1.529]]
149 4098
siam score:  -0.44859585
siam score:  -0.46181694
first move QE:  -0.040931203053681794
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.49170822
siam score:  -0.5011922
151 4110
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
153 4126
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.087]
 [-0.087]
 [-0.087]
 [ 0.159]
 [-0.087]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.081]
 [1.081]
 [1.196]
 [0.959]
 [1.305]] [[1.192]
 [0.515]
 [0.592]
 [0.432]
 [0.666]]
using explorer policy with actor:  1
in main func line 156:  156
156 4154
156 4155
using explorer policy with actor:  1
156 4158
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.49854064
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
163 4191
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.566]
 [1.23 ]
 [0.566]
 [0.469]
 [0.501]] [[0.442]
 [1.302]
 [0.442]
 [0.315]
 [0.357]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.037319886131998214
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
169 4215
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
first move QE:  -0.03500802333249592
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
start point for exploration sampling:  20047
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
180 4245
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
siam score:  -0.52478296
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.038]
 [-0.18 ]
 [ 0.188]
 [ 0.048]
 [ 0.157]] [[0.145]
 [0.   ]
 [0.245]
 [0.152]
 [0.225]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.292 0.042 0.083 0.25  0.333]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.296]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.754]
 [1.347]
 [1.272]
 [1.343]
 [1.449]] [[0.378]
 [0.94 ]
 [0.868]
 [0.937]
 [1.037]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.51878
siam score:  -0.5219801
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.991]
 [3.184]
 [4.337]
 [4.732]
 [4.265]] [[1.31 ]
 [0.772]
 [1.54 ]
 [1.804]
 [1.492]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
line 256 mcts: sample exp_bonus -0.09293035734536889
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.25 ]
 [ 0.973]
 [ 0.27 ]
 [ 0.601]
 [ 0.475]] [[0.08 ]
 [1.563]
 [0.711]
 [1.112]
 [0.959]]
siam score:  -0.5168306
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
211 4361
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.4  ]
 [-0.006]
 [ 0.347]
 [ 0.631]
 [ 0.335]] [[0.179]
 [0.69 ]
 [1.148]
 [1.516]
 [1.132]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.535181
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
in main func line 156:  218
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.592]
 [ 0.374]
 [ 0.253]
 [ 0.167]
 [ 0.062]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20047
first move QE:  -0.02325719118059345
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
220 4447
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
223 4452
start point for exploration sampling:  20047
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 2.5224872033068713
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.375]
 [ 0.012]
 [ 0.183]
 [ 0.163]
 [ 0.078]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.051]
 [1.051]
 [1.051]
 [2.358]
 [0.702]] [[0.845]
 [0.845]
 [0.845]
 [1.679]
 [0.622]]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.397]
 [0.397]
 [0.397]
 [0.376]
 [0.533]] [[0.963]
 [0.963]
 [0.963]
 [0.936]
 [1.145]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.078]
 [2.078]
 [2.078]
 [2.564]
 [2.078]] [[1.098]
 [1.098]
 [1.098]
 [1.697]
 [1.098]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.153]
 [3.153]
 [3.153]
 [3.153]
 [4.285]] [[0.89 ]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [1.316]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.626]
 [0.357]
 [0.626]
 [0.314]
 [0.4  ]] [[1.331]
 [0.793]
 [1.331]
 [0.708]
 [0.879]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.072]
 [ 0.22 ]
 [ 0.304]
 [ 0.669]
 [ 0.489]] [[0.032]
 [0.518]
 [0.659]
 [1.267]
 [0.966]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
249 4528
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20047
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.264]
 [ 0.143]
 [ 0.179]
 [-0.008]
 [ 0.12 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.014463633710134273
siam score:  -0.45918706
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.76 ]
 [1.138]
 [1.138]
 [1.138]
 [1.211]] [[1.754]
 [0.397]
 [0.397]
 [0.397]
 [0.459]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.452]
 [ 0.783]
 [ 0.065]
 [ 0.302]
 [ 0.224]] [[0.   ]
 [0.413]
 [0.173]
 [0.252]
 [0.226]]
siam score:  -0.4306409
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.522]
 [-0.101]
 [ 0.275]
 [-0.19 ]
 [-0.275]] [[0.   ]
 [0.141]
 [0.266]
 [0.111]
 [0.082]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.227]
 [3.227]
 [3.227]
 [3.227]
 [3.364]] [[1.832]
 [1.832]
 [1.832]
 [1.832]
 [1.939]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.4189260996742721
Printing some Q and Qe and total Qs values:  [[1.5]
 [0. ]
 [1.5]
 [1.5]
 [1.5]] [[ 0.   ]
 [-0.256]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[3.086]
 [0.   ]
 [3.086]
 [3.086]
 [3.086]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
siam score:  -0.54698735
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.031]
 [6.76 ]
 [6.031]
 [6.031]
 [7.215]] [[1.385]
 [1.712]
 [1.385]
 [1.385]
 [1.916]]
line 256 mcts: sample exp_bonus 8.585961303154615
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.357]
 [1.408]
 [0.576]
 [0.434]
 [1.408]] [[0.187]
 [1.238]
 [0.406]
 [0.265]
 [1.238]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.021742911621648014, 0.456514176756704, 0.021742911621648014, 0.021742911621648014, 0.021742911621648014, 0.456514176756704]
siam score:  -0.49268982
274 4723
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.763]
 [3.763]
 [3.763]
 [3.763]
 [4.795]] [[1.235]
 [1.235]
 [1.235]
 [1.235]
 [1.691]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.086]
 [1.131]
 [1.498]
 [1.487]
 [1.24 ]] [[0.761]
 [0.821]
 [1.311]
 [1.296]
 [0.966]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.001]] [[1.066]
 [0.807]
 [1.066]
 [1.066]
 [0.927]] [[1.856]
 [1.431]
 [1.856]
 [1.856]
 [1.627]]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.015154270335628264, 0.3181790629977051, 0.3181790629977051, 0.015154270335628264, 0.015154270335628264, 0.3181790629977051]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.285]
 [4.285]
 [4.285]
 [4.285]
 [4.285]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
line 256 mcts: sample exp_bonus 1.393781259785112
siam score:  -0.49631187
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.015154270335628264, 0.3181790629977051, 0.3181790629977051, 0.015154270335628264, 0.015154270335628264, 0.3181790629977051]
282 4762
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.015154270335628264, 0.3181790629977051, 0.3181790629977051, 0.015154270335628264, 0.015154270335628264, 0.3181790629977051]
282 4765
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.007109292591599374
siam score:  -0.5217701
line 256 mcts: sample exp_bonus 0.7302249912494393
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.851]] [[0.637]
 [0.637]
 [0.637]
 [0.637]
 [1.283]]
from probs:  [0.015154270335628264, 0.3181790629977051, 0.3181790629977051, 0.015154270335628264, 0.015154270335628264, 0.3181790629977051]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.015154270335628264, 0.3181790629977051, 0.3181790629977051, 0.015154270335628264, 0.015154270335628264, 0.3181790629977051]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20047
288 4800
from probs:  [0.015154270335628264, 0.3181790629977051, 0.3181790629977051, 0.015154270335628264, 0.015154270335628264, 0.3181790629977051]
siam score:  -0.5596404
289 4807
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.015154270335628264, 0.3181790629977051, 0.3181790629977051, 0.015154270335628264, 0.015154270335628264, 0.3181790629977051]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]] [[-0.039]
 [ 0.184]
 [ 1.272]
 [ 1.173]
 [ 0.086]] [[0.001]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.1927766351822306
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.015154270335628264, 0.3181790629977051, 0.3181790629977051, 0.015154270335628264, 0.015154270335628264, 0.3181790629977051]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[-0.518]
 [-0.05 ]
 [ 0.082]
 [ 0.03 ]
 [-0.031]] [[0.001]
 [0.468]
 [0.601]
 [0.548]
 [0.489]]
from probs:  [0.015154270335628264, 0.3181790629977051, 0.3181790629977051, 0.015154270335628264, 0.015154270335628264, 0.3181790629977051]
using another actor
Printing some Q and Qe and total Qs values:  [[0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.105]] [[4.332]
 [4.332]
 [4.332]
 [4.332]
 [4.283]] [[1.46 ]
 [1.46 ]
 [1.46 ]
 [1.46 ]
 [1.431]]
using another actor
from probs:  [0.015154270335628264, 0.3181790629977051, 0.3181790629977051, 0.015154270335628264, 0.015154270335628264, 0.3181790629977051]
using another actor
303 4884
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.015154270335628264, 0.3181790629977051, 0.3181790629977051, 0.015154270335628264, 0.015154270335628264, 0.3181790629977051]
using another actor
siam score:  -0.6741319
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.015154270335628264, 0.3181790629977051, 0.3181790629977051, 0.015154270335628264, 0.015154270335628264, 0.3181790629977051]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.002]
 [0.003]
 [0.003]] [[0.771]
 [0.796]
 [0.711]
 [0.909]
 [0.909]] [[0.877]
 [0.927]
 [0.759]
 [1.155]
 [1.157]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.015154270335628264, 0.3181790629977051, 0.3181790629977051, 0.015154270335628264, 0.015154270335628264, 0.3181790629977051]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.015154270335628264, 0.3181790629977051, 0.3181790629977051, 0.015154270335628264, 0.015154270335628264, 0.3181790629977051]
siam score:  -0.6892961
siam score:  -0.69637895
line 256 mcts: sample exp_bonus 2.9465984366150155
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.015154270335628264, 0.3181790629977051, 0.3181790629977051, 0.015154270335628264, 0.015154270335628264, 0.3181790629977051]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
from probs:  [0.015154270335628264, 0.3181790629977051, 0.3181790629977051, 0.015154270335628264, 0.015154270335628264, 0.3181790629977051]
from probs:  [0.015154270335628264, 0.3181790629977051, 0.3181790629977051, 0.015154270335628264, 0.015154270335628264, 0.3181790629977051]
from probs:  [0.015154270335628264, 0.3181790629977051, 0.3181790629977051, 0.015154270335628264, 0.015154270335628264, 0.3181790629977051]
318 4947
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]] [[0.76]
 [0.76]
 [0.76]
 [0.76]
 [0.76]] [[1.437]
 [1.437]
 [1.437]
 [1.437]
 [1.437]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7328914
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.024]
 [0.024]
 [0.023]
 [0.023]] [[1.313]
 [1.313]
 [1.313]
 [2.1  ]
 [1.631]] [[0.977]
 [0.977]
 [0.977]
 [1.865]
 [1.335]]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.76862055
from probs:  [0.009435742503908827, 0.1981128514992182, 0.3867899604945277, 0.009435742503908827, 0.009435742503908827, 0.3867899604945277]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.    0.083 0.417 0.208 0.292]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.009435742503908827, 0.1981128514992182, 0.3867899604945277, 0.009435742503908827, 0.009435742503908827, 0.3867899604945277]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.014]
 [0.014]
 [0.007]
 [0.013]] [[0.079]
 [0.504]
 [0.548]
 [0.34 ]
 [0.435]] [[0.013]
 [0.014]
 [0.014]
 [0.007]
 [0.013]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.03457549588365374
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.008]
 [0.01 ]
 [0.009]
 [0.008]] [[-0.541]
 [ 0.152]
 [ 0.242]
 [ 0.427]
 [ 0.118]] [[0.007]
 [0.008]
 [0.01 ]
 [0.009]
 [0.008]]
from probs:  [0.009435742503908827, 0.1981128514992182, 0.3867899604945277, 0.009435742503908827, 0.009435742503908827, 0.3867899604945277]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.009435742503908827, 0.1981128514992182, 0.3867899604945277, 0.009435742503908827, 0.009435742503908827, 0.3867899604945277]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 2.510669385298002
rdn probs:  [0.009435742503908827, 0.1981128514992182, 0.3867899604945277, 0.009435742503908827, 0.009435742503908827, 0.3867899604945277]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.009435742503908827, 0.1981128514992182, 0.3867899604945277, 0.009435742503908827, 0.009435742503908827, 0.3867899604945277]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.009435742503908827, 0.1981128514992182, 0.3867899604945277, 0.009435742503908827, 0.009435742503908827, 0.3867899604945277]
340 5020
start point for exploration sampling:  20047
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.009435742503908827, 0.1981128514992182, 0.3867899604945277, 0.009435742503908827, 0.009435742503908827, 0.3867899604945277]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.008]
 [0.001]
 [0.006]
 [0.006]] [[-0.553]
 [-0.232]
 [-0.091]
 [-0.236]
 [-0.277]] [[0.113]
 [0.438]
 [0.566]
 [0.43 ]
 [0.388]]
using explorer policy with actor:  1
from probs:  [0.009435742503908827, 0.1981128514992182, 0.3867899604945277, 0.009435742503908827, 0.009435742503908827, 0.3867899604945277]
from probs:  [0.009435742503908827, 0.1981128514992182, 0.3867899604945277, 0.009435742503908827, 0.009435742503908827, 0.3867899604945277]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.19197453190973024
using another actor
from probs:  [0.009435742503908827, 0.1981128514992182, 0.3867899604945277, 0.009435742503908827, 0.009435742503908827, 0.3867899604945277]
from probs:  [0.009435742503908827, 0.1981128514992182, 0.3867899604945277, 0.009435742503908827, 0.009435742503908827, 0.3867899604945277]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.009435742503908827, 0.1981128514992182, 0.3867899604945277, 0.009435742503908827, 0.009435742503908827, 0.3867899604945277]
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.068]
 [0.065]
 [0.031]
 [0.05 ]] [[2.983]
 [3.179]
 [2.983]
 [2.435]
 [3.454]] [[1.513]
 [1.663]
 [1.513]
 [1.069]
 [1.848]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.051]
 [0.052]
 [0.045]
 [0.038]
 [0.071]] [[0.484]
 [0.721]
 [1.567]
 [1.881]
 [4.855]] [[0.007]
 [0.1  ]
 [0.427]
 [0.547]
 [1.72 ]]
start point for exploration sampling:  20047
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.007938019864691498, 0.16666666666666666, 0.32539531346864187, 0.16666666666666666, 0.007938019864691498, 0.32539531346864187]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.005377384818140648, 0.112903572717158, 0.3279559485151928, 0.3279559485151928, 0.005377384818140648, 0.22042976061617536]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.005377384818140648, 0.112903572717158, 0.3279559485151928, 0.3279559485151928, 0.005377384818140648, 0.22042976061617536]
using another actor
354 5095
first move QE:  -0.009209161398503614
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.44690116667323476, 0.2699109561427707, 0.004425640347074613, 0.1814158508775387]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.011]
 [0.005]
 [0.005]
 [0.008]] [[0.954]
 [1.934]
 [0.954]
 [0.765]
 [1.096]] [[0.556]
 [1.22 ]
 [0.556]
 [0.43 ]
 [0.656]]
siam score:  -0.7534932
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.44690116667323476, 0.2699109561427707, 0.004425640347074613, 0.1814158508775387]
from probs:  [0.004425640347074613, 0.09292074561230663, 0.44690116667323476, 0.2699109561427707, 0.004425640347074613, 0.1814158508775387]
siam score:  -0.7493205
from probs:  [0.004425640347074613, 0.09292074561230663, 0.44690116667323476, 0.2699109561427707, 0.004425640347074613, 0.1814158508775387]
from probs:  [0.004425640347074613, 0.09292074561230663, 0.44690116667323476, 0.2699109561427707, 0.004425640347074613, 0.1814158508775387]
line 256 mcts: sample exp_bonus 0.13115137125036083
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.002]] [[-0.242]
 [ 0.001]
 [ 0.056]
 [-0.   ]
 [-0.26 ]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.004425640347074613, 0.09292074561230663, 0.44690116667323476, 0.2699109561427707, 0.004425640347074613, 0.1814158508775387]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.44690116667323476, 0.2699109561427707, 0.004425640347074613, 0.1814158508775387]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[-0.106]
 [-0.106]
 [-0.106]
 [-0.106]
 [-0.106]] [[0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.252]]
using another actor
from probs:  [0.004425640347074613, 0.09292074561230663, 0.44690116667323476, 0.2699109561427707, 0.004425640347074613, 0.1814158508775387]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.44690116667323476, 0.2699109561427707, 0.004425640347074613, 0.1814158508775387]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.44690116667323476, 0.2699109561427707, 0.004425640347074613, 0.1814158508775387]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.004]
 [0.004]] [[-0.008]
 [-0.008]
 [-0.008]
 [-0.016]
 [-0.128]] [[0.608]
 [0.608]
 [0.608]
 [0.6  ]
 [0.45 ]]
siam score:  -0.7133527
start point for exploration sampling:  20047
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.44690116667323476, 0.2699109561427707, 0.004425640347074613, 0.1814158508775387]
from probs:  [0.004425640347074613, 0.09292074561230663, 0.44690116667323476, 0.2699109561427707, 0.004425640347074613, 0.1814158508775387]
from probs:  [0.004425640347074613, 0.09292074561230663, 0.44690116667323476, 0.2699109561427707, 0.004425640347074613, 0.1814158508775387]
from probs:  [0.004425640347074613, 0.09292074561230663, 0.44690116667323476, 0.2699109561427707, 0.004425640347074613, 0.1814158508775387]
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.311]
 [0.44 ]
 [0.298]
 [0.363]] [[3.17 ]
 [3.17 ]
 [2.244]
 [2.911]
 [3.23 ]] [[1.778]
 [1.778]
 [1.321]
 [1.605]
 [1.865]]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9857105051529843
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
364 5231
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.003497188226393535, 0.1433567411751991, 0.3531460705984074, 0.2832162941240046, 0.003497188226393535, 0.21328651764960188]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.141]] [[3.483]
 [3.483]
 [3.483]
 [3.483]
 [4.129]] [[1.232]
 [1.232]
 [1.232]
 [1.232]
 [1.427]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.003497188226393535, 0.1433567411751991, 0.3531460705984074, 0.2832162941240046, 0.003497188226393535, 0.21328651764960188]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 0.49767501557523575
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.003497188226393535, 0.1433567411751991, 0.3531460705984074, 0.2832162941240046, 0.003497188226393535, 0.21328651764960188]
from probs:  [0.003497188226393535, 0.1433567411751991, 0.3531460705984074, 0.2832162941240046, 0.003497188226393535, 0.21328651764960188]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.045]
 [0.019]
 [0.015]
 [0.014]] [[0.472]
 [1.616]
 [0.589]
 [0.412]
 [0.553]] [[0.3  ]
 [1.502]
 [0.421]
 [0.237]
 [0.376]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.0030680869562801335, 0.12576702173907003, 0.3711648913046498, 0.30981542391325484, 0.0030680869562801335, 0.187116489130465]
from probs:  [0.0030680869562801335, 0.12576702173907003, 0.3711648913046498, 0.30981542391325484, 0.0030680869562801335, 0.187116489130465]
Printing some Q and Qe and total Qs values:  [[0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.286]] [[2.456]
 [2.456]
 [2.456]
 [2.456]
 [2.338]] [[2.094]
 [2.094]
 [2.094]
 [2.094]
 [2.062]]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.001]
 [0.004]
 [0.003]
 [0.003]] [[-0.76 ]
 [-0.159]
 [-0.258]
 [-0.096]
 [-0.275]] [[0.002]
 [1.   ]
 [0.842]
 [1.108]
 [0.81 ]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.001]
 [0.004]
 [0.003]
 [0.002]] [[-0.754]
 [-0.252]
 [-0.254]
 [-0.107]
 [-0.194]] [[0.002]
 [0.835]
 [0.837]
 [1.079]
 [0.933]]
from probs:  [0.0028907415046063355, 0.11849727691311951, 0.34971034773014587, 0.34971034773014587, 0.0028907415046063355, 0.1763005446173761]
start point for exploration sampling:  20047
line 256 mcts: sample exp_bonus -0.1674117122126548
from probs:  [0.0028907415046063355, 0.11849727691311951, 0.34971034773014587, 0.34971034773014587, 0.0028907415046063355, 0.1763005446173761]
using another actor
from probs:  [0.0028907415046063355, 0.11849727691311951, 0.34971034773014587, 0.34971034773014587, 0.0028907415046063355, 0.1763005446173761]
from probs:  [0.0028907415046063355, 0.11849727691311951, 0.34971034773014587, 0.34971034773014587, 0.0028907415046063355, 0.1763005446173761]
siam score:  -0.7025883
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.002732778005951988, 0.11202203711309511, 0.33060055532738136, 0.38524518488095294, 0.002732778005951988, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.5  ]
 [1.5  ]
 [0.007]
 [0.004]
 [0.003]] [[ 0.   ]
 [ 0.   ]
 [-0.21 ]
 [-0.044]
 [-0.194]] [[3.203]
 [3.203]
 [0.007]
 [0.167]
 [0.013]]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.015]
 [0.009]
 [0.007]] [[0.933]
 [0.933]
 [0.409]
 [0.482]
 [0.933]] [[0.434]
 [0.434]
 [0.275]
 [0.287]
 [0.434]]
373 5321
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using another actor
using explorer policy with actor:  0
from probs:  [0.002463539589440879, 0.10098541583577636, 0.34729010645161507, 0.3965510445747828, 0.002463539589440879, 0.15024635395894412]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.01830081250303928
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.012]
 [0.006]
 [0.006]] [[0.209]
 [0.209]
 [0.55 ]
 [0.209]
 [0.233]] [[0.953]
 [0.953]
 [1.425]
 [0.953]
 [0.987]]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.002347880779634718, 0.14319255439709067, 0.3309854525536986, 0.37793367709285053, 0.002347880779634718, 0.14319255439709067]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.002146346467815118, 0.1309013796669163, 0.3025747572657179, 0.3454931016654183, 0.002146346467815118, 0.2167380684663171]
377 5370
from probs:  [0.002146346467815118, 0.1309013796669163, 0.3025747572657179, 0.3454931016654183, 0.002146346467815118, 0.2167380684663171]
378 5380
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.002146346467815118, 0.1309013796669163, 0.30257475726571786, 0.3454931016654183, 0.002146346467815118, 0.2167380684663171]
using explorer policy with actor:  1
using another actor
using explorer policy with actor:  1
using another actor
381 5431
line 256 mcts: sample exp_bonus 1.424373531836771
line 256 mcts: sample exp_bonus 0.1758548350398793
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.002146346467815118, 0.1309013796669163, 0.30257475726571786, 0.3454931016654183, 0.002146346467815118, 0.2167380684663171]
381 5452
siam score:  -0.70908576
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.002146346467815118, 0.1309013796669163, 0.30257475726571786, 0.3454931016654183, 0.002146346467815118, 0.2167380684663171]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.002146346467815118, 0.1309013796669163, 0.30257475726571786, 0.3454931016654183, 0.002146346467815118, 0.2167380684663171]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using another actor
using another actor
siam score:  -0.7319557
siam score:  -0.73744535
from probs:  [0.0020580196708022687, 0.12551450491770058, 0.29012315191356497, 0.3724274754114972, 0.0020580196708022687, 0.20781882841563276]
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.564]
 [0.427]
 [0.304]
 [0.257]] [[1.057]
 [1.298]
 [0.742]
 [1.083]
 [1.248]] [[1.063]
 [1.607]
 [0.914]
 [1.056]
 [1.143]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0019015166308778038, 0.1159696974248855, 0.26806060515022906, 0.38212878594423677, 0.0399242435622137, 0.19201515128755728]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0019015166308778038, 0.1159696974248855, 0.26806060515022906, 0.38212878594423677, 0.0399242435622137, 0.19201515128755728]
385 5511
start point for exploration sampling:  20047
from probs:  [0.0019015166308778038, 0.1159696974248855, 0.26806060515022906, 0.38212878594423677, 0.0399242435622137, 0.19201515128755728]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0019015166308778038, 0.1159696974248855, 0.26806060515022906, 0.38212878594423677, 0.0399242435622137, 0.19201515128755728]
using explorer policy with actor:  1
386 5542
using another actor
from probs:  [0.0019015166308778038, 0.1159696974248855, 0.26806060515022906, 0.38212878594423677, 0.0399242435622137, 0.19201515128755728]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0019015166308778038, 0.1159696974248855, 0.26806060515022906, 0.38212878594423677, 0.0399242435622137, 0.19201515128755728]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0019015166308778038, 0.1159696974248855, 0.26806060515022906, 0.38212878594423677, 0.0399242435622137, 0.19201515128755728]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.083]
 [0.135]
 [0.083]
 [0.083]
 [0.054]] [[0.587]
 [1.016]
 [0.587]
 [0.587]
 [0.891]] [[0.544]
 [1.078]
 [0.544]
 [0.544]
 [0.79 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0019015166308778038, 0.1159696974248855, 0.26806060515022906, 0.38212878594423677, 0.0399242435622137, 0.19201515128755728]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.155]
 [0.559]
 [0.066]
 [0.483]] [[ 1.313]
 [-0.021]
 [ 2.045]
 [ 0.467]
 [ 1.313]] [[1.649]
 [0.266]
 [2.264]
 [0.482]
 [1.649]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
391 5606
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.007]
 [0.006]
 [0.006]
 [0.006]] [[-0.216]
 [-0.239]
 [-0.216]
 [-0.216]
 [-0.216]] [[0.456]
 [0.427]
 [0.456]
 [0.456]
 [0.456]]
from probs:  [0.0018318641601414445, 0.11172173249782494, 0.2948715130606307, 0.36813142528575304, 0.0384618202727026, 0.18498164472294723]
from probs:  [0.0018318641601414445, 0.11172173249782494, 0.2948715130606307, 0.36813142528575304, 0.0384618202727026, 0.18498164472294723]
from probs:  [0.0018318641601414445, 0.11172173249782494, 0.2948715130606307, 0.36813142528575304, 0.0384618202727026, 0.18498164472294723]
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0018391332867809707, 0.11172415554003809, 0.2948658592954666, 0.368122540797638, 0.03846747403786667, 0.1849808370422095]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0018391332867809707, 0.11172415554003809, 0.2948658592954666, 0.368122540797638, 0.03846747403786667, 0.1849808370422095]
from probs:  [0.0018391332867809707, 0.11172415554003809, 0.2948658592954666, 0.368122540797638, 0.03846747403786667, 0.1849808370422095]
from probs:  [0.0018391332867809707, 0.11172415554003809, 0.2948658592954666, 0.368122540797638, 0.03846747403786667, 0.1849808370422095]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 2.713370001339041
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.045]] [[3.625]
 [3.625]
 [3.625]
 [3.625]
 [2.79 ]] [[3.149]
 [3.149]
 [3.149]
 [3.149]
 [2.021]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0017741491472714637, 0.14311059273532448, 0.28444703632337753, 0.3551152581174041, 0.03710826004428472, 0.17844470363233775]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  20047
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0017136005938550032, 0.13822648286100947, 0.30886758569495254, 0.3429958062617412, 0.03584182116064362, 0.1723547034277981]
using explorer policy with actor:  1
398 5674
maxi score, test score, baseline:  0.0021 0.0 0.0021
actor:  1 policy actor:  1  step number:  93 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
in main func line 156:  399
using another actor
from probs:  [0.0016570484779109952, 0.1336647430289155, 0.29867436121767116, 0.36467820849317345, 0.03465897211566212, 0.16666666666666666]
from probs:  [0.0016570484779109952, 0.1336647430289155, 0.29867436121767116, 0.36467820849317345, 0.03465897211566212, 0.16666666666666666]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0016570484779109952, 0.1336647430289155, 0.29867436121767116, 0.36467820849317345, 0.03465897211566212, 0.16666666666666666]
maxi score, test score, baseline:  0.0021 0.0 0.0021
402 5693
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.067]] [[3.525]
 [3.525]
 [3.525]
 [3.525]
 [3.27 ]] [[2.02 ]
 [2.02 ]
 [2.02 ]
 [2.02 ]
 [1.822]]
siam score:  -0.6853133
maxi score, test score, baseline:  0.0021 0.0 0.0021
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.685976
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
408 5716
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0014223474656691797, 0.14306033535223844, 0.2846983232388077, 0.36968111597074926, 0.05807754262029689, 0.14306033535223844]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0014223474656691797, 0.14306033535223844, 0.2846983232388077, 0.36968111597074926, 0.05807754262029689, 0.14306033535223844]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.002]
 [0.006]
 [0.005]] [[-0.518]
 [-0.518]
 [ 1.239]
 [-0.518]
 [ 0.457]] [[0.006]
 [0.006]
 [0.002]
 [0.006]
 [0.005]]
from probs:  [0.0014223474656691797, 0.14306033535223844, 0.2846983232388077, 0.36968111597074926, 0.05807754262029689, 0.14306033535223844]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.0013514528209859536, 0.1353908153985649, 0.2694301779761438, 0.4034695405537228, 0.054967197852017535, 0.1353908153985649]
from probs:  [0.0013514528209859536, 0.1353908153985649, 0.2694301779761438, 0.4034695405537228, 0.054967197852017535, 0.1353908153985649]
using explorer policy with actor:  0
using explorer policy with actor:  1
from probs:  [0.0013514528209859536, 0.1353908153985649, 0.2694301779761438, 0.4034695405537228, 0.054967197852017535, 0.1353908153985649]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.0013514528209859536, 0.1353908153985649, 0.2694301779761438, 0.4034695405537228, 0.054967197852017535, 0.1353908153985649]
413 5791
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.001]
 [0.005]
 [0.004]
 [0.004]] [[-0.615]
 [-0.366]
 [-0.363]
 [-0.116]
 [-0.364]] [[0.003]
 [0.329]
 [0.342]
 [0.669]
 [0.339]]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.001316169126825167, 0.1318560356056474, 0.2623959020844696, 0.3929357685632919, 0.05353211571835407, 0.15796400890141185]
using another actor
from probs:  [0.001316169126825167, 0.1318560356056474, 0.2623959020844696, 0.3929357685632919, 0.05353211571835407, 0.15796400890141185]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.001316169126825167, 0.1318560356056474, 0.2623959020844696, 0.3929357685632919, 0.05353211571835407, 0.15796400890141185]
415 5831
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.001316169126825167, 0.1318560356056474, 0.2623959020844696, 0.3929357685632919, 0.05353211571835407, 0.15796400890141185]
siam score:  -0.73246443
from probs:  [0.001316169126825167, 0.1318560356056474, 0.2623959020844696, 0.3929357685632919, 0.05353211571835407, 0.15796400890141185]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
from probs:  [0.001316169126825167, 0.1318560356056474, 0.2623959020844696, 0.3929357685632919, 0.05353211571835407, 0.15796400890141185]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.001316169126825167, 0.1318560356056474, 0.2623959020844696, 0.3929357685632919, 0.05353211571835407, 0.15796400890141185]
siam score:  -0.7287967
maxi score, test score, baseline:  0.0041 0.0 0.0041
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.0012826809274249693, 0.1285011314960724, 0.25571958206471984, 0.40838172274709683, 0.05217006115488395, 0.15394482160980189]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.002]
 [0.001]
 [0.005]
 [0.005]] [[-0.128]
 [-0.074]
 [-0.201]
 [-0.128]
 [-0.128]] [[1.363]
 [1.467]
 [1.209]
 [1.363]
 [1.363]]
from probs:  [0.0012826809274249693, 0.1285011314960724, 0.25571958206471984, 0.40838172274709683, 0.05217006115488395, 0.15394482160980189]
Printing some Q and Qe and total Qs values:  [[0.102]
 [0.102]
 [0.102]
 [0.102]
 [0.053]] [[1.522]
 [1.522]
 [1.522]
 [1.522]
 [1.736]] [[1.106]
 [1.106]
 [1.106]
 [1.106]
 [1.357]]
using another actor
maxi score, test score, baseline:  0.0041 0.0 0.0041
from probs:  [0.0012826809274249693, 0.1285011314960724, 0.25571958206471984, 0.40838172274709683, 0.05217006115488395, 0.15394482160980189]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
siam score:  -0.7143449
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.0012826809274249693, 0.1285011314960724, 0.25571958206471984, 0.40838172274709683, 0.05217006115488395, 0.15394482160980189]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.002]
 [0.002]
 [0.005]
 [0.004]] [[-0.661]
 [-0.346]
 [-0.333]
 [-0.272]
 [-0.392]] [[0.049]
 [0.676]
 [0.702]
 [0.829]
 [0.589]]
418 5907
using another actor
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.0012826809274249693, 0.1285011314960724, 0.25571958206471984, 0.40838172274709683, 0.05217006115488395, 0.15394482160980189]
using another actor
from probs:  [0.0012826809274249693, 0.1285011314960724, 0.25571958206471984, 0.40838172274709683, 0.05217006115488395, 0.15394482160980189]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.0012826809274249693, 0.1285011314960724, 0.25571958206471984, 0.40838172274709683, 0.05217006115488395, 0.15394482160980189]
using another actor
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.0012826809274249693, 0.1285011314960724, 0.25571958206471984, 0.40838172274709683, 0.05217006115488395, 0.15394482160980189]
siam score:  -0.7165242
from probs:  [0.0012826809274249693, 0.1285011314960724, 0.25571958206471984, 0.40838172274709683, 0.05217006115488395, 0.15394482160980189]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.0012826809274249693, 0.1285011314960724, 0.25571958206471984, 0.40838172274709683, 0.05217006115488395, 0.15394482160980189]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.001]
 [0.005]
 [0.004]
 [0.004]] [[-0.599]
 [-0.391]
 [-0.421]
 [-0.41 ]
 [-0.536]] [[0.16 ]
 [0.432]
 [0.4  ]
 [0.414]
 [0.246]]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.0012508545713345901, 0.12531271364283364, 0.2493745727143327, 0.3982488036001316, 0.07568797001423404, 0.15012508545713343]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
using another actor
from probs:  [0.0012205693507779493, 0.12227868933801359, 0.24333680932524923, 0.3886065533099321, 0.09806706534056647, 0.14649031333546073]
from probs:  [0.0012205693507779493, 0.12227868933801359, 0.24333680932524923, 0.3886065533099321, 0.09806706534056647, 0.14649031333546073]
siam score:  -0.7500452
siam score:  -0.74930435
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.0012205693507779493, 0.12227868933801359, 0.24333680932524923, 0.3886065533099321, 0.09806706534056647, 0.14649031333546073]
from probs:  [0.0012205693507779493, 0.12227868933801359, 0.24333680932524923, 0.3886065533099321, 0.09806706534056647, 0.14649031333546073]
from probs:  [0.0012205693507779493, 0.12227868933801359, 0.24333680932524923, 0.3886065533099321, 0.09806706534056647, 0.14649031333546073]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.0012205693507779493, 0.12227868933801359, 0.24333680932524923, 0.3886065533099321, 0.09806706534056647, 0.14649031333546073]
423 6026
423 6029
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.2634964550816248
using another actor
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.0011641952361436888, 0.1166310357690667, 0.2320978763019897, 0.3937514530480819, 0.1166310357690667, 0.1397244038756513]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
from probs:  [0.0011641952361436888, 0.1166310357690667, 0.2320978763019897, 0.3937514530480819, 0.1166310357690667, 0.1397244038756513]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.007]
 [0.01 ]
 [0.006]
 [0.006]] [[ 0.   ]
 [-0.635]
 [-0.76 ]
 [ 0.   ]
 [ 0.   ]] [[0.254]
 [0.043]
 [0.007]
 [0.254]
 [0.254]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.0011379169022454308, 0.11399842810525991, 0.2268589393082744, 0.38486365499249464, 0.11399842810525991, 0.1591426325864657]
from probs:  [0.0011379169022454308, 0.11399842810525991, 0.2268589393082744, 0.38486365499249464, 0.11399842810525991, 0.1591426325864657]
maxi score, test score, baseline:  0.0041 0.0 0.0041
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.003]
 [0.004]
 [0.011]
 [0.012]] [[-0.621]
 [-0.566]
 [-0.515]
 [-0.673]
 [-0.62 ]] [[0.096]
 [0.17 ]
 [0.256]
 [0.007]
 [0.098]]
from probs:  [0.0011379169022454308, 0.11399842810525991, 0.2268589393082744, 0.38486365499249475, 0.11399842810525991, 0.1591426325864657]
from probs:  [0.0011379169022454308, 0.11399842810525991, 0.2268589393082744, 0.38486365499249475, 0.11399842810525991, 0.1591426325864657]
from probs:  [0.0011379169022454308, 0.11399842810525991, 0.2268589393082744, 0.38486365499249475, 0.11399842810525991, 0.1591426325864657]
428 6117
from probs:  [0.0011379169022454308, 0.11399842810525991, 0.2268589393082744, 0.38486365499249475, 0.11399842810525991, 0.1591426325864657]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.0011379169022454308, 0.11399842810525991, 0.2268589393082744, 0.38486365499249475, 0.11399842810525991, 0.1591426325864657]
maxi score, test score, baseline:  0.0041 0.0 0.0041
from probs:  [0.0011379169022454308, 0.11399842810525991, 0.2268589393082744, 0.38486365499249475, 0.11399842810525991, 0.1591426325864657]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.0011379169022454308, 0.11399842810525991, 0.2268589393082744, 0.38486365499249475, 0.11399842810525991, 0.1591426325864657]
maxi score, test score, baseline:  0.0041 0.0 0.0041
siam score:  -0.78143716
start point for exploration sampling:  20047
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.012]] [[-0.554]
 [-0.276]
 [-0.554]
 [-0.554]
 [-0.601]] [[0.106]
 [0.199]
 [0.106]
 [0.106]
 [0.089]]
using explorer policy with actor:  1
from probs:  [0.0011379169022454308, 0.11399842810525991, 0.2268589393082744, 0.38486365499249475, 0.11399842810525991, 0.1591426325864657]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.003]
 [0.017]
 [0.012]
 [0.012]] [[-0.789]
 [-0.234]
 [-0.432]
 [-0.479]
 [-0.62 ]] [[0.011]
 [0.355]
 [0.251]
 [0.21 ]
 [0.115]]
using another actor
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.007]] [[-0.081]
 [-0.081]
 [-0.081]
 [-0.081]
 [-0.269]] [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.007]]
428 6203
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.0011127986962993519, 0.1335558930725932, 0.2218512893234557, 0.3763682327624653, 0.11148204400987753, 0.1556297421353088]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]] [[-0.399]
 [-0.399]
 [-0.399]
 [-0.399]
 [-0.399]] [[0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.015]
 [0.015]
 [0.012]
 [0.017]] [[-0.249]
 [-0.055]
 [-0.249]
 [-0.309]
 [-0.432]] [[0.015]
 [0.015]
 [0.015]
 [0.012]
 [0.017]]
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
rdn probs:  [0.0011262379775376994, 0.13355858092884088, 0.22184680956304298, 0.3763512096728967, 0.11148652377029034, 0.1556306380873914]
using another actor
429 6238
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.001718213058419244, 0.1336769759450172, 0.2216494845360825, 0.37560137457044673, 0.11168384879725088, 0.1556701030927835]
using another actor
from probs:  [0.001718213058419244, 0.1336769759450172, 0.2216494845360825, 0.37560137457044673, 0.11168384879725088, 0.1556701030927835]
429 6248
first move QE:  -0.05760731890911179
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.024]
 [0.053]
 [0.018]
 [0.012]] [[0.117]
 [0.064]
 [0.325]
 [0.532]
 [0.938]] [[0.594]
 [0.57 ]
 [0.801]
 [0.869]
 [1.128]]
from probs:  [0.001718213058419244, 0.1336769759450172, 0.2216494845360825, 0.37560137457044673, 0.11168384879725088, 0.1556701030927835]
using another actor
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0017182130584192442, 0.1336769759450172, 0.2216494845360825, 0.3756013745704468, 0.11168384879725089, 0.15567010309278353]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
434 6318
maxi score, test score, baseline:  0.0141 0.2 0.2
siam score:  -0.763165
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
maxi score, test score, baseline:  0.0141 0.2 0.2
435 6336
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0016812373907195697, 0.13080026899798256, 0.2168796234028245, 0.36751849361129796, 0.13080026899798256, 0.152320107599193]
from probs:  [0.0016812373907195697, 0.13080026899798256, 0.2168796234028245, 0.36751849361129796, 0.13080026899798256, 0.152320107599193]
using another actor
siam score:  -0.7781224
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0016812373907195697, 0.13080026899798256, 0.2168796234028245, 0.36751849361129796, 0.13080026899798256, 0.152320107599193]
using another actor
from probs:  [0.0016812373907195697, 0.13080026899798256, 0.2168796234028245, 0.36751849361129796, 0.13080026899798256, 0.152320107599193]
using another actor
from probs:  [0.0016812373907195697, 0.13080026899798256, 0.2168796234028245, 0.36751849361129796, 0.13080026899798256, 0.152320107599193]
using another actor
from probs:  [0.0016812373907195697, 0.13080026899798256, 0.2168796234028245, 0.36751849361129796, 0.13080026899798256, 0.152320107599193]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0016812373907195695, 0.13080026899798253, 0.21687962340282446, 0.36751849361129796, 0.13080026899798253, 0.152320107599193]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0016812373907195695, 0.13080026899798253, 0.21687962340282446, 0.36751849361129796, 0.13080026899798253, 0.152320107599193]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0016812373907195695, 0.13080026899798253, 0.21687962340282446, 0.36751849361129796, 0.13080026899798253, 0.152320107599193]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.022712310730743913, 0.12804476629361422, 0.21231073074391044, 0.3597761685319289, 0.12804476629361422, 0.14911125740618827]
using explorer policy with actor:  1
using another actor
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.02224371373307544, 0.12540296582849775, 0.20793036750483557, 0.3729851708575113, 0.12540296582849775, 0.1460348162475822]
using explorer policy with actor:  0
444 6451
444 6452
using another actor
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.12738433668706756
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.02224371373307544, 0.12540296582849775, 0.20793036750483557, 0.3729851708575113, 0.12540296582849775, 0.1460348162475822]
using another actor
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.014]
 [0.019]
 [0.013]
 [0.014]] [[-1.166]
 [-1.166]
 [-0.447]
 [-0.823]
 [-1.166]] [[0.014]
 [0.014]
 [0.019]
 [0.013]
 [0.014]]
using another actor
from probs:  [0.02224371373307544, 0.12540296582849775, 0.20793036750483557, 0.3729851708575113, 0.12540296582849775, 0.1460348162475822]
siam score:  -0.8070717
line 256 mcts: sample exp_bonus 1.4280928592101028
actor:  0 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0161 0.2 0.2
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.01 ]
 [0.01 ]
 [0.01 ]
 [0.011]] [[ 0.612]
 [ 0.612]
 [ 0.612]
 [ 0.612]
 [-0.136]] [[0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.205]]
using explorer policy with actor:  1
siam score:  -0.79560083
from probs:  [0.02224371373307544, 0.12540296582849775, 0.20793036750483562, 0.3729851708575113, 0.12540296582849775, 0.1460348162475822]
448 6526
siam score:  -0.79580426
448 6534
using another actor
449 6558
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.02224371373307544, 0.12540296582849775, 0.20793036750483562, 0.3729851708575113, 0.12540296582849775, 0.1460348162475822]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[-0.668]
 [-0.668]
 [-0.668]
 [-0.668]
 [-0.668]] [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]]
Printing some Q and Qe and total Qs values:  [[0.997]
 [0.997]
 [0.997]
 [0.003]
 [0.997]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.322]
 [ 0.   ]] [[0.997]
 [0.997]
 [0.997]
 [0.003]
 [0.997]]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]] [[-0.45]
 [-0.45]
 [-0.45]
 [-0.45]
 [-0.45]] [[1.265]
 [1.265]
 [1.265]
 [1.265]
 [1.265]]
from probs:  [0.02224371373307544, 0.12540296582849775, 0.20793036750483562, 0.3729851708575113, 0.12540296582849775, 0.1460348162475822]
from probs:  [0.02224371373307544, 0.12540296582849775, 0.20793036750483562, 0.3729851708575113, 0.12540296582849775, 0.1460348162475822]
siam score:  -0.8092346
from probs:  [0.02224371373307544, 0.12540296582849775, 0.20793036750483562, 0.3729851708575113, 0.12540296582849775, 0.1460348162475822]
from probs:  [0.02224371373307544, 0.12540296582849775, 0.20793036750483562, 0.3729851708575113, 0.12540296582849775, 0.1460348162475822]
start point for exploration sampling:  20047
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0161 0.2 0.2
start point for exploration sampling:  20047
maxi score, test score, baseline:  0.0161 0.2 0.2
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.006]
 [0.033]
 [0.017]
 [0.015]] [[-0.633]
 [-0.033]
 [ 0.267]
 [ 1.053]
 [ 0.956]] [[0.014]
 [0.71 ]
 [1.104]
 [2.014]
 [1.897]]
maxi score, test score, baseline:  0.0161 0.2 0.2
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [0.1  ]
 [0.1  ]
 [0.001]
 [0.1  ]] [[0.   ]
 [0.   ]
 [0.   ]
 [1.677]
 [0.   ]] [[0.265]
 [0.265]
 [0.265]
 [1.185]
 [0.265]]
from probs:  [0.021794061907770057, 0.12286797220467466, 0.22394188250157926, 0.3654453569172457, 0.12286797220467466, 0.14308275426405556]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.018099999999999998 0.2 0.2
probs:  [0.021794061907770057, 0.12286797220467466, 0.22394188250157926, 0.3654453569172457, 0.12286797220467466, 0.14308275426405556]
455 6671
siam score:  -0.81911314
maxi score, test score, baseline:  0.018099999999999998 0.2 0.2
probs:  [0.021794061907770057, 0.12286797220467466, 0.22394188250157926, 0.3654453569172457, 0.12286797220467466, 0.14308275426405556]
start point for exploration sampling:  20047
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.018099999999999998 0.2 0.2
probs:  [0.020947176684881608, 0.13752276867030963, 0.23466909532483302, 0.3512446873102611, 0.11809350333940498, 0.13752276867030963]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.018099999999999998 0.2 0.2
probs:  [0.020947176684881608, 0.13752276867030963, 0.23466909532483302, 0.3512446873102611, 0.11809350333940498, 0.13752276867030963]
using another actor
from probs:  [0.020947176684881608, 0.13752276867030963, 0.23466909532483302, 0.3512446873102611, 0.11809350333940498, 0.13752276867030963]
maxi score, test score, baseline:  0.018099999999999998 0.2 0.2
from probs:  [0.020947176684881608, 0.13752276867030963, 0.23466909532483302, 0.3512446873102611, 0.11809350333940498, 0.13752276867030963]
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.326]
 [0.35 ]
 [0.157]
 [0.247]] [[1.976]
 [3.032]
 [3.03 ]
 [1.777]
 [2.828]] [[0.24 ]
 [0.326]
 [0.35 ]
 [0.157]
 [0.247]]
actor:  0 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0201 0.2 0.2
probs:  [0.020947176684881608, 0.13752276867030963, 0.23466909532483302, 0.3512446873102611, 0.11809350333940498, 0.13752276867030963]
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.78 ]
 [0.676]
 [0.676]
 [0.676]] [[1.228]
 [2.014]
 [1.228]
 [1.228]
 [1.228]] [[1.311]
 [2.317]
 [1.311]
 [1.311]
 [1.311]]
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.397]
 [0.403]
 [0.267]
 [0.321]] [[0.867]
 [1.844]
 [0.338]
 [0.751]
 [0.987]] [[0.652]
 [1.979]
 [0.363]
 [0.631]
 [0.956]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.020947176684881608, 0.13752276867030963, 0.23466909532483302, 0.3512446873102611, 0.11809350333940498, 0.13752276867030963]
maxi score, test score, baseline:  0.0201 0.2 0.2
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0201 0.2 0.2
maxi score, test score, baseline:  0.0201 0.2 0.2
probs:  [0.05549295774647887, 0.12760563380281686, 0.23577464788732394, 0.34394366197183096, 0.10957746478873238, 0.12760563380281686]
siam score:  -0.83860743
maxi score, test score, baseline:  0.0201 0.2 0.2
maxi score, test score, baseline:  0.0201 0.2 0.2
probs:  [0.05549295774647887, 0.12760563380281686, 0.23577464788732394, 0.34394366197183096, 0.10957746478873238, 0.12760563380281686]
siam score:  -0.8402933
using another actor
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0201 0.2 0.2
maxi score, test score, baseline:  0.0201 0.2 0.2
probs:  [0.05451023796347538, 0.1253458771444383, 0.23159933591588272, 0.3555617044825678, 0.10763696734919757, 0.1253458771444383]
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.036]
 [0.029]
 [0.029]
 [0.029]] [[-0.466]
 [-0.246]
 [-0.466]
 [-0.466]
 [-0.466]] [[0.279]
 [0.367]
 [0.279]
 [0.279]
 [0.279]]
maxi score, test score, baseline:  0.0201 0.2 0.2
maxi score, test score, baseline:  0.0201 0.2 0.2
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
469 6734
line 256 mcts: sample exp_bonus -0.33517118251119754
first move QE:  -0.06656116017925434
470 6751
from probs:  [0.053561718325176734, 0.12316476345840131, 0.22756933115823824, 0.36677542142468733, 0.10576400217509518, 0.12316476345840131]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0201 0.2 0.2
using another actor
Printing some Q and Qe and total Qs values:  [[0.257]
 [0.746]
 [0.257]
 [0.257]
 [0.257]] [[1.046]
 [1.327]
 [1.046]
 [1.046]
 [1.046]] [[1.162]
 [2.001]
 [1.162]
 [1.162]
 [1.162]]
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.361]
 [0.528]
 [0.422]
 [0.097]] [[1.574]
 [1.503]
 [2.074]
 [1.574]
 [1.288]] [[1.753]
 [1.606]
 [2.293]
 [1.753]
 [1.043]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using another actor
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0201 0.2 0.2
probs:  [0.05090439276485789, 0.11705426356589148, 0.2493540051679587, 0.3485788113695091, 0.11705426356589148, 0.11705426356589148]
siam score:  -0.81612426
maxi score, test score, baseline:  0.0201 0.2 0.2
probs:  [0.05090439276485789, 0.11705426356589148, 0.2493540051679587, 0.3485788113695091, 0.11705426356589148, 0.11705426356589148]
from probs:  [0.05090439276485789, 0.11705426356589148, 0.2493540051679587, 0.3485788113695091, 0.11705426356589148, 0.11705426356589148]
from probs:  [0.05090439276485789, 0.11705426356589148, 0.2493540051679587, 0.3485788113695091, 0.11705426356589148, 0.11705426356589148]
from probs:  [0.05090439276485789, 0.11705426356589148, 0.2493540051679587, 0.3485788113695091, 0.11705426356589148, 0.11705426356589148]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0201 0.2 0.2
probs:  [0.05090439276485789, 0.11705426356589148, 0.2493540051679587, 0.3485788113695091, 0.11705426356589148, 0.11705426356589148]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus 0.3367647696878846
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.634]] [[1.819]
 [1.819]
 [1.819]
 [1.819]
 [1.819]] [[1.725]
 [1.725]
 [1.725]
 [1.725]
 [1.725]]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0201 0.2 0.2
using another actor
from probs:  [0.04849827671097982, 0.12727720334810438, 0.2533234859675037, 0.33210241260462825, 0.11152141802067947, 0.12727720334810438]
start point for exploration sampling:  20047
from probs:  [0.04849827671097982, 0.12727720334810438, 0.2533234859675037, 0.33210241260462825, 0.11152141802067947, 0.12727720334810438]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
using another actor
479 6812
from probs:  [0.0763986835919135, 0.12153267512929009, 0.24188998589562766, 0.3171133051245886, 0.12153267512929009, 0.12153267512929009]
maxi score, test score, baseline:  0.0201 0.2 0.2
probs:  [0.0763986835919135, 0.12153267512929009, 0.24188998589562766, 0.3171133051245886, 0.12153267512929009, 0.12153267512929009]
from probs:  [0.0763986835919135, 0.12153267512929009, 0.24188998589562766, 0.3171133051245886, 0.12153267512929009, 0.12153267512929009]
Printing some Q and Qe and total Qs values:  [[1.09 ]
 [1.064]
 [1.09 ]
 [1.09 ]
 [1.09 ]] [[1.709]
 [1.962]
 [1.709]
 [1.709]
 [1.709]] [[2.289]
 [2.371]
 [2.289]
 [2.289]
 [2.289]]
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.044]
 [0.088]
 [0.044]
 [0.044]] [[0.341]
 [0.341]
 [0.544]
 [0.341]
 [0.341]] [[0.417]
 [0.417]
 [0.911]
 [0.417]
 [0.417]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
482 6826
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.6370870978510532
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.0201 0.2 0.2
using explorer policy with actor:  1
siam score:  -0.806461
Printing some Q and Qe and total Qs values:  [[0.185]
 [0.185]
 [0.262]
 [0.185]
 [0.185]] [[0.993]
 [0.993]
 [1.168]
 [0.993]
 [0.993]] [[1.326]
 [1.326]
 [1.761]
 [1.326]
 [1.326]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.078]
 [0.078]
 [0.214]
 [0.071]
 [0.091]] [[0.305]
 [0.305]
 [0.398]
 [0.304]
 [0.326]] [[0.516]
 [0.516]
 [0.873]
 [0.503]
 [0.563]]
Printing some Q and Qe and total Qs values:  [[0.061]
 [0.195]
 [0.376]
 [0.139]
 [0.157]] [[1.485]
 [1.531]
 [1.069]
 [1.238]
 [1.404]] [[1.328]
 [1.656]
 [1.143]
 [1.021]
 [1.355]]
line 256 mcts: sample exp_bonus 1.6870351098758767
maxi score, test score, baseline:  0.0201 0.2 0.2
maxi score, test score, baseline:  0.0201 0.2 0.2
siam score:  -0.8192574
maxi score, test score, baseline:  0.0201 0.2 0.2
probs:  [0.07206208425720621, 0.12882483370288247, 0.2565410199556541, 0.29911308203991127, 0.12882483370288247, 0.11463414634146342]
maxi score, test score, baseline:  0.0201 0.2 0.2
maxi score, test score, baseline:  0.0201 0.2 0.2
using another actor
using another actor
from probs:  [0.07206208425720621, 0.12882483370288247, 0.2565410199556541, 0.29911308203991127, 0.12882483370288247, 0.11463414634146342]
488 6842
maxi score, test score, baseline:  0.0201 0.2 0.2
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.619]
 [0.759]
 [0.619]
 [0.33 ]] [[0.89 ]
 [0.89 ]
 [1.454]
 [0.89 ]
 [0.886]] [[1.308]
 [1.308]
 [2.047]
 [1.308]
 [0.796]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.07105378224748579, 0.14101442938347178, 0.2529514648010494, 0.29492785308264097, 0.12702229995627456, 0.11303017052907739]
maxi score, test score, baseline:  0.0201 0.2 0.2
using another actor
maxi score, test score, baseline:  0.0201 0.2 0.2
probs:  [0.07007330746011212, 0.13906856403622248, 0.2632600258732212, 0.2908581285036653, 0.12526951272100043, 0.11147046140577835]
Printing some Q and Qe and total Qs values:  [[0.972]
 [0.611]
 [0.373]
 [0.245]
 [0.188]] [[1.939]
 [1.97 ]
 [2.511]
 [1.504]
 [1.448]] [[2.159]
 [1.694]
 [1.738]
 [0.888]
 [0.773]]
Printing some Q and Qe and total Qs values:  [[0.928]
 [1.212]
 [0.928]
 [0.928]
 [0.928]] [[1.503]
 [1.317]
 [1.503]
 [1.503]
 [1.503]] [[2.153]
 [2.526]
 [2.153]
 [2.153]
 [2.153]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.08161980696600923, 0.13533361309274022, 0.2696181284095678, 0.28304657994125054, 0.12190516156105749, 0.10847671002937474]
UNIT TEST: sample policy line 217 mcts : [0.042 0.083 0.667 0.125 0.083]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
497 6925
Printing some Q and Qe and total Qs values:  [[0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]] [[0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]] [[0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]]
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]] [[0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]] [[1.883]
 [1.883]
 [1.883]
 [1.883]
 [1.883]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
498 6927
maxi score, test score, baseline:  0.0201 0.2 0.2
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.544]
 [0.682]
 [0.349]
 [0.557]] [[1.052]
 [0.975]
 [1.197]
 [0.87 ]
 [0.873]] [[0.581]
 [1.074]
 [1.499]
 [0.615]
 [1.033]]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0201 0.2 0.2
siam score:  -0.8293188
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0201 0.2 0.2
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.049]
 [0.136]
 [0.11 ]
 [0.103]] [[0.622]
 [0.412]
 [0.479]
 [0.739]
 [0.308]] [[0.866]
 [0.457]
 [0.767]
 [1.234]
 [0.358]]
502 6946
using explorer policy with actor:  0
siam score:  -0.83996063
maxi score, test score, baseline:  0.0201 0.2 0.2
using explorer policy with actor:  1
502 6970
from probs:  [0.0956344802071772, 0.1548279689234184, 0.2613762486126526, 0.2613762486126526, 0.11931187569367367, 0.10747317795042542]
502 6992
siam score:  -0.8436213
from probs:  [0.0956344802071772, 0.1548279689234184, 0.2613762486126526, 0.2613762486126526, 0.11931187569367367, 0.10747317795042542]
from probs:  [0.0956344802071772, 0.1548279689234184, 0.2613762486126526, 0.2613762486126526, 0.11931187569367367, 0.10747317795042542]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.2938],
        [0.3802],
        [0.3532],
        [0.4544],
        [0.0755],
        [0.2962],
        [0.0000],
        [0.4636],
        [0.1379],
        [0.3231]], dtype=torch.float64)
0.0 0.29376334496616463
0.0 0.3801645537733411
0.0 0.35322670861410727
0.0 0.4543691695564855
0.0 0.07548310575512067
0.0 0.2962214852222354
0.0 0.0
0.0 0.4635792979160159
0.0 0.13788166799357457
0.0 0.3231208720842257
maxi score, test score, baseline:  0.0201 0.2 0.2
using another actor
503 7013
using explorer policy with actor:  1
using another actor
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0201 0.2 0.2
probs:  [0.09451553930530163, 0.16471663619744056, 0.2583180987202925, 0.25831809872029243, 0.1179159049360146, 0.1062157221206581]
from probs:  [0.09451553930530163, 0.16471663619744056, 0.2583180987202925, 0.25831809872029243, 0.1179159049360146, 0.1062157221206581]
using another actor
from probs:  [0.09451553930530163, 0.16471663619744056, 0.2583180987202925, 0.25831809872029243, 0.1179159049360146, 0.1062157221206581]
508 7027
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0201 0.2 0.2
siam score:  -0.84355164
siam score:  -0.84375215
maxi score, test score, baseline:  0.0201 0.2 0.2
probs:  [0.09342247921937116, 0.17437658113480306, 0.25533068305023493, 0.25533068305023493, 0.11655222262378027, 0.1049873509215757]
maxi score, test score, baseline:  0.0201 0.2 0.2
probs:  [0.09342247921937116, 0.17437658113480306, 0.25533068305023493, 0.25533068305023493, 0.11655222262378027, 0.1049873509215757]
510 7051
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0201 0.2 0.2
maxi score, test score, baseline:  0.0201 0.2 0.2
using another actor
using another actor
maxi score, test score, baseline:  0.0201 0.2 0.2
maxi score, test score, baseline:  0.0201 0.2 0.2
probs:  [0.0923544122901036, 0.18381564844587353, 0.25241157556270094, 0.25241157556270094, 0.11521972132904608, 0.10378706680957485]
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.02 ]
 [0.053]
 [0.045]
 [0.045]] [[-0.746]
 [-0.201]
 [-0.359]
 [-0.42 ]
 [-0.482]] [[0.05 ]
 [1.099]
 [0.848]
 [0.712]
 [0.587]]
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.022]
 [0.022]
 [0.001]
 [0.022]] [[-0.297]
 [-0.297]
 [-0.297]
 [-0.174]
 [-0.297]] [[0.022]
 [0.022]
 [0.022]
 [0.001]
 [0.022]]
using another actor
maxi score, test score, baseline:  0.0201 0.2 0.2
probs:  [0.0923544122901036, 0.18381564844587353, 0.25241157556270094, 0.25241157556270094, 0.11521972132904608, 0.10378706680957485]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.09131049099258212, 0.19304132815259625, 0.24955845990815964, 0.24955845990815964, 0.11391734369480748, 0.10261391734369481]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
actor:  0 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.08617074283133568
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.054]
 [0.011]
 [0.025]
 [0.027]] [[-0.559]
 [-0.343]
 [-0.21 ]
 [-0.453]
 [-0.591]] [[0.019]
 [0.054]
 [0.011]
 [0.025]
 [0.027]]
maxi score, test score, baseline:  0.022099999999999998 0.2 0.2
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.022099999999999998 0.2 0.2
probs:  [0.08929188255613126, 0.21088082901554403, 0.24404145077720205, 0.24404145077720205, 0.11139896373056994, 0.1003454231433506]
from probs:  [0.08929188255613126, 0.21088082901554403, 0.24404145077720205, 0.24404145077720205, 0.11139896373056994, 0.1003454231433506]
Printing some Q and Qe and total Qs values:  [[0.027]
 [0.027]
 [0.027]
 [0.027]
 [0.027]] [[-0.47]
 [-0.47]
 [-0.47]
 [-0.47]
 [-0.47]] [[0.027]
 [0.027]
 [0.027]
 [0.027]
 [0.027]]
from probs:  [0.08929188255613126, 0.21088082901554403, 0.24404145077720205, 0.24404145077720205, 0.11139896373056994, 0.1003454231433506]
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.113]
 [0.048]
 [0.032]
 [0.064]] [[-0.544]
 [-0.212]
 [-0.149]
 [-0.216]
 [-0.331]] [[0.025]
 [0.297]
 [0.189]
 [0.136]
 [0.162]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
Printing some Q and Qe and total Qs values:  [[1.5  ]
 [0.018]
 [0.042]
 [0.03 ]
 [0.041]] [[ 0.   ]
 [-0.164]
 [-0.253]
 [-0.352]
 [-0.411]] [[3.356]
 [0.228]
 [0.186]
 [0.064]
 [0.028]]
siam score:  -0.84581584
maxi score, test score, baseline:  0.022099999999999998 0.2 0.2
line 256 mcts: sample exp_bonus 0.6669267902414144
maxi score, test score, baseline:  0.022099999999999998 0.2 0.2
probs:  [0.08929188255613126, 0.21088082901554403, 0.24404145077720205, 0.24404145077720207, 0.11139896373056994, 0.1003454231433506]
maxi score, test score, baseline:  0.022099999999999998 0.2 0.2
Printing some Q and Qe and total Qs values:  [[0.11 ]
 [0.078]
 [0.453]
 [0.15 ]
 [0.161]] [[0.068]
 [0.125]
 [0.888]
 [0.256]
 [0.327]] [[0.098]
 [0.053]
 [1.057]
 [0.241]
 [0.286]]
Printing some Q and Qe and total Qs values:  [[0.08 ]
 [0.08 ]
 [0.202]
 [0.08 ]
 [0.08 ]] [[0.371]
 [0.371]
 [0.38 ]
 [0.371]
 [0.371]] [[0.599]
 [0.599]
 [0.856]
 [0.599]
 [0.599]]
maxi score, test score, baseline:  0.022099999999999998 0.2 0.2
probs:  [0.08929188255613126, 0.21088082901554403, 0.24404145077720205, 0.24404145077720205, 0.11139896373056994, 0.1003454231433506]
maxi score, test score, baseline:  0.022099999999999998 0.2 0.2
probs:  [0.08929188255613126, 0.21088082901554403, 0.24404145077720205, 0.24404145077720205, 0.11139896373056994, 0.1003454231433506]
Printing some Q and Qe and total Qs values:  [[0.659]
 [0.809]
 [0.659]
 [0.659]
 [0.659]] [[2.079]
 [3.593]
 [2.079]
 [2.079]
 [2.079]] [[0.924]
 [2.213]
 [0.924]
 [0.924]
 [0.924]]
line 256 mcts: sample exp_bonus 3.2597636516905437
maxi score, test score, baseline:  0.022099999999999998 0.2 0.2
probs:  [0.08929188255613126, 0.210880829015544, 0.24404145077720205, 0.24404145077720205, 0.11139896373056996, 0.1003454231433506]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.022099999999999998 0.2 0.2
maxi score, test score, baseline:  0.022099999999999998 0.2 0.2
probs:  [0.08929188255613126, 0.210880829015544, 0.24404145077720205, 0.24404145077720207, 0.11139896373056996, 0.1003454231433506]
Printing some Q and Qe and total Qs values:  [[0.056]
 [0.106]
 [0.011]
 [0.061]
 [0.062]] [[-0.322]
 [-0.26 ]
 [-0.175]
 [-0.278]
 [-0.29 ]] [[0.732]
 [0.916]
 [0.837]
 [0.801]
 [0.787]]
using explorer policy with actor:  1
using another actor
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.09817505914160188, 0.2171341669482933, 0.23876309564041903, 0.23876309564041903, 0.10898952348766475, 0.09817505914160188]
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.515]
 [0.515]
 [0.515]
 [0.532]] [[1.836]
 [1.836]
 [1.836]
 [1.836]
 [1.957]] [[1.906]
 [1.906]
 [1.906]
 [1.906]
 [2.107]]
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0241 0.2 0.2
532 7256
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0241 0.2 0.2
probs:  [0.0950900163666121, 0.220785597381342, 0.23126022913256952, 0.24173486088379703, 0.10556464811783961, 0.1055646481178396]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  0.0241 0.2 0.2
from probs:  [0.10447035957240036, 0.21849692257855519, 0.22886297376093293, 0.2392290249433106, 0.10447035957240039, 0.10447035957240038]
maxi score, test score, baseline:  0.0241 0.2 0.2
probs:  [0.10447035957240036, 0.21849692257855519, 0.22886297376093293, 0.2392290249433106, 0.10447035957240039, 0.10447035957240038]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
using another actor
Printing some Q and Qe and total Qs values:  [[0.184]
 [0.184]
 [0.624]
 [0.184]
 [0.184]] [[0.692]
 [0.692]
 [1.109]
 [0.692]
 [0.692]] [[0.755]
 [0.755]
 [1.984]
 [0.755]
 [0.755]]
using explorer policy with actor:  1
using another actor
siam score:  -0.8306879
using another actor
maxi score, test score, baseline:  0.0241 0.2 0.2
from probs:  [0.10447035957240036, 0.2184969225785552, 0.22886297376093293, 0.2392290249433106, 0.10447035957240039, 0.10447035957240038]
maxi score, test score, baseline:  0.0241 0.2 0.2
from probs:  [0.10447035957240038, 0.2184969225785552, 0.22886297376093293, 0.2392290249433106, 0.10447035957240039, 0.10447035957240038]
from probs:  [0.10447035957240038, 0.2184969225785552, 0.22886297376093293, 0.2392290249433106, 0.10447035957240039, 0.10447035957240038]
maxi score, test score, baseline:  0.0241 0.2 0.2
probs:  [0.10447035957240038, 0.2184969225785552, 0.22886297376093293, 0.2392290249433106, 0.10447035957240039, 0.10447035957240038]
from probs:  [0.10447035957240038, 0.2184969225785552, 0.22886297376093293, 0.2392290249433106, 0.10447035957240039, 0.10447035957240038]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.11365822378967616, 0.21625521000320613, 0.2265149086245591, 0.23677460724591212, 0.10339852516832318, 0.10339852516832317]
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.017]] [[-0.327]
 [-0.109]
 [-0.152]
 [-0.099]
 [-0.327]] [[0.366]
 [0.478]
 [0.449]
 [0.484]
 [0.366]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.12142632736412187, 0.22196041470311026, 0.22196041470311023, 0.23201382343700905, 0.10131950989632421, 0.1013195098963242]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8145397
siam score:  -0.8143475
maxi score, test score, baseline:  0.026099999999999998 0.2 0.2
from probs:  [0.12021772939346809, 0.22970451010886467, 0.21975116640746495, 0.22970451010886464, 0.10031104199066873, 0.10031104199066872]
actor:  0 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
545 7402
using another actor
actor:  0 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0301 0.2 0.2
from probs:  [0.12021772939346811, 0.22970451010886472, 0.219751166407465, 0.2297045101088647, 0.10031104199066875, 0.10031104199066875]
maxi score, test score, baseline:  0.0301 0.2 0.2
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0301 0.2 0.2
probs:  [0.12021772939346811, 0.22970451010886472, 0.219751166407465, 0.2297045101088647, 0.10031104199066875, 0.10031104199066875]
Starting evaluation
line 256 mcts: sample exp_bonus -1.518091018663172
rdn probs:  [0.12021772939346811, 0.22970451010886472, 0.219751166407465, 0.2297045101088647, 0.10031104199066875, 0.10031104199066875]
start point for exploration sampling:  20047
545 7466
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.22903665898762748
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.518]
 [0.355]
 [0.355]
 [0.355]] [[0.803]
 [0.761]
 [0.803]
 [0.803]
 [0.803]] [[0.735]
 [1.048]
 [0.735]
 [0.735]
 [0.735]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8092714
UNIT TEST: sample policy line 217 mcts : [0.042 0.667 0.208 0.042 0.042]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [0.12757320854628293, 0.22530685384724228, 0.21553348931714636, 0.23508021837733825, 0.09825311495599513, 0.09825311495599513]
maxi score, test score, baseline:  0.0301 0.0 0.0301
from probs:  [0.12757320854628293, 0.22530685384724228, 0.21553348931714636, 0.23508021837733825, 0.09825311495599513, 0.09825311495599513]
using another actor
maxi score, test score, baseline:  0.0301 0.0 0.0301
using explorer policy with actor:  1
548 7507
549 7511
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [0.12757320854628293, 0.22530685384724225, 0.2155334893171463, 0.23508021837733822, 0.09825311495599512, 0.09825311495599512]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using another actor
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.1347133673833504, 0.2209872754483043, 0.2209872754483043, 0.2305732652332992, 0.09636940824337095, 0.09636940824337095]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [0.14292923894385243, 0.21888900765685798, 0.21888900765685798, 0.2283839787459837, 0.095454383498224, 0.095454383498224]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.14158489446425737, 0.2262358756473887, 0.2168302110714852, 0.22623587564738873, 0.09455657158474004, 0.09455657158474004]
Printing some Q and Qe and total Qs values:  [[0.162]
 [0.162]
 [0.675]
 [0.162]
 [0.162]] [[0.356]
 [0.356]
 [0.314]
 [0.356]
 [0.356]] [[0.619]
 [0.619]
 [1.247]
 [0.619]
 [0.619]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [0.14026560324856463, 0.2241278046943005, 0.2241278046943005, 0.2241278046943005, 0.09367549133426696, 0.09367549133426696]
from probs:  [0.14026560324856463, 0.2241278046943005, 0.2241278046943005, 0.2241278046943005, 0.09367549133426696, 0.09367549133426696]
using another actor
from probs:  [0.14026560324856463, 0.2241278046943005, 0.2241278046943005, 0.22412780469430052, 0.09367549133426696, 0.09367549133426696]
using another actor
from probs:  [0.14026560324856463, 0.2241278046943005, 0.2241278046943005, 0.22412780469430052, 0.09367549133426696, 0.09367549133426696]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
from probs:  [0.13897067142169625, 0.23129065557159756, 0.2220586571566075, 0.2220586571566075, 0.09281067934674558, 0.09281067934674558]
line 256 mcts: sample exp_bonus 0.4597672191764487
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [0.13897067142169625, 0.23129065557159756, 0.2220586571566075, 0.2220586571566075, 0.09281067934674558, 0.09281067934674558]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.082]
 [0.116]
 [0.188]
 [0.082]
 [0.057]] [[-0.618]
 [-0.206]
 [-0.347]
 [-0.618]
 [-0.294]] [[0.395]
 [0.736]
 [0.786]
 [0.395]
 [0.56 ]]
start point for exploration sampling:  20047
siam score:  -0.8096955
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [0.15458049449141037, 0.22709752754294824, 0.21803289841150603, 0.21803289841150605, 0.09112809057131468, 0.09112809057131468]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]] [[1.09]
 [1.09]
 [1.09]
 [1.09]
 [1.09]] [[0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]]
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]] [[1.806]
 [1.806]
 [1.806]
 [1.806]
 [1.806]] [[2.17]
 [2.17]
 [2.17]
 [2.17]
 [2.17]]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [0.15931287517886134, 0.22990927346179224, 0.2210847236764259, 0.21226017389105956, 0.08871647689593047, 0.08871647689593047]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
567 7644
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [0.17092850629542494, 0.22207058184052433, 0.22207058184052433, 0.21354690258300776, 0.08569171372025933, 0.08569171372025933]
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.078]
 [0.218]
 [0.044]
 [0.077]] [[-0.185]
 [ 0.006]
 [ 0.275]
 [ 0.191]
 [-0.009]] [[0.754]
 [1.043]
 [1.592]
 [1.161]
 [1.026]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [0.17793552024981274, 0.22019372118661065, 0.22019372118661065, 0.21174208099925107, 0.08496747818885747, 0.08496747818885747]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.014]
 [0.032]
 [0.042]
 [0.03 ]] [[-0.029]
 [-0.078]
 [-0.049]
 [-0.015]
 [-0.044]] [[0.066]
 [0.01 ]
 [0.052]
 [0.085]
 [0.052]]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [0.17644427670994142, 0.2183483197525476, 0.22672912836106882, 0.20996751114402637, 0.08425538201620796, 0.08425538201620796]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [0.18328897549478892, 0.21653359315103338, 0.22484474756509445, 0.20822243873697227, 0.08355512252605554, 0.08355512252605554]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [0.18328897549478892, 0.21653359315103338, 0.22484474756509445, 0.20822243873697227, 0.08355512252605554, 0.08355512252605554]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.173]
 [0.176]
 [0.137]
 [0.309]] [[0.651]
 [0.815]
 [0.953]
 [2.601]
 [0.651]] [[0.861]
 [0.698]
 [0.798]
 [1.815]
 [0.861]]
576 7670
from probs:  [0.18328897549478892, 0.21653359315103338, 0.22484474756509445, 0.20822243873697227, 0.08355512252605554, 0.08355512252605554]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.18177818889775135, 0.22299143134616398, 0.22299143134616398, 0.20650613436679893, 0.08286640702156091, 0.08286640702156091]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [0.18177818889775135, 0.22299143134616398, 0.22299143134616398, 0.20650613436679893, 0.08286640702156091, 0.08286640702156091]
577 7689
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.18846736712841838, 0.22116841782104588, 0.22116841782104588, 0.20481789247473214, 0.08218895237737893, 0.08218895237737893]
siam score:  -0.84285104
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.19504775515691736, 0.21937440243427517, 0.2193744024342751, 0.2031566375827033, 0.08152340119591452, 0.08152340119591452]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.338]
 [1.338]
 [1.338]
 [1.338]
 [1.338]] [[1.501]
 [1.501]
 [1.501]
 [1.501]
 [1.501]] [[1.93]
 [1.93]
 [1.93]
 [1.93]
 [1.93]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
actor:  1 policy actor:  1  step number:  87 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
from probs:  [0.1904155824466257, 0.22208080348657105, 0.22208080348657103, 0.19833188770661203, 0.08750361406680325, 0.07958730880681693]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [0.19677416336215875, 0.22033655208036995, 0.22033655208036992, 0.19677416336215875, 0.0868163493438398, 0.07896221977110275]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [0.19677416336215875, 0.22033655208036995, 0.22033655208036992, 0.19677416336215875, 0.0868163493438398, 0.07896221977110275]
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.001]
 [0.002]
 [0.   ]
 [0.001]] [[-0.491]
 [-0.615]
 [-0.489]
 [-0.409]
 [-0.563]] [[1.323]
 [1.055]
 [1.292]
 [1.442]
 [1.151]]
from probs:  [0.19677416336215875, 0.22033655208036995, 0.22033655208036992, 0.19677416336215875, 0.0868163493438398, 0.07896221977110275]
from probs:  [0.19677416336215875, 0.22033655208036995, 0.22033655208036992, 0.19677416336215875, 0.0868163493438398, 0.07896221977110275]
from probs:  [0.19677416336215875, 0.22033655208036995, 0.22033655208036992, 0.19677416336215875, 0.0868163493438398, 0.07896221977110275]
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.585]
 [0.61 ]
 [0.527]
 [0.735]] [[-0.106]
 [-0.106]
 [-0.182]
 [-0.013]
 [-0.261]] [[0.585]
 [0.585]
 [0.61 ]
 [0.527]
 [0.735]]
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
588 7750
siam score:  -0.8601048
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [0.2030332618523442, 0.21861894550334893, 0.21861894550334893, 0.1952404200268419, 0.08614063446980921, 0.07834779264430687]
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [0.2030332618523442, 0.21861894550334893, 0.21861894550334893, 0.1952404200268419, 0.08614063446980921, 0.07834779264430687]
maxi score, test score, baseline:  0.0361 0.0 0.0361
from probs:  [0.20303326185234424, 0.21861894550334893, 0.21861894550334893, 0.1952404200268419, 0.08614063446980921, 0.07834779264430687]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.2075906613018924, 0.22293715929010208, 0.21526391029599726, 0.19224416331368271, 0.08481867739621518, 0.07714542840211035]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [0.2075910807681404, 0.22293773605619302, 0.21526440841216668, 0.19224442548008774, 0.0848178384637192, 0.07714451081969288]
from probs:  [0.2075910807681404, 0.22293773605619302, 0.21526440841216668, 0.19224442548008774, 0.0848178384637192, 0.07714451081969288]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
from probs:  [0.23118845237392374, 0.21629880951840288, 0.20885398809064243, 0.18651952380736114, 0.08229202381871513, 0.07484720239095471]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
start point for exploration sampling:  20047
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [0.23686982028801412, 0.21470040335495705, 0.20731059771060464, 0.18514118077754757, 0.08168390175661443, 0.07429409611226204]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [0.2334199639537542, 0.218855608182026, 0.2042912524102978, 0.18244471875270554, 0.08049422835060821, 0.08049422835060818]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [0.2334199639537542, 0.218855608182026, 0.2042912524102978, 0.18244471875270554, 0.08049422835060821, 0.08049422835060818]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [0.2334199639537542, 0.218855608182026, 0.2042912524102978, 0.18244471875270554, 0.08049422835060821, 0.08049422835060818]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
Printing some Q and Qe and total Qs values:  [[1.075]
 [1.075]
 [1.075]
 [1.075]
 [1.075]] [[1.475]
 [1.475]
 [1.475]
 [1.475]
 [1.475]] [[2.61]
 [2.61]
 [2.61]
 [2.61]
 [2.61]]
start point for exploration sampling:  20047
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [0.2317324470523921, 0.22450291589842264, 0.20281432243651412, 0.18112572897460566, 0.07991229281903275, 0.07991229281903273]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
in main func line 156:  602
using another actor
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
607 7830
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [0.23927220589319564, 0.22521952088160938, 0.20414049336423, 0.1760351233410575, 0.07766632825995373, 0.0776663282599537]
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.23927152444906558, 0.22521897132989158, 0.20414014165113062, 0.17603503541278262, 0.0776671635785648, 0.0776671635785648]
actor:  0 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0381 0.0 0.0381
probs:  [0.23927083876281627, 0.22521841835710993, 0.20413978774855035, 0.1760349469371376, 0.07766800409719295, 0.07766800409719295]
from probs:  [0.23927083876281627, 0.22521841835710993, 0.20413978774855035, 0.1760349469371376, 0.07766800409719295, 0.07766800409719295]
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.683]
 [0.683]
 [0.548]
 [0.548]] [[ 0.282]
 [-0.129]
 [-0.117]
 [ 0.282]
 [ 0.282]] [[0.548]
 [0.683]
 [0.683]
 [0.548]
 [0.548]]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.23927083876281627, 0.22521841835710993, 0.20413978774855035, 0.1760349469371376, 0.07766800409719295, 0.07766800409719295]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
siam score:  -0.85883594
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.2428832274447753, 0.2220968926871093, 0.20131055792944333, 0.1805242231717773, 0.07659254938344731, 0.0765925493834473]
from probs:  [0.2428832274447753, 0.2220968926871093, 0.20131055792944333, 0.1805242231717773, 0.07659254938344731, 0.0765925493834473]
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
probs:  [0.2428832274447753, 0.2220968926871093, 0.20131055792944333, 0.1805242231717773, 0.07659254938344731, 0.0765925493834473]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.85523117
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
probs:  [0.236333225394886, 0.22959130035667122, 0.19588167516559737, 0.17565590005095305, 0.08126894951594625, 0.08126894951594624]
maxi score, test score, baseline:  0.040100000000000004 0.0 0.040100000000000004
probs:  [0.236333225394886, 0.22959130035667122, 0.19588167516559737, 0.17565590005095305, 0.08126894951594625, 0.08126894951594624]
620 7856
from probs:  [0.236333225394886, 0.22959130035667122, 0.19588167516559737, 0.17565590005095305, 0.08126894951594625, 0.08126894951594624]
from probs:  [0.23793738138396578, 0.22436200715209928, 0.19721125868836628, 0.17684819734056653, 0.08182057771750106, 0.08182057771750105]
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0441 0.0 0.0441
probs:  [0.24814261663200923, 0.2213560029447733, 0.19456938925753742, 0.17447942899211047, 0.0807262810867848, 0.0807262810867848]
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.043]
 [0.012]
 [0.023]
 [0.047]] [[1.57 ]
 [1.351]
 [1.319]
 [1.468]
 [1.347]] [[1.357]
 [1.112]
 [1.031]
 [1.199]
 [1.114]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.002]] [[ 0.011]
 [ 0.011]
 [ 0.011]
 [ 0.011]
 [-0.017]] [[0.093]
 [0.093]
 [0.093]
 [0.093]
 [0.053]]
maxi score, test score, baseline:  0.0441 0.0 0.0441
probs:  [0.24814261663200923, 0.2213560029447733, 0.19456938925753742, 0.17447942899211047, 0.0807262810867848, 0.0807262810867848]
maxi score, test score, baseline:  0.0441 0.0 0.0441
probs:  [0.24814261663200923, 0.2213560029447733, 0.19456938925753742, 0.17447942899211047, 0.0807262810867848, 0.0807262810867848]
Printing some Q and Qe and total Qs values:  [[0.131]
 [0.229]
 [0.007]
 [0.089]
 [0.131]] [[-0.563]
 [-0.636]
 [-0.465]
 [-0.466]
 [-0.563]] [[0.582]
 [0.706]
 [0.432]
 [0.596]
 [0.582]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
first move QE:  -0.12299695322333146
maxi score, test score, baseline:  0.0441 0.0 0.0441
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
in main func line 156:  627
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
628 7872
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0461 0.0 0.0461
probs:  [0.24499468035785152, 0.2321188424908075, 0.19349132888967518, 0.16773965315558703, 0.08404670701980044, 0.0776087880862784]
Printing some Q and Qe and total Qs values:  [[0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.086]] [[4.175]
 [4.175]
 [4.175]
 [4.175]
 [5.734]] [[1.183]
 [1.183]
 [1.183]
 [1.183]
 [1.657]]
using another actor
maxi score, test score, baseline:  0.0461 0.0 0.0461
maxi score, test score, baseline:  0.0461 0.0 0.0461
probs:  [0.24499468035785152, 0.2321188424908075, 0.19349132888967518, 0.16773965315558703, 0.08404670701980044, 0.0776087880862784]
630 7883
Printing some Q and Qe and total Qs values:  [[1.067]
 [1.217]
 [1.067]
 [1.067]
 [1.067]] [[0.597]
 [0.141]
 [0.597]
 [0.597]
 [0.597]] [[2.567]
 [2.716]
 [2.567]
 [2.567]
 [2.567]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0461 0.0 0.0461
start point for exploration sampling:  20047
maxi score, test score, baseline:  0.0461 0.0 0.0461
start point for exploration sampling:  20047
maxi score, test score, baseline:  0.0461 0.0 0.0461
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
636 7892
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0461 0.0 0.0461
using another actor
from probs:  [0.24983280693649493, 0.2437474795996782, 0.1889795335683279, 0.16463822422106109, 0.0794436415056273, 0.07335831416881058]
maxi score, test score, baseline:  0.0441 0.0 0.0441
641 7908
siam score:  -0.88664633
siam score:  -0.88579345
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  0
643 7912
maxi score, test score, baseline:  0.0441 0.0 0.0441
maxi score, test score, baseline:  0.0441 0.0 0.0441
maxi score, test score, baseline:  0.0441 0.0 0.0441
probs:  [0.24832236886093634, 0.24832236886093634, 0.18783666353184764, 0.1636423814002122, 0.07896239393948813, 0.07291382340657926]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.24832236886093634, 0.24832236886093634, 0.18783666353184764, 0.1636423814002122, 0.07896239393948813, 0.07291382340657926]
first move QE:  -0.12760136707270542
from probs:  [0.24682940380244142, 0.2528416090876245, 0.1867073509506103, 0.16265852980987788, 0.07848765581731443, 0.07247545053213132]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using another actor
Printing some Q and Qe and total Qs values:  [[0.03]
 [0.03]
 [0.03]
 [0.03]
 [0.03]] [[0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]] [[0.784]
 [0.784]
 [0.784]
 [0.784]
 [0.784]]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]] [[0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]] [[0.933]
 [0.933]
 [0.933]
 [0.933]
 [0.933]]
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.535]
 [0.608]
 [0.506]
 [0.506]] [[-0.039]
 [ 0.101]
 [-0.276]
 [-0.039]
 [-0.039]] [[1.106]
 [1.21 ]
 [1.23 ]
 [1.106]
 [1.106]]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.24535428348254537, 0.2513305581774221, 0.1855915365337767, 0.16766271244914613, 0.07801859202599322, 0.07204231733111635]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus -0.3880900279106041
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.716]
 [0.748]
 [0.716]
 [0.716]
 [0.716]] [[0.447]
 [0.543]
 [0.447]
 [0.447]
 [0.447]] [[2.266]
 [2.459]
 [2.266]
 [2.266]
 [2.266]]
line 256 mcts: sample exp_bonus 1.018861624138308
maxi score, test score, baseline:  0.0461 0.0 0.0461
probs:  [0.24690321304207474, 0.2586451466579881, 0.1823225781545512, 0.16470967773068113, 0.07664517561133082, 0.07077420880337412]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0461 0.0 0.0461
probs:  [0.24984056848037278, 0.26144622919856436, 0.1802066041712235, 0.16279811309393616, 0.0757556577074995, 0.06995282734840372]
maxi score, test score, baseline:  0.0461 0.0 0.0461
probs:  [0.24984056848037278, 0.26144622919856436, 0.1802066041712235, 0.16279811309393616, 0.0757556577074995, 0.06995282734840372]
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.401]
 [0.515]
 [0.401]
 [0.401]] [[0.055]
 [0.055]
 [0.305]
 [0.055]
 [0.055]] [[1.048]
 [1.048]
 [1.359]
 [1.048]
 [1.048]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.24984056848037278, 0.26144622919856436, 0.1802066041712235, 0.16279811309393616, 0.0757556577074995, 0.06995282734840372]
654 7943
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0461 0.0 0.0461
probs:  [0.25844678351537703, 0.25844678351537703, 0.17813918127275546, 0.1609304093636223, 0.07488654981795635, 0.06915029251491195]
using explorer policy with actor:  1
start point for exploration sampling:  20047
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0461 0.0 0.0461
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0461 0.0 0.0461
maxi score, test score, baseline:  0.0461 0.0 0.0461
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.2555153795909397, 0.2668577684748894, 0.17611865740329147, 0.15910507407736688, 0.07403715744774382, 0.06836596300576894]
siam score:  -0.8700378
maxi score, test score, baseline:  0.0461 0.0 0.0461
probs:  [0.2555153795909397, 0.2668577684748894, 0.17611865740329147, 0.15910507407736685, 0.07403715744774382, 0.06836596300576894]
actor:  0 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.2555146807755115, 0.26685698044898104, 0.17611858306122463, 0.15910513355102032, 0.07403788599999869, 0.06836673616326391]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.111]
 [1.111]
 [1.111]
 [1.111]
 [1.111]] [[0.274]
 [0.274]
 [0.274]
 [0.274]
 [0.274]] [[1.866]
 [1.866]
 [1.866]
 [1.866]
 [1.866]]
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.8  ]
 [0.8  ]
 [0.8  ]
 [0.8  ]] [[0.396]
 [0.296]
 [0.296]
 [0.296]
 [0.296]] [[1.093]
 [1.271]
 [1.271]
 [1.271]
 [1.271]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.2512402180114411, 0.27354533045401896, 0.17874860257306302, 0.15644349013048517, 0.07279931847081815, 0.06722304036017368]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [0.24710641572049508, 0.2745290574433911, 0.18129207558554458, 0.15935396220722775, 0.0716015086939604, 0.06611698034938118]
siam score:  -0.8726166
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.418]
 [0.489]
 [0.418]
 [0.418]] [[-0.021]
 [-0.021]
 [ 0.244]
 [-0.021]
 [-0.021]] [[0.994]
 [0.994]
 [1.312]
 [0.994]
 [0.994]]
siam score:  -0.875558
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [0.24710641572049508, 0.2745290574433911, 0.18129207558554458, 0.15935396220722775, 0.0716015086939604, 0.06611698034938118]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.87864715
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.2444253007404089, 0.27697542663174285, 0.18475006993962997, 0.157624965030185, 0.0708246293199611, 0.06539960833807211]
actor:  0 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.050100000000000006 0.0 0.050100000000000006
using another actor
maxi score, test score, baseline:  0.050100000000000006 0.0 0.050100000000000006
probs:  [0.24442471199921872, 0.2769745914407521, 0.18474993302307413, 0.15762503348846293, 0.07082535497770719, 0.06540037507078494]
671 7981
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.939]
 [1.228]
 [1.367]
 [1.132]
 [0.998]] [[1.819]
 [1.007]
 [0.714]
 [1.462]
 [1.293]] [[2.807]
 [2.18 ]
 [2.007]
 [2.638]
 [2.192]]
from probs:  [0.24850157585031446, 0.2754801173394291, 0.1837530762764393, 0.15677453478732464, 0.07044320202215772, 0.06504749372433478]
maxi score, test score, baseline:  0.050100000000000006 0.0 0.050100000000000006
maxi score, test score, baseline:  0.050100000000000006 0.0 0.050100000000000006
siam score:  -0.8971419
Printing some Q and Qe and total Qs values:  [[0.963]
 [1.222]
 [0.804]
 [0.627]
 [0.746]] [[0.821]
 [0.336]
 [0.996]
 [0.759]
 [1.061]] [[1.783]
 [1.817]
 [1.642]
 [1.051]
 [1.591]]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.050100000000000006 0.0 0.050100000000000006
Printing some Q and Qe and total Qs values:  [[0.918]
 [0.918]
 [0.953]
 [0.918]
 [0.918]] [[0.542]
 [0.542]
 [0.94 ]
 [0.542]
 [0.542]] [[1.516]
 [1.516]
 [1.986]
 [1.516]
 [1.516]]
679 7995
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
first move QE:  -0.12222959058054315
maxi score, test score, baseline:  0.0521 0.0 0.0521
probs:  [0.24716732301987687, 0.279367585561161, 0.1827667979373087, 0.15593324581957196, 0.07006587904281444, 0.0646991686192671]
maxi score, test score, baseline:  0.0521 0.0 0.0521
maxi score, test score, baseline:  0.0521 0.0 0.0521
siam score:  -0.89567626
actor:  0 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.24716732301987687, 0.279367585561161, 0.1827667979373087, 0.15593324581957196, 0.07006587904281444, 0.0646991686192671]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.24716609789481928, 0.27936587038608035, 0.18276655291229718, 0.15593340916957965, 0.07006734919288356, 0.06470072044434004]
maxi score, test score, baseline:  0.056100000000000004 0.0 0.056100000000000004
maxi score, test score, baseline:  0.056100000000000004 0.0 0.056100000000000004
probs:  [0.24716609789481928, 0.27936587038608035, 0.18276655291229718, 0.15593340916957965, 0.07006734919288356, 0.06470072044434004]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.056100000000000004 0.0 0.056100000000000004
probs:  [0.24584672976711563, 0.2778746204594321, 0.18179094838248275, 0.16043902125427176, 0.0696933309593752, 0.06435534917732245]
line 256 mcts: sample exp_bonus 0.19017611479083404
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.056100000000000004 0.0 0.056100000000000004
maxi score, test score, baseline:  0.056100000000000004 0.0 0.056100000000000004
probs:  [0.24985101140207663, 0.27639920653039907, 0.18082570406843856, 0.1595871479657807, 0.06932328452948476, 0.06401364550382029]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
684 8010
actor:  1 policy actor:  1  step number:  99 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.056100000000000004 0.0 0.056100000000000004
probs:  [0.24853139938480015, 0.28550256899944104, 0.1798706558147527, 0.15874427317781503, 0.06895714697082996, 0.05839395565236112]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.058100000000000006 0.0 0.058100000000000006
maxi score, test score, baseline:  0.058100000000000006 0.0 0.058100000000000006
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.058100000000000006 0.0 0.058100000000000006
probs:  [0.251159312369489, 0.28774375690060794, 0.1779904233072511, 0.157085026432326, 0.06823708971389424, 0.05778439127643168]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.251159312369489, 0.28774375690060794, 0.1779904233072511, 0.157085026432326, 0.06823708971389424, 0.05778439127643168]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.157]
 [1.172]
 [1.157]
 [1.157]
 [1.157]] [[0.196]
 [0.268]
 [0.196]
 [0.196]
 [0.196]] [[1.981]
 [2.035]
 [1.981]
 [1.981]
 [1.981]]
maxi score, test score, baseline:  0.058100000000000006 0.0 0.058100000000000006
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.06 ]
 [1.183]
 [1.122]
 [1.06 ]
 [1.06 ]] [[0.989]
 [1.03 ]
 [0.999]
 [0.989]
 [0.989]] [[1.975]
 [2.277]
 [2.113]
 [1.975]
 [1.975]]
Printing some Q and Qe and total Qs values:  [[0.66 ]
 [0.695]
 [0.631]
 [0.66 ]
 [0.66 ]] [[1.561]
 [1.352]
 [2.005]
 [1.561]
 [1.561]] [[1.532]
 [1.413]
 [1.863]
 [1.532]
 [1.532]]
siam score:  -0.90254885
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.058100000000000006 0.0 0.058100000000000006
probs:  [0.25242783355614373, 0.28844752364972415, 0.18038845336898301, 0.1546601033021399, 0.06718371307487325, 0.056892373048136005]
siam score:  -0.9062949
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
from probs:  [0.25242783355614373, 0.28844752364972415, 0.18038845336898301, 0.1546601033021399, 0.06718371307487325, 0.056892373048136005]
using another actor
from probs:  [0.25242783355614373, 0.28844752364972415, 0.18038845336898301, 0.1546601033021399, 0.06718371307487325, 0.056892373048136005]
maxi score, test score, baseline:  0.058100000000000006 0.0 0.058100000000000006
probs:  [0.25242783355614373, 0.28844752364972415, 0.18038845336898301, 0.1546601033021399, 0.06718371307487325, 0.056892373048136005]
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
using another actor
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.35 ]
 [0.35 ]
 [0.057]
 [0.067]
 [0.35 ]] [[ 0.   ]
 [ 0.   ]
 [-0.139]
 [ 1.269]
 [ 0.   ]] [[0.35 ]
 [0.35 ]
 [0.057]
 [0.067]
 [0.35 ]]
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]] [[0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]] [[0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.25242655800939595, 0.2884457123733422, 0.18038824928150335, 0.15466028187868458, 0.06718519270910073, 0.056894005747973216]
700 8038
maxi score, test score, baseline:  0.0621 0.0 0.0621
maxi score, test score, baseline:  0.0621 0.0 0.0621
probs:  [0.25625357476627125, 0.28696908611470706, 0.17946479639518162, 0.15386853693815175, 0.0668412547842502, 0.056602751001438245]
Printing some Q and Qe and total Qs values:  [[0.757]
 [0.847]
 [0.766]
 [0.564]
 [0.699]] [[1.03 ]
 [1.528]
 [1.098]
 [1.784]
 [0.847]] [[1.192]
 [1.703]
 [1.255]
 [1.308]
 [0.954]]
Printing some Q and Qe and total Qs values:  [[0.776]
 [0.646]
 [0.669]
 [0.75 ]
 [0.75 ]] [[0.843]
 [1.275]
 [1.066]
 [1.658]
 [1.658]] [[1.232]
 [1.115]
 [1.092]
 [1.452]
 [1.452]]
actor:  1 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8951053
maxi score, test score, baseline:  0.0621 0.0 0.0621
using another actor
from probs:  [0.26137282665767714, 0.286969086114707, 0.17434554450377562, 0.15386853693815172, 0.06684125478425018, 0.05660275100143824]
Printing some Q and Qe and total Qs values:  [[0.194]
 [0.13 ]
 [0.216]
 [0.167]
 [0.186]] [[-0.865]
 [-0.058]
 [ 0.362]
 [ 0.124]
 [-0.019]] [[0.194]
 [0.13 ]
 [0.216]
 [0.167]
 [0.186]]
using explorer policy with actor:  1
siam score:  -0.9029626
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.26 ]
 [0.26 ]
 [0.189]
 [0.329]
 [0.357]] [[ 0.   ]
 [ 0.   ]
 [ 0.148]
 [-0.137]
 [-0.16 ]] [[0.26 ]
 [0.26 ]
 [0.189]
 [0.329]
 [0.357]]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0621 0.0 0.0621
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.309]
 [0.309]
 [0.279]
 [0.309]] [[-0.441]
 [-0.441]
 [-0.441]
 [-0.402]
 [-0.441]] [[0.309]
 [0.309]
 [0.309]
 [0.279]
 [0.309]]
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.692]
 [0.676]
 [0.676]
 [0.676]] [[1.379]
 [0.865]
 [1.379]
 [1.379]
 [1.379]] [[0.676]
 [0.692]
 [0.676]
 [0.676]
 [0.676]]
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.309]
 [0.309]
 [0.279]
 [0.309]] [[-0.368]
 [-0.368]
 [-0.368]
 [-0.357]
 [-0.368]] [[0.309]
 [0.309]
 [0.309]
 [0.279]
 [0.309]]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.07010000000000001 0.0 0.07010000000000001
probs:  [0.2613699695430092, 0.2869654568068855, 0.17434531284582958, 0.15386892303472852, 0.0668442663375489, 0.05660607143199836]
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.307]
 [0.667]
 [0.602]
 [0.58 ]] [[0.663]
 [0.07 ]
 [0.634]
 [1.311]
 [1.15 ]] [[0.545]
 [0.307]
 [0.667]
 [0.602]
 [0.58 ]]
Printing some Q and Qe and total Qs values:  [[0.931]
 [1.014]
 [0.879]
 [0.931]
 [0.933]] [[1.15 ]
 [1.286]
 [0.543]
 [1.15 ]
 [1.438]] [[1.764]
 [2.01 ]
 [1.187]
 [1.764]
 [2.002]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.289176451630566
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]] [[0]
 [0]
 [0]
 [0]
 [0]] [[0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]]
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.529]
 [0.614]
 [0.529]
 [0.529]] [[0.964]
 [0.964]
 [1.031]
 [0.964]
 [0.964]] [[0.529]
 [0.529]
 [0.614]
 [0.529]
 [0.529]]
siam score:  -0.9120357
actor:  0 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  93 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn probs:  [0.26135451210086436, 0.2869458216776746, 0.1743440595397097, 0.15387101187826155, 0.06686055931710688, 0.056624035486382805]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1121 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]] [[-0.297]
 [-0.297]
 [-0.297]
 [-0.297]
 [-0.297]] [[1.323]
 [1.323]
 [1.323]
 [1.323]
 [1.323]]
maxi score, test score, baseline:  0.1121 1.0 1.0
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
siam score:  -0.91478944
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.521]
 [0.993]
 [0.663]
 [0.621]] [[0.244]
 [0.576]
 [0.87 ]
 [1.008]
 [0.766]] [[0.466]
 [0.741]
 [1.467]
 [1.249]
 [1.002]]
actor:  1 policy actor:  1  step number:  91 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.261]
 [1.296]
 [1.261]
 [1.261]
 [1.261]] [[1.806]
 [1.681]
 [1.806]
 [1.806]
 [1.806]] [[2.701]
 [2.647]
 [2.701]
 [2.701]
 [2.701]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.915]
 [0.915]
 [0.915]
 [0.915]
 [0.982]] [[1.618]
 [1.618]
 [1.618]
 [1.618]
 [1.996]] [[1.351]
 [1.351]
 [1.351]
 [1.351]
 [1.865]]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.91201407
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.9092088
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.042 0.25  0.167 0.25  0.292]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.02 ]
 [0.373]
 [1.135]
 [1.001]
 [0.965]] [[1.451]
 [0.665]
 [1.43 ]
 [1.619]
 [1.526]] [[1.986]
 [0.196]
 [2.14 ]
 [2.129]
 [1.978]]
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.616]
 [0.708]
 [0.641]
 [0.641]] [[2.192]
 [0.762]
 [2.34 ]
 [2.192]
 [2.192]] [[1.766]
 [0.467]
 [2.015]
 [1.766]
 [1.766]]
siam score:  -0.9091473
first move QE:  -0.12501259876029472
siam score:  -0.91093534
first move QE:  -0.12504860622537803
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.698]
 [0.698]
 [0.909]
 [0.247]
 [0.698]] [[ 0.   ]
 [ 0.   ]
 [ 0.377]
 [-0.224]
 [ 0.   ]] [[1.144]
 [1.144]
 [1.722]
 [0.243]
 [1.144]]
using explorer policy with actor:  1
start point for exploration sampling:  20047
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.054]
 [1.084]
 [1.054]
 [1.054]
 [1.054]] [[2.517]
 [2.412]
 [2.517]
 [2.517]
 [2.517]] [[2.469]
 [2.412]
 [2.469]
 [2.469]
 [2.469]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1181 1.0 1.0
siam score:  -0.8912413
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.237]
 [0.365]
 [0.554]
 [0.739]
 [0.739]] [[0.311]
 [0.8  ]
 [1.39 ]
 [0.594]
 [0.594]] [[0.518]
 [1.244]
 [2.191]
 [1.773]
 [1.773]]
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [1.318]
 [1.142]
 [1.142]
 [1.142]] [[0.383]
 [1.398]
 [1.102]
 [1.102]
 [1.102]] [[0.603]
 [2.687]
 [2.223]
 [2.223]
 [2.223]]
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.9485946939886984
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
UNIT TEST: sample policy line 217 mcts : [0.    0.458 0.    0.042 0.5  ]
UNIT TEST: sample policy line 217 mcts : [0.042 0.    0.958 0.    0.   ]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.4883131425994616
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
749 8086
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
749 8088
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1201 1.0 1.0
maxi score, test score, baseline:  0.1201 1.0 1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1201 1.0 1.0
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
753 8156
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8761983
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.995]
 [1.035]
 [1.159]
 [0.74 ]
 [0.818]] [[0.039]
 [0.215]
 [0.032]
 [0.359]
 [0.175]] [[1.191]
 [1.328]
 [1.516]
 [0.787]
 [0.882]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.052]
 [0.052]
 [0.096]
 [0.038]
 [0.037]] [[ 0.   ]
 [ 0.   ]
 [-0.415]
 [-0.22 ]
 [-0.588]] [[0.052]
 [0.052]
 [0.096]
 [0.038]
 [0.037]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1201 1.0 1.0
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.8757384
maxi score, test score, baseline:  0.1201 1.0 1.0
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20047
first move QE:  -0.10527318927092605
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.97 ]
 [0.586]
 [0.586]
 [0.586]
 [0.586]] [[0.989]
 [0.424]
 [0.424]
 [0.424]
 [0.424]] [[1.822]
 [0.734]
 [0.734]
 [0.734]
 [0.734]]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
siam score:  -0.8789047
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.053]
 [0.072]
 [0.055]
 [0.058]] [[-0.807]
 [ 0.   ]
 [-0.544]
 [-0.724]
 [-0.936]] [[0.047]
 [0.053]
 [0.072]
 [0.055]
 [0.058]]
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.082]
 [0.08 ]
 [0.212]
 [0.082]
 [0.126]] [[0.061]
 [0.046]
 [0.068]
 [0.061]
 [0.049]] [[0.817]
 [0.791]
 [1.024]
 [0.817]
 [0.864]]
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.02 ]
 [0.02 ]
 [0.02 ]
 [0.017]] [[-0.115]
 [-0.115]
 [-0.115]
 [-0.115]
 [ 0.015]] [[0.135]
 [0.135]
 [0.135]
 [0.135]
 [0.358]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
776 8229
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.226]
 [0.277]
 [0.413]
 [0.045]
 [0.21 ]] [[-0.41 ]
 [-0.317]
 [-0.351]
 [-0.044]
 [-0.163]] [[0.272]
 [0.404]
 [0.665]
 [0.03 ]
 [0.321]]
maxi score, test score, baseline:  0.1181 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.441]
 [1.058]
 [0.441]
 [0.441]] [[-0.263]
 [-0.263]
 [-0.25 ]
 [-0.263]
 [-0.263]] [[0.828]
 [0.828]
 [2.067]
 [0.828]
 [0.828]]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1181 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]] [[-0.159]
 [-0.159]
 [-0.159]
 [-0.159]
 [-0.159]] [[0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]]
maxi score, test score, baseline:  0.1181 1.0 1.0
actor:  0 policy actor:  0  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.88054305
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.617]
 [0.622]
 [0.412]
 [0.45 ]] [[-0.092]
 [-0.094]
 [-0.008]
 [ 0.091]
 [-0.096]] [[0.249]
 [0.858]
 [0.896]
 [0.509]
 [0.522]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1221 1.0 1.0
siam score:  -0.8752556
siam score:  -0.8752307
maxi score, test score, baseline:  0.1221 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.02 ]
 [1.145]
 [1.02 ]
 [1.02 ]
 [1.02 ]] [[0.251]
 [0.116]
 [0.251]
 [0.251]
 [0.251]] [[2.048]
 [2.254]
 [2.048]
 [2.048]
 [2.048]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.097]
 [1.136]
 [1.132]
 [0.925]
 [0.925]] [[2.265]
 [1.246]
 [1.654]
 [1.353]
 [1.353]] [[1.305]
 [2.29 ]
 [2.661]
 [1.998]
 [1.998]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.125 0.292 0.125 0.292 0.167]
788 8247
Printing some Q and Qe and total Qs values:  [[1.25]
 [1.25]
 [1.25]
 [1.25]
 [1.25]] [[1.052]
 [1.052]
 [1.052]
 [1.052]
 [1.052]] [[2.347]
 [2.347]
 [2.347]
 [2.347]
 [2.347]]
first move QE:  -0.10778694350112214
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
790 8251
siam score:  -0.8821809
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1221 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.88819724
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8907365
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8991917
maxi score, test score, baseline:  0.1261 1.0 1.0
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.338]
 [1.405]
 [1.406]
 [1.221]
 [1.221]] [[ 0.906]
 [ 0.641]
 [-0.039]
 [ 0.768]
 [ 0.768]] [[2.235]
 [2.105]
 [1.426]
 [1.863]
 [1.863]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.2500],
        [0.9305],
        [0.4737],
        [0.8814],
        [0.7196],
        [0.8668],
        [0.9166],
        [0.5647],
        [0.8380],
        [0.0000]], dtype=torch.float64)
0.0 0.24999713320486086
0.0 0.9304774692564821
0.0 0.47374711537433134
0.0 0.8814247477070853
0.0 0.7196092367477557
0.0 0.8667971529613988
0.0 0.9165953098957919
0.0 0.564721234167573
0.0 0.8379876702254258
0.0 0.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.9087988
maxi score, test score, baseline:  0.1261 1.0 1.0
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.138]
 [0.138]
 [0.832]
 [0.138]
 [0.138]] [[-0.755]
 [-0.755]
 [-0.618]
 [-0.755]
 [-0.755]] [[0.173]
 [0.173]
 [1.607]
 [0.173]
 [0.173]]
Printing some Q and Qe and total Qs values:  [[1.179]
 [1.332]
 [1.179]
 [1.179]
 [1.179]] [[0.242]
 [0.078]
 [0.242]
 [0.242]
 [0.242]] [[1.884]
 [2.134]
 [1.884]
 [1.884]
 [1.884]]
start point for exploration sampling:  20047
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.85 ]
 [1.002]
 [1.219]
 [0.85 ]
 [0.85 ]] [[0.409]
 [0.386]
 [0.127]
 [0.409]
 [0.409]] [[1.924]
 [2.22 ]
 [2.567]
 [1.924]
 [1.924]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
siam score:  -0.91081727
maxi score, test score, baseline:  0.1281 1.0 1.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.9119424
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.178]
 [1.348]
 [1.313]
 [1.118]
 [1.106]] [[1.461]
 [1.347]
 [1.073]
 [1.388]
 [1.627]] [[2.357]
 [2.521]
 [2.141]
 [2.166]
 [2.425]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1341 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1341 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.921]
 [0.93 ]
 [0.963]
 [0.907]
 [0.921]] [[1.515]
 [1.374]
 [0.306]
 [1.266]
 [1.515]] [[0.921]
 [0.93 ]
 [0.963]
 [0.907]
 [0.921]]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1381 1.0 1.0
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.9064748
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1381 1.0 1.0
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1381 1.0 1.0
siam score:  -0.9062819
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1381 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.15 ]
 [1.314]
 [1.15 ]
 [1.15 ]
 [1.108]] [[1.034]
 [0.854]
 [1.034]
 [1.034]
 [1.378]] [[2.099]
 [2.247]
 [2.099]
 [2.099]
 [2.358]]
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.102]
 [0.285]
 [0.004]
 [0.164]
 [0.18 ]] [[-0.447]
 [-0.408]
 [-0.454]
 [-0.576]
 [-0.582]] [[0.102]
 [0.285]
 [0.004]
 [0.164]
 [0.18 ]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
860 8312
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.547]
 [1.08 ]
 [0.547]
 [0.547]] [[-0.628]
 [-0.628]
 [-1.084]
 [-0.628]
 [-0.628]] [[1.009]
 [1.009]
 [1.675]
 [1.009]
 [1.009]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1401 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.098]
 [1.276]
 [1.098]
 [1.098]
 [1.098]] [[0.625]
 [0.816]
 [0.625]
 [0.625]
 [0.625]] [[2.005]
 [2.554]
 [2.005]
 [2.005]
 [2.005]]
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1401 1.0 1.0
maxi score, test score, baseline:  0.1401 1.0 1.0
maxi score, test score, baseline:  0.1401 1.0 1.0
maxi score, test score, baseline:  0.1401 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.569]
 [0.591]
 [0.551]
 [0.551]] [[0.907]
 [0.571]
 [0.971]
 [0.907]
 [0.907]] [[0.551]
 [0.569]
 [0.591]
 [0.551]
 [0.551]]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.785]
 [0.788]
 [0.785]
 [0.785]
 [0.785]] [[0.003]
 [0.063]
 [0.003]
 [0.003]
 [0.003]] [[1.054]
 [1.079]
 [1.054]
 [1.054]
 [1.054]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.89087343
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
start point for exploration sampling:  20047
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
using explorer policy with actor:  1
first move QE:  -0.11706989530885055
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.9084287
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[1.286]
 [1.333]
 [1.286]
 [1.286]
 [1.286]] [[1.297]
 [1.363]
 [1.297]
 [1.297]
 [1.297]] [[2.599]
 [2.759]
 [2.599]
 [2.599]
 [2.599]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.621]
 [0.939]
 [0.621]
 [0.621]] [[-0.192]
 [-0.192]
 [-0.508]
 [-0.192]
 [-0.192]] [[1.425]
 [1.425]
 [1.85 ]
 [1.425]
 [1.425]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1981 1.0 1.0
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1981 1.0 1.0
maxi score, test score, baseline:  0.1981 1.0 1.0
maxi score, test score, baseline:  0.1981 1.0 1.0
start point for exploration sampling:  20047
maxi score, test score, baseline:  0.1981 1.0 1.0
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.044]
 [1.044]
 [1.044]
 [1.044]
 [1.044]] [[1.089]
 [1.089]
 [1.089]
 [1.089]
 [1.089]] [[2.011]
 [2.011]
 [2.011]
 [2.011]
 [2.011]]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.844]
 [0.922]
 [0.844]
 [0.844]
 [0.844]] [[ 0.056]
 [-0.008]
 [ 0.056]
 [ 0.056]
 [ 0.056]] [[0.844]
 [0.922]
 [0.844]
 [0.844]
 [0.844]]
Printing some Q and Qe and total Qs values:  [[0.814]
 [0.901]
 [0.83 ]
 [0.814]
 [0.814]] [[-0.034]
 [-0.261]
 [ 0.057]
 [-0.034]
 [-0.034]] [[0.814]
 [0.901]
 [0.83 ]
 [0.814]
 [0.814]]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
898 8346
Printing some Q and Qe and total Qs values:  [[0.622]
 [0.985]
 [0.622]
 [0.622]
 [0.622]] [[1.694]
 [1.666]
 [1.694]
 [1.694]
 [1.694]] [[2.003]
 [2.591]
 [2.003]
 [2.003]
 [2.003]]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
siam score:  -0.90151906
in main func line 156:  907
907 8348
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.973]
 [0.91 ]
 [1.053]
 [1.11 ]
 [1.11 ]] [[1.773]
 [2.641]
 [2.515]
 [2.871]
 [2.871]] [[1.472]
 [1.939]
 [1.991]
 [2.257]
 [2.257]]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
910 8351
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus 1.4862340146638524
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.91666275
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
UNIT TEST: sample policy line 217 mcts : [0.125 0.458 0.292 0.042 0.083]
Printing some Q and Qe and total Qs values:  [[0.269]
 [0.269]
 [0.269]
 [0.261]
 [0.269]] [[-0.694]
 [-0.694]
 [-0.694]
 [-0.856]
 [-0.694]] [[0.269]
 [0.269]
 [0.269]
 [0.261]
 [0.269]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.884]
 [0.889]
 [0.44 ]
 [0.884]
 [0.704]] [[0.973]
 [1.778]
 [0.323]
 [0.973]
 [1.844]] [[0.884]
 [0.889]
 [0.44 ]
 [0.884]
 [0.704]]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.91135377
using explorer policy with actor:  1
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.269]
 [1.3  ]
 [1.269]
 [1.125]
 [1.123]] [[1.779]
 [1.748]
 [1.779]
 [1.768]
 [1.921]] [[2.4  ]
 [2.415]
 [2.4  ]
 [2.176]
 [2.323]]
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.138]
 [1.138]
 [1.138]
 [1.138]
 [1.138]] [[0.11]
 [0.11]
 [0.11]
 [0.11]
 [0.11]] [[1.562]
 [1.562]
 [1.562]
 [1.562]
 [1.562]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2161 1.0 1.0
maxi score, test score, baseline:  0.2161 1.0 1.0
line 256 mcts: sample exp_bonus 0.39740070450228326
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.91165376
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2181 1.0 1.0
935 8384
maxi score, test score, baseline:  0.2181 1.0 1.0
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.9049822
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.8217],
        [0.6645],
        [0.6291],
        [0.6693],
        [0.8144],
        [0.8209],
        [0.6736],
        [0.5015],
        [0.5750],
        [0.9123]], dtype=torch.float64)
0.0 0.8216829750799917
0.0 0.6644983632153821
0.0 0.6290542391333009
0.0 0.6693132161825475
0.0 0.8144210113962302
0.0 0.8209050377475028
0.0 0.673602987761058
0.0 0.5015198834158582
0.0 0.5749546200698228
0.0 0.9123354609796895
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
942 8392
siam score:  -0.9048836
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
943 8393
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.282]
 [1.282]
 [1.282]
 [1.282]
 [1.282]] [[2.352]
 [2.352]
 [2.352]
 [2.352]
 [2.352]] [[2.552]
 [2.552]
 [2.552]
 [2.552]
 [2.552]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]] [[2.366]
 [2.366]
 [2.366]
 [2.366]
 [2.366]] [[2.422]
 [2.422]
 [2.422]
 [2.422]
 [2.422]]
siam score:  -0.9049838
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.13 ]
 [0.   ]
 [0.211]
 [0.151]
 [0.156]] [[-0.363]
 [-0.277]
 [-0.35 ]
 [-0.307]
 [-0.321]] [[0.13 ]
 [0.   ]
 [0.211]
 [0.151]
 [0.156]]
Printing some Q and Qe and total Qs values:  [[0.826]
 [0.345]
 [1.231]
 [0.932]
 [1.023]] [[ 0.198]
 [-0.004]
 [ 0.072]
 [ 0.422]
 [ 0.175]] [[1.198]
 [0.229]
 [1.84 ]
 [1.517]
 [1.534]]
Printing some Q and Qe and total Qs values:  [[1.025]
 [1.025]
 [1.025]
 [1.025]
 [1.025]] [[3.752]
 [3.752]
 [3.752]
 [3.752]
 [3.752]] [[2.258]
 [2.258]
 [2.258]
 [2.258]
 [2.258]]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
siam score:  -0.9033281
siam score:  -0.9042047
Printing some Q and Qe and total Qs values:  [[1.396]
 [1.396]
 [1.396]
 [1.396]
 [1.396]] [[-0.111]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]] [[2.037]
 [2.037]
 [2.037]
 [2.037]
 [2.037]]
siam score:  -0.9034082
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
956 8400
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.125102135314081
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
958 8401
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2301 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
start point for exploration sampling:  20047
970 8405
Printing some Q and Qe and total Qs values:  [[0.906]
 [1.173]
 [1.24 ]
 [1.019]
 [1.019]] [[1.206]
 [1.003]
 [1.189]
 [0.721]
 [0.721]] [[1.627]
 [1.874]
 [2.231]
 [1.234]
 [1.234]]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.157]
 [1.151]
 [1.157]
 [1.157]
 [1.157]] [[1.734]
 [1.603]
 [1.734]
 [1.734]
 [1.734]] [[1.807]
 [1.681]
 [1.807]
 [1.807]
 [1.807]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.201]] [[3.932]
 [3.932]
 [3.932]
 [3.932]
 [8.427]] [[0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [1.897]]
siam score:  -0.897378
maxi score, test score, baseline:  0.2321 1.0 1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.144]
 [0.148]
 [0.139]
 [0.139]
 [0.149]] [[1.815]
 [1.349]
 [1.716]
 [1.716]
 [1.915]] [[1.009]
 [0.55 ]
 [0.899]
 [0.899]
 [1.118]]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.862]
 [0.813]
 [0.812]
 [0.862]
 [0.862]] [[1.934]
 [1.682]
 [1.916]
 [1.934]
 [1.934]] [[0.862]
 [0.813]
 [0.812]
 [0.862]
 [0.862]]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
first move QE:  -0.12732923874733837
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8963982
Printing some Q and Qe and total Qs values:  [[1.263]
 [1.394]
 [1.263]
 [1.263]
 [1.33 ]] [[ 0.   ]
 [-0.382]
 [ 0.   ]
 [ 0.   ]
 [-0.256]] [[1.711]
 [1.845]
 [1.711]
 [1.711]
 [1.759]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8959094
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.14 ]
 [0.151]
 [0.14 ]
 [0.166]
 [0.158]] [[4.873]
 [3.383]
 [3.952]
 [1.89 ]
 [3.137]] [[1.985]
 [1.139]
 [1.458]
 [0.294]
 [1.002]]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.24 ]
 [0.249]
 [0.249]
 [0.249]] [[6.508]
 [6.482]
 [6.508]
 [6.508]
 [6.508]] [[1.713]
 [1.696]
 [1.713]
 [1.713]
 [1.713]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.913]
 [1.255]
 [1.419]
 [1.311]
 [1.359]] [[2.759]
 [2.042]
 [1.428]
 [1.642]
 [2.039]] [[2.464]
 [2.264]
 [1.904]
 [1.955]
 [2.413]]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.89]
 [0.89]
 [0.89]
 [0.89]
 [0.89]] [[1.232]
 [1.232]
 [1.232]
 [1.232]
 [1.232]] [[2.008]
 [2.008]
 [2.008]
 [2.008]
 [2.008]]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8895256
1015 8429
first move QE:  -0.13013148226519994
in main func line 156:  1017
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2501 1.0 1.0
siam score:  -0.8902388
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8930215
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.307]
 [1.307]
 [1.307]
 [1.307]
 [1.307]] [[2.3]
 [2.3]
 [2.3]
 [2.3]
 [2.3]] [[1.361]
 [1.361]
 [1.361]
 [1.361]
 [1.361]]
maxi score, test score, baseline:  0.2501 1.0 1.0
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2501 1.0 1.0
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  90 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2501 1.0 1.0
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2501 1.0 1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2521 1.0 1.0
maxi score, test score, baseline:  0.2521 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.002]
 [1.002]
 [1.096]
 [1.002]
 [1.002]] [[0.979]
 [0.979]
 [1.43 ]
 [0.979]
 [0.979]] [[1.845]
 [1.845]
 [2.38 ]
 [1.845]
 [1.845]]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8916797
maxi score, test score, baseline:  0.2521 1.0 1.0
line 256 mcts: sample exp_bonus -0.8892465667816529
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.89473844
siam score:  -0.89412844
maxi score, test score, baseline:  0.2541 1.0 1.0
maxi score, test score, baseline:  0.2541 1.0 1.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
1050 8448
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.020451813566094174
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2561 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.135]
 [0.124]
 [0.058]
 [0.124]
 [0.124]] [[2.951]
 [4.224]
 [0.65 ]
 [4.224]
 [4.224]] [[0.883]
 [1.336]
 [0.011]
 [1.336]
 [1.336]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
Printing some Q and Qe and total Qs values:  [[1.007]
 [1.007]
 [1.225]
 [1.007]
 [1.007]] [[0.861]
 [0.861]
 [1.097]
 [0.861]
 [0.861]] [[1.611]
 [1.611]
 [2.361]
 [1.611]
 [1.611]]
maxi score, test score, baseline:  0.2561 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.    0.25  0.417 0.    0.333]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2561 1.0 1.0
maxi score, test score, baseline:  0.2561 1.0 1.0
maxi score, test score, baseline:  0.2561 1.0 1.0
maxi score, test score, baseline:  0.2561 1.0 1.0
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
in main func line 156:  1061
maxi score, test score, baseline:  0.2581 1.0 1.0
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
siam score:  -0.9067517
maxi score, test score, baseline:  0.2581 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.229]
 [1.368]
 [1.399]
 [1.085]
 [1.105]] [[2.753]
 [2.578]
 [1.863]
 [2.765]
 [2.588]] [[2.309]
 [2.304]
 [1.795]
 [2.189]
 [2.074]]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.90584815
siam score:  -0.9033952
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.9001957
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2621 1.0 1.0
line 256 mcts: sample exp_bonus 1.9289551229406596
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2681 1.0 1.0
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.646]
 [0.755]
 [0.542]
 [0.646]] [[0.289]
 [0.289]
 [0.743]
 [0.913]
 [0.289]] [[0.646]
 [0.646]
 [0.755]
 [0.542]
 [0.646]]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  20047
siam score:  -0.8945662
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.888]
 [0.907]
 [0.899]
 [0.888]
 [0.888]] [[0.016]
 [0.057]
 [0.179]
 [0.016]
 [0.016]] [[0.888]
 [0.907]
 [0.899]
 [0.888]
 [0.888]]
line 256 mcts: sample exp_bonus 0.2521377854956474
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8951683
maxi score, test score, baseline:  0.3201 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]] [[5.788]
 [5.788]
 [5.788]
 [5.788]
 [5.788]] [[2.041]
 [2.041]
 [2.041]
 [2.041]
 [2.041]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.89644325
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.968]
 [1.166]
 [1.192]
 [0.968]
 [0.968]] [[1.497]
 [0.422]
 [0.699]
 [1.497]
 [1.497]] [[1.763]
 [1.452]
 [1.683]
 [1.763]
 [1.763]]
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.787]
 [0.811]
 [0.568]
 [0.681]] [[1.408]
 [0.921]
 [1.016]
 [1.971]
 [1.4  ]] [[1.189]
 [1.156]
 [1.268]
 [1.417]
 [1.264]]
maxi score, test score, baseline:  0.3221 1.0 1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.43698389750411876
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.046]
 [1.046]
 [1.046]
 [1.046]
 [1.046]] [[1.121]
 [1.121]
 [1.121]
 [1.121]
 [1.121]] [[1.998]
 [1.998]
 [1.998]
 [1.998]
 [1.998]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3261 1.0 1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 6.018686011294342
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.8323923802481388
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.092]
 [1.159]
 [1.092]
 [1.092]
 [1.092]] [[1.332]
 [1.855]
 [1.332]
 [1.332]
 [1.332]] [[1.635]
 [2.153]
 [1.635]
 [1.635]
 [1.635]]
maxi score, test score, baseline:  0.3301 1.0 1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3301 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3301 1.0 1.0
actor:  0 policy actor:  0  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]] [[-0.957]
 [-0.957]
 [-0.957]
 [-0.957]
 [-0.957]] [[1.15]
 [1.15]
 [1.15]
 [1.15]
 [1.15]]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.13664097484203902
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
maxi score, test score, baseline:  0.3321 1.0 1.0
maxi score, test score, baseline:  0.3321 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8943637
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
UNIT TEST: sample policy line 217 mcts : [0.167 0.458 0.083 0.25  0.042]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.352]
 [1.397]
 [1.416]
 [1.319]
 [1.377]] [[-0.29 ]
 [-0.416]
 [-0.595]
 [-0.164]
 [-0.337]] [[1.694]
 [1.7  ]
 [1.619]
 [1.712]
 [1.713]]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.021]
 [0.879]
 [0.311]
 [0.31 ]] [[-1.432]
 [-0.462]
 [-1.629]
 [-1.357]
 [-1.26 ]] [[0.244]
 [0.288]
 [1.227]
 [0.272]
 [0.334]]
siam score:  -0.89223677
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
line 256 mcts: sample exp_bonus 4.16343684366238
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.334]
 [1.334]
 [1.334]
 [1.334]
 [1.334]] [[0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.035]] [[1.968]
 [1.968]
 [1.968]
 [1.968]
 [1.968]]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
1121 8512
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.89292926
1126 8513
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
1128 8514
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3401 1.0 1.0
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.845]] [[1.12 ]
 [1.12 ]
 [1.12 ]
 [1.12 ]
 [2.736]] [[1.285]
 [1.285]
 [1.285]
 [1.285]
 [2.391]]
Printing some Q and Qe and total Qs values:  [[1.408]
 [1.408]
 [1.408]
 [1.408]
 [1.408]] [[1.108]
 [1.108]
 [1.108]
 [1.108]
 [1.108]] [[1.89]
 [1.89]
 [1.89]
 [1.89]
 [1.89]]
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.309]
 [1.309]
 [1.309]
 [1.309]
 [1.309]] [[0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]] [[2.924]
 [2.924]
 [2.924]
 [2.924]
 [2.924]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3401 1.0 1.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
using explorer policy with actor:  1
first move QE:  -0.14213951389086535
maxi score, test score, baseline:  0.3421 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.292 0.375 0.042 0.25 ]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus -0.03239994661400898
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
Printing some Q and Qe and total Qs values:  [[0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.252]] [[3.531]
 [3.531]
 [3.531]
 [3.531]
 [4.152]] [[1.301]
 [1.301]
 [1.301]
 [1.301]
 [1.762]]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.841]
 [1.05 ]
 [0.987]
 [0.627]
 [0.834]] [[0.585]
 [0.391]
 [0.491]
 [1.533]
 [0.939]] [[1.301]
 [1.592]
 [1.532]
 [1.506]
 [1.524]]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.083]
 [0.017]
 [0.011]
 [0.018]
 [0.009]] [[-0.08 ]
 [-0.132]
 [-0.128]
 [ 0.015]
 [-0.059]] [[0.249]
 [0.012]
 [0.007]
 [0.308]
 [0.142]]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
first move QE:  -0.1432291302670743
first move QE:  -0.1432291302670743
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
1152 8535
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.0661992234956181
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
1158 8539
Printing some Q and Qe and total Qs values:  [[1.019]
 [1.083]
 [1.057]
 [0.837]
 [1.003]] [[0.708]
 [1.382]
 [1.057]
 [1.398]
 [1.028]] [[0.674]
 [1.091]
 [0.893]
 [0.899]
 [0.835]]
Printing some Q and Qe and total Qs values:  [[0.889]
 [0.889]
 [1.033]
 [0.889]
 [0.889]] [[0.352]
 [0.352]
 [0.624]
 [0.352]
 [0.352]] [[0.844]
 [0.844]
 [1.276]
 [0.844]
 [0.844]]
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.435]
 [1.064]
 [0.025]
 [0.438]
 [0.517]] [[-0.685]
 [-1.282]
 [-0.425]
 [-0.559]
 [-0.502]] [[0.669]
 [1.53 ]
 [0.022]
 [0.759]
 [0.956]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
siam score:  -0.8963188
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.89676255
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.077]
 [1.077]
 [1.189]
 [1.077]
 [1.077]] [[0.763]
 [0.763]
 [1.024]
 [0.763]
 [0.763]] [[2.114]
 [2.114]
 [2.773]
 [2.114]
 [2.114]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
UNIT TEST: sample policy line 217 mcts : [0.042 0.375 0.167 0.417 0.   ]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.325]] [[4.796]
 [4.796]
 [4.796]
 [4.796]
 [4.796]] [[1.556]
 [1.556]
 [1.556]
 [1.556]
 [1.556]]
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 1.0231363828338658
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.89007413
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.2245903291106035
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.227]
 [0.328]
 [0.892]
 [0.385]
 [0.269]] [[2.672]
 [1.719]
 [1.466]
 [2.301]
 [2.808]] [[1.097]
 [0.598]
 [1.178]
 [1.06 ]
 [1.243]]
Printing some Q and Qe and total Qs values:  [[1.196]
 [0.154]
 [1.064]
 [1.018]
 [1.066]] [[1.862]
 [0.5  ]
 [2.295]
 [1.865]
 [2.269]] [[1.804]
 [0.234]
 [1.969]
 [1.677]
 [1.955]]
Printing some Q and Qe and total Qs values:  [[0.871]
 [0.871]
 [1.021]
 [0.871]
 [0.871]] [[0.008]
 [0.008]
 [0.47 ]
 [0.008]
 [0.008]] [[0.73 ]
 [0.73 ]
 [1.586]
 [0.73 ]
 [0.73 ]]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.224]
 [0.56 ]
 [0.98 ]
 [0.56 ]
 [0.56 ]] [[4.166]
 [3.631]
 [3.74 ]
 [3.631]
 [3.631]] [[1.466]
 [1.448]
 [1.884]
 [1.448]
 [1.448]]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
siam score:  -0.88370574
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Printing some Q and Qe and total Qs values:  [[1.01]
 [1.01]
 [1.01]
 [1.01]
 [1.01]] [[-0.378]
 [-0.378]
 [-0.378]
 [-0.378]
 [-0.378]] [[1.838]
 [1.838]
 [1.838]
 [1.838]
 [1.838]]
maxi score, test score, baseline:  0.3581 1.0 1.0
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
UNIT TEST: sample policy line 217 mcts : [0.042 0.292 0.125 0.208 0.333]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3581 1.0 1.0
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3601 1.0 1.0
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3601 1.0 1.0
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.87675416
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3641 1.0 1.0
start point for exploration sampling:  20047
siam score:  -0.88045555
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.202]
 [1.058]
 [0.617]
 [0.841]
 [0.842]] [[0.36 ]
 [0.726]
 [0.52 ]
 [1.342]
 [1.296]] [[0.129]
 [2.008]
 [1.024]
 [2.172]
 [2.132]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3641 1.0 1.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3641 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.799]
 [0.768]
 [0.768]
 [0.768]] [[0.706]
 [1.967]
 [1.901]
 [1.901]
 [1.901]] [[0.434]
 [0.799]
 [0.768]
 [0.768]
 [0.768]]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 7.628628932030009
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus -0.06290466846672987
Printing some Q and Qe and total Qs values:  [[0.792]
 [0.792]
 [0.792]
 [0.791]
 [0.782]] [[1.854]
 [1.854]
 [1.854]
 [2.078]
 [2.221]] [[1.813]
 [1.813]
 [1.813]
 [1.959]
 [2.038]]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
maxi score, test score, baseline:  0.3681 1.0 1.0
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.87716615
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.1410001778452907
maxi score, test score, baseline:  0.3681 1.0 1.0
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.009]
 [0.846]
 [0.279]
 [0.274]] [[-0.454]
 [-0.251]
 [-0.984]
 [-0.831]
 [-0.583]] [[0.517]
 [0.188]
 [0.956]
 [0.155]
 [0.351]]
Printing some Q and Qe and total Qs values:  [[0.825]
 [0.825]
 [0.825]
 [0.875]
 [0.825]] [[1.568]
 [1.568]
 [1.568]
 [1.716]
 [1.568]] [[2.065]
 [2.065]
 [2.065]
 [2.279]
 [2.065]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
1228 8594
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3701 1.0 1.0
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3701 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.852]
 [0.913]
 [1.081]
 [0.913]
 [0.913]] [[0.622]
 [0.513]
 [0.783]
 [0.513]
 [0.513]] [[1.598]
 [1.542]
 [2.279]
 [1.542]
 [1.542]]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.1  ]
 [1.095]
 [1.155]
 [1.1  ]
 [1.1  ]] [[0.591]
 [0.561]
 [0.424]
 [0.591]
 [0.591]] [[2.205]
 [2.174]
 [2.117]
 [2.205]
 [2.205]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3701 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.3721 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]] [[-0.198]
 [-0.198]
 [-0.198]
 [-0.198]
 [-0.198]] [[0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]]
maxi score, test score, baseline:  0.3721 1.0 1.0
siam score:  -0.8937166
start point for exploration sampling:  20047
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3741 1.0 1.0
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3741 1.0 1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 5.024004055277149
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3741 1.0 1.0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.231464071921309
maxi score, test score, baseline:  0.3741 1.0 1.0
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.248]
 [1.436]
 [1.203]
 [1.203]
 [1.218]] [[1.893]
 [0.998]
 [1.47 ]
 [1.47 ]
 [1.893]] [[1.998]
 [1.493]
 [1.594]
 [1.594]
 [1.961]]
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
1263 8616
siam score:  -0.8825145
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.878672
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3821 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3841 1.0 1.0
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.458 0.042 0.042 0.    0.458]
maxi score, test score, baseline:  0.3841 1.0 1.0
Starting evaluation
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.667 0.042 0.    0.    0.292]
Printing some Q and Qe and total Qs values:  [[0.998]
 [0.998]
 [0.998]
 [0.998]
 [0.998]] [[-0.146]
 [-0.146]
 [-0.146]
 [-0.146]
 [-0.146]] [[0.998]
 [0.998]
 [0.998]
 [0.998]
 [0.998]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.14528150765487
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4361 1.0 1.0
first move QE:  -0.14692400587828072
siam score:  -0.88030607
1285 8626
Printing some Q and Qe and total Qs values:  [[0.861]
 [0.688]
 [0.951]
 [0.651]
 [0.664]] [[ 0.424]
 [ 0.032]
 [ 0.63 ]
 [-0.131]
 [ 0.048]] [[1.072]
 [0.487]
 [1.379]
 [0.284]
 [0.475]]
in main func line 156:  1286
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4381 1.0 1.0
maxi score, test score, baseline:  0.4381 1.0 1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.072]
 [1.184]
 [1.136]
 [1.072]
 [1.072]] [[0.889]
 [0.525]
 [1.075]
 [0.889]
 [0.889]] [[1.936]
 [1.917]
 [2.187]
 [1.936]
 [1.936]]
Printing some Q and Qe and total Qs values:  [[0.22]
 [0.22]
 [0.22]
 [0.22]
 [0.22]] [[2.893]
 [2.893]
 [2.893]
 [2.893]
 [2.893]] [[1.983]
 [1.983]
 [1.983]
 [1.983]
 [1.983]]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.326]
 [1.326]
 [1.326]
 [1.326]
 [1.326]] [[1.383]
 [1.383]
 [1.383]
 [1.383]
 [1.383]] [[2.42]
 [2.42]
 [2.42]
 [2.42]
 [2.42]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.258062775766101
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.501]
 [1.501]
 [1.501]
 [1.501]
 [1.501]] [[0.22]
 [0.22]
 [0.22]
 [0.22]
 [0.22]] [[2.971]
 [2.971]
 [2.971]
 [2.971]
 [2.971]]
siam score:  -0.87911683
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4401 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4401 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4401 1.0 1.0
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.25 ]] [[2.244]
 [2.244]
 [2.244]
 [2.244]
 [7.159]] [[0.436]
 [0.436]
 [0.436]
 [0.436]
 [1.745]]
siam score:  -0.87544423
maxi score, test score, baseline:  0.4421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4421 1.0 1.0
maxi score, test score, baseline:  0.4421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.87395203
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1310 8639
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
maxi score, test score, baseline:  0.4441 1.0 1.0
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.732]
 [1.375]
 [1.363]
 [1.363]
 [1.363]] [[3.238]
 [1.713]
 [1.951]
 [1.951]
 [1.951]] [[2.126]
 [1.635]
 [1.781]
 [1.781]
 [1.781]]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.586]] [[3.222]
 [3.222]
 [3.222]
 [3.222]
 [3.48 ]] [[2.101]
 [2.101]
 [2.101]
 [2.101]
 [2.225]]
maxi score, test score, baseline:  0.4441 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4441 1.0 1.0
maxi score, test score, baseline:  0.4441 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.22 ]] [[2.818]
 [2.818]
 [2.818]
 [2.818]
 [2.921]] [[1.961]
 [1.961]
 [1.961]
 [1.961]
 [2.08 ]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4461 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.036]
 [1.013]
 [1.007]
 [1.036]
 [1.036]] [[0.322]
 [0.393]
 [0.498]
 [0.322]
 [0.322]] [[1.472]
 [1.451]
 [1.473]
 [1.472]
 [1.472]]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8795712
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1327 8648
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4521 1.0 1.0
using explorer policy with actor:  1
1334 8649
maxi score, test score, baseline:  0.4521 1.0 1.0
siam score:  -0.88105774
siam score:  -0.8857362
first move QE:  -0.15042671725017417
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4521 1.0 1.0
siam score:  -0.8860858
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4541 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus -0.3955700451762436
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8808458
start point for exploration sampling:  20047
Printing some Q and Qe and total Qs values:  [[1.248]
 [1.248]
 [1.262]
 [1.248]
 [1.248]] [[1.626]
 [1.626]
 [1.992]
 [1.626]
 [1.626]] [[1.6  ]
 [1.6  ]
 [1.791]
 [1.6  ]
 [1.6  ]]
siam score:  -0.8795922
maxi score, test score, baseline:  0.4581 1.0 1.0
siam score:  -0.8812523
maxi score, test score, baseline:  0.4581 1.0 1.0
start point for exploration sampling:  20047
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8792714
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4581 1.0 1.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4601 1.0 1.0
siam score:  -0.8776599
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4601 1.0 1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
1360 8669
maxi score, test score, baseline:  0.4601 1.0 1.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4621 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.167 0.042 0.25  0.5  ]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  20047
maxi score, test score, baseline:  0.4661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.032]
 [1.032]
 [1.126]
 [0.926]
 [0.987]] [[1.38 ]
 [1.38 ]
 [1.261]
 [1.327]
 [1.437]] [[0.898]
 [0.898]
 [0.882]
 [0.802]
 [0.904]]
Printing some Q and Qe and total Qs values:  [[0.615]
 [0.465]
 [0.844]
 [0.375]
 [0.495]] [[4.   ]
 [3.736]
 [3.715]
 [5.14 ]
 [4.038]] [[1.534]
 [1.282]
 [1.5  ]
 [2.083]
 [1.485]]
line 256 mcts: sample exp_bonus -0.24657093306751915
1377 8675
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.5199],
        [0.7007],
        [0.0000],
        [0.6847],
        [0.6734],
        [0.7954],
        [0.8560],
        [0.0000],
        [0.6405]], dtype=torch.float64)
0.0 0.0
0.0 0.5199039217400563
0.0 0.7007352803934468
0.9801 0.9801
0.0 0.684747408208134
0.0 0.6733686921623315
0.0 0.7954075588800789
0.0 0.8559571368530678
0.0 0.0
0.0 0.6404843460396592
maxi score, test score, baseline:  0.4661 1.0 1.0
siam score:  -0.8656447
maxi score, test score, baseline:  0.4661 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.048]
 [1.048]
 [1.225]
 [1.048]
 [1.048]] [[0.672]
 [0.672]
 [0.685]
 [0.672]
 [0.672]] [[1.553]
 [1.553]
 [1.876]
 [1.553]
 [1.553]]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1379 8676
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 0.7551861030567388
actor:  1 policy actor:  1  step number:  96 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4681 1.0 1.0
maxi score, test score, baseline:  0.4681 1.0 1.0
maxi score, test score, baseline:  0.4681 1.0 1.0
maxi score, test score, baseline:  0.4681 1.0 1.0
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.96 ]
 [0.883]
 [0.747]
 [0.96 ]] [[1.643]
 [0.593]
 [0.354]
 [0.588]
 [0.593]] [[2.462]
 [1.819]
 [1.373]
 [1.411]
 [1.819]]
maxi score, test score, baseline:  0.4681 1.0 1.0
actor:  0 policy actor:  0  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.33993161669556266
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.929]
 [0.786]
 [0.786]
 [0.812]] [[0.89 ]
 [0.319]
 [0.643]
 [0.643]
 [0.867]] [[1.301]
 [1.281]
 [1.211]
 [1.211]
 [1.412]]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.032]
 [1.044]
 [1.032]
 [0.978]
 [1.032]] [[0.873]
 [0.612]
 [0.873]
 [0.688]
 [0.873]] [[1.717]
 [1.567]
 [1.717]
 [1.485]
 [1.717]]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.346]
 [1.346]
 [1.331]
 [1.346]
 [1.346]] [[2.332]
 [2.332]
 [2.335]
 [2.332]
 [2.332]] [[2.304]
 [2.304]
 [2.292]
 [2.304]
 [2.304]]
Printing some Q and Qe and total Qs values:  [[1.202]
 [1.341]
 [1.294]
 [1.041]
 [1.129]] [[1.595]
 [1.427]
 [1.355]
 [1.107]
 [1.988]] [[1.949]
 [1.982]
 [1.866]
 [1.35 ]
 [2.183]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.976]
 [1.049]
 [0.976]
 [0.976]
 [0.948]] [[0.341]
 [0.04 ]
 [0.341]
 [0.341]
 [0.585]] [[1.514]
 [1.46 ]
 [1.514]
 [1.514]
 [1.621]]
first move QE:  -0.15398266450132145
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]] [[0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]] [[0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]]
Printing some Q and Qe and total Qs values:  [[0.979]
 [1.06 ]
 [0.979]
 [0.979]
 [1.074]] [[1.661]
 [2.787]
 [1.661]
 [1.661]
 [2.3  ]] [[0.521]
 [1.004]
 [0.521]
 [0.521]
 [0.815]]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8760029
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
1416 8700
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
start point for exploration sampling:  20047
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4821 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.079]
 [0.965]
 [0.834]
 [0.786]
 [1.004]] [[0.042]
 [2.494]
 [0.649]
 [0.456]
 [0.816]] [[1.248]
 [2.618]
 [1.164]
 [0.946]
 [1.604]]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.162]
 [0.162]
 [0.162]
 [0.162]
 [0.162]] [[1.544]
 [1.544]
 [1.544]
 [1.544]
 [1.544]] [[2.896]
 [2.896]
 [2.896]
 [2.896]
 [2.896]]
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.259]
 [0.267]
 [0.331]
 [0.347]] [[2.229]
 [2.041]
 [1.7  ]
 [1.705]
 [1.9  ]] [[1.198]
 [1.023]
 [0.751]
 [0.818]
 [0.993]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
start point for exploration sampling:  20047
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]] [[2.129]
 [2.129]
 [2.129]
 [2.129]
 [2.129]] [[2.168]
 [2.168]
 [2.168]
 [2.168]
 [2.168]]
Printing some Q and Qe and total Qs values:  [[1.125]
 [1.233]
 [1.292]
 [1.295]
 [1.125]] [[ 0.257]
 [-0.17 ]
 [-0.293]
 [-0.056]
 [ 0.257]] [[2.385]
 [2.458]
 [2.535]
 [2.621]
 [2.385]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4881 1.0 1.0
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4881 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.058]
 [1.214]
 [1.029]
 [1.058]
 [1.058]] [[0.289]
 [0.323]
 [0.402]
 [0.289]
 [0.289]] [[1.525]
 [1.86 ]
 [1.543]
 [1.525]
 [1.525]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.062]
 [1.302]
 [1.287]
 [1.1  ]
 [1.222]] [[0.244]
 [0.399]
 [0.384]
 [0.474]
 [0.443]] [[1.672]
 [2.254]
 [2.214]
 [1.901]
 [2.123]]
maxi score, test score, baseline:  0.4901 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  91 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
first move QE:  -0.15762703148620658
maxi score, test score, baseline:  0.4901 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.152]
 [1.246]
 [1.171]
 [0.659]
 [1.152]] [[0.811]
 [0.589]
 [0.9  ]
 [1.752]
 [0.811]] [[1.776]
 [1.671]
 [1.864]
 [2.144]
 [1.776]]
Printing some Q and Qe and total Qs values:  [[0.982]
 [0.982]
 [0.982]
 [0.982]
 [0.91 ]] [[1.051]
 [1.051]
 [1.051]
 [1.051]
 [1.165]] [[2.01 ]
 [2.01 ]
 [2.01 ]
 [2.01 ]
 [2.016]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.615]
 [1.037]
 [1.301]
 [1.151]
 [1.106]] [[3.171]
 [1.76 ]
 [1.308]
 [1.72 ]
 [1.233]] [[2.081]
 [1.654]
 [1.712]
 [1.796]
 [1.364]]
maxi score, test score, baseline:  0.4901 1.0 1.0
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
in main func line 156:  1443
siam score:  -0.8688068
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]] [[2.168]
 [2.168]
 [2.168]
 [2.168]
 [2.168]] [[1.711]
 [1.711]
 [1.711]
 [1.711]
 [1.711]]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4921 1.0 1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4941 1.0 1.0
line 256 mcts: sample exp_bonus 0.8787918199860459
Printing some Q and Qe and total Qs values:  [[1.213]
 [1.213]
 [1.248]
 [1.213]
 [1.213]] [[-0.183]
 [-0.183]
 [-0.074]
 [-0.183]
 [-0.183]] [[1.617]
 [1.617]
 [1.76 ]
 [1.617]
 [1.617]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.225]
 [0.883]
 [0.93 ]
 [0.676]] [[0.348]
 [0.482]
 [0.134]
 [0.378]
 [0.154]] [[0.646]
 [0.426]
 [1.277]
 [1.697]
 [0.889]]
Printing some Q and Qe and total Qs values:  [[1.161]
 [1.257]
 [1.161]
 [1.161]
 [1.161]] [[0.305]
 [0.296]
 [0.305]
 [0.305]
 [0.305]] [[1.709]
 [1.895]
 [1.709]
 [1.709]
 [1.709]]
Printing some Q and Qe and total Qs values:  [[1.05 ]
 [1.116]
 [1.05 ]
 [1.05 ]
 [1.05 ]] [[0.968]
 [0.351]
 [0.968]
 [0.968]
 [0.968]] [[1.847]
 [1.568]
 [1.847]
 [1.847]
 [1.847]]
Printing some Q and Qe and total Qs values:  [[1.037]
 [1.175]
 [1.05 ]
 [1.05 ]
 [1.05 ]] [[0.619]
 [0.336]
 [0.961]
 [0.961]
 [0.961]] [[1.685]
 [1.774]
 [1.939]
 [1.939]
 [1.939]]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.434]
 [0.423]
 [0.304]
 [0.315]] [[2.145]
 [3.559]
 [2.699]
 [4.927]
 [3.585]] [[0.371]
 [1.34 ]
 [0.835]
 [2.057]
 [1.286]]
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4941 1.0 1.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4961 1.0 1.0
siam score:  -0.8667772
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4961 1.0 1.0
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8678884
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
siam score:  -0.868791
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.014]
 [0.905]
 [0.685]
 [0.609]] [[-0.262]
 [-0.042]
 [-0.337]
 [-0.358]
 [-0.19 ]] [[0.889]
 [0.247]
 [1.186]
 [0.841]
 [0.933]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.648]
 [0.703]
 [0.64 ]
 [0.619]] [[-0.435]
 [-0.706]
 [-1.061]
 [-0.435]
 [-0.523]] [[0.64 ]
 [0.648]
 [0.703]
 [0.64 ]
 [0.619]]
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.635]
 [0.78 ]
 [0.635]
 [0.574]] [[-0.313]
 [ 0.   ]
 [-0.971]
 [ 0.   ]
 [-0.255]] [[0.443]
 [0.635]
 [0.78 ]
 [0.635]
 [0.574]]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.8692085
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 1.627027714822275
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.875 0.    0.042 0.042]
line 256 mcts: sample exp_bonus 1.9766890325906457
maxi score, test score, baseline:  0.5421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5421 1.0 1.0
start point for exploration sampling:  20047
maxi score, test score, baseline:  0.5421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.687]
 [1.049]
 [0.883]
 [0.643]
 [0.883]] [[-0.246]
 [-0.078]
 [ 0.   ]
 [-0.266]
 [ 0.   ]] [[0.587]
 [1.479]
 [1.226]
 [0.48 ]
 [1.226]]
Printing some Q and Qe and total Qs values:  [[1.043]
 [1.136]
 [1.05 ]
 [0.675]
 [1.105]] [[1.287]
 [0.899]
 [0.532]
 [1.742]
 [1.107]] [[1.736]
 [1.611]
 [1.277]
 [1.562]
 [1.701]]
actor:  1 policy actor:  1  step number:  91 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5421 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5421 1.0 1.0
maxi score, test score, baseline:  0.5421 1.0 1.0
siam score:  -0.86284506
maxi score, test score, baseline:  0.5421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]] [[-0.089]
 [-0.089]
 [-0.089]
 [-0.089]
 [-0.089]] [[1.472]
 [1.472]
 [1.472]
 [1.472]
 [1.472]]
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.5421 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5421 1.0 1.0
maxi score, test score, baseline:  0.5421 1.0 1.0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.502624049044095
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.998]
 [0.11 ]
 [0.796]
 [0.814]] [[-0.218]
 [-1.171]
 [-0.948]
 [-0.29 ]
 [-0.714]] [[1.217]
 [1.857]
 [0.156]
 [1.748]
 [1.644]]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1486 8750
Printing some Q and Qe and total Qs values:  [[0.662]
 [0.107]
 [0.936]
 [0.647]
 [0.646]] [[-0.059]
 [-0.179]
 [ 0.175]
 [-0.063]
 [-0.418]] [[0.651]
 [0.098]
 [1.066]
 [0.635]
 [0.344]]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5461 1.0 1.0
line 256 mcts: sample exp_bonus 0.8272017627564746
Printing some Q and Qe and total Qs values:  [[1.388]
 [1.36 ]
 [1.388]
 [1.388]
 [1.348]] [[1.548]
 [1.619]
 [1.548]
 [1.548]
 [2.148]] [[2.484]
 [2.498]
 [2.484]
 [2.484]
 [2.987]]
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Printing some Q and Qe and total Qs values:  [[1.197]
 [1.336]
 [1.197]
 [1.145]
 [1.085]] [[1.565]
 [1.693]
 [1.565]
 [1.809]
 [1.704]] [[2.239]
 [2.647]
 [2.239]
 [2.381]
 [2.155]]
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.6922],
        [0.1799],
        [0.1837],
        [0.1609],
        [0.0000],
        [0.2519],
        [0.8931],
        [0.4209],
        [0.1744],
        [0.4561]], dtype=torch.float64)
0.0 0.6922310136409926
0.0 0.17993264201826198
0.0 0.18370785089138955
0.0 0.16086309089904094
0.9509900498999999 0.9509900498999999
0.0 0.2518660849186438
0.0 0.8930540080767402
0.0 0.4208754167618539
0.0 0.17437567359788647
0.0 0.4560841071178024
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
in main func line 156:  1492
actor:  0 policy actor:  0  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5501 1.0 1.0
using explorer policy with actor:  1
siam score:  -0.85085297
siam score:  -0.8527143
siam score:  -0.85386884
maxi score, test score, baseline:  0.5501 1.0 1.0
maxi score, test score, baseline:  0.5501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.96 ]
 [0.96 ]
 [1.127]
 [0.96 ]
 [0.96 ]] [[0.138]
 [0.138]
 [0.404]
 [0.138]
 [0.138]] [[1.291]
 [1.291]
 [1.607]
 [1.291]
 [1.291]]
maxi score, test score, baseline:  0.5501 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.103]
 [1.202]
 [1.042]
 [0.963]
 [1.133]] [[1.108]
 [1.237]
 [1.421]
 [1.634]
 [1.212]] [[1.875]
 [2.145]
 [2.031]
 [2.077]
 [2.009]]
maxi score, test score, baseline:  0.5501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.349]
 [0.249]
 [0.3  ]
 [0.31 ]] [[2.325]
 [1.892]
 [0.971]
 [2.19 ]
 [2.679]] [[1.434]
 [1.058]
 [0.084]
 [1.294]
 [1.771]]
using explorer policy with actor:  1
1502 8761
siam score:  -0.8542941
maxi score, test score, baseline:  0.5501 1.0 1.0
actor:  1 policy actor:  1  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5501 1.0 1.0
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus -0.21082190588882618
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]] [[-0.546]
 [-0.546]
 [-0.546]
 [-0.546]
 [-0.546]] [[0.972]
 [0.972]
 [0.972]
 [0.972]
 [0.972]]
Printing some Q and Qe and total Qs values:  [[0.942]
 [1.065]
 [0.942]
 [0.715]
 [0.932]] [[1.242]
 [0.812]
 [1.242]
 [1.305]
 [1.349]] [[2.319]
 [2.135]
 [2.319]
 [1.929]
 [2.407]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5501 1.0 1.0
maxi score, test score, baseline:  0.5501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1511 8770
Printing some Q and Qe and total Qs values:  [[0.712]
 [1.194]
 [1.257]
 [1.016]
 [1.118]] [[1.666]
 [1.43 ]
 [1.055]
 [2.369]
 [1.203]] [[0.968]
 [1.455]
 [1.381]
 [1.618]
 [1.272]]
Printing some Q and Qe and total Qs values:  [[0.954]
 [1.147]
 [0.954]
 [0.954]
 [0.954]] [[2.632]
 [2.279]
 [2.632]
 [2.632]
 [2.632]] [[1.675]
 [1.769]
 [1.675]
 [1.675]
 [1.675]]
actor:  1 policy actor:  1  step number:  90 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5501 1.0 1.0
maxi score, test score, baseline:  0.5501 1.0 1.0
maxi score, test score, baseline:  0.5501 1.0 1.0
maxi score, test score, baseline:  0.5501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
first move QE:  -0.15784972010387996
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
Printing some Q and Qe and total Qs values:  [[1.112]
 [1.069]
 [1.069]
 [1.069]
 [1.069]] [[0.066]
 [0.285]
 [0.285]
 [0.285]
 [0.285]] [[1.58 ]
 [1.857]
 [1.857]
 [1.857]
 [1.857]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5521 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5521 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.873]
 [0.945]
 [0.942]
 [0.873]
 [0.873]] [[0.875]
 [0.777]
 [0.608]
 [0.875]
 [0.875]] [[0.873]
 [0.945]
 [0.942]
 [0.873]
 [0.873]]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.196]
 [1.049]
 [1.017]
 [1.049]
 [1.049]] [[0.635]
 [1.087]
 [0.937]
 [1.087]
 [1.087]] [[1.509]
 [1.768]
 [1.591]
 [1.768]
 [1.768]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5541 1.0 1.0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5956496999651014
maxi score, test score, baseline:  0.5541 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.931]
 [0.931]
 [0.931]
 [0.931]
 [0.931]] [[2.276]
 [2.276]
 [2.276]
 [2.276]
 [2.276]] [[1.616]
 [1.616]
 [1.616]
 [1.616]
 [1.616]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20047
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.313]
 [0.981]
 [0.611]
 [0.782]] [[0.949]
 [0.035]
 [0.273]
 [0.949]
 [0.843]] [[2.441]
 [0.626]
 [2.279]
 [2.441]
 [2.64 ]]
maxi score, test score, baseline:  0.5541 1.0 1.0
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.773]
 [1.174]
 [0.773]
 [0.898]
 [0.773]] [[2.837]
 [2.704]
 [2.837]
 [3.836]
 [2.837]] [[1.666]
 [1.822]
 [1.666]
 [2.18 ]
 [1.666]]
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.765]
 [0.941]
 [0.684]
 [0.758]] [[-0.108]
 [ 0.191]
 [ 1.147]
 [-0.188]
 [-0.065]] [[0.021]
 [0.648]
 [1.27 ]
 [0.395]
 [0.507]]
Printing some Q and Qe and total Qs values:  [[1.059]
 [1.059]
 [1.056]
 [1.059]
 [1.059]] [[1.37 ]
 [1.37 ]
 [1.381]
 [1.37 ]
 [1.37 ]] [[2.248]
 [2.248]
 [2.255]
 [2.248]
 [2.248]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.108]
 [1.108]
 [1.077]
 [1.108]
 [1.108]] [[1.102]
 [1.102]
 [1.074]
 [1.102]
 [1.102]] [[2.28 ]
 [2.28 ]
 [2.189]
 [2.28 ]
 [2.28 ]]
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.349]] [[4.476]
 [4.476]
 [4.476]
 [4.476]
 [4.356]] [[2.128]
 [2.128]
 [2.128]
 [2.128]
 [2.072]]
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
