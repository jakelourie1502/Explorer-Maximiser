dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 25}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
res_block_channels:[32, 64, 64]
res_block_ds:[False, False, False]
reward_support:[-1, 1, 41]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64, 64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 41]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:2
max_workers:32
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
resampling:False
resampling_use_max:False
resampling_assess_best_child:False
rs_start:1000
ep_to_batch_ratio:[11, 12]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
env:<class 'game_play.frozen_lakeGym_Image.gridWorld'>
same_env_each_time:True
channels:3
env_size:[8, 8]
observable_size:[8, 8]
game_modes:1
env_map:[['S' 'H' 'F' 'F' 'F' 'H' 'F' 'F']
 ['F' 'F' 'F' 'H' 'F' 'H' 'H' 'F']
 ['H' 'F' 'F' 'H' 'F' 'F' 'F' 'H']
 ['H' 'F' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'H' 'H' 'F' 'H' 'F']
 ['H' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'G']]
max_steps:100
actions_size:5
optimal_score:1
total_frames:205000
exp_gamma:0.95
atari_env:False
reward_clipping:False
memory_size:100
image_size:[48, 48]
timesteps_in_obs:2
store_prev_actions:True
running_reward_in_obs:False
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 1, 6]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
state_size:[6, 6]
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 64)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:25
detach_expV_calc:True
use_new_episode_expV:True
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['S' 'H' 'F' 'F' 'F' 'H' 'F' 'F']
 ['F' 'F' 'F' 'H' 'F' 'H' 'H' 'F']
 ['H' 'F' 'F' 'H' 'F' 'F' 'F' 'H']
 ['H' 'F' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'H' 'H' 'F' 'H' 'F']
 ['H' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'G']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1 110
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.011779873970557343
2 185
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
deleting a thread, now have 2 threads
Frames:  1150 train batches done:  32 episodes:  200
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.1004893866745394
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
2 200
2 205
2 206
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
2 252
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.34403947
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
3 321
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
siam score:  -0.25834864
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
3 369
3 390
3 403
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
siam score:  -0.3809426
siam score:  -0.35920405
siam score:  -0.35430306
6 488
6 500
6 505
first move QE:  -0.23191663476066393
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
9 581
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus -0.12467539086937904
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.2969155
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.2010693450759729
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[ 1.5]
 [-0. ]
 [ 1.5]
 [ 1.5]
 [ 0. ]] [[0.   ]
 [0.045]
 [0.   ]
 [0.   ]
 [0.072]] [[2.931]
 [0.022]
 [2.931]
 [2.931]
 [0.076]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
9 656
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.22082368
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
14 794
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
16 852
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.53052497
first move QE:  -0.06420448699232162
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.06420448699232162
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.  0.2 0.2 0.4 0.2]
siam score:  -0.36169174
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.2633855
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.3618496
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
21 990
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.47025645
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.08]
 [0.08]
 [0.08]
 [0.08]
 [0.08]] [[0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
23 1191
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
25 1222
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.   ]
 [ 0.051]
 [ 0.051]
 [ 0.118]
 [ 0.051]] [[0.   ]
 [0.085]
 [0.085]
 [0.197]
 [0.085]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]] [[0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.49420032
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.0391487914255791
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.133]
 [-0.001]
 [-0.085]
 [-0.143]
 [-0.209]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.133]
 [-0.001]
 [-0.078]
 [-0.143]
 [-0.246]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.005]
 [0.002]
 [0.001]
 [0.001]] [[ 0.031]
 [ 0.038]
 [ 0.004]
 [-0.186]
 [-0.417]] [[0.601]
 [0.616]
 [0.566]
 [0.31 ]
 [0.001]]
26 1355
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[ 0.]
 [ 0.]
 [ 0.]
 [-0.]
 [-0.]] [[0.006]
 [0.001]
 [0.001]
 [0.005]
 [0.001]] [[0.029]
 [0.02 ]
 [0.02 ]
 [0.026]
 [0.021]]
line 256 mcts: sample exp_bonus 0.0059876045913540565
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.083]
 [0.031]
 [0.068]
 [0.017]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.089]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.   ]] [[0.072]
 [0.012]
 [0.012]
 [0.012]
 [0.012]]
26 1417
siam score:  -0.46442214
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.643]
 [ 0.   ]
 [-0.115]
 [-0.167]
 [-0.102]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.48992988
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.021634402025003958
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
27 1494
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.49189872
28 1555
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.48416978
29 1602
maxi score, test score, baseline:  0.0001 0.0 0.0001
29 1614
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
29 1678
siam score:  -0.4923763
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.417 0.083 0.25  0.083 0.167]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
line 256 mcts: sample exp_bonus -0.01206189646026167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.47342494
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
34 1724
siam score:  -0.45759216
34 1728
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.01 ]
 [ 0.   ]
 [-0.002]
 [-0.005]
 [-0.   ]] [[0.004]
 [0.022]
 [0.018]
 [0.013]
 [0.021]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
36 1777
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.012]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.006]] [[0.   ]
 [0.002]
 [0.002]
 [0.002]
 [0.005]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
37 1804
siam score:  -0.36423576
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.51611406
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
40 1888
40 1894
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
42 1911
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.01 ]
 [0.01 ]
 [0.003]
 [0.01 ]
 [0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
42 1941
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.125 0.083 0.417 0.125 0.25 ]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.32329574
43 1995
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.30018803
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.00406480876879083
siam score:  -0.24611755
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.019]
 [-0.005]
 [ 0.009]
 [ 0.009]
 [ 0.008]] [[0.03 ]
 [0.006]
 [0.02 ]
 [0.02 ]
 [0.019]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]] [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]]
44 2091
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
siam score:  -0.17649445
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.11889925
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.208 0.125 0.292 0.125 0.25 ]
siam score:  -0.2003477
siam score:  -0.2086133
first move QE:  -0.010705180195912337
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
45 2198
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
45 2209
45 2216
siam score:  -0.17538542
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
46 2289
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.007]
 [0.   ]
 [0.   ]] [[0.005]
 [0.006]
 [0.014]
 [0.005]
 [0.005]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.003]
 [-0.006]
 [ 0.   ]
 [-0.002]
 [ 0.002]] [[0.006]
 [0.003]
 [0.009]
 [0.007]
 [0.01 ]]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.14147204
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.1330048
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [-0.001]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.001]
 [0.001]
 [0.003]
 [0.003]
 [0.003]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.18989837
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
first move QE:  -0.009823152082070631
47 2443
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
49 2550
49 2561
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
UNIT TEST: sample policy line 217 mcts : [0.333 0.083 0.292 0.208 0.083]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
49 2606
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.003]
 [0.001]
 [0.008]
 [0.004]] [[0.016]
 [0.019]
 [0.015]
 [0.028]
 [0.021]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
50 2643
first move QE:  -0.00909284060341012
50 2653
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.3970211
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
50 2692
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
using explorer policy with actor:  1
50 2698
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.009032848301018725
siam score:  -0.3495703
maxi score, test score, baseline:  0.0001 0.0 0.0001
50 2746
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.0001999688136312879
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.003]
 [0.001]
 [0.002]
 [0.002]] [[0.005]
 [0.006]
 [0.005]
 [0.006]
 [0.006]]
53 2859
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.4001054
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.00039999268349270525
53 2894
maxi score, test score, baseline:  0.0001 0.0 0.0001
54 2913
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]] [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.007]
 [-0.007]
 [-0.007]
 [-0.01 ]
 [-0.01 ]] [[0.004]
 [0.004]
 [0.003]
 [0.002]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.008]
 [-0.007]
 [-0.007]
 [-0.008]
 [-0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.003]
 [-0.003]
 [ 0.005]
 [ 0.006]
 [ 0.006]] [[0.022]
 [0.012]
 [0.026]
 [0.028]
 [0.029]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.3533475
maxi score, test score, baseline:  0.0001 0.0 0.0001
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 3.4916101842941487e-07
0.0 0.0
0.0 3.2567211532940537e-07
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
0.0 3.280759886299917e-07
0.0 3.2993902719705477e-07
0.0 2.990797048077068e-07
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.00220855236683088
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [-0.005]
 [-0.004]
 [-0.004]
 [-0.004]] [[0.002]
 [0.002]
 [0.002]
 [0.003]
 [0.002]]
59 3065
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.004]
 [-0.006]
 [-0.002]
 [-0.   ]
 [-0.002]] [[0.005]
 [0.002]
 [0.006]
 [0.008]
 [0.006]]
line 256 mcts: sample exp_bonus -0.002855508522525881
59 3076
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.003]
 [-0.006]
 [ 0.001]
 [ 0.   ]
 [ 0.001]] [[0.011]
 [0.004]
 [0.02 ]
 [0.017]
 [0.018]]
59 3094
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
59 3102
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.008090587058318707
first move QE:  -0.008109630629483888
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
60 3167
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.002]
 [-0.003]
 [-0.002]
 [-0.003]
 [-0.003]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
61 3202
using explorer policy with actor:  1
siam score:  -0.452374
siam score:  -0.44829097
63 3224
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.004]
 [0.005]
 [0.002]
 [0.002]] [[-0.   ]
 [ 0.003]
 [ 0.006]
 [ 0.001]
 [-0.   ]]
64 3243
STARTED EXPV TRAINING ON FRAME NO.  20019
Starting evaluation
siam score:  -0.42781198
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.249]
 [-0.095]
 [-0.041]
 [-0.001]
 [ 0.009]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.41121066
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
73 3372
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.267]
 [-0.038]
 [ 0.139]
 [ 0.797]
 [ 0.725]] [[0.198]
 [0.351]
 [0.469]
 [0.907]
 [0.859]]
73 3392
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20019
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.152]
 [0.152]
 [0.152]
 [0.152]
 [1.249]] [[0.592]
 [0.592]
 [0.592]
 [0.592]
 [1.689]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.138]
 [0.138]
 [0.138]
 [0.268]
 [0.571]] [[1.182]
 [1.182]
 [1.182]
 [1.39 ]
 [1.879]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.748]
 [-0.533]
 [-0.402]
 [-0.011]
 [-0.003]] [[0.   ]
 [0.43 ]
 [0.691]
 [1.475]
 [1.49 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.562]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.02 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.581]
 [-0.343]
 [ 0.119]
 [-0.003]
 [-0.026]] [[0.   ]
 [0.396]
 [1.166]
 [0.964]
 [0.924]]
78 3450
siam score:  -0.36100295
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20019
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
81 3481
siam score:  -0.42140442
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.126]
 [ 0.067]
 [ 0.12 ]
 [ 0.656]
 [ 0.911]] [[0.   ]
 [0.164]
 [0.21 ]
 [0.668]
 [0.886]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.13885335391593928
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.06]
 [0.06]
 [0.06]
 [0.06]
 [0.06]] [[1.052]
 [1.052]
 [1.052]
 [1.052]
 [1.052]]
siam score:  -0.43227416
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.4511065
92 3511
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.785]
 [0.785]
 [0.785]
 [0.774]
 [1.107]] [[0.835]
 [0.835]
 [0.835]
 [0.813]
 [1.478]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.64]
 [1.64]
 [1.64]
 [1.64]
 [1.64]] [[1.246]
 [1.246]
 [1.246]
 [1.246]
 [1.246]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]] [[0.494]
 [0.494]
 [0.494]
 [0.494]
 [0.494]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.07 ]
 [2.07 ]
 [2.07 ]
 [2.07 ]
 [1.994]] [[1.809]
 [1.809]
 [1.809]
 [1.809]
 [1.724]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9616999926078804
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.601]
 [1.01 ]
 [1.837]
 [2.897]
 [4.188]] [[0.057]
 [0.249]
 [0.636]
 [1.133]
 [1.738]]
siam score:  -0.41203716
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.012]
 [0.406]
 [0.493]
 [1.447]
 [4.903]] [[0.   ]
 [0.157]
 [0.192]
 [0.571]
 [1.947]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.125]
 [ 0.119]
 [ 0.25 ]
 [ 0.325]
 [ 0.855]] [[0.   ]
 [0.081]
 [0.125]
 [0.15 ]
 [0.327]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.059]
 [1.059]
 [1.059]
 [1.059]
 [1.948]] [[0.965]
 [0.965]
 [0.965]
 [0.965]
 [2.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20019
siam score:  -0.4473682
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.5973874533578879
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.358]
 [0.514]
 [0.658]
 [0.902]
 [1.998]] [[0.   ]
 [0.149]
 [0.287]
 [0.521]
 [1.57 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.237]
 [0.603]
 [0.303]
 [1.462]
 [2.35 ]] [[0.161]
 [0.398]
 [0.203]
 [0.953]
 [1.528]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.887594339196266
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.576]
 [1.576]
 [1.576]
 [1.41 ]
 [1.482]] [[0.923]
 [0.923]
 [0.923]
 [0.772]
 [0.837]]
first move QE:  0.030152271121220275
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.117]
 [1.117]
 [1.117]
 [1.117]
 [1.117]] [[0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.59]
 [0.59]
 [0.59]
 [0.59]
 [0.59]] [[0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]]
134 3612
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.4708345
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
136 3632
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.628]
 [ 0.152]
 [ 0.121]
 [ 0.021]
 [ 0.009]] [[0.   ]
 [1.253]
 [1.203]
 [1.041]
 [1.022]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.208 0.458 0.25 ]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.44 ]
 [1.006]
 [0.365]
 [0.812]
 [1.646]] [[1.483]
 [0.532]
 [0.106]
 [0.403]
 [0.956]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.595]
 [1.595]
 [1.595]
 [1.595]
 [1.595]] [[1.674]
 [1.674]
 [1.674]
 [1.674]
 [1.674]]
138 3662
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
139 3670
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
start point for exploration sampling:  20019
siam score:  -0.43236715
siam score:  -0.42697316
siam score:  -0.42531875
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.668]
 [1.668]
 [1.668]
 [1.546]
 [2.067]] [[1.107]
 [1.107]
 [1.107]
 [0.973]
 [1.545]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-2.19 ]
 [-0.557]
 [-0.984]
 [-1.329]
 [-0.203]] [[0.   ]
 [1.087]
 [0.803]
 [0.574]
 [1.323]]
150 3792
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
151 3801
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.4890234
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.5305078
first move QE:  0.04554074645711934
start point for exploration sampling:  20019
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
157 3842
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus -3.869317719036621
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
157 3885
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.411]
 [-0.411]
 [-0.214]
 [-0.233]
 [-0.19 ]] [[1.411]
 [1.411]
 [1.689]
 [1.663]
 [1.724]]
siam score:  -0.49818465
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.48106614
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.2563193063403014
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20019
siam score:  -0.40111268
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.378]
 [1.378]
 [1.378]
 [1.378]
 [1.378]] [[1.276]
 [1.276]
 [1.276]
 [1.276]
 [1.276]]
siam score:  -0.4007778
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.39024162
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.327]
 [2.327]
 [2.327]
 [2.327]
 [2.698]] [[0.939]
 [0.939]
 [0.939]
 [0.939]
 [1.268]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.576]
 [2.576]
 [2.576]
 [2.576]
 [3.09 ]] [[1.274]
 [1.274]
 [1.274]
 [1.274]
 [1.779]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.31946072
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.902]
 [0.902]
 [0.902]
 [0.902]
 [2.417]] [[-0.245]
 [-0.245]
 [-0.245]
 [-0.245]
 [ 1.269]]
Starting evaluation
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.42279503
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.861]
 [2.861]
 [2.861]
 [2.255]
 [2.887]] [[0.777]
 [0.777]
 [0.777]
 [0.575]
 [0.786]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.007]
 [3.007]
 [3.007]
 [3.007]
 [3.399]] [[1.383]
 [1.383]
 [1.383]
 [1.383]
 [1.613]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.554]
 [1.505]
 [1.744]
 [2.449]
 [3.055]] [[0.284]
 [0.25 ]
 [0.419]
 [0.92 ]
 [1.351]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
202 4153
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
213 4179
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
217 4183
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-4.66 ]
 [-0.203]
 [-4.374]
 [ 0.802]
 [ 2.328]] [[0.   ]
 [0.882]
 [0.057]
 [1.081]
 [1.383]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.501]
 [4.501]
 [4.501]
 [4.501]
 [5.083]] [[1.879]
 [1.879]
 [1.879]
 [1.879]
 [1.993]]
using explorer policy with actor:  1
219 4189
maxi score, test score, baseline:  0.0001 0.0 0.0001
220 4205
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.254]] [[1.074]
 [1.074]
 [1.074]
 [1.074]
 [0.938]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.5209429
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.02907918428362339
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.47004157
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
230 4323
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.47173432
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000]], dtype=torch.float64)
0.0 1.44993582859736e-20
0.0 1.159948662877888e-19
0.0 0.0
0.0 1.2243902552599928e-19
0.0 -6.919363223696829e-12
0.0 9.021822933494684e-20
0.0 0.0
0.0 2.335285120137924e-12
0.0 0.0
0.0 0.0
UNIT TEST: sample policy line 217 mcts : [0.    0.125 0.    0.333 0.542]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.078]
 [0.431]
 [0.623]
 [1.18 ]
 [1.18 ]] [[0.032]
 [0.456]
 [0.686]
 [1.355]
 [1.355]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.104]
 [2.104]
 [0.703]
 [1.168]
 [1.61 ]] [[2.   ]
 [2.   ]
 [0.   ]
 [0.664]
 [1.295]]
siam score:  -0.5322679
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.371]
 [0.368]
 [0.72 ]
 [1.334]
 [1.296]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.5565045
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.833]
 [1.366]
 [0.883]
 [2.392]
 [2.384]] [[0.36 ]
 [0.792]
 [0.4  ]
 [1.624]
 [1.618]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.504]
 [0.504]
 [0.504]
 [0.958]
 [1.032]] [[0.264]
 [0.264]
 [0.264]
 [0.415]
 [0.44 ]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.783]
 [1.269]
 [0.587]
 [1.269]
 [1.269]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.5542556
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
259 4384
siam score:  -0.53313315
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.499]
 [2.499]
 [2.499]
 [2.499]
 [2.437]] [[2.   ]
 [2.   ]
 [2.   ]
 [2.   ]
 [1.926]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.426]
 [2.426]
 [2.426]
 [2.426]
 [1.92 ]] [[1.524]
 [1.524]
 [1.524]
 [1.524]
 [1.116]]
siam score:  -0.5350123
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.362]
 [2.362]
 [2.362]
 [2.362]
 [2.362]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.622]
 [0.612]
 [0.378]
 [1.116]
 [1.97 ]] [[1.223]
 [0.251]
 [0.027]
 [0.736]
 [1.557]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.029]
 [0.918]
 [1.048]
 [2.085]
 [2.366]] [[0.302]
 [0.224]
 [0.315]
 [1.038]
 [1.234]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.808]
 [2.808]
 [2.808]
 [2.808]
 [2.808]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.187]
 [2.187]
 [2.187]
 [2.187]
 [2.187]] [[4.374]
 [4.374]
 [4.374]
 [4.374]
 [4.374]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.5945718
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.937]
 [0.537]
 [0.419]
 [1.354]
 [1.849]] [[0.451]
 [0.164]
 [0.08 ]
 [0.752]
 [1.108]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.042 0.042 0.833]
siam score:  -0.5771039
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.5670201
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.807]
 [1.961]
 [1.859]
 [2.304]
 [2.568]] [[1.965]
 [1.119]
 [1.017]
 [1.462]
 [1.726]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.5725642
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.052]
 [1.118]
 [1.338]
 [2.355]
 [2.701]] [[0.491]
 [0.535]
 [0.681]
 [1.358]
 [1.589]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.27 ]
 [0.142]
 [0.764]
 [1.386]
 [1.616]] [[1.143]
 [0.166]
 [0.705]
 [1.244]
 [1.443]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.72 ]
 [0.594]
 [1.611]
 [2.664]
 [2.69 ]] [[0.698]
 [0.063]
 [0.637]
 [1.23 ]
 [1.245]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.701]
 [3.701]
 [3.139]
 [3.701]
 [4.348]] [[1.174]
 [1.174]
 [0.809]
 [1.174]
 [1.594]]
siam score:  -0.60437644
first move QE:  0.0849970169908774
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.5835228
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.261]
 [2.261]
 [1.415]
 [2.261]
 [3.296]] [[1.211]
 [1.211]
 [0.566]
 [1.211]
 [2.   ]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.56345195
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.5339006
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.818]
 [0.317]
 [1.505]
 [1.547]
 [1.548]] [[0.568]
 [0.144]
 [1.149]
 [1.185]
 [1.186]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.791]
 [1.999]
 [2.791]
 [2.941]
 [3.236]] [[0.802]
 [0.275]
 [0.802]
 [0.902]
 [1.099]]
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.419]
 [0.471]
 [1.846]
 [1.642]
 [1.416]] [[0.356]
 [0.039]
 [0.499]
 [0.431]
 [0.355]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.5359532
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.    0.083 0.833]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.432]
 [0.722]
 [1.708]
 [1.847]
 [1.813]] [[1.084]
 [0.454]
 [1.328]
 [1.451]
 [1.421]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-4.415]
 [-4.415]
 [-4.415]
 [-4.415]
 [-3.811]] [[0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.885]]
340 4499
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus -2.9623156914387954
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.5490846
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-4.056]
 [-4.056]
 [-4.056]
 [-4.056]
 [-3.296]] [[0.615]
 [0.615]
 [0.615]
 [0.615]
 [1.105]]
siam score:  -0.56901526
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.63 ]
 [-0.63 ]
 [-0.63 ]
 [-0.63 ]
 [ 0.119]] [[1.427]
 [1.427]
 [1.427]
 [1.427]
 [1.998]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
344 4541
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6336395
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
347 4559
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.791]
 [3.179]
 [3.159]
 [3.473]
 [4.148]] [[0.527]
 [0.738]
 [0.727]
 [0.898]
 [1.266]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.7339965516765705
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.419]
 [4.419]
 [4.419]
 [4.419]
 [5.253]] [[1.27 ]
 [1.27 ]
 [1.27 ]
 [1.27 ]
 [1.836]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
Sims:  25 1 epoch:  40107 pick best:  True frame count:  40107
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.91]
 [1.91]
 [1.91]
 [1.91]
 [2.61]] [[1.091]
 [1.091]
 [1.091]
 [1.091]
 [1.852]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.611]
 [3.611]
 [3.611]
 [3.611]
 [7.236]] [[0.672]
 [0.672]
 [0.672]
 [0.672]
 [1.94 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
369 4606
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.149]
 [2.326]
 [2.166]
 [2.177]
 [2.855]] [[0.   ]
 [0.787]
 [0.68 ]
 [0.687]
 [1.14 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6259218
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
first move QE:  0.1264212613373784
siam score:  -0.6312946
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6247094
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
378 4637
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
381 4644
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.042 0.25  0.625]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.976]
 [2.848]
 [2.766]
 [4.014]
 [4.141]] [[1.611]
 [1.088]
 [1.05 ]
 [1.629]
 [1.688]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.491]
 [1.64 ]
 [1.64 ]
 [1.64 ]
 [2.495]] [[0.712]
 [0.848]
 [0.848]
 [0.848]
 [1.626]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
388 4676
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.08289399554238402
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.258]
 [-0.099]] [[0.172]
 [0.172]
 [0.172]
 [0.   ]
 [0.106]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.154]
 [ 0.155]
 [-0.154]
 [ 0.12 ]
 [ 0.116]] [[1.381]
 [1.793]
 [1.381]
 [1.747]
 [1.741]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
388 4703
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-1.193]
 [-0.247]
 [-0.624]
 [-0.605]
 [-0.553]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.388]
 [ 0.236]
 [ 0.   ]
 [ 0.   ]
 [ 0.133]] [[0.   ]
 [0.416]
 [0.259]
 [0.259]
 [0.347]]
start point for exploration sampling:  20019
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.565]
 [0.565]
 [0.565]
 [0.767]
 [1.408]] [[0.658]
 [0.658]
 [0.658]
 [0.905]
 [1.691]]
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.096]
 [1.096]
 [1.096]
 [1.096]
 [1.419]] [[1.368]
 [1.368]
 [1.368]
 [1.368]
 [1.846]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.252]
 [ 0.051]
 [ 0.044]
 [ 0.384]
 [ 0.582]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.63795
maxi score, test score, baseline:  0.0001 0.0 0.0001
396 4749
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.263]
 [1.263]
 [1.263]
 [1.263]
 [1.396]] [[1.471]
 [1.471]
 [1.471]
 [1.471]
 [1.65 ]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
398 4754
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.167]
 [ 0.632]
 [ 0.   ]
 [ 1.308]
 [ 0.844]] [[0.   ]
 [0.267]
 [0.056]
 [0.493]
 [0.338]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.078]
 [1.078]
 [1.078]
 [1.078]
 [1.576]] [[1.048]
 [1.048]
 [1.048]
 [1.048]
 [1.515]]
using explorer policy with actor:  1
using explorer policy with actor:  1
first move QE:  0.12793664557671908
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7357002390830379
siam score:  -0.63120383
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.311]
 [0.558]
 [0.927]
 [1.602]
 [1.663]] [[0.163]
 [0.41 ]
 [0.78 ]
 [1.454]
 [1.515]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.717]
 [2.717]
 [2.717]
 [2.717]
 [2.717]] [[1.427]
 [1.427]
 [1.427]
 [1.427]
 [1.427]]
using explorer policy with actor:  1
siam score:  -0.6276136
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.503]
 [1.503]
 [1.032]
 [1.05 ]
 [1.228]] [[1.375]
 [1.375]
 [0.83 ]
 [0.851]
 [1.057]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.044]
 [2.479]
 [2.685]
 [3.297]
 [3.607]] [[0.694]
 [0.927]
 [1.036]
 [1.363]
 [1.529]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20019
415 4785
siam score:  -0.6204362
first move QE:  0.1359609062975199
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
418 4790
line 256 mcts: sample exp_bonus 4.711070701282956
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6365903
siam score:  -0.63658506
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.452]
 [2.452]
 [2.452]
 [2.452]
 [3.053]] [[1.249]
 [1.249]
 [1.249]
 [1.249]
 [2.   ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.195]
 [2.76 ]
 [2.712]
 [2.687]
 [3.041]] [[0.354]
 [0.818]
 [0.779]
 [0.758]
 [1.049]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.54 ]
 [0.588]
 [1.299]
 [1.576]
 [1.669]] [[0.724]
 [0.09 ]
 [0.563]
 [0.748]
 [0.81 ]]
431 4813
maxi score, test score, baseline:  0.0001 0.0 0.0001
in main func line 156:  433
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6568949
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.746]
 [3.746]
 [3.746]
 [3.862]
 [3.767]] [[1.609]
 [1.609]
 [1.609]
 [1.691]
 [1.624]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.858]
 [2.323]
 [2.38 ]
 [2.323]
 [2.323]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.345]
 [3.345]
 [3.345]
 [2.906]
 [3.448]] [[1.537]
 [1.537]
 [1.537]
 [1.223]
 [1.61 ]]
siam score:  -0.6236849
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.239]
 [1.855]
 [4.239]
 [3.329]
 [3.76 ]] [[2.   ]
 [0.   ]
 [2.   ]
 [1.236]
 [1.598]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
444 4844
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.956]
 [1.167]
 [2.956]
 [2.303]
 [2.461]] [[1.63 ]
 [0.439]
 [1.63 ]
 [1.196]
 [1.301]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.4575469460646655
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.677]
 [0.957]
 [2.51 ]
 [1.878]
 [2.131]] [[0.699]
 [0.177]
 [1.302]
 [0.844]
 [1.027]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.485]
 [3.125]
 [3.377]
 [3.15 ]
 [3.233]] [[0.61 ]
 [1.169]
 [1.389]
 [1.191]
 [1.263]]
450 4848
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 1.8811984325234197
siam score:  -0.6122447
line 256 mcts: sample exp_bonus 1.907326946532575
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.119]
 [2.119]
 [2.119]
 [2.119]
 [2.909]] [[0.598]
 [0.598]
 [0.598]
 [0.598]
 [1.494]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.349]
 [2.318]
 [2.434]
 [3.19 ]
 [2.977]] [[1.991]
 [0.824]
 [0.955]
 [1.811]
 [1.57 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.5828857
UNIT TEST: sample policy line 217 mcts : [0.125 0.042 0.    0.625 0.208]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.193]
 [2.115]
 [1.944]
 [2.445]
 [2.331]] [[0.145]
 [0.939]
 [0.792]
 [1.224]
 [1.125]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.208]
 [2.32 ]
 [3.208]
 [2.648]
 [2.828]] [[1.658]
 [1.021]
 [1.658]
 [1.257]
 [1.386]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.227]
 [3.227]
 [3.227]
 [3.227]
 [2.837]] [[2.   ]
 [2.   ]
 [2.   ]
 [2.   ]
 [1.658]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 1.5971279076383977
siam score:  -0.519314
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.967]
 [1.967]
 [1.967]
 [1.967]
 [1.937]] [[1.118]
 [1.118]
 [1.118]
 [1.118]
 [1.085]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.5400583
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.0657826907035703
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.092]
 [1.092]
 [1.244]
 [1.137]
 [1.058]] [[0.487]
 [0.487]
 [0.588]
 [0.517]
 [0.464]]
495 4927
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.074]
 [1.074]
 [1.074]
 [1.074]
 [1.074]] [[2.149]
 [2.149]
 [2.149]
 [2.149]
 [2.149]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.5537749
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.566]
 [3.566]
 [3.566]
 [2.915]
 [2.966]] [[1.999]
 [1.999]
 [1.999]
 [1.349]
 [1.4  ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
499 4933
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.1671309177194638
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
first move QE:  0.19494150962258597
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.19617704683457077
line 256 mcts: sample exp_bonus 1.959425841594221
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.966]
 [1.966]
 [1.966]
 [1.966]
 [3.177]] [[0.521]
 [0.521]
 [0.521]
 [0.521]
 [1.521]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.38]
 [4.38]
 [4.38]
 [4.38]
 [4.38]] [[1.576]
 [1.576]
 [1.576]
 [1.576]
 [1.576]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.082]
 [3.082]
 [3.082]
 [3.082]
 [3.082]] [[4.112]
 [4.112]
 [4.112]
 [4.112]
 [4.112]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.2015436470045753
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
520 4961
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.162]
 [3.162]
 [3.162]
 [4.333]
 [3.938]] [[1.251]
 [1.251]
 [1.251]
 [1.741]
 [1.576]]
siam score:  -0.6219336
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6198126
siam score:  -0.6182061
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
Starting evaluation
line 256 mcts: sample exp_bonus -0.1023959798869517
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.5871806
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6392876
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.71 ]
 [1.655]
 [1.71 ]
 [1.71 ]
 [2.12 ]] [[1.074]
 [1.011]
 [1.074]
 [1.074]
 [1.551]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
544 4990
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.733]
 [2.733]
 [2.733]
 [2.043]
 [2.767]] [[1.633]
 [1.633]
 [1.633]
 [0.923]
 [1.668]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.976]
 [1.997]
 [2.415]
 [2.415]
 [2.6  ]] [[0.797]
 [0.823]
 [1.316]
 [1.316]
 [1.535]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 2.4909160014229204
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
556 4996
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.777]
 [3.651]
 [3.126]
 [3.632]
 [4.125]] [[1.713]
 [1.074]
 [0.777]
 [1.063]
 [1.343]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.463]
 [4.463]
 [4.463]
 [4.463]
 [5.251]] [[1.497]
 [1.497]
 [1.497]
 [1.497]
 [2.   ]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.391960008932553
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.767]
 [2.045]
 [2.489]
 [2.045]
 [2.375]] [[0.327]
 [0.512]
 [0.808]
 [0.512]
 [0.732]]
564 5011
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.66448927
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.911]
 [2.739]
 [3.911]
 [3.121]
 [3.426]] [[0.873]
 [0.481]
 [0.873]
 [0.609]
 [0.71 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
579 5033
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.383]
 [2.383]
 [2.383]
 [2.383]
 [2.276]] [[1.682]
 [1.682]
 [1.682]
 [1.682]
 [1.538]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 2.3809990537991066
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.23686226347860995
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.206]
 [0.628]
 [1.412]
 [0.694]
 [1.618]] [[1.225]
 [0.846]
 [1.36 ]
 [0.89 ]
 [1.496]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.865]
 [1.607]
 [2.223]
 [1.68 ]
 [2.422]] [[1.421]
 [1.301]
 [1.587]
 [1.335]
 [1.68 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.4697999692669814
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.66163754
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.356]
 [3.356]
 [3.356]
 [3.356]
 [6.658]] [[0.797]
 [0.797]
 [0.797]
 [0.797]
 [1.934]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
605 5069
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
606 5071
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.63666236
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.745]
 [1.745]
 [1.745]
 [1.745]
 [1.745]] [[3.491]
 [3.491]
 [3.491]
 [3.491]
 [3.491]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.718]
 [1.718]
 [1.718]
 [1.718]
 [1.718]] [[0.85]
 [0.85]
 [0.85]
 [0.85]
 [0.85]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
613 5088
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.527]
 [2.527]
 [2.527]
 [2.527]
 [2.522]] [[1.737]
 [1.737]
 [1.737]
 [1.737]
 [1.731]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.6673315
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.549]
 [1.939]
 [2.06 ]
 [2.175]
 [2.284]] [[0.423]
 [0.913]
 [1.065]
 [1.209]
 [1.346]]
624 5096
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.753]
 [0.196]
 [1.377]
 [1.992]
 [1.812]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.673749
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 1.7926848717463228
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [1.314]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.231]
 [1.645]
 [2.25 ]
 [2.009]
 [2.094]] [[0.364]
 [0.64 ]
 [1.043]
 [0.883]
 [0.939]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.67913955
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5000862326313527
635 5113
635 5115
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6897362
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.336]
 [3.201]
 [3.205]
 [3.052]
 [3.104]] [[0.209]
 [0.767]
 [0.768]
 [0.722]
 [0.738]]
643 5125
644 5125
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.602]
 [5.602]
 [5.602]
 [5.602]
 [5.952]] [[1.87]
 [1.87]
 [1.87]
 [1.87]
 [2.  ]]
first move QE:  0.263264948248032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
649 5132
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.828]
 [1.828]
 [1.828]
 [1.885]
 [2.362]] [[0.736]
 [0.736]
 [0.736]
 [0.775]
 [1.093]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.6966714212676552
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.699858
first move QE:  0.26905256608237993
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 2.269696455207759
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.403]
 [2.403]
 [2.403]
 [2.258]
 [2.314]] [[1.705]
 [1.705]
 [1.705]
 [1.56 ]
 [1.616]]
using explorer policy with actor:  1
siam score:  -0.69312257
siam score:  -0.6918049
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.056]
 [2.056]
 [2.056]
 [2.056]
 [1.983]] [[1.767]
 [1.767]
 [1.767]
 [1.767]
 [1.682]]
668 5173
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.68776107
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.968]
 [1.679]
 [1.679]
 [1.829]
 [1.942]] [[0.261]
 [1.104]
 [1.104]
 [1.282]
 [1.416]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.451]
 [2.451]
 [2.451]
 [2.451]
 [2.242]] [[1.944]
 [1.944]
 [1.944]
 [1.944]
 [1.693]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
676 5182
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
676 5185
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.964]
 [2.964]
 [2.964]
 [2.964]
 [3.695]] [[1.417]
 [1.417]
 [1.417]
 [1.417]
 [1.971]]
siam score:  -0.6259242
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.822]
 [2.822]
 [2.363]
 [2.774]
 [2.9  ]] [[1.358]
 [1.358]
 [1.018]
 [1.323]
 [1.415]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
688 5200
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6037975
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.448]
 [1.448]
 [1.448]
 [1.861]
 [1.448]] [[0.756]
 [0.756]
 [0.756]
 [1.437]
 [0.756]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.629]
 [1.629]
 [1.629]
 [1.629]
 [3.194]] [[0.543]
 [0.543]
 [0.543]
 [0.543]
 [1.066]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.66495204
Starting evaluation
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.824]
 [1.824]
 [1.824]
 [1.824]
 [1.761]] [[0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.596]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6565937
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
718 5233
first move QE:  0.29306172646826906
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.686399983763304
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.7392976
using explorer policy with actor:  1
siam score:  -0.7282848
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.486]
 [3.486]
 [3.486]
 [3.486]
 [4.327]] [[1.275]
 [1.275]
 [1.275]
 [1.275]
 [1.97 ]]
first move QE:  0.2962352862918126
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.563]
 [1.563]
 [1.563]
 [1.563]
 [1.65 ]] [[1.745]
 [1.745]
 [1.745]
 [1.745]
 [1.846]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.193]
 [1.038]
 [0.574]
 [1.088]
 [1.243]] [[0.075]
 [0.358]
 [0.202]
 [0.374]
 [0.426]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.8052860628374616
in main func line 156:  737
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
in main func line 156:  739
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.863]
 [0.863]
 [1.219]
 [0.883]
 [1.073]] [[0.622]
 [0.622]
 [0.978]
 [0.642]
 [0.832]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 2.066475627645918
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.89 ]
 [2.89 ]
 [2.89 ]
 [2.89 ]
 [3.317]] [[1.305]
 [1.305]
 [1.305]
 [1.305]
 [1.596]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.86 ]
 [2.86 ]
 [2.86 ]
 [2.671]
 [3.432]] [[0.692]
 [0.692]
 [0.692]
 [0.629]
 [0.883]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.117]
 [3.117]
 [3.117]
 [3.117]
 [3.082]] [[2.044]
 [2.044]
 [2.044]
 [2.044]
 [2.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.188]
 [1.188]
 [1.188]
 [1.188]
 [1.409]] [[0.902]
 [0.902]
 [0.902]
 [0.902]
 [1.048]]
753 5306
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.689]
 [1.375]
 [2.323]
 [1.483]
 [2.622]] [[0.7  ]
 [0.387]
 [1.334]
 [0.494]
 [1.633]]
UNIT TEST: sample policy line 217 mcts : [0.042 0.    0.    0.25  0.708]
start point for exploration sampling:  20019
siam score:  -0.7443782
start point for exploration sampling:  20019
siam score:  -0.7444886
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.839]
 [0.839]
 [0.839]
 [0.839]
 [1.594]] [[0.835]
 [0.835]
 [0.835]
 [0.835]
 [1.687]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.134]
 [1.5  ]
 [2.112]
 [1.367]
 [2.438]] [[0.248]
 [0.715]
 [1.495]
 [0.546]
 [1.91 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.02 ]
 [0.469]
 [0.702]
 [0.702]
 [1.034]] [[0.   ]
 [0.299]
 [0.454]
 [0.454]
 [0.675]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 3.0117815670023727
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.363]
 [2.363]
 [2.363]
 [2.363]
 [1.78 ]] [[1.901]
 [1.901]
 [1.901]
 [1.901]
 [1.307]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.792]
 [1.792]
 [1.792]
 [1.792]
 [1.792]] [[1.683]
 [1.683]
 [1.683]
 [1.683]
 [1.683]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.64866936
779 5344
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.205]
 [0.208]
 [1.05 ]
 [1.19 ]
 [1.487]] [[0.108]
 [0.111]
 [0.967]
 [1.109]
 [1.411]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.562]
 [1.562]
 [1.562]
 [1.562]
 [1.799]] [[1.255]
 [1.255]
 [1.255]
 [1.255]
 [1.492]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
in main func line 156:  785
785 5355
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.015]
 [0.762]
 [0.427]
 [0.678]
 [0.723]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
line 256 mcts: sample exp_bonus 3.0802225470802287
using explorer policy with actor:  1
789 5363
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.396]
 [1.199]
 [2.931]
 [1.451]
 [2.006]] [[0.469]
 [0.316]
 [1.665]
 [0.512]
 [0.944]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
first move QE:  0.3226988575179846
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.394]
 [3.394]
 [3.394]
 [3.394]
 [4.732]] [[1.244]
 [1.244]
 [1.244]
 [1.244]
 [1.759]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6362697
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.425]
 [0.487]
 [0.456]
 [0.464]
 [3.011]] [[0.059]
 [0.084]
 [0.072]
 [0.075]
 [1.074]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.448]
 [4.448]
 [4.448]
 [1.045]
 [1.559]] [[2.469]
 [2.469]
 [2.469]
 [0.203]
 [0.545]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.438]
 [1.438]
 [1.438]
 [1.438]
 [1.438]] [[1.752]
 [1.752]
 [1.752]
 [1.752]
 [1.752]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
807 5400
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.914]
 [0.914]
 [1.029]
 [0.914]
 [1.553]] [[0.425]
 [0.425]
 [0.54 ]
 [0.425]
 [1.064]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.564]
 [1.358]
 [0.465]
 [1.204]
 [1.197]] [[0.166]
 [1.496]
 [0.   ]
 [1.238]
 [1.225]]
809 5408
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.839]
 [1.866]
 [1.625]
 [1.461]
 [2.004]] [[0.369]
 [1.229]
 [1.026]
 [0.889]
 [1.344]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.892]
 [0.892]
 [0.892]
 [0.808]
 [0.653]] [[0.78 ]
 [0.78 ]
 [0.78 ]
 [0.641]
 [0.382]]
siam score:  -0.7316739
start point for exploration sampling:  20019
line 256 mcts: sample exp_bonus 1.7965233838880224
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.73649156
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.68578476
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.526]
 [2.526]
 [2.526]
 [2.526]
 [2.778]] [[1.545]
 [1.545]
 [1.545]
 [1.545]
 [1.796]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.65774125
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.59 ]] [[-0.394]
 [-0.394]
 [-0.394]
 [-0.394]
 [ 0.182]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.352]
 [2.352]
 [2.352]
 [2.352]
 [3.48 ]] [[1.025]
 [1.025]
 [1.025]
 [1.025]
 [1.928]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.332]
 [2.332]
 [2.332]
 [2.332]
 [2.232]] [[1.99 ]
 [1.99 ]
 [1.99 ]
 [1.99 ]
 [1.873]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.334]
 [2.334]
 [2.334]
 [2.334]
 [2.14 ]] [[1.992]
 [1.992]
 [1.992]
 [1.992]
 [1.764]]
first move QE:  0.3336294592649999
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7442679
siam score:  -0.74237335
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.11 ]
 [2.11 ]
 [2.11 ]
 [2.11 ]
 [2.137]] [[1.701]
 [1.701]
 [1.701]
 [1.701]
 [1.734]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.463]
 [1.463]
 [1.463]
 [1.463]
 [1.463]] [[1.418]
 [1.418]
 [1.418]
 [1.418]
 [1.418]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.149]
 [1.149]
 [1.149]
 [1.149]
 [1.149]] [[0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]]
854 5493
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 0.0
0.0 1.2888318476420977e-19
0.0 1.159948662877888e-19
0.0 1.8043645866989369e-19
0.0 1.159948662877888e-19
0.0 6.444159238210488e-20
0.0 3.222079619105244e-20
0.0 1.5465982171705174e-19
0.0 1.2888318476420977e-19
0.0 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.396]
 [1.396]
 [1.396]
 [1.396]
 [1.868]] [[1.141]
 [1.141]
 [1.141]
 [1.141]
 [1.662]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.75 ]
 [1.429]
 [1.031]
 [1.286]
 [1.665]] [[0.336]
 [1.215]
 [0.7  ]
 [1.029]
 [1.52 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.3341793317808913
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
862 5513
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.432]
 [1.432]
 [1.432]
 [1.432]
 [1.804]] [[1.47 ]
 [1.47 ]
 [1.47 ]
 [1.47 ]
 [1.987]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
866 5534
in main func line 156:  867
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.157]
 [2.157]
 [2.157]
 [2.157]
 [1.874]] [[1.454]
 [1.454]
 [1.454]
 [1.454]
 [1.172]]
870 5541
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.257]
 [3.241]
 [2.842]
 [2.609]
 [3.083]] [[0.528]
 [1.264]
 [0.966]
 [0.792]
 [1.146]]
873 5542
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.7641458
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7535654
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.042 0.083 0.083 0.042 0.75 ]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.3357410600721345
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.446]
 [1.023]
 [1.023]
 [1.023]
 [1.163]] [[0.312]
 [1.082]
 [1.082]
 [1.082]
 [1.268]]
in main func line 156:  893
894 5586
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.192]
 [0.192]
 [0.059]
 [0.192]
 [0.555]] [[0.055]
 [0.055]
 [0.01 ]
 [0.055]
 [0.176]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.457]
 [1.457]
 [1.457]
 [1.457]
 [2.73 ]] [[0.743]
 [0.743]
 [0.743]
 [0.743]
 [1.967]]
in main func line 156:  898
in main func line 156:  901
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7100904
siam score:  -0.70871216
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.69338197
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.021]
 [1.021]
 [1.021]
 [1.021]
 [1.159]] [[0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.875]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.018]
 [0.376]
 [0.362]
 [0.273]
 [0.464]] [[0.   ]
 [0.608]
 [0.586]
 [0.433]
 [0.758]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.207]
 [2.207]
 [2.207]
 [2.207]
 [2.207]] [[1.951]
 [1.951]
 [1.951]
 [1.951]
 [1.951]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]] [[0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.67]] [[0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.375]]
using explorer policy with actor:  1
start point for exploration sampling:  20019
first move QE:  0.3387471412456877
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
using explorer policy with actor:  1
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.8018233303731228
932 5672
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.71328473
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
935 5674
line 256 mcts: sample exp_bonus 0.8198420299016094
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.475]
 [1.475]
 [1.475]
 [1.475]
 [1.398]] [[0.961]
 [0.961]
 [0.961]
 [0.961]
 [0.841]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.6537020070783655
start point for exploration sampling:  20019
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.8270471219994031
siam score:  -0.768479
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.283]
 [1.283]
 [1.283]
 [1.283]
 [1.283]] [[0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]]
siam score:  -0.7765249
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.167]
 [ 0.364]
 [ 0.242]
 [ 0.286]
 [ 0.705]] [[0.   ]
 [0.754]
 [0.582]
 [0.644]
 [1.238]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.3399155641731206
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.088]
 [0.387]
 [0.353]
 [0.188]
 [0.212]] [[0.395]
 [0.893]
 [0.836]
 [0.562]
 [0.602]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7697972
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.734]] [[1.194]
 [1.194]
 [1.194]
 [1.194]
 [1.424]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.97 ]] [[0.319]
 [0.319]
 [0.319]
 [0.319]
 [1.005]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.24]
 [0.24]
 [0.24]
 [0.24]
 [0.24]] [[0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.426]
 [0.767]
 [0.63 ]
 [0.661]
 [0.738]] [[0.546]
 [1.221]
 [0.951]
 [1.012]
 [1.164]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.118]
 [2.126]
 [2.118]
 [2.118]
 [1.743]] [[1.02 ]
 [1.025]
 [1.02 ]
 [1.02 ]
 [0.779]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
first move QE:  0.33955044402281054
965 5791
967 5796
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.869]
 [2.869]
 [2.869]
 [2.869]
 [3.924]] [[0.968]
 [0.968]
 [0.968]
 [0.968]
 [1.611]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.015]
 [0.356]
 [0.365]
 [0.325]
 [0.392]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
971 5800
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.3404710893905985
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.765]
 [1.588]
 [1.661]
 [1.765]
 [1.584]] [[1.078]
 [0.902]
 [0.975]
 [1.078]
 [0.897]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.227]
 [1.969]
 [2.632]
 [3.227]
 [3.55 ]] [[1.658]
 [0.415]
 [1.071]
 [1.658]
 [1.977]]
start point for exploration sampling:  20019
974 5802
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.691]
 [1.691]
 [1.164]
 [1.691]
 [2.086]] [[1.244]
 [1.244]
 [0.668]
 [1.244]
 [1.676]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.127]
 [1.322]
 [0.935]
 [1.415]
 [1.821]] [[1.708]
 [0.787]
 [0.345]
 [0.894]
 [1.358]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7739124
using explorer policy with actor:  1
983 5818
siam score:  -0.7754638
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.6804786724414991
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.765]
 [1.966]
 [2.047]
 [1.832]
 [2.49 ]] [[0.111]
 [0.897]
 [0.95 ]
 [0.809]
 [1.24 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.083 0.417 0.042 0.125 0.333]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.062]
 [1.959]
 [1.678]
 [1.015]
 [1.933]] [[0.096]
 [0.693]
 [0.506]
 [0.064]
 [0.676]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.861]
 [0.861]
 [0.861]
 [0.861]
 [1.71 ]] [[0.588]
 [0.588]
 [0.588]
 [0.588]
 [1.239]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.72109634
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.276]
 [2.276]
 [2.276]
 [2.276]
 [3.154]] [[0.983]
 [0.983]
 [0.983]
 [0.983]
 [1.734]]
999 5841
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
first move QE:  0.3440305659706399
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.755774
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.054]
 [1.054]
 [1.054]
 [1.054]
 [1.122]] [[0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.529]]
siam score:  -0.76329803
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.192]
 [1.192]
 [1.192]
 [1.192]
 [2.519]] [[0.24 ]
 [0.24 ]
 [0.24 ]
 [0.24 ]
 [0.618]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.77207017
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1022 5893
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.478]
 [1.478]
 [1.478]
 [1.478]
 [4.069]] [[0.442]
 [0.442]
 [0.442]
 [0.442]
 [1.896]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.395]
 [3.395]
 [3.395]
 [3.395]
 [7.097]] [[0.735]
 [0.735]
 [0.735]
 [0.735]
 [1.646]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.885]
 [1.885]
 [1.885]
 [1.885]
 [1.649]] [[1.544]
 [1.544]
 [1.544]
 [1.544]
 [1.309]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.499]
 [3.499]
 [3.499]
 [3.499]
 [3.696]] [[1.476]
 [1.476]
 [1.476]
 [1.476]
 [1.578]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.182]
 [0.182]
 [0.182]
 [0.182]
 [0.182]] [[0.303]
 [0.303]
 [0.303]
 [0.303]
 [0.303]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.165]
 [3.17 ]
 [3.449]
 [4.165]
 [3.045]] [[1.605]
 [1.137]
 [1.268]
 [1.605]
 [1.078]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.422]
 [1.422]
 [1.422]
 [1.422]
 [1.705]] [[1.397]
 [1.397]
 [1.397]
 [1.397]
 [1.859]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.431]
 [0.889]
 [0.431]
 [0.431]
 [0.806]] [[-0.005]
 [ 0.453]
 [-0.005]
 [-0.005]
 [ 0.37 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7815581
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7789572
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
siam score:  -0.78143156
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.236]
 [ 0.497]
 [ 0.172]
 [ 0.202]
 [ 0.166]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
siam score:  -0.805618
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.27910657895175967
1045 5980
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]] [[0.15]
 [0.15]
 [0.15]
 [0.15]
 [0.15]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.611]] [[0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.786]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.924]
 [1.924]
 [0.937]
 [0.83 ]
 [1.534]] [[1.991]
 [1.991]
 [0.347]
 [0.169]
 [1.342]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5628683683563231
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.0260395828408257
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.921]
 [2.921]
 [2.255]
 [2.921]
 [2.68 ]] [[1.804]
 [1.804]
 [1.021]
 [1.804]
 [1.521]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.751]
 [0.751]
 [0.751]
 [0.751]
 [1.215]] [[0.704]
 [0.704]
 [0.704]
 [0.704]
 [1.322]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.142]
 [2.142]
 [2.142]
 [2.142]
 [2.128]] [[1.943]
 [1.943]
 [1.943]
 [1.943]
 [1.93 ]]
siam score:  -0.7711821
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.792]
 [2.792]
 [0.286]
 [0.322]
 [0.591]] [[2.639]
 [2.639]
 [0.133]
 [0.169]
 [0.438]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.894]
 [0.894]
 [0.894]
 [0.894]
 [0.894]] [[0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]]
start point for exploration sampling:  20019
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.892]
 [0.892]
 [0.892]
 [0.892]
 [1.043]] [[0.898]
 [0.898]
 [0.898]
 [0.898]
 [1.201]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1066 6051
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.256]
 [0.138]
 [0.289]
 [0.249]] [[0.   ]
 [0.338]
 [0.18 ]
 [0.382]
 [0.328]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.38864323913809684
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.    0.125 0.042 0.375 0.458]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1078 6081
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.693]] [[0.151]
 [0.151]
 [0.151]
 [0.151]
 [0.707]]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.814]
 [0.814]
 [0.814]
 [0.814]
 [1.025]] [[1.309]
 [1.309]
 [1.309]
 [1.309]
 [1.641]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.685]
 [2.384]
 [2.231]
 [1.96 ]
 [4.099]] [[0.   ]
 [0.995]
 [0.906]
 [0.747]
 [2.   ]]
using explorer policy with actor:  1
siam score:  -0.8196132
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 2.6976502979918084
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]] [[0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.231]]
UNIT TEST: sample policy line 217 mcts : [0.833 0.042 0.042 0.042 0.042]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.233]
 [1.233]
 [1.233]
 [1.233]
 [1.57 ]] [[0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.973]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1090 6108
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.78 ]
 [4.78 ]
 [4.78 ]
 [4.78 ]
 [5.618]] [[1.178]
 [1.178]
 [1.178]
 [1.178]
 [1.434]]
line 256 mcts: sample exp_bonus 0.7912981576458435
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.338]
 [2.338]
 [2.338]
 [2.338]
 [2.203]] [[1.102]
 [1.102]
 [1.102]
 [1.102]
 [1.007]]
first move QE:  0.34792993676460626
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
1095 6113
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.086]
 [ 0.226]
 [ 0.226]
 [ 0.226]
 [ 0.182]] [[0.   ]
 [0.519]
 [0.519]
 [0.519]
 [0.446]]
1095 6147
using explorer policy with actor:  1
1095 6150
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.8300147
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]] [[0.78]
 [0.78]
 [0.78]
 [0.78]
 [0.78]]
first move QE:  0.345976888049026
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 3.093196434341035e-19
0.0 1.6649718022640682e-12
0.0 2.4487805105199856e-19
0.0 3.6087291733978737e-19
0.0 0.0
0.0 3.093196434341035e-19
0.0 6.444159238210488e-20
0.0 0.0
0.0 0.0
0.0 3.3083205810220398e-12
using explorer policy with actor:  1
1102 6205
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.169]
 [1.169]
 [1.169]
 [1.169]
 [1.083]] [[1.686]
 [1.686]
 [1.686]
 [1.686]
 [1.514]]
siam score:  -0.8081569
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.228]
 [1.228]
 [1.228]
 [1.228]
 [0.987]] [[0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.367]]
siam score:  -0.8083793
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1104 6257
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.082]
 [1.096]
 [1.053]
 [0.93 ]
 [1.003]] [[1.464]
 [1.485]
 [1.419]
 [1.23 ]
 [1.343]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.199]
 [0.902]
 [0.399]
 [0.414]
 [0.408]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]] [[0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.81526357
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.34 ]
 [0.421]
 [0.444]
 [0.477]
 [0.506]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.885]
 [0.885]
 [0.885]
 [0.885]
 [0.885]] [[0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.269]
 [0.269]
 [0.269]
 [0.269]
 [0.269]] [[0.43]
 [0.43]
 [0.43]
 [0.43]
 [0.43]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.507]
 [0.507]
 [0.507]
 [0.507]
 [1.328]] [[0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [1.404]]
siam score:  -0.8011081
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.1964339176984513
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7043984955388829
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.80191016
maxi score, test score, baseline:  0.0001 0.0 0.0001
1107 6388
1108 6391
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.79928535
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.35100506100620166
1108 6429
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9103103343242722
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.188]
 [0.188]
 [0.188]
 [0.188]
 [0.188]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.095]
 [ 0.095]
 [ 0.043]
 [-0.117]
 [ 0.095]] [[0.08 ]
 [0.08 ]
 [0.062]
 [0.009]
 [0.08 ]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.75209725
1109 6503
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7483848
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1110 6542
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.373]] [[0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.174]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.788]
 [1.125]
 [0.695]
 [0.788]
 [1.025]] [[1.208]
 [1.876]
 [1.022]
 [1.208]
 [1.678]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.393]] [[0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.388]]
1113 6597
1113 6602
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1114 6616
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
1117 6646
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.3582088454844963
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20019
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.008]
 [0.977]
 [1.008]
 [0.919]
 [0.974]] [[0.962]
 [0.906]
 [0.962]
 [0.8  ]
 [0.9  ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.027]
 [1.027]
 [1.027]
 [1.027]
 [1.027]] [[0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.24577795086408055
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7788422
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.127037883121811
using explorer policy with actor:  1
1136 6776
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.752]
 [3.063]
 [3.006]
 [3.035]
 [4.124]] [[0.095]
 [0.789]
 [0.759]
 [0.774]
 [1.351]]
actor:  1 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.962]
 [2.962]
 [2.962]
 [2.962]
 [2.962]] [[1.695]
 [1.695]
 [1.695]
 [1.695]
 [1.695]]
1140 6790
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.038467456235825086, 0.038467456235825086, 0.8076627188208747, 0.038467456235825086, 0.038467456235825086, 0.038467456235825086]
siam score:  -0.7577223
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.038467456235825086, 0.038467456235825086, 0.8076627188208747, 0.038467456235825086, 0.038467456235825086, 0.038467456235825086]
1148 6814
1149 6814
from probs:  [0.038467456235825086, 0.038467456235825086, 0.8076627188208747, 0.038467456235825086, 0.038467456235825086, 0.038467456235825086]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.922]
 [0.922]
 [0.922]
 [0.922]
 [1.581]] [[0.721]
 [0.721]
 [0.721]
 [0.721]
 [1.288]]
using another actor
from probs:  [0.038467456235825086, 0.038467456235825086, 0.8076627188208747, 0.038467456235825086, 0.038467456235825086, 0.038467456235825086]
using another actor
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.889]] [[0.539]
 [0.539]
 [0.539]
 [0.539]
 [1.37 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.038467456235825086, 0.038467456235825086, 0.8076627188208747, 0.038467456235825086, 0.038467456235825086, 0.038467456235825086]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.667]
 [2.667]
 [2.667]
 [2.667]
 [2.643]] [[0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.772]]
siam score:  -0.76109856
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
from probs:  [0.038467456235825086, 0.038467456235825086, 0.8076627188208747, 0.038467456235825086, 0.038467456235825086, 0.038467456235825086]
start point for exploration sampling:  20019
1162 6840
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.76614076
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.05 ]
 [3.05 ]
 [3.05 ]
 [3.05 ]
 [7.028]] [[0.519]
 [0.519]
 [0.519]
 [0.519]
 [1.557]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
from probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
from probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.794]
 [1.794]
 [1.794]
 [1.794]
 [2.061]] [[1.256]
 [1.256]
 [1.256]
 [1.256]
 [1.614]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.392]
 [0.925]
 [0.836]
 [0.727]
 [1.005]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
rdn probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
1170 6864
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
using explorer policy with actor:  1
from probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
using explorer policy with actor:  0
1172 6872
using another actor
from probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
from probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
from probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.767]
 [1.767]
 [1.767]
 [1.767]
 [1.817]] [[0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.98 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.17 ]
 [0.17 ]
 [0.501]
 [0.17 ]
 [0.17 ]] [[0.06]
 [0.06]
 [0.28]
 [0.06]
 [0.06]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.324]
 [0.446]
 [0.324]
 [0.324]
 [0.22 ]] [[0.057]
 [0.098]
 [0.057]
 [0.057]
 [0.022]]
using explorer policy with actor:  1
from probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
using explorer policy with actor:  1
from probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7661379
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.003]] [[5.457]
 [5.457]
 [5.457]
 [5.457]
 [7.   ]] [[1.139]
 [1.139]
 [1.139]
 [1.139]
 [1.813]]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[2.493]
 [1.969]
 [2.068]
 [2.493]
 [2.126]] [[2.002]
 [1.474]
 [1.574]
 [2.002]
 [1.633]]
in main func line 156:  1182
using another actor
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.985]
 [0.985]
 [0.985]
 [0.985]
 [1.328]] [[0.884]
 [0.884]
 [0.884]
 [0.884]
 [1.384]]
siam score:  -0.7699487
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
from probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.796]
 [1.796]
 [2.144]
 [0.742]
 [1.879]] [[1.014]
 [1.014]
 [1.348]
 [0.001]
 [1.093]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.603]
 [1.603]
 [1.603]
 [1.603]
 [2.107]] [[1.37 ]
 [1.37 ]
 [1.37 ]
 [1.37 ]
 [1.854]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20019
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[1.771]
 [0.606]
 [1.39 ]
 [1.437]
 [1.395]] [[1.166]
 [0.   ]
 [0.786]
 [0.832]
 [0.79 ]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.277]
 [1.277]
 [1.277]
 [1.277]
 [1.184]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.7619449
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using another actor
1203 6946
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.   ]
 [0.003]
 [0.003]
 [0.004]] [[ 0.15 ]
 [ 0.33 ]
 [-0.134]
 [ 0.211]
 [-0.266]] [[0.594]
 [0.707]
 [0.404]
 [0.634]
 [0.317]]
using another actor
1203 6971
from probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
from probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.001]
 [0.002]
 [0.002]
 [0.002]] [[ 0.   ]
 [ 0.554]
 [ 0.   ]
 [ 0.   ]
 [-0.598]] [[0.002]
 [0.001]
 [0.002]
 [0.002]
 [0.002]]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.001]
 [0.001]
 [0.   ]
 [0.002]] [[-0.165]
 [ 0.399]
 [ 0.32 ]
 [ 0.381]
 [ 0.092]] [[0.002]
 [0.001]
 [0.001]
 [0.   ]
 [0.002]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.078]] [[0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.096]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
from probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
from probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
using another actor
using another actor
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.528]
 [1.528]
 [1.528]
 [1.528]
 [1.846]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[1.217]
 [1.272]
 [1.217]
 [1.217]
 [1.371]] [[0.961]
 [1.045]
 [0.961]
 [0.961]
 [1.195]]
first move QE:  0.356610633644706
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.3564609285443809
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
siam score:  -0.8404903
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.001]
 [0.002]
 [0.002]
 [0.002]] [[1.299]
 [2.126]
 [1.947]
 [1.788]
 [2.038]] [[0.496]
 [1.322]
 [1.145]
 [0.986]
 [1.234]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[1.126]
 [0.508]
 [1.263]
 [0.843]
 [1.017]] [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
1219 7081
using another actor
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[2.476]
 [2.476]
 [2.476]
 [2.476]
 [2.476]] [[1.736]
 [1.736]
 [1.736]
 [1.736]
 [1.736]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.35570182145442264
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
using another actor
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.002]
 [0.001]
 [0.002]] [[1.097]
 [0.527]
 [1.229]
 [1.039]
 [1.106]] [[0.808]
 [0.046]
 [0.984]
 [0.73 ]
 [0.821]]
using explorer policy with actor:  1
1225 7101
line 256 mcts: sample exp_bonus 5.092694202726936
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
1228 7107
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.85 ]
 [1.237]
 [0.85 ]
 [0.85 ]
 [1.074]] [[0.69 ]
 [1.076]
 [0.69 ]
 [0.69 ]
 [0.914]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[0.165]
 [0.331]
 [0.486]
 [0.334]
 [0.712]] [[0.151]
 [0.437]
 [0.707]
 [0.445]
 [1.098]]
1228 7125
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.002]
 [0.003]
 [0.002]
 [0.003]] [[0.959]
 [0.922]
 [0.959]
 [0.711]
 [0.947]] [[0.844]
 [0.806]
 [0.844]
 [0.595]
 [0.832]]
siam score:  -0.8051224
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
from probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.81662214
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
using explorer policy with actor:  1
in main func line 156:  1236
siam score:  -0.8190714
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[2.424]
 [2.424]
 [2.424]
 [2.424]
 [2.424]] [[2.001]
 [2.001]
 [2.001]
 [2.001]
 [2.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
from probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
Sims:  25 1 epoch:  94990 pick best:  False frame count:  94990
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670397, 0.02174291162164801, 0.45651417675670397, 0.02174291162164801, 0.02174291162164801, 0.02174291162164801]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.35713794908244695
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2441849646986885, 0.2441849646986885, 0.47673985879475395, 0.01163007060262309, 0.01163007060262309, 0.01163007060262309]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2441849646986885, 0.2441849646986885, 0.47673985879475395, 0.01163007060262309, 0.01163007060262309, 0.01163007060262309]
siam score:  -0.77726823
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2441849646986885, 0.2441849646986885, 0.47673985879475395, 0.01163007060262309, 0.01163007060262309, 0.01163007060262309]
from probs:  [0.2441849646986885, 0.2441849646986885, 0.47673985879475395, 0.01163007060262309, 0.01163007060262309, 0.01163007060262309]
from probs:  [0.2441849646986885, 0.2441849646986885, 0.47673985879475395, 0.01163007060262309, 0.01163007060262309, 0.01163007060262309]
from probs:  [0.2441849646986885, 0.2441849646986885, 0.47673985879475395, 0.01163007060262309, 0.01163007060262309, 0.01163007060262309]
from probs:  [0.2441849646986885, 0.2441849646986885, 0.47673985879475395, 0.01163007060262309, 0.01163007060262309, 0.01163007060262309]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.002]
 [0.002]] [[0.561]
 [0.841]
 [0.627]
 [0.619]
 [0.71 ]] [[0.287]
 [0.567]
 [0.352]
 [0.346]
 [0.436]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 6.342133064698237
siam score:  -0.77062804
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
from probs:  [0.2441849646986885, 0.2441849646986885, 0.47673985879475395, 0.01163007060262309, 0.01163007060262309, 0.01163007060262309]
using explorer policy with actor:  1
in main func line 156:  1266
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2441849646986885, 0.2441849646986885, 0.47673985879475395, 0.01163007060262309, 0.01163007060262309, 0.01163007060262309]
using another actor
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[3.148]
 [3.148]
 [3.148]
 [3.148]
 [3.475]] [[1.31 ]
 [1.31 ]
 [1.31 ]
 [1.31 ]
 [1.567]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2441849646986885, 0.2441849646986885, 0.47673985879475395, 0.01163007060262309, 0.01163007060262309, 0.01163007060262309]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2441849646986885, 0.2441849646986885, 0.47673985879475395, 0.01163007060262309, 0.01163007060262309, 0.01163007060262309]
1273 7219
in main func line 156:  1276
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.002]
 [0.003]
 [0.003]
 [0.002]] [[1.762]
 [1.52 ]
 [1.762]
 [1.762]
 [1.687]] [[1.46 ]
 [1.136]
 [1.46 ]
 [1.46 ]
 [1.359]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2441849646986885, 0.2441849646986885, 0.47673985879475395, 0.01163007060262309, 0.01163007060262309, 0.01163007060262309]
using another actor
1279 7227
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2441849646986885, 0.2441849646986885, 0.47673985879475395, 0.01163007060262309, 0.01163007060262309, 0.01163007060262309]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.19811285149921826, 0.19811285149921826, 0.38678996049452774, 0.009435742503908828, 0.19811285149921826, 0.009435742503908828]
line 256 mcts: sample exp_bonus 3.1512161047285154
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
1282 7234
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[2.834]
 [2.834]
 [2.834]
 [2.834]
 [3.407]] [[1.441]
 [1.441]
 [1.441]
 [1.441]
 [2.001]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  100 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]] [[3.681]
 [3.681]
 [3.681]
 [3.681]
 [3.681]] [[1.888]
 [1.888]
 [1.888]
 [1.888]
 [1.888]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1438358041174326, 0.28082097941283696, 0.4178061547082413, 0.006850628822028332, 0.1438358041174326, 0.006850628822028332]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[0.541]
 [0.322]
 [0.428]
 [0.445]
 [0.498]] [[0.123]
 [0.049]
 [0.085]
 [0.091]
 [0.108]]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.8  ]
 [0.362]
 [0.497]
 [0.402]
 [0.334]] [[0.184]
 [0.038]
 [0.083]
 [0.051]
 [0.029]]
Printing some Q and Qe and total Qs values:  [[0.031]
 [0.031]
 [0.031]
 [0.031]
 [0.02 ]] [[2.265]
 [2.265]
 [2.265]
 [2.265]
 [2.533]] [[1.449]
 [1.449]
 [1.449]
 [1.449]
 [1.637]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.11290357271715797, 0.22042976061617534, 0.32795594851519266, 0.22042976061617534, 0.11290357271715797, 0.005377384818140646]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
1295 7299
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.03 ]] [[2.78]
 [2.78]
 [2.78]
 [2.78]
 [2.88]] [[1.506]
 [1.506]
 [1.506]
 [1.506]
 [1.611]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.11290357271715797, 0.22042976061617534, 0.32795594851519266, 0.22042976061617534, 0.11290357271715797, 0.005377384818140646]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.3543566361891712
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.09292074561230662, 0.18141585087753867, 0.2699109561427707, 0.18141585087753867, 0.09292074561230662, 0.18141585087753867]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.09292074561230662, 0.18141585087753867, 0.2699109561427707, 0.18141585087753867, 0.09292074561230662, 0.18141585087753867]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.587]
 [0.587]
 [0.693]
 [0.587]
 [0.677]] [[0.202]
 [0.202]
 [0.308]
 [0.202]
 [0.292]]
Printing some Q and Qe and total Qs values:  [[0.14 ]
 [0.14 ]
 [0.14 ]
 [0.14 ]
 [0.175]] [[1.547]
 [1.547]
 [1.547]
 [1.547]
 [1.822]] [[1.16 ]
 [1.16 ]
 [1.16 ]
 [1.16 ]
 [1.505]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.76839036
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.09292074561230662, 0.18141585087753867, 0.2699109561427707, 0.18141585087753867, 0.09292074561230662, 0.18141585087753867]
using another actor
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[ 0.351]
 [ 0.428]
 [ 0.351]
 [ 0.245]
 [-0.135]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
start point for exploration sampling:  20019
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
1303 7354
from probs:  [0.0853662503054295, 0.16666666666666666, 0.3292674993891409, 0.16666666666666666, 0.0853662503054295, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5891861043812394
from probs:  [0.0853662503054295, 0.16666666666666666, 0.3292674993891409, 0.16666666666666666, 0.0853662503054295, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.002]
 [0.001]
 [0.001]] [[0.094]
 [0.094]
 [0.552]
 [0.094]
 [0.144]] [[0.001]
 [0.001]
 [0.002]
 [0.001]
 [0.001]]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.78041613
using another actor
from probs:  [0.0853662503054295, 0.16666666666666666, 0.3292674993891409, 0.16666666666666666, 0.0853662503054295, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.001]
 [0.002]
 [0.001]] [[0.275]
 [0.659]
 [0.653]
 [0.275]
 [0.545]] [[0.002]
 [0.002]
 [0.001]
 [0.002]
 [0.001]]
from probs:  [0.0853662503054295, 0.16666666666666666, 0.3292674993891409, 0.16666666666666666, 0.0853662503054295, 0.16666666666666666]
from probs:  [0.0853662503054295, 0.16666666666666666, 0.3292674993891409, 0.16666666666666666, 0.0853662503054295, 0.16666666666666666]
1312 7409
1312 7417
using another actor
actor:  1 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus -0.2317815986947537
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]] [[0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]]
1313 7422
1313 7424
siam score:  -0.7867563
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.3500532432478561
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.043]
 [0.087]
 [0.034]
 [0.028]] [[2.115]
 [2.444]
 [3.3  ]
 [3.202]
 [7.299]] [[0.314]
 [0.436]
 [0.743]
 [0.677]
 [2.006]]
from probs:  [0.0789477642047912, 0.15413539488639874, 0.30451065624961376, 0.2293230255680063, 0.0789477642047912, 0.15413539488639874]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.001]
 [0.002]] [[-0.096]
 [ 0.4  ]
 [ 0.135]
 [ 0.308]
 [ 0.391]] [[0.002]
 [0.002]
 [0.002]
 [0.001]
 [0.002]]
from probs:  [0.0789477642047912, 0.15413539488639874, 0.30451065624961376, 0.2293230255680063, 0.0789477642047912, 0.15413539488639874]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.005]] [[1.301]
 [1.301]
 [1.301]
 [1.301]
 [0.402]] [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.005]]
using explorer policy with actor:  0
using explorer policy with actor:  0
first move QE:  0.3493862660734216
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.004]] [[0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.871]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.004]]
using explorer policy with actor:  0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[1.298]
 [1.298]
 [1.298]
 [1.298]
 [1.015]] [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.01 ]
 [0.01 ]
 [0.01 ]
 [0.008]] [[1.362]
 [1.362]
 [1.362]
 [1.362]
 [0.934]] [[0.01 ]
 [0.01 ]
 [0.01 ]
 [0.01 ]
 [0.008]]
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.329]
 [0.181]
 [0.181]
 [0.382]] [[0.825]
 [1.688]
 [3.564]
 [3.564]
 [1.043]] [[0.478]
 [1.625]
 [3.832]
 [3.832]
 [0.872]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  0
using explorer policy with actor:  0
rdn probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.001]
 [0.004]
 [0.005]
 [0.004]] [[ 0.068]
 [ 0.302]
 [ 0.212]
 [ 0.001]
 [-0.064]] [[0.004]
 [0.001]
 [0.004]
 [0.005]
 [0.004]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
using another actor
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.01 ]
 [0.006]
 [0.008]
 [0.008]] [[0.037]
 [0.527]
 [0.19 ]
 [0.363]
 [0.418]] [[0.005]
 [0.5  ]
 [0.155]
 [0.333]
 [0.388]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
start point for exploration sampling:  20019
using another actor
from probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
maxi score, test score, baseline:  0.0001 0.0 0.0001
1320 7493
1320 7495
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.001]] [[0.088]
 [0.088]
 [0.088]
 [0.088]
 [1.586]] [[-0.261]
 [-0.261]
 [-0.261]
 [-0.261]
 [ 1.724]]
from probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
maxi score, test score, baseline:  0.0001 0.0 0.0001
1321 7553
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
using another actor
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.003]
 [0.013]
 [0.015]
 [0.015]] [[-0.144]
 [ 0.388]
 [-0.038]
 [-0.123]
 [ 0.333]] [[0.016]
 [0.003]
 [0.013]
 [0.015]
 [0.015]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
using another actor
1322 7616
using another actor
from probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.016]
 [0.013]
 [0.019]
 [0.017]] [[-0.072]
 [ 0.504]
 [ 0.   ]
 [-0.057]
 [ 0.267]] [[0.077]
 [0.263]
 [0.09 ]
 [0.082]
 [0.186]]
from probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
line 256 mcts: sample exp_bonus -0.22880394441538338
1323 7651
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
from probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
from probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.041]
 [0.041]
 [0.041]
 [0.056]] [[1.826]
 [1.826]
 [1.826]
 [1.826]
 [6.213]] [[0.618]
 [0.618]
 [0.618]
 [0.618]
 [2.01 ]]
from probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
1325 7682
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
from probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
from probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.019]
 [0.005]
 [0.015]
 [0.018]] [[-0.278]
 [ 0.234]
 [ 0.121]
 [-0.043]
 [-0.068]] [[0.016]
 [0.019]
 [0.005]
 [0.015]
 [0.018]]
siam score:  -0.8805444
using another actor
from probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
1327 7721
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
from probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
siam score:  -0.8827578
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
from probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
1330 7780
using another actor
from probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
from probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
from probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
from probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
1330 7813
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
Printing some Q and Qe and total Qs values:  [[0.1]
 [0.1]
 [0.1]
 [0.1]
 [0.1]] [[2.062]
 [2.062]
 [2.062]
 [2.062]
 [2.062]] [[0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]]
Printing some Q and Qe and total Qs values:  [[0.098]
 [0.222]
 [0.217]
 [0.203]
 [0.116]] [[0.957]
 [2.043]
 [1.117]
 [0.597]
 [4.396]] [[0.14 ]
 [0.66 ]
 [0.26 ]
 [0.03 ]
 [1.626]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.003]
 [0.002]
 [0.002]
 [0.003]] [[0.518]
 [0.537]
 [0.518]
 [0.518]
 [0.496]] [[0.361]
 [0.375]
 [0.361]
 [0.361]
 [0.348]]
from probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
using another actor
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.001]
 [0.016]
 [0.015]
 [0.015]] [[-0.111]
 [ 0.265]
 [ 0.031]
 [-0.209]
 [-0.074]] [[0.205]
 [0.929]
 [0.491]
 [0.01 ]
 [0.28 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
using another actor
1338 7888
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
from probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
from probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
siam score:  -0.87003005
siam score:  -0.8689705
siam score:  -0.8664515
UNIT TEST: sample policy line 217 mcts : [0.042 0.208 0.292 0.292 0.167]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07342696470079631, 0.1433567411751991, 0.2832162941240046, 0.2832162941240046, 0.07342696470079631, 0.1433567411751991]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.8655945
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06441755434767509, 0.12576702173907006, 0.3098154239132549, 0.24846595652185993, 0.12576702173907006, 0.12576702173907006]
1346 7947
from probs:  [0.06441755434767509, 0.12576702173907006, 0.3098154239132549, 0.24846595652185993, 0.12576702173907006, 0.12576702173907006]
using another actor
from probs:  [0.06441755434767509, 0.12576702173907006, 0.3098154239132549, 0.24846595652185993, 0.12576702173907006, 0.12576702173907006]
from probs:  [0.06441755434767509, 0.12576702173907006, 0.3098154239132549, 0.24846595652185993, 0.12576702173907006, 0.12576702173907006]
from probs:  [0.06441755434767509, 0.12576702173907006, 0.3098154239132549, 0.24846595652185993, 0.12576702173907006, 0.12576702173907006]
from probs:  [0.06441755434767509, 0.12576702173907006, 0.3098154239132549, 0.24846595652185993, 0.12576702173907006, 0.12576702173907006]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
from probs:  [0.06441755434767509, 0.12576702173907006, 0.3098154239132549, 0.24846595652185993, 0.12576702173907006, 0.12576702173907006]
from probs:  [0.06441755434767509, 0.12576702173907006, 0.3098154239132549, 0.24846595652185993, 0.12576702173907006, 0.12576702173907006]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06441755434767509, 0.12576702173907006, 0.3098154239132549, 0.24846595652185993, 0.12576702173907006, 0.12576702173907006]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06441755434767509, 0.12576702173907006, 0.3098154239132549, 0.24846595652185993, 0.12576702173907006, 0.12576702173907006]
from probs:  [0.06441755434767509, 0.12576702173907006, 0.3098154239132549, 0.24846595652185993, 0.12576702173907006, 0.12576702173907006]
1347 7997
1347 7998
using another actor
from probs:  [0.06441755434767509, 0.12576702173907006, 0.3098154239132549, 0.24846595652185993, 0.12576702173907006, 0.12576702173907006]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
from probs:  [0.06441755434767509, 0.12576702173907006, 0.3098154239132549, 0.24846595652185993, 0.12576702173907006, 0.12576702173907006]
siam score:  -0.8746945
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5373331627514064
from probs:  [0.06441755434767509, 0.12576702173907006, 0.3098154239132549, 0.24846595652185993, 0.12576702173907006, 0.12576702173907006]
from probs:  [0.06441755434767509, 0.12576702173907006, 0.3098154239132549, 0.24846595652185993, 0.12576702173907006, 0.12576702173907006]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 2.5204917043470876
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]]
1356 8057
siam score:  -0.8736325
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.05440449413165007, 0.15803111493320388, 0.31347104613553456, 0.20984442533398073, 0.15803111493320388, 0.10621780453242698]
Printing some Q and Qe and total Qs values:  [[1.5  ]
 [1.5  ]
 [0.007]
 [0.009]
 [0.007]] [[ 0.   ]
 [ 0.   ]
 [-0.158]
 [-0.039]
 [ 0.144]] [[3.154]
 [3.154]
 [0.011]
 [0.135]
 [0.313]]
from probs:  [0.05440449413165007, 0.15803111493320388, 0.31347104613553456, 0.20984442533398073, 0.15803111493320388, 0.10621780453242698]
from probs:  [0.05440449413165007, 0.15803111493320388, 0.31347104613553456, 0.20984442533398073, 0.15803111493320388, 0.10621780453242698]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.05440449413165007, 0.15803111493320388, 0.31347104613553456, 0.20984442533398073, 0.15803111493320388, 0.10621780453242698]
Printing some Q and Qe and total Qs values:  [[0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.125]] [[3.286]
 [3.286]
 [3.286]
 [3.286]
 [3.286]] [[0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.   ]
 [0.012]
 [0.011]
 [0.014]] [[ 0.022]
 [ 0.14 ]
 [-0.125]
 [-0.309]
 [-0.12 ]] [[0.553]
 [0.739]
 [0.319]
 [0.01 ]
 [0.331]]
siam score:  -0.87206376
1361 8126
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
from probs:  [0.05440449413165007, 0.15803111493320388, 0.31347104613553456, 0.20984442533398073, 0.15803111493320388, 0.10621780453242698]
1362 8138
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using another actor
from probs:  [0.0962443298579387, 0.1431925543970907, 0.3309854525536986, 0.19014077893624265, 0.1431925543970907, 0.0962443298579387]
from probs:  [0.0962443298579387, 0.1431925543970907, 0.3309854525536986, 0.19014077893624265, 0.1431925543970907, 0.0962443298579387]
from probs:  [0.0962443298579387, 0.1431925543970907, 0.3309854525536986, 0.19014077893624265, 0.1431925543970907, 0.0962443298579387]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
start point for exploration sampling:  20019
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08798303526721594, 0.13090137966691637, 0.302574757265718, 0.21673806846631716, 0.13090137966691637, 0.13090137966691637]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
1371 8186
from probs:  [0.08102787111061603, 0.12055346905956249, 0.27865586085534827, 0.2786558608553483, 0.12055346905956249, 0.12055346905956249]
from probs:  [0.08102787111061603, 0.12055346905956249, 0.27865586085534827, 0.2786558608553483, 0.12055346905956249, 0.12055346905956249]
from probs:  [0.08102787111061603, 0.12055346905956249, 0.27865586085534827, 0.2786558608553483, 0.12055346905956249, 0.12055346905956249]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.009]
 [0.029]
 [0.022]
 [0.022]] [[0.536]
 [0.473]
 [1.084]
 [0.451]
 [0.657]] [[0.012]
 [0.009]
 [0.029]
 [0.022]
 [0.022]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.08102787111061603, 0.12055346905956249, 0.27865586085534827, 0.27865586085534827, 0.12055346905956249, 0.12055346905956249]
using explorer policy with actor:  0
from probs:  [0.08102787111061603, 0.12055346905956249, 0.27865586085534827, 0.27865586085534827, 0.12055346905956249, 0.12055346905956249]
using another actor
Printing some Q and Qe and total Qs values:  [[0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.124]] [[2.313]
 [2.313]
 [2.313]
 [2.313]
 [2.338]] [[1.096]
 [1.096]
 [1.096]
 [1.096]
 [1.108]]
using another actor
from probs:  [0.08102787111061603, 0.12055346905956249, 0.27865586085534827, 0.27865586085534827, 0.12055346905956249, 0.12055346905956249]
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.043]
 [0.043]
 [0.043]
 [0.063]] [[4.222]
 [4.222]
 [4.222]
 [4.222]
 [5.36 ]] [[0.974]
 [0.974]
 [0.974]
 [0.974]
 [1.373]]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.006]
 [0.004]
 [0.007]
 [0.005]] [[0.734]
 [0.618]
 [0.359]
 [0.734]
 [0.559]] [[0.238]
 [0.197]
 [0.106]
 [0.238]
 [0.175]]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08102787111061603, 0.12055346905956249, 0.27865586085534827, 0.27865586085534827, 0.12055346905956249, 0.12055346905956249]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08102787111061603, 0.12055346905956249, 0.27865586085534827, 0.27865586085534827, 0.12055346905956249, 0.12055346905956249]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
using explorer policy with actor:  0
using another actor
from probs:  [0.08102787111061603, 0.12055346905956249, 0.27865586085534827, 0.27865586085534827, 0.12055346905956249, 0.12055346905956249]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08102787111061603, 0.12055346905956249, 0.27865586085534827, 0.27865586085534827, 0.12055346905956249, 0.12055346905956249]
1377 8262
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.013]
 [0.004]
 [0.004]
 [0.004]] [[0.518]
 [0.76 ]
 [0.518]
 [0.518]
 [0.518]] [[0.157]
 [0.416]
 [0.157]
 [0.157]
 [0.157]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0779469704935496, 0.1159696974248855, 0.26806060515022906, 0.26806060515022906, 0.1159696974248855, 0.15399242435622137]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  94 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.06996606835857748, 0.10409569129084427, 0.2747438059521781, 0.2747438059521781, 0.13822531422311102, 0.13822531422311102]
UNIT TEST: sample policy line 217 mcts : [0.375 0.417 0.    0.042 0.167]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06996606835857748, 0.10409569129084427, 0.2747438059521781, 0.2747438059521781, 0.13822531422311102, 0.13822531422311102]
Printing some Q and Qe and total Qs values:  [[0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.245]] [[1.245]
 [1.245]
 [1.245]
 [1.245]
 [2.137]] [[0.728]
 [0.728]
 [0.728]
 [0.728]
 [1.737]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.36131775784434905
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.4364],
        [0.3620],
        [0.2636],
        [0.1114],
        [0.4426],
        [0.0933],
        [0.4230],
        [0.2472],
        [0.0000]], dtype=torch.float64)
0.0 0.0
0.0 0.43637100152008834
0.0 0.36198351735059997
0.0 0.26362039000973736
0.0 0.11144135781373592
0.0 0.4426090614904212
0.0 0.09327924311014295
0.0 0.42300433296243933
0.0 0.24723114093885565
0.970299 0.970299
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.89143693
from probs:  [0.06549540163535351, 0.09744422217155768, 0.2571883248525784, 0.28913714538878266, 0.1293930427077618, 0.161341863243966]
siam score:  -0.892525
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.424]
 [0.405]
 [0.356]
 [0.421]] [[2.709]
 [1.965]
 [1.63 ]
 [2.709]
 [2.497]] [[0.952]
 [0.839]
 [0.689]
 [0.952]
 [1.011]]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0634676839897175, 0.125387073595887, 0.24922585280822598, 0.28018554761131076, 0.125387073595887, 0.15634676839897174]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.0634676839897175, 0.125387073595887, 0.24922585280822598, 0.28018554761131076, 0.125387073595887, 0.15634676839897174]
1393 8342
line 256 mcts: sample exp_bonus 1.1324897434037389
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0634676839897175, 0.125387073595887, 0.24922585280822598, 0.28018554761131076, 0.125387073595887, 0.15634676839897174]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0634676839897175, 0.125387073595887, 0.24922585280822598, 0.28018554761131076, 0.125387073595887, 0.15634676839897174]
from probs:  [0.0634676839897175, 0.125387073595887, 0.24922585280822598, 0.28018554761131076, 0.125387073595887, 0.15634676839897174]
using explorer policy with actor:  1
from probs:  [0.0634676839897175, 0.125387073595887, 0.24922585280822598, 0.28018554761131076, 0.125387073595887, 0.15634676839897174]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0634676839897175, 0.125387073595887, 0.24922585280822598, 0.28018554761131076, 0.125387073595887, 0.15634676839897174]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06156175096819856, 0.1216217027958946, 0.27177158236513477, 0.27177158236513477, 0.1216217027958946, 0.15165167870974264]
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.751]
 [0.751]
 [0.556]
 [0.638]] [[2.852]
 [2.198]
 [2.198]
 [2.166]
 [2.458]] [[1.891]
 [1.418]
 [1.418]
 [1.132]
 [1.498]]
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.815]
 [0.815]
 [0.815]
 [1.061]] [[1.076]
 [1.076]
 [1.076]
 [1.076]
 [2.144]] [[1.187]
 [1.187]
 [1.187]
 [1.187]
 [2.364]]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
in main func line 156:  1401
1401 8418
using another actor
1402 8421
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus -0.05838719555962785
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7404178304774442
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.039]
 [0.029]
 [0.026]
 [0.039]] [[ 0.401]
 [ 0.237]
 [ 0.321]
 [-0.108]
 [-0.277]] [[0.026]
 [0.039]
 [0.029]
 [0.026]
 [0.039]]
from probs:  [0.07760827848104132, 0.15394403978300591, 0.255725054852292, 0.255725054852292, 0.10305353224836283, 0.15394403978300591]
rdn beta is 0 so we're just using the maxi policy
using another actor
first move QE:  0.34269807794946644
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.07568523570029957, 0.17493770584542728, 0.2493770584542731, 0.2493770584542731, 0.10049835323658149, 0.15012458830914535]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.07568523570029957, 0.17493770584542728, 0.2493770584542731, 0.2493770584542731, 0.10049835323658149, 0.15012458830914535]
using explorer policy with actor:  1
1414 8475
actor:  1 policy actor:  1  step number:  100 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus 3.5580735933948864
siam score:  -0.89216286
siam score:  -0.8921524
maxi score, test score, baseline:  0.0021 0.0 0.0021
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8909189
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0935356220828672, 0.18591167839924544, 0.2551937206365292, 0.2320997065574346, 0.0935356220828672, 0.13972365024105632]
1418 8480
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Printing some Q and Qe and total Qs values:  [[0.059]
 [0.074]
 [0.181]
 [0.125]
 [0.122]] [[0.476]
 [0.997]
 [1.416]
 [0.063]
 [0.829]] [[0.279]
 [0.681]
 [1.109]
 [0.047]
 [0.61 ]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.09142426873356337, 0.18171514625328733, 0.2494333043930803, 0.22686058501314932, 0.11399698811349436, 0.13656970749342534]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.08940612926677853, 0.1777038862952221, 0.2439272040665548, 0.2218527648094439, 0.11148056852388943, 0.1556294470381112]
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.081]] [[1.551]
 [1.551]
 [1.551]
 [1.551]
 [1.437]] [[0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.081]]
actor:  0 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8868043
maxi score, test score, baseline:  0.0041 0.0 0.0041
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.352]
 [0.219]
 [0.219]
 [0.303]] [[2.666]
 [3.528]
 [2.666]
 [2.666]
 [2.99 ]] [[0.967]
 [1.463]
 [0.967]
 [0.967]
 [1.17 ]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.08940819494716191, 0.17770359119802448, 0.2439251383861714, 0.22185128932345577, 0.11148204400987755, 0.15562974213530884]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
from probs:  [0.08747723565097862, 0.17386570584991107, 0.23865705849911037, 0.21705994094937728, 0.13067147075044483, 0.15226858830017795]
1426 8529
actor:  1 policy actor:  1  step number:  92 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
from probs:  [0.085627919409943, 0.1701900904604373, 0.23361171874830794, 0.2124711759856844, 0.12790900493519014, 0.1701900904604373]
siam score:  -0.8919893
from probs:  [0.08385517548669914, 0.18736953946165855, 0.22877528505164232, 0.20807241225665046, 0.1252609210766829, 0.16666666666666669]
maxi score, test score, baseline:  0.0041 0.0 0.0041
using another actor
from probs:  [0.08385517548669914, 0.18736953946165855, 0.22877528505164232, 0.20807241225665046, 0.1252609210766829, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.026]
 [0.036]
 [0.026]
 [0.016]] [[ 0.279]
 [ 0.279]
 [ 0.195]
 [ 0.279]
 [-0.108]] [[0.026]
 [0.026]
 [0.036]
 [0.026]
 [0.016]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
first move QE:  0.3389132523003492
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.08215434454208834, 0.18356913109158232, 0.24441800302127875, 0.20385208840148114, 0.12272025916188593, 0.16328617378168353]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.08215434454208834, 0.18356913109158232, 0.24441800302127875, 0.20385208840148114, 0.12272025916188593, 0.16328617378168353]
start point for exploration sampling:  20019
from probs:  [0.08215434454208834, 0.18356913109158232, 0.24441800302127875, 0.20385208840148114, 0.12272025916188593, 0.16328617378168353]
from probs:  [0.08215434454208834, 0.18356913109158232, 0.24441800302127875, 0.20385208840148114, 0.12272025916188593, 0.16328617378168353]
from probs:  [0.08215434454208834, 0.18356913109158232, 0.24441800302127875, 0.20385208840148114, 0.12272025916188593, 0.16328617378168353]
1434 8574
from probs:  [0.08215434454208834, 0.18356913109158232, 0.24441800302127875, 0.20385208840148114, 0.12272025916188593, 0.16328617378168353]
from probs:  [0.08215434454208834, 0.18356913109158232, 0.24441800302127875, 0.20385208840148114, 0.12272025916188593, 0.16328617378168353]
from probs:  [0.08215434454208834, 0.18356913109158232, 0.24441800302127875, 0.20385208840148114, 0.12272025916188593, 0.16328617378168353]
maxi score, test score, baseline:  0.0041 0.0 0.0041
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.603]
 [0.512]
 [0.512]
 [0.569]] [[1.472]
 [1.715]
 [1.472]
 [1.472]
 [1.983]] [[1.201]
 [1.544]
 [1.201]
 [1.201]
 [1.656]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.08215434454208834, 0.18356913109158232, 0.24441800302127875, 0.20385208840148114, 0.12272025916188593, 0.16328617378168353]
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]] [[0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]] [[0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.219]]
siam score:  -0.87548417
maxi score, test score, baseline:  0.0041 0.0 0.0041
Starting evaluation
line 256 mcts: sample exp_bonus -0.05295189580181059
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus 0.13329194646910875
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.008]] [[0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.218]] [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.008]]
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]] [[1.843]
 [1.843]
 [1.843]
 [1.843]
 [1.843]] [[0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.9733830923286999
Printing some Q and Qe and total Qs values:  [[0.303]
 [0.3  ]
 [0.39 ]
 [0.303]
 [0.321]] [[1.841]
 [3.076]
 [2.305]
 [1.841]
 [1.969]] [[0.303]
 [0.3  ]
 [0.39 ]
 [0.303]
 [0.321]]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.08215643328220822, 0.18356871334355837, 0.24441608138036847, 0.20385116935582837, 0.12272134530674828, 0.16328625733128835]
actor:  0 policy actor:  1  step number:  97 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
rdn probs:  [0.08215643328220822, 0.18356871334355837, 0.24441608138036847, 0.20385116935582837, 0.12272134530674828, 0.16328625733128835]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
start point for exploration sampling:  20019
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.07904036221081112, 0.21534794691991976, 0.23482045902122098, 0.19587543481861852, 0.11798538641341358, 0.15693041061601604]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.003]
 [0.029]
 [0.019]
 [0.018]] [[-0.267]
 [ 0.084]
 [-0.168]
 [-0.241]
 [-0.456]] [[0.084]
 [0.181]
 [0.15 ]
 [0.105]
 [0.031]]
maxi score, test score, baseline:  0.0081 0.08 0.08
probs:  [0.07904036221081112, 0.21534794691991976, 0.23482045902122098, 0.19587543481861852, 0.11798538641341358, 0.15693041061601604]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.0081 0.08 0.08
probs:  [0.07753064577277897, 0.21123467711361052, 0.23033525301944358, 0.19213410120777744, 0.13483237349027818, 0.15393294939611127]
using another actor
maxi score, test score, baseline:  0.0081 0.08 0.08
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0081 0.08 0.08
probs:  [0.07607752130241457, 0.22601817569841803, 0.22601817569841803, 0.18853301209941717, 0.13230526670091586, 0.1510478485004163]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.08 0.08
using explorer policy with actor:  1
siam score:  -0.8766809
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0081 0.08 0.08
probs:  [0.09307562557601684, 0.22185994748465404, 0.22185994748465404, 0.18506442693932915, 0.12987114612134174, 0.14826890639400422]
maxi score, test score, baseline:  0.0081 0.08 0.08
1450 8653
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0081 0.08 0.08
probs:  [0.10945957483138392, 0.21785195936139332, 0.21785195936139332, 0.18172116451805687, 0.12752497225305215, 0.1455903696747204]
maxi score, test score, baseline:  0.0081 0.08 0.08
probs:  [0.10945957483138392, 0.21785195936139332, 0.21785195936139332, 0.18172116451805687, 0.12752497225305215, 0.1455903696747204]
maxi score, test score, baseline:  0.0081 0.08 0.08
probs:  [0.10945957483138392, 0.21785195936139332, 0.21785195936139332, 0.18172116451805687, 0.12752497225305215, 0.1455903696747204]
siam score:  -0.8780883
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0081 0.08 0.08
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8790559
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using another actor
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0081 0.08 0.08
probs:  [0.09563770905130616, 0.23769562428202717, 0.2376956242820272, 0.17455877306837342, 0.11142192185471961, 0.1429903474615465]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.875985
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using another actor
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0081 0.08 0.08
probs:  [0.08995804156551267, 0.22357951754816793, 0.2532731788776469, 0.16419219488921008, 0.10480487223025214, 0.16419219488921008]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.10327161603451374, 0.2203086325861806, 0.24956788672409735, 0.1617901243103472, 0.10327161603451374, 0.1617901243103472]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.463]] [[2.681]
 [2.681]
 [2.681]
 [2.681]
 [2.309]] [[2.178]
 [2.178]
 [2.178]
 [2.178]
 [1.752]]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
1471 8682
siam score:  -0.8766942
siam score:  -0.87928873
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0081 0.08 0.08
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.291]] [[3.33 ]
 [3.33 ]
 [3.33 ]
 [3.33 ]
 [3.134]] [[2.126]
 [2.126]
 [2.126]
 [2.126]
 [1.916]]
maxi score, test score, baseline:  0.0081 0.08 0.08
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  91 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.86745864
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0081 0.08 0.08
maxi score, test score, baseline:  0.0081 0.08 0.08
probs:  [0.12183634815106714, 0.2159800170338261, 0.24287820814318584, 0.1621836348151067, 0.09493815704170744, 0.1621836348151067]
maxi score, test score, baseline:  0.0081 0.08 0.08
probs:  [0.12183634815106714, 0.2159800170338261, 0.24287820814318584, 0.1621836348151067, 0.09493815704170744, 0.1621836348151067]
using explorer policy with actor:  1
using explorer policy with actor:  1
1479 8693
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.072]
 [0.368]
 [0.204]
 [0.231]] [[1.138]
 [1.241]
 [1.526]
 [1.138]
 [1.242]] [[0.907]
 [0.881]
 [1.41 ]
 [0.907]
 [1.028]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.152]
 [0.152]
 [0.152]
 [0.152]
 [0.183]] [[7.847]
 [7.847]
 [7.847]
 [7.847]
 [8.45 ]] [[1.747]
 [1.747]
 [1.747]
 [1.747]
 [2.029]]
Printing some Q and Qe and total Qs values:  [[0.151]
 [0.151]
 [0.151]
 [0.151]
 [0.16 ]] [[9.596]
 [9.596]
 [9.596]
 [9.596]
 [9.105]] [[1.957]
 [1.957]
 [1.957]
 [1.957]
 [1.822]]
maxi score, test score, baseline:  0.0081 0.08 0.08
maxi score, test score, baseline:  0.0081 0.08 0.08
probs:  [0.12183634815106714, 0.2159800170338261, 0.24287820814318584, 0.1621836348151067, 0.09493815704170744, 0.1621836348151067]
maxi score, test score, baseline:  0.0081 0.08 0.08
probs:  [0.12183634815106714, 0.2159800170338261, 0.24287820814318584, 0.1621836348151067, 0.09493815704170744, 0.1621836348151067]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0081 0.08 0.08
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.86908346
siam score:  -0.8719523
maxi score, test score, baseline:  0.0081 0.08 0.08
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.251694355066297
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16456638517336253, 0.2149731410126612, 0.22757482997248593, 0.15196469621353786, 0.08895625141441452, 0.15196469621353786]
Printing some Q and Qe and total Qs values:  [[0.25 ]
 [0.559]
 [0.645]
 [0.367]
 [0.424]] [[1.871]
 [2.188]
 [1.971]
 [2.193]
 [3.308]] [[0.06 ]
 [0.455]
 [0.39 ]
 [0.325]
 [1.006]]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
using another actor
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.032]
 [0.047]
 [0.032]
 [0.032]] [[-0.378]
 [ 0.   ]
 [-0.004]
 [ 0.   ]
 [ 0.   ]] [[0.025]
 [0.032]
 [0.047]
 [0.032]
 [0.032]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  0
from probs:  [0.15857157613249012, 0.20714211933754917, 0.23142739094007875, 0.1464289403312254, 0.09785839712616634, 0.15857157613249015]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0101 0.08 0.08
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.0101 0.08 0.08
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0101 0.08 0.08
probs:  [0.16280660661318036, 0.19754714709455687, 0.22070750741547457, 0.13964624629226274, 0.10490570581088625, 0.1743867867736392]
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.502]
 [0.47 ]
 [0.47 ]
 [0.422]] [[1.275]
 [2.207]
 [1.728]
 [1.728]
 [1.968]] [[0.715]
 [1.688]
 [1.32 ]
 [1.32 ]
 [1.382]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.107]
 [0.285]
 [0.138]
 [0.161]] [[0.291]
 [0.639]
 [0.992]
 [0.091]
 [0.335]] [[0.197]
 [0.467]
 [1.038]
 [0.076]
 [0.309]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0101 0.08 0.08
maxi score, test score, baseline:  0.0101 0.08 0.08
probs:  [0.1720225981836406, 0.21487005031943227, 0.21487005031943227, 0.1398870090817969, 0.09703955694600533, 0.1613107351496927]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0101 0.08 0.08
maxi score, test score, baseline:  0.0101 0.08 0.08
probs:  [0.17019944503991902, 0.2125927855189478, 0.2125927855189478, 0.1384044396806475, 0.0960110992016188, 0.17019944503991907]
maxi score, test score, baseline:  0.0101 0.08 0.08
probs:  [0.17019944503991902, 0.2125927855189478, 0.2125927855189478, 0.1384044396806475, 0.0960110992016188, 0.17019944503991907]
1509 8757
using another actor
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.133]
 [0.133]
 [0.133]
 [0.133]
 [0.106]] [[1.405]
 [1.405]
 [1.405]
 [1.405]
 [0.541]] [[0.133]
 [0.133]
 [0.133]
 [0.133]
 [0.106]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.495]
 [0.406]
 [0.337]
 [0.345]] [[1.506]
 [2.249]
 [1.453]
 [1.489]
 [2.109]] [[0.716]
 [1.704]
 [0.995]
 [0.88 ]
 [1.311]]
maxi score, test score, baseline:  0.0101 0.08 0.08
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.28686190899321
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
in main func line 156:  1513
using another actor
from probs:  [0.17522645411209598, 0.226585178784672, 0.20604168891564162, 0.13413947437403517, 0.0930524946359744, 0.16495470917758082]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0101 0.08 0.08
probs:  [0.1817641294938021, 0.22202403036616342, 0.20189407992998282, 0.14150422862144085, 0.09117935253098927, 0.16163417905762154]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0101 0.08 0.08
siam score:  -0.87722594
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7761980893180497
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0101 0.08 0.08
probs:  [0.17995290793501292, 0.2198116317400518, 0.1998822698375324, 0.1400941841299741, 0.09027077937367553, 0.16998822698375327]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0101 0.08 0.08
probs:  [0.18804379248925238, 0.21764288978206336, 0.19791015825352273, 0.13871196366790073, 0.08938013484654911, 0.16831106096071174]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1520 8775
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.149]
 [0.039]
 [0.039]
 [0.05 ]] [[-0.17 ]
 [ 0.291]
 [-0.17 ]
 [-0.17 ]
 [-0.023]] [[0.473]
 [1.   ]
 [0.473]
 [0.473]
 [0.592]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0101 0.08 0.08
probs:  [0.1922206019545862, 0.21138605342052588, 0.20180332768755604, 0.13472424755676723, 0.09639334462488792, 0.16347242475567672]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0101 0.08 0.08
probs:  [0.19039608845826048, 0.21887139460817295, 0.19988785717489796, 0.13344547615843538, 0.09547840129188535, 0.16192078230834792]
maxi score, test score, baseline:  0.0101 0.08 0.08
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.119]
 [0.089]
 [0.089]
 [0.089]] [[0.149]
 [0.577]
 [0.149]
 [0.149]
 [0.149]] [[0.681]
 [1.169]
 [0.681]
 [0.681]
 [0.681]]
maxi score, test score, baseline:  0.0101 0.08 0.08
1526 8787
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
in main func line 156:  1529
maxi score, test score, baseline:  0.0101 0.08 0.08
using explorer policy with actor:  1
start point for exploration sampling:  20019
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
siam score:  -0.8716213
start point for exploration sampling:  20019
siam score:  -0.8722715
from probs:  [0.18512460855380805, 0.21281152138452003, 0.1943535794973787, 0.14820872477952526, 0.09283489911810115, 0.16666666666666663]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0101 0.08 0.08
probs:  [0.19257629843470445, 0.2108654502709664, 0.19257629843470447, 0.14685341884404957, 0.09198596333526368, 0.16514257068031152]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.19257629843470445, 0.2108654502709664, 0.19257629843470447, 0.14685341884404957, 0.09198596333526368, 0.16514257068031152]
Printing some Q and Qe and total Qs values:  [[0.26 ]
 [0.232]
 [0.26 ]
 [0.26 ]
 [0.158]] [[2.975]
 [1.223]
 [2.975]
 [2.975]
 [6.691]] [[0.634]
 [0.055]
 [0.634]
 [0.634]
 [1.759]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
1539 8807
from probs:  [0.18911749976657918, 0.21605849948647432, 0.19809783300654427, 0.14421583356675408, 0.09033383412696391, 0.16217650004668413]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0121 0.08 0.08
probs:  [0.18911749976657918, 0.21605849948647432, 0.19809783300654427, 0.14421583356675408, 0.09033383412696391, 0.16217650004668413]
maxi score, test score, baseline:  0.0121 0.08 0.08
probs:  [0.18911749976657918, 0.21605849948647432, 0.19809783300654427, 0.14421583356675408, 0.09033383412696391, 0.16217650004668413]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.764]] [[1.798]
 [1.798]
 [1.798]
 [1.798]
 [2.343]] [[1.162]
 [1.162]
 [1.162]
 [1.162]
 [1.901]]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.0121 0.08 0.08
probs:  [0.19290089017828363, 0.2103903725193616, 0.20164563134882257, 0.1404324431550497, 0.09670873730235481, 0.15792192549612769]
line 256 mcts: sample exp_bonus 1.1461785222824583
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0121 0.08 0.08
probs:  [0.19122864517186286, 0.20856651235200133, 0.19989757876193207, 0.13921504363144743, 0.09587037568110125, 0.16522184440165516]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.88211006
maxi score, test score, baseline:  0.0121 0.08 0.08
first move QE:  0.34736510464817005
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0121 0.08 0.08
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
1551 8824
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.18481988010484884, 0.20995509871156265, 0.20995509871156265, 0.13454944289142123, 0.09265741188023155, 0.16806306770037296]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.568]
 [0.552]
 [0.552]
 [0.507]] [[2.893]
 [2.737]
 [2.893]
 [2.893]
 [2.878]] [[1.653]
 [1.554]
 [1.653]
 [1.653]
 [1.603]]
maxi score, test score, baseline:  0.0121 0.08 0.08
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.002]
 [0.037]
 [0.025]
 [0.006]] [[0.302]
 [0.374]
 [0.302]
 [0.28 ]
 [0.305]] [[0.155]
 [0.248]
 [0.198]
 [0.137]
 [0.14 ]]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.18177392848519777, 0.2147352269983566, 0.20649490237006687, 0.13233198071545957, 0.09937068220230073, 0.1652932792286184]
first move QE:  0.3476003024295064
maxi score, test score, baseline:  0.0121 0.08 0.08
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.272]
 [0.278]
 [0.278]
 [0.256]] [[2.495]
 [2.155]
 [2.495]
 [2.495]
 [2.42 ]] [[1.25 ]
 [1.032]
 [1.25 ]
 [1.25 ]
 [1.177]]
maxi score, test score, baseline:  0.0121 0.08 0.08
probs:  [0.18028829441255764, 0.212980201002696, 0.20480722435516138, 0.1312504345273501, 0.09855852793721176, 0.17211531776502306]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
UNIT TEST: sample policy line 217 mcts : [0. 0. 0. 0. 1.]
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.55 ]
 [0.58 ]
 [0.497]
 [0.503]] [[1.257]
 [0.514]
 [0.825]
 [0.485]
 [0.925]] [[1.607]
 [0.987]
 [1.359]
 [0.853]
 [1.303]]
actor:  0 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0141 0.08 0.08
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.126]
 [0.437]
 [0.148]
 [0.162]] [[0.243]
 [0.351]
 [0.903]
 [0.006]
 [0.413]] [[0.388]
 [0.257]
 [1.009]
 [0.065]
 [0.342]]
using explorer policy with actor:  1
from probs:  [0.1937241061838269, 0.21691619719853564, 0.20918550019363277, 0.12414783313970056, 0.09322504512008883, 0.1628013181642152]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.19223797266432224, 0.2229235398615089, 0.20758075626291558, 0.12319544647065221, 0.09250987927346553, 0.16155240546713553]
maxi score, test score, baseline:  0.0141 0.08 0.08
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using another actor
from probs:  [0.19838745655630946, 0.22122642527685232, 0.2060004461298238, 0.1222575608211667, 0.09180560252710958, 0.1603225086887381]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0161 0.08 0.08
probs:  [0.1954121157567209, 0.21790855417502428, 0.21040974136892313, 0.12792280050181093, 0.09042873647130534, 0.15791805172621537]
from probs:  [0.1954121157567209, 0.21790855417502428, 0.21040974136892313, 0.12792280050181093, 0.09042873647130534, 0.15791805172621537]
1568 8877
Printing some Q and Qe and total Qs values:  [[0.941]
 [0.941]
 [0.941]
 [0.941]
 [0.885]] [[2.029]
 [2.029]
 [2.029]
 [2.029]
 [1.77 ]] [[2.085]
 [2.085]
 [2.085]
 [2.085]
 [1.723]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0161 0.08 0.08
probs:  [0.19111275933538052, 0.2277818983384513, 0.2057804149366088, 0.12510830912985313, 0.08843917012678235, 0.16177744813292388]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]] [[-0.448]
 [-0.448]
 [-0.448]
 [-0.448]
 [-0.448]] [[0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
1574 8887
maxi score, test score, baseline:  0.0161 0.08 0.08
probs:  [0.1869985146198037, 0.23723013897461287, 0.2013504072926063, 0.12241499759219193, 0.08653526591018537, 0.16547067561059978]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.018099999999999998 0.08 0.08
siam score:  -0.8910135
line 256 mcts: sample exp_bonus -0.021813771468531754
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.180521453312119, 0.22901320657120217, 0.2082310266030237, 0.11817491340758346, 0.0835379467939526, 0.180521453312119]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.018099999999999998 0.08 0.08
probs:  [0.17927951360665864, 0.22743765646844616, 0.20679845238482297, 0.12424163605032999, 0.08296322788308352, 0.17927951360665864]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.018099999999999998 0.08 0.08
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
1587 8892
maxi score, test score, baseline:  0.018099999999999998 0.08 0.08
probs:  [0.1836325616897903, 0.22435070974528704, 0.20399163571753867, 0.12934169761579464, 0.08183719155104842, 0.17684620368054085]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
1588 8900
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]] [[2.027]
 [2.027]
 [2.027]
 [2.027]
 [2.027]] [[5.456]
 [5.456]
 [5.456]
 [5.456]
 [5.456]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.18661951909476662, 0.21987427314159988, 0.2132233223322333, 0.1267609618104668, 0.08020430614490022, 0.17331761747603333]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.018099999999999998 0.08 0.08
probs:  [0.18538652245321077, 0.2184215620765239, 0.21842156207652394, 0.1259234511312472, 0.0796743956586088, 0.17217250660388556]
maxi score, test score, baseline:  0.018099999999999998 0.08 0.08
probs:  [0.18538652245321077, 0.2184215620765239, 0.21842156207652394, 0.1259234511312472, 0.0796743956586088, 0.17217250660388556]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.018099999999999998 0.08 0.08
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.018099999999999998 0.08 0.08
maxi score, test score, baseline:  0.018099999999999998 0.08 0.08
probs:  [0.19601045306904818, 0.21557297733730257, 0.2155729773373026, 0.12428119741878228, 0.0786353074595221, 0.16992708737804244]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
from probs:  [0.19992394974599215, 0.21279773674444072, 0.21279773674444075, 0.129118121254525, 0.0776229732607307, 0.16773948224987073]
maxi score, test score, baseline:  0.018099999999999998 0.08 0.08
actor:  0 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.1973828845333702, 0.21009304365062684, 0.2100930436506268, 0.13383208894708706, 0.08299145247806056, 0.16560748674022863]
actor:  0 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0241 0.08 0.08
probs:  [0.19613642196742248, 0.20876631709631788, 0.21508126466076558, 0.1329869463229457, 0.08246736580736423, 0.1645616841451841]
siam score:  -0.87579226
maxi score, test score, baseline:  0.0241 0.08 0.08
probs:  [0.19613642196742248, 0.20876631709631788, 0.21508126466076558, 0.1329869463229457, 0.08246736580736423, 0.1645616841451841]
using another actor
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  0.3495945236204337
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8710866
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0241 0.08 0.08
probs:  [0.19506058532265205, 0.21331381874435695, 0.21939822988492522, 0.12813206277640074, 0.07945677365185434, 0.16463852961981057]
maxi score, test score, baseline:  0.0241 0.08 0.08
probs:  [0.19506058532265205, 0.21331381874435695, 0.21939822988492522, 0.12813206277640074, 0.07945677365185434, 0.16463852961981057]
maxi score, test score, baseline:  0.0241 0.08 0.08
Printing some Q and Qe and total Qs values:  [[0.085]
 [0.169]
 [0.085]
 [0.085]
 [0.085]] [[-0.36 ]
 [ 0.506]
 [-0.36 ]
 [-0.36 ]
 [-0.36 ]] [[0.528]
 [1.272]
 [0.528]
 [0.528]
 [0.528]]
line 256 mcts: sample exp_bonus 0.7070308815706312
from probs:  [0.19506058532265205, 0.21331381874435695, 0.21939822988492524, 0.12813206277640074, 0.07945677365185434, 0.16463852961981057]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0241 0.08 0.08
probs:  [0.19388093400783102, 0.21807139386664376, 0.2180713938666438, 0.1273571693960959, 0.07897624967847036, 0.16364285918431506]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
from probs:  [0.19872672624898438, 0.21676050976403807, 0.2167605097640381, 0.1265915921887695, 0.0785015028152929, 0.16265915921887697]
maxi score, test score, baseline:  0.0241 0.08 0.08
probs:  [0.19872672624898438, 0.21676050976403807, 0.2167605097640381, 0.1265915921887695, 0.0785015028152929, 0.16265915921887697]
maxi score, test score, baseline:  0.0241 0.08 0.08
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.617]
 [0.444]
 [0.444]
 [0.48 ]] [[1.407]
 [2.641]
 [1.859]
 [1.859]
 [1.554]] [[0.718]
 [1.842]
 [1.148]
 [1.148]
 [1.004]]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.62 ]] [[2.701]
 [2.701]
 [2.701]
 [2.701]
 [2.709]] [[2.28 ]
 [2.28 ]
 [2.28 ]
 [2.28 ]
 [2.283]]
maxi score, test score, baseline:  0.0241 0.08 0.08
maxi score, test score, baseline:  0.0241 0.08 0.08
maxi score, test score, baseline:  0.0241 0.08 0.08
probs:  [0.19292802259326064, 0.21627145008356638, 0.22210730695614284, 0.12289774012234339, 0.08204674201430834, 0.16374873823037847]
siam score:  -0.8756297
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.491]
 [0.588]
 [0.491]
 [0.506]] [[1.896]
 [1.896]
 [1.954]
 [1.896]
 [1.759]] [[1.803]
 [1.803]
 [2.008]
 [1.803]
 [1.688]]
maxi score, test score, baseline:  0.0241 0.08 0.08
probs:  [0.19292802259326064, 0.21627145008356638, 0.22210730695614284, 0.12289774012234339, 0.08204674201430834, 0.16374873823037847]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8754151
maxi score, test score, baseline:  0.0241 0.08 0.08
maxi score, test score, baseline:  0.0241 0.08 0.08
maxi score, test score, baseline:  0.0241 0.08 0.08
probs:  [0.19180865473729236, 0.21501664372556223, 0.22081864097262974, 0.12798668501955024, 0.08157070704301046, 0.16279866850195504]
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.026099999999999998 0.08 0.08
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.3858988570729078
maxi score, test score, baseline:  0.026099999999999998 0.08 0.08
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
actor:  0 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.19656966915563936, 0.21899692102236895, 0.21899692102236898, 0.12928791355545075, 0.07882659685530925, 0.1573219783888627]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.19547368476525237, 0.2177758923254478, 0.2233514442154967, 0.12856706208466617, 0.07838709507422652, 0.15644482153491046]
siam score:  -0.8772972
start point for exploration sampling:  20019
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0281 0.08 0.08
maxi score, test score, baseline:  0.0281 0.08 0.08
probs:  [0.19331797595284378, 0.22088829590406156, 0.22088829590406156, 0.12714920806992125, 0.07752263215772934, 0.16023359201138254]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0281 0.08 0.08
probs:  [0.19331797595284378, 0.22088829590406156, 0.22088829590406156, 0.12714920806992125, 0.07752263215772934, 0.16023359201138254]
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
in main func line 156:  1644
maxi score, test score, baseline:  0.0301 0.08 0.08
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.3527390975056313
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.257]
 [0.277]
 [0.257]
 [0.228]
 [0.206]] [[1.809]
 [2.129]
 [1.809]
 [2.192]
 [2.522]] [[0.871]
 [1.124]
 [0.871]
 [1.068]
 [1.243]]
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.555]
 [0.449]
 [0.378]
 [0.426]] [[2.533]
 [3.213]
 [2.283]
 [1.596]
 [2.696]] [[0.381]
 [0.555]
 [0.449]
 [0.378]
 [0.426]]
Starting evaluation
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]] [[0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]] [[0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]]
actor:  0 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.87432474
actor:  0 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn probs:  [0.18914612070064507, 0.23230667244588363, 0.21612146554141917, 0.12440529308278729, 0.07584967236939395, 0.162170775859871]
maxi score, test score, baseline:  0.0821 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.77604173012206
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0821 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0821 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.193]
 [0.214]
 [0.193]
 [0.193]
 [0.448]] [[2.99 ]
 [2.549]
 [2.99 ]
 [2.99 ]
 [3.223]] [[0.875]
 [0.706]
 [0.875]
 [0.875]
 [1.129]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.184]
 [0.163]
 [0.367]
 [0.264]] [[2.022]
 [2.257]
 [1.956]
 [2.735]
 [3.092]] [[0.679]
 [0.969]
 [0.653]
 [1.688]
 [1.881]]
rdn beta is 0 so we're just using the maxi policy
1658 8990
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0821 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0821 1.0 1.0
1660 8993
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0821 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1665 8998
Printing some Q and Qe and total Qs values:  [[0.201]
 [0.49 ]
 [0.202]
 [0.378]
 [0.352]] [[-0.292]
 [-0.116]
 [-0.181]
 [-0.5  ]
 [-0.362]] [[0.112]
 [0.439]
 [0.191]
 [0.09 ]
 [0.17 ]]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.008]
 [1.008]
 [1.008]
 [1.008]
 [0.968]] [[1.785]
 [1.785]
 [1.785]
 [1.785]
 [2.485]] [[2.058]
 [2.058]
 [2.058]
 [2.058]
 [2.53 ]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0821 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0821 1.0 1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
1670 9001
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.333]
 [0.33 ]
 [0.33 ]
 [0.262]] [[0.765]
 [0.6  ]
 [0.765]
 [0.765]
 [0.827]] [[1.262]
 [1.157]
 [1.262]
 [1.262]
 [1.166]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0821 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0821 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0821 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.475]] [[1.02 ]
 [1.02 ]
 [1.02 ]
 [1.02 ]
 [1.394]] [[0.887]
 [0.887]
 [0.887]
 [0.887]
 [0.964]]
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1677 9010
first move QE:  0.34801951473773934
maxi score, test score, baseline:  0.0821 1.0 1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.08410000000000001 1.0 1.0
maxi score, test score, baseline:  0.08410000000000001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.08410000000000001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.08410000000000001 1.0 1.0
siam score:  -0.88809234
maxi score, test score, baseline:  0.08410000000000001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0861 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
UNIT TEST: sample policy line 217 mcts : [0.5   0.042 0.042 0.125 0.292]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0881 1.0 1.0
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.963]
 [0.939]
 [0.963]
 [0.963]
 [0.943]] [[2.819]
 [0.649]
 [2.819]
 [2.819]
 [2.515]] [[2.341]
 [0.849]
 [2.341]
 [2.341]
 [2.1  ]]
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0921 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0941 1.0 1.0
maxi score, test score, baseline:  0.0941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.0941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20019
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.627]
 [0.834]
 [0.627]
 [0.627]] [[1.994]
 [1.994]
 [3.144]
 [1.994]
 [1.994]] [[1.215]
 [1.215]
 [1.957]
 [1.215]
 [1.215]]
Printing some Q and Qe and total Qs values:  [[0.87 ]
 [0.877]
 [0.87 ]
 [0.87 ]
 [0.721]] [[1.556]
 [3.634]
 [1.556]
 [1.556]
 [2.564]] [[1.291]
 [1.998]
 [1.291]
 [1.291]
 [1.329]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.116]
 [0.158]
 [0.382]
 [0.217]
 [0.186]] [[0.175]
 [0.925]
 [1.718]
 [1.07 ]
 [0.028]] [[0.116]
 [0.158]
 [0.382]
 [0.217]
 [0.186]]
Printing some Q and Qe and total Qs values:  [[0.869]
 [0.888]
 [0.869]
 [0.869]
 [0.872]] [[2.989]
 [2.742]
 [2.989]
 [2.989]
 [2.94 ]] [[2.073]
 [1.917]
 [2.073]
 [2.073]
 [2.042]]
start point for exploration sampling:  20019
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.721]
 [0.763]
 [0.721]
 [0.716]] [[1.484]
 [1.484]
 [1.521]
 [1.484]
 [1.866]] [[1.354]
 [1.354]
 [1.464]
 [1.354]
 [1.599]]
Printing some Q and Qe and total Qs values:  [[0.768]
 [1.113]
 [1.171]
 [0.84 ]
 [0.934]] [[1.69 ]
 [2.282]
 [1.057]
 [1.23 ]
 [2.261]] [[1.367]
 [2.43 ]
 [1.745]
 [1.208]
 [2.065]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.087]] [[-0.171]
 [-0.171]
 [-0.171]
 [-0.171]
 [-0.302]] [[0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.087]]
siam score:  -0.888509
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 0.6235198939222204
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.938]
 [1.065]
 [1.441]
 [0.81 ]
 [1.087]] [[1.838]
 [1.48 ]
 [0.663]
 [0.948]
 [1.596]] [[1.872]
 [1.675]
 [1.324]
 [0.871]
 [1.812]]
start point for exploration sampling:  20019
start point for exploration sampling:  20019
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0961 1.0 1.0
maxi score, test score, baseline:  0.0961 1.0 1.0
maxi score, test score, baseline:  0.0961 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20019
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]] [[0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]] [[0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0961 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0961 1.0 1.0
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 1.8986469449139016
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.947]
 [0.961]
 [0.947]
 [0.616]
 [0.545]] [[1.165]
 [2.478]
 [1.165]
 [0.995]
 [3.143]] [[1.248]
 [1.714]
 [1.248]
 [0.529]
 [1.104]]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.10010000000000001 1.0 1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1021 1.0 1.0
start point for exploration sampling:  20019
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.9027971
first move QE:  0.34272420077598953
Printing some Q and Qe and total Qs values:  [[0.124]
 [0.007]
 [0.232]
 [0.141]
 [0.156]] [[-0.739]
 [-0.689]
 [-1.2  ]
 [-1.009]
 [-0.913]] [[0.124]
 [0.007]
 [0.232]
 [0.141]
 [0.156]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1021 1.0 1.0
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.95 ]] [[2.124]
 [2.124]
 [2.124]
 [2.124]
 [2.541]] [[1.935]
 [1.935]
 [1.935]
 [1.935]
 [2.314]]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
in main func line 156:  1748
maxi score, test score, baseline:  0.1021 1.0 1.0
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.1021 1.0 1.0
maxi score, test score, baseline:  0.1021 1.0 1.0
siam score:  -0.8921993
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8902266
Printing some Q and Qe and total Qs values:  [[0.999]
 [1.031]
 [1.049]
 [0.999]
 [0.96 ]] [[3.608]
 [2.172]
 [2.675]
 [3.608]
 [3.426]] [[2.426]
 [1.265]
 [1.709]
 [2.426]
 [2.225]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.05 ]
 [1.187]
 [1.165]
 [1.12 ]
 [1.121]] [[3.841]
 [1.302]
 [1.589]
 [2.489]
 [2.104]] [[1.964]
 [0.771]
 [0.899]
 [1.325]
 [1.124]]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1061 1.0 1.0
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1061 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.904]
 [0.904]
 [0.904]
 [0.904]
 [0.865]] [[1.877]
 [1.877]
 [1.877]
 [1.877]
 [2.598]] [[1.716]
 [1.716]
 [1.716]
 [1.716]
 [2.337]]
maxi score, test score, baseline:  0.1061 1.0 1.0
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1061 1.0 1.0
line 256 mcts: sample exp_bonus 1.660549097177564
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 2.0811892328749018
maxi score, test score, baseline:  0.1061 1.0 1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.977]] [[1.999]
 [1.999]
 [1.999]
 [1.999]
 [2.671]] [[1.322]
 [1.322]
 [1.322]
 [1.322]
 [2.044]]
maxi score, test score, baseline:  0.1061 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.721]
 [0.561]
 [0.561]
 [0.561]] [[3.047]
 [3.143]
 [3.047]
 [3.047]
 [3.047]] [[2.145]
 [2.498]
 [2.145]
 [2.145]
 [2.145]]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8884747
maxi score, test score, baseline:  0.1061 1.0 1.0
siam score:  -0.8904158
maxi score, test score, baseline:  0.1061 1.0 1.0
line 256 mcts: sample exp_bonus 1.6049911822720109
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.1061 1.0 1.0
maxi score, test score, baseline:  0.1061 1.0 1.0
siam score:  -0.90288544
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.8196851834500272
Printing some Q and Qe and total Qs values:  [[1.156]
 [1.156]
 [1.156]
 [1.156]
 [1.006]] [[2.686]
 [2.686]
 [2.686]
 [2.686]
 [3.18 ]] [[2.181]
 [2.181]
 [2.181]
 [2.181]
 [2.325]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1061 1.0 1.0
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1061 1.0 1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
in main func line 156:  1794
maxi score, test score, baseline:  0.1081 1.0 1.0
line 256 mcts: sample exp_bonus 1.598470701328942
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1081 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.06 ]
 [1.112]
 [1.119]
 [1.091]
 [1.061]] [[4.22 ]
 [1.289]
 [1.758]
 [2.257]
 [3.617]] [[2.562]
 [1.356]
 [1.569]
 [1.749]
 [2.301]]
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1081 1.0 1.0
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
start point for exploration sampling:  20019
Printing some Q and Qe and total Qs values:  [[0.933]
 [0.305]
 [1.02 ]
 [0.828]
 [0.908]] [[1.592]
 [0.906]
 [1.653]
 [1.288]
 [1.788]] [[1.67 ]
 [0.217]
 [1.851]
 [1.319]
 [1.738]]
Printing some Q and Qe and total Qs values:  [[0.956]
 [1.083]
 [0.956]
 [0.956]
 [0.956]] [[0.449]
 [1.661]
 [0.449]
 [0.449]
 [0.449]] [[1.682]
 [2.58 ]
 [1.682]
 [1.682]
 [1.682]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
1807 9107
siam score:  -0.8942405
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1101 1.0 1.0
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1101 1.0 1.0
maxi score, test score, baseline:  0.1101 1.0 1.0
1817 9115
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.968]
 [1.18 ]
 [0.968]
 [0.968]
 [1.086]] [[0.718]
 [0.189]
 [0.718]
 [0.718]
 [1.412]] [[1.787]
 [1.68 ]
 [1.787]
 [1.787]
 [2.716]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.495]
 [0.489]
 [0.423]
 [0.442]] [[1.212]
 [1.145]
 [1.393]
 [1.297]
 [2.036]] [[0.297]
 [0.495]
 [0.489]
 [0.423]
 [0.442]]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
in main func line 156:  1819
Printing some Q and Qe and total Qs values:  [[0.853]
 [0.853]
 [0.984]
 [0.853]
 [0.853]] [[0.357]
 [0.357]
 [0.385]
 [0.357]
 [0.357]] [[1.252]
 [1.252]
 [1.525]
 [1.252]
 [1.252]]
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.925]
 [0.994]
 [1.014]
 [1.018]
 [1.033]] [[2.197]
 [2.623]
 [2.558]
 [3.092]
 [2.816]] [[1.365]
 [1.755]
 [1.719]
 [2.142]
 [1.936]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.917]
 [1.187]
 [0.917]
 [0.917]
 [0.917]] [[2.565]
 [2.815]
 [2.565]
 [2.565]
 [2.565]] [[1.884]
 [2.44 ]
 [1.884]
 [1.884]
 [1.884]]
line 256 mcts: sample exp_bonus 0.947602218859056
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
maxi score, test score, baseline:  0.11410000000000001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.3424802628027248
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.258]
 [1.135]
 [1.038]
 [0.805]
 [0.981]] [[0.681]
 [0.908]
 [1.192]
 [0.859]
 [1.893]] [[0.177]
 [2.007]
 [1.908]
 [1.33 ]
 [2.027]]
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.915]
 [0.844]
 [0.915]
 [0.915]
 [0.972]] [[2.643]
 [2.141]
 [2.643]
 [2.643]
 [2.576]] [[2.16 ]
 [1.784]
 [2.16 ]
 [2.16 ]
 [2.187]]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1181 1.0 1.0
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 3.4971166301688763
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.34 ]] [[0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.549]] [[0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.34 ]]
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.487]
 [0.632]
 [0.542]
 [0.52 ]] [[0.942]
 [1.19 ]
 [1.288]
 [0.979]
 [1.084]] [[0.46 ]
 [0.487]
 [0.632]
 [0.542]
 [0.52 ]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.878]] [[0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.772]] [[0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.878]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 1.4033567342998399
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  0.3410478128506889
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1857 9145
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.861]
 [0.861]
 [1.024]
 [0.897]
 [0.861]] [[0.759]
 [0.759]
 [1.278]
 [1.04 ]
 [0.759]] [[0.743]
 [0.743]
 [1.245]
 [0.977]
 [0.743]]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.943]
 [0.943]
 [0.943]
 [0.943]
 [0.943]] [[3.038]
 [3.038]
 [3.038]
 [3.038]
 [3.038]] [[2.364]
 [2.364]
 [2.364]
 [2.364]
 [2.364]]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.5194895194865847
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1861 1.0 1.0
maxi score, test score, baseline:  0.1861 1.0 1.0
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1861 1.0 1.0
siam score:  -0.90794146
maxi score, test score, baseline:  0.1861 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.129]
 [1.129]
 [1.129]
 [1.129]
 [1.196]] [[1.73 ]
 [1.73 ]
 [1.73 ]
 [1.73 ]
 [2.784]] [[1.218]
 [1.218]
 [1.218]
 [1.218]
 [2.311]]
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.1881 1.0 1.0
first move QE:  0.34003879364511325
siam score:  -0.9106589
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1881 1.0 1.0
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.90371275
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
siam score:  -0.9044807
Printing some Q and Qe and total Qs values:  [[1.003]
 [1.003]
 [1.003]
 [1.003]
 [1.033]] [[2.455]
 [2.455]
 [2.455]
 [2.455]
 [2.575]] [[1.996]
 [1.996]
 [1.996]
 [1.996]
 [2.152]]
maxi score, test score, baseline:  0.1901 1.0 1.0
maxi score, test score, baseline:  0.1901 1.0 1.0
siam score:  -0.90465605
maxi score, test score, baseline:  0.1901 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.143]
 [1.223]
 [1.143]
 [1.143]
 [1.143]] [[1.552]
 [2.616]
 [1.552]
 [1.552]
 [1.552]] [[1.836]
 [2.631]
 [1.836]
 [1.836]
 [1.836]]
maxi score, test score, baseline:  0.1901 1.0 1.0
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.128]
 [1.204]
 [1.128]
 [1.128]
 [1.128]] [[1.681]
 [1.286]
 [1.681]
 [1.681]
 [1.681]] [[2.833]
 [2.853]
 [2.833]
 [2.833]
 [2.833]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1921 1.0 1.0
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.1921 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.121]
 [1.23 ]
 [1.121]
 [1.121]
 [1.115]] [[2.446]
 [2.144]
 [2.446]
 [2.446]
 [2.29 ]] [[2.627]
 [2.479]
 [2.627]
 [2.627]
 [2.45 ]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1906 9166
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1941 1.0 1.0
maxi score, test score, baseline:  0.1941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1941 1.0 1.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1981 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.766]
 [0.819]
 [0.729]
 [0.766]
 [0.828]] [[ 0.   ]
 [-0.077]
 [-0.143]
 [ 0.   ]
 [ 0.054]] [[0.766]
 [0.819]
 [0.729]
 [0.766]
 [0.828]]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2001 1.0 1.0
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.9018762
in main func line 156:  1924
siam score:  -0.90201235
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]] [[-0.54]
 [-0.54]
 [-0.54]
 [-0.54]
 [-0.54]] [[0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]]
using explorer policy with actor:  1
in main func line 156:  1927
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20019
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.7213],
        [0.6482],
        [0.8151],
        [0.8914],
        [0.6503],
        [0.7722],
        [0.4590],
        [0.7715],
        [0.0863],
        [0.0000]], dtype=torch.float64)
0.0 0.7212954374487865
0.0 0.6482121739233399
0.0 0.8151028381761082
0.0 0.8913757983550252
0.0 0.6503463015059189
0.0 0.7721889664845083
0.0 0.45897154740416135
0.0 0.7715276459988838
0.0 0.08628583224312249
0.0 0.0
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.958]
 [0.958]
 [0.954]
 [0.958]
 [0.958]] [[1.595]
 [1.595]
 [1.938]
 [1.595]
 [1.595]] [[1.838]
 [1.838]
 [2.136]
 [1.838]
 [1.838]]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 2.037341308601511
Printing some Q and Qe and total Qs values:  [[0.937]
 [1.043]
 [1.012]
 [0.991]
 [0.983]] [[2.474]
 [2.547]
 [2.625]
 [2.414]
 [2.878]] [[1.361]
 [1.498]
 [1.534]
 [1.356]
 [1.704]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
siam score:  -0.9019713
siam score:  -0.90142107
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.829]
 [0.877]
 [0.816]
 [0.816]] [[2.015]
 [1.951]
 [2.809]
 [2.015]
 [2.015]] [[1.733]
 [1.694]
 [2.49 ]
 [1.733]
 [1.733]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.066]
 [0.056]
 [0.065]
 [0.066]
 [0.071]] [[4.99 ]
 [5.187]
 [5.695]
 [4.99 ]
 [5.765]] [[0.697]
 [0.783]
 [1.039]
 [0.697]
 [1.078]]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.339]
 [0.502]
 [0.339]
 [0.292]] [[0.809]
 [0.809]
 [1.021]
 [0.809]
 [0.747]] [[0.339]
 [0.339]
 [0.502]
 [0.339]
 [0.292]]
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.88489985
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1959 9199
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.114]
 [0.114]
 [0.114]
 [0.143]] [[1.737]
 [1.737]
 [1.737]
 [1.737]
 [0.955]] [[1.744]
 [1.744]
 [1.744]
 [1.744]
 [1.027]]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.944]
 [1.199]
 [0.944]
 [0.944]
 [0.944]] [[1.676]
 [2.201]
 [1.676]
 [1.676]
 [1.676]] [[1.187]
 [1.789]
 [1.187]
 [1.187]
 [1.187]]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.293]
 [0.746]
 [0.458]
 [0.431]] [[0.732]
 [0.601]
 [1.156]
 [0.732]
 [0.703]] [[0.796]
 [0.553]
 [1.377]
 [0.796]
 [0.75 ]]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.337]
 [0.951]
 [0.388]
 [0.388]] [[0.128]
 [0.336]
 [0.519]
 [0.128]
 [0.128]] [[0.72 ]
 [0.744]
 [1.627]
 [0.72 ]
 [0.72 ]]
Printing some Q and Qe and total Qs values:  [[1.251]
 [1.055]
 [1.055]
 [1.055]
 [1.055]] [[2.388]
 [1.798]
 [1.798]
 [1.798]
 [1.798]] [[2.789]
 [2.004]
 [2.004]
 [2.004]
 [2.004]]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.872258
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2161 1.0 1.0
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.269]
 [1.269]
 [1.269]
 [1.269]
 [1.224]] [[2.046]
 [2.046]
 [2.046]
 [2.046]
 [1.7  ]] [[2.414]
 [2.414]
 [2.414]
 [2.414]
 [2.104]]
maxi score, test score, baseline:  0.2161 1.0 1.0
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.88143003
Printing some Q and Qe and total Qs values:  [[1.033]
 [1.058]
 [1.117]
 [1.033]
 [1.039]] [[2.561]
 [1.971]
 [2.259]
 [2.561]
 [2.312]] [[2.397]
 [2.072]
 [2.317]
 [2.397]
 [2.254]]
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2161 1.0 1.0
first move QE:  0.33933385478835437
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]] [[5.294]
 [5.294]
 [5.294]
 [5.294]
 [5.294]] [[1.439]
 [1.439]
 [1.439]
 [1.439]
 [1.439]]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.114]
 [1.212]
 [1.076]
 [1.114]
 [1.123]] [[1.513]
 [1.217]
 [1.107]
 [1.513]
 [1.908]] [[1.904]
 [1.904]
 [1.594]
 [1.904]
 [2.157]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
in main func line 156:  1986
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.122]
 [1.117]
 [1.103]
 [0.8  ]
 [1.006]] [[6.888]
 [1.8  ]
 [1.948]
 [1.861]
 [3.055]] [[1.636]
 [0.412]
 [0.453]
 [0.311]
 [0.765]]
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.099]
 [0.646]
 [0.22 ]
 [0.082]] [[7.066]
 [7.066]
 [3.393]
 [4.626]
 [8.159]] [[1.418]
 [1.418]
 [0.273]
 [0.547]
 [1.826]]
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8833596
maxi score, test score, baseline:  0.2201 1.0 1.0
maxi score, test score, baseline:  0.2201 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus 2.2532707348066836
Printing some Q and Qe and total Qs values:  [[0.062]
 [0.129]
 [0.129]
 [0.129]
 [0.189]] [[7.255]
 [6.23 ]
 [6.23 ]
 [6.23 ]
 [5.599]] [[1.829]
 [1.481]
 [1.481]
 [1.481]
 [1.275]]
maxi score, test score, baseline:  0.2201 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]] [[6.663]
 [6.663]
 [6.663]
 [6.663]
 [6.802]] [[1.562]
 [1.562]
 [1.562]
 [1.562]
 [1.617]]
Printing some Q and Qe and total Qs values:  [[0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.076]] [[6.449]
 [6.449]
 [6.449]
 [6.449]
 [5.735]] [[2.018]
 [2.018]
 [2.018]
 [2.018]
 [1.601]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.116]
 [1.294]
 [1.257]
 [0.92 ]
 [1.104]] [[5.679]
 [2.23 ]
 [2.462]
 [3.04 ]
 [3.606]] [[1.764]
 [0.834]
 [0.914]
 [1.013]
 [1.32 ]]
maxi score, test score, baseline:  0.2201 1.0 1.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
Printing some Q and Qe and total Qs values:  [[1.041]
 [1.027]
 [1.041]
 [1.041]
 [1.005]] [[2.882]
 [2.539]
 [2.882]
 [2.882]
 [2.95 ]] [[2.072]
 [1.818]
 [2.072]
 [2.072]
 [2.083]]
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.1]
 [0.1]
 [0.1]
 [0.1]
 [0.1]] [[6.047]
 [6.047]
 [6.047]
 [6.047]
 [6.047]] [[2.08]
 [2.08]
 [2.08]
 [2.08]
 [2.08]]
maxi score, test score, baseline:  0.2221 1.0 1.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.093]
 [1.161]
 [1.139]
 [1.093]
 [1.035]] [[2.612]
 [2.5  ]
 [1.901]
 [2.612]
 [2.655]] [[2.091]
 [2.083]
 [1.544]
 [2.091]
 [2.055]]
maxi score, test score, baseline:  0.2241 1.0 1.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8982906
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2261 1.0 1.0
siam score:  -0.8987313
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
2013 9225
maxi score, test score, baseline:  0.2261 1.0 1.0
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
2013 9227
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.2301 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.36 ]
 [0.364]
 [0.601]
 [0.36 ]
 [0.36 ]] [[-0.04 ]
 [ 0.291]
 [ 0.716]
 [-0.04 ]
 [-0.04 ]] [[0.644]
 [0.813]
 [1.17 ]
 [0.644]
 [0.644]]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
UNIT TEST: sample policy line 217 mcts : [0.917 0.    0.    0.042 0.042]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2341 1.0 1.0
maxi score, test score, baseline:  0.2341 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.938]
 [0.938]
 [1.18 ]
 [0.938]
 [0.938]] [[1.162]
 [1.162]
 [1.656]
 [1.162]
 [1.162]] [[1.681]
 [1.681]
 [2.182]
 [1.681]
 [1.681]]
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.393]
 [1.149]
 [0.511]
 [0.511]] [[0.367]
 [0.704]
 [1.439]
 [0.367]
 [0.367]] [[0.526]
 [0.606]
 [1.54 ]
 [0.526]
 [0.526]]
maxi score, test score, baseline:  0.2341 1.0 1.0
using explorer policy with actor:  1
2029 9241
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8769523
siam score:  -0.8761183
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.304]
 [1.304]
 [1.304]
 [1.304]
 [1.304]] [[0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]] [[2.212]
 [2.212]
 [2.212]
 [2.212]
 [2.212]]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2030 9244
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
in main func line 156:  2033
using explorer policy with actor:  1
siam score:  -0.8852742
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.907]
 [1.024]
 [1.163]
 [1.089]
 [1.024]] [[0.518]
 [0.343]
 [0.811]
 [0.665]
 [0.343]] [[1.151]
 [1.326]
 [1.761]
 [1.564]
 [1.326]]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
siam score:  -0.8955402
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
2045 9250
Printing some Q and Qe and total Qs values:  [[1.146]
 [1.239]
 [1.146]
 [1.146]
 [1.146]] [[1.65 ]
 [1.597]
 [1.65 ]
 [1.65 ]
 [1.65 ]] [[2.397]
 [2.531]
 [2.397]
 [2.397]
 [2.397]]
actor:  1 policy actor:  1  step number:  93 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
line 256 mcts: sample exp_bonus 1.52725643529552
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 0.3868695993067251
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus -0.2439117495055978
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.89771485
line 256 mcts: sample exp_bonus 2.236674434363291
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.978]
 [1.325]
 [0.978]
 [0.978]
 [0.991]] [[1.909]
 [1.382]
 [1.909]
 [1.909]
 [2.105]] [[2.118]
 [1.905]
 [2.118]
 [2.118]
 [2.364]]
Printing some Q and Qe and total Qs values:  [[0.177]
 [0.177]
 [0.177]
 [0.177]
 [0.181]] [[1.398]
 [1.398]
 [1.398]
 [1.398]
 [1.509]] [[1.946]
 [1.946]
 [1.946]
 [1.946]
 [2.075]]
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.059]
 [1.059]
 [1.17 ]
 [1.059]
 [1.059]] [[1.126]
 [1.126]
 [1.83 ]
 [1.126]
 [1.126]] [[1.494]
 [1.494]
 [1.964]
 [1.494]
 [1.494]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.069]
 [1.069]
 [1.069]
 [1.069]
 [1.036]] [[2.203]
 [2.203]
 [2.203]
 [2.203]
 [2.65 ]] [[2.1  ]
 [2.1  ]
 [2.1  ]
 [2.1  ]
 [2.343]]
maxi score, test score, baseline:  0.2981 1.0 1.0
maxi score, test score, baseline:  0.2981 1.0 1.0
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.162]
 [1.162]
 [1.162]
 [1.128]
 [1.205]] [[0.721]
 [0.721]
 [0.721]
 [1.023]
 [0.898]] [[1.829]
 [1.829]
 [1.829]
 [2.061]
 [2.092]]
maxi score, test score, baseline:  0.2981 1.0 1.0
siam score:  -0.86519516
maxi score, test score, baseline:  0.2981 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.18 ]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.184]] [[1.723]
 [1.723]
 [1.723]
 [1.723]
 [2.318]] [[0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [1.126]]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.482]
 [1.315]
 [1.224]
 [1.005]
 [1.178]] [[2.323]
 [1.437]
 [1.216]
 [1.643]
 [1.623]] [[1.369]
 [1.852]
 [1.602]
 [1.596]
 [1.797]]
Printing some Q and Qe and total Qs values:  [[0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.075]] [[4.157]
 [4.157]
 [4.157]
 [4.157]
 [3.95 ]] [[1.49 ]
 [1.49 ]
 [1.49 ]
 [1.49 ]
 [1.335]]
maxi score, test score, baseline:  0.2981 1.0 1.0
2067 9260
siam score:  -0.86656874
Printing some Q and Qe and total Qs values:  [[1.363]
 [1.356]
 [1.363]
 [1.363]
 [1.375]] [[0.503]
 [0.517]
 [0.503]
 [0.503]
 [0.888]] [[2.161]
 [2.161]
 [2.161]
 [2.161]
 [2.57 ]]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.05 ]
 [0.069]
 [0.323]
 [0.069]
 [0.097]] [[2.815]
 [3.968]
 [1.942]
 [3.968]
 [3.268]] [[1.466]
 [2.807]
 [0.779]
 [2.807]
 [2.038]]
actor:  1 policy actor:  1  step number:  95 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.87345386
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.238]
 [1.238]
 [1.238]
 [1.238]
 [1.238]] [[1.507]
 [1.507]
 [1.507]
 [1.507]
 [1.507]] [[2.863]
 [2.863]
 [2.863]
 [2.863]
 [2.863]]
2080 9267
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2081 9267
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3041 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3081 1.0 1.0
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
2086 9274
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3081 1.0 1.0
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
start point for exploration sampling:  20019
siam score:  -0.88677
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8890064
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.40610531787841836
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.3101 1.0 1.0
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8803157
siam score:  -0.8812923
using explorer policy with actor:  1
2100 9279
line 256 mcts: sample exp_bonus -0.944225404618181
first move QE:  0.3340065421752864
maxi score, test score, baseline:  0.3121 1.0 1.0
siam score:  -0.88098764
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8810364
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.88096267
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.188]] [[7.143]
 [7.143]
 [7.143]
 [7.143]
 [7.409]] [[1.894]
 [1.894]
 [1.894]
 [1.894]
 [2.047]]
Printing some Q and Qe and total Qs values:  [[0.217]
 [0.313]
 [0.293]
 [0.277]
 [0.266]] [[1.989]
 [2.766]
 [1.777]
 [1.36 ]
 [3.256]] [[0.488]
 [1.071]
 [0.426]
 [0.146]
 [1.337]]
2106 9282
Printing some Q and Qe and total Qs values:  [[0.919]
 [0.919]
 [1.045]
 [0.919]
 [0.919]] [[0.737]
 [0.737]
 [1.136]
 [0.737]
 [0.737]] [[1.712]
 [1.712]
 [2.362]
 [1.712]
 [1.712]]
Printing some Q and Qe and total Qs values:  [[0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.137]] [[5.972]
 [5.972]
 [5.972]
 [5.972]
 [6.626]] [[1.759]
 [1.759]
 [1.759]
 [1.759]
 [1.99 ]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.261]
 [1.381]
 [1.354]
 [1.261]
 [1.158]] [[1.097]
 [0.399]
 [0.391]
 [1.097]
 [0.873]] [[1.976]
 [1.983]
 [1.926]
 [1.976]
 [1.696]]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
in main func line 156:  2113
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3221 1.0 1.0
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3221 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.134]
 [1.134]
 [1.399]
 [1.134]
 [1.134]] [[2.143]
 [2.143]
 [1.283]
 [2.143]
 [2.143]] [[3.391]
 [3.391]
 [2.773]
 [3.391]
 [3.391]]
Printing some Q and Qe and total Qs values:  [[1.227]
 [1.399]
 [1.411]
 [1.227]
 [1.267]] [[0.596]
 [0.476]
 [0.615]
 [0.596]
 [1.193]] [[1.817]
 [2.069]
 [2.18 ]
 [1.817]
 [2.273]]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.884427
maxi score, test score, baseline:  0.3221 1.0 1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.048]
 [1.157]
 [1.184]
 [1.048]
 [1.063]] [[0.89 ]
 [0.77 ]
 [0.876]
 [0.89 ]
 [1.101]] [[1.769]
 [1.906]
 [2.032]
 [1.769]
 [1.94 ]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.05 ]
 [1.008]
 [1.16 ]
 [1.05 ]
 [1.063]] [[0.921]
 [0.997]
 [1.027]
 [0.921]
 [1.729]] [[1.835]
 [1.812]
 [2.037]
 [1.835]
 [2.227]]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3221 1.0 1.0
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3221 1.0 1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
start point for exploration sampling:  20019
siam score:  -0.8762242
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 5.18900158944844
maxi score, test score, baseline:  0.3221 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8732048
maxi score, test score, baseline:  0.3221 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.127]
 [1.148]
 [1.127]
 [1.127]
 [1.104]] [[2.889]
 [2.674]
 [2.889]
 [2.889]
 [2.922]] [[2.352]
 [2.207]
 [2.352]
 [2.352]
 [2.351]]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.197]
 [1.197]
 [1.197]
 [1.197]
 [1.197]] [[0.876]
 [0.876]
 [0.876]
 [0.876]
 [0.876]] [[1.983]
 [1.983]
 [1.983]
 [1.983]
 [1.983]]
maxi score, test score, baseline:  0.3241 1.0 1.0
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3241 1.0 1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.082]
 [1.128]
 [1.082]
 [1.082]
 [1.066]] [[2.734]
 [2.788]
 [2.734]
 [2.734]
 [2.851]] [[2.109]
 [2.206]
 [2.109]
 [2.109]
 [2.197]]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.114]
 [0.114]
 [0.044]
 [0.072]] [[4.5  ]
 [4.5  ]
 [4.5  ]
 [3.263]
 [4.792]] [[1.675]
 [1.675]
 [1.675]
 [0.681]
 [1.856]]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3241 1.0 1.0
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.88490754
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3241 1.0 1.0
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.082]
 [0.082]
 [0.086]
 [0.082]
 [0.078]] [[3.865]
 [3.865]
 [6.335]
 [3.865]
 [5.945]] [[0.56 ]
 [0.56 ]
 [1.54 ]
 [0.56 ]
 [1.382]]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.88301927
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
2163 9316
Printing some Q and Qe and total Qs values:  [[1.081]
 [1.081]
 [1.081]
 [1.081]
 [1.081]] [[3.198]
 [3.198]
 [3.198]
 [3.198]
 [3.198]] [[2.558]
 [2.558]
 [2.558]
 [2.558]
 [2.558]]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3321 1.0 1.0
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 0.10138855063801608
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3321 1.0 1.0
maxi score, test score, baseline:  0.3321 1.0 1.0
maxi score, test score, baseline:  0.3321 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.395299771011726
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
UNIT TEST: sample policy line 217 mcts : [0.042 0.333 0.    0.042 0.583]
siam score:  -0.8758581
maxi score, test score, baseline:  0.3321 1.0 1.0
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3321 1.0 1.0
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.3341 1.0 1.0
first move QE:  0.32467368167988286
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]] [[0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]] [[0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]]
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.3381 1.0 1.0
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 2.028877828278488
maxi score, test score, baseline:  0.3381 1.0 1.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2206 9346
siam score:  -0.8706613
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.296]] [[3.311]
 [3.311]
 [3.311]
 [3.311]
 [4.042]] [[1.483]
 [1.483]
 [1.483]
 [1.483]
 [2.121]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3381 1.0 1.0
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.3381 1.0 1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.975]
 [0.984]
 [1.013]
 [0.975]
 [0.931]] [[1.366]
 [1.039]
 [0.631]
 [1.366]
 [1.192]] [[1.857]
 [1.657]
 [1.444]
 [1.857]
 [1.653]]
Printing some Q and Qe and total Qs values:  [[1.236]
 [1.236]
 [1.236]
 [1.236]
 [1.211]] [[1.193]
 [1.193]
 [1.193]
 [1.193]
 [1.479]] [[2.326]
 [2.326]
 [2.326]
 [2.326]
 [2.467]]
maxi score, test score, baseline:  0.3381 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3401 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.842]
 [1.013]
 [0.842]
 [0.842]
 [0.741]] [[0.241]
 [0.512]
 [0.241]
 [0.241]
 [0.38 ]] [[0.977]
 [1.304]
 [0.977]
 [0.977]
 [0.969]]
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3401 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.265]] [[3.312]
 [3.312]
 [3.312]
 [3.312]
 [4.489]] [[0.997]
 [0.997]
 [0.997]
 [0.997]
 [1.71 ]]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.854]
 [1.075]
 [0.971]
 [0.993]
 [0.918]] [[2.503]
 [1.161]
 [1.324]
 [2.271]
 [1.923]] [[1.947]
 [0.978]
 [1.029]
 [1.866]
 [1.501]]
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3401 1.0 1.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]] [[6.06]
 [6.06]
 [6.06]
 [6.06]
 [6.06]] [[1.698]
 [1.698]
 [1.698]
 [1.698]
 [1.698]]
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]] [[2.901]
 [2.901]
 [2.901]
 [2.901]
 [2.901]] [[1.533]
 [1.533]
 [1.533]
 [1.533]
 [1.533]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.446]
 [0.364]
 [0.364]
 [0.414]] [[2.209]
 [1.674]
 [1.583]
 [1.583]
 [1.572]] [[0.96 ]
 [0.868]
 [0.644]
 [0.644]
 [0.738]]
2232 9355
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3401 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.215]
 [0.274]
 [0.258]
 [0.097]
 [0.202]] [[6.026]
 [4.439]
 [3.558]
 [3.365]
 [5.05 ]] [[1.5  ]
 [0.977]
 [0.652]
 [0.469]
 [1.143]]
maxi score, test score, baseline:  0.3401 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.447]
 [0.447]
 [0.447]
 [0.425]] [[1.307]
 [1.307]
 [1.307]
 [1.307]
 [1.304]] [[1.305]
 [1.305]
 [1.305]
 [1.305]
 [1.28 ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.549]
 [1.179]
 [1.179]
 [1.179]
 [1.192]] [[5.726]
 [3.069]
 [3.069]
 [3.069]
 [3.056]] [[1.799]
 [1.136]
 [1.136]
 [1.136]
 [1.138]]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.725]
 [0.725]
 [0.725]
 [0.725]
 [0.624]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.406]] [[0.823]
 [0.823]
 [0.823]
 [0.823]
 [1.163]]
siam score:  -0.8607284
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.441]
 [0.568]
 [0.441]
 [0.473]] [[6.395]
 [7.298]
 [6.143]
 [7.298]
 [7.12 ]] [[1.68 ]
 [2.029]
 [1.737]
 [2.029]
 [1.99 ]]
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]] [[5.439]
 [5.439]
 [5.439]
 [5.439]
 [5.439]] [[2.081]
 [2.081]
 [2.081]
 [2.081]
 [2.081]]
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.54 ]] [[2.282]
 [2.282]
 [2.282]
 [2.282]
 [2.669]] [[1.644]
 [1.644]
 [1.644]
 [1.644]
 [1.945]]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.06 ]
 [0.671]
 [0.551]
 [0.592]] [[1.377]
 [1.635]
 [1.794]
 [1.377]
 [1.823]] [[0.551]
 [0.06 ]
 [0.671]
 [0.551]
 [0.592]]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.002]
 [0.368]
 [0.179]
 [0.198]] [[-0.659]
 [-0.081]
 [-0.346]
 [-0.367]
 [-0.327]] [[0.204]
 [0.002]
 [0.368]
 [0.179]
 [0.198]]
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.596]
 [0.664]
 [0.57 ]
 [0.573]] [[0.788]
 [1.045]
 [0.481]
 [0.788]
 [1.346]] [[0.57 ]
 [0.596]
 [0.664]
 [0.57 ]
 [0.573]]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.741]
 [0.623]
 [0.605]
 [0.552]] [[1.714]
 [0.869]
 [0.787]
 [0.576]
 [1.655]] [[0.602]
 [0.741]
 [0.623]
 [0.605]
 [0.552]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 0.4893944644179547
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8694678
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.378]
 [0.39 ]
 [0.378]
 [0.387]] [[-0.33 ]
 [-0.33 ]
 [ 0.329]
 [-0.33 ]
 [ 1.115]] [[0.378]
 [0.378]
 [0.39 ]
 [0.378]
 [0.387]]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
2247 9362
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20019
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
2251 9365
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3961 1.0 1.0
2254 9370
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
line 256 mcts: sample exp_bonus 0.6512554401247506
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3961 1.0 1.0
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3981 1.0 1.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.838597331636662
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]] [[2.941]
 [2.941]
 [2.941]
 [2.941]
 [2.941]] [[2.112]
 [2.112]
 [2.112]
 [2.112]
 [2.112]]
first move QE:  0.32713048734216066
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3981 1.0 1.0
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4001 1.0 1.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.154]
 [1.049]
 [1.151]
 [1.154]
 [1.075]] [[0.939]
 [1.062]
 [1.125]
 [0.939]
 [1.38 ]] [[2.609]
 [2.452]
 [2.662]
 [2.609]
 [2.6  ]]
Printing some Q and Qe and total Qs values:  [[1.052]
 [0.974]
 [1.181]
 [1.052]
 [1.034]] [[2.887]
 [1.998]
 [2.621]
 [2.887]
 [2.872]] [[2.274]
 [1.403]
 [2.21 ]
 [2.274]
 [2.237]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.98 ]
 [0.98 ]
 [1.086]
 [0.98 ]
 [0.98 ]] [[0.125]
 [0.125]
 [0.621]
 [0.125]
 [0.125]] [[1.602]
 [1.602]
 [2.145]
 [1.602]
 [1.602]]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8797708
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.9872988244346566
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.6141088641118126
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4041 1.0 1.0
siam score:  -0.8827675
maxi score, test score, baseline:  0.4041 1.0 1.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4061 1.0 1.0
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4061 1.0 1.0
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.04 ]
 [1.04 ]
 [1.157]
 [1.04 ]
 [1.021]] [[1.806]
 [1.806]
 [2.545]
 [1.806]
 [1.894]] [[1.652]
 [1.652]
 [2.466]
 [1.652]
 [1.712]]
Printing some Q and Qe and total Qs values:  [[0.864]
 [0.864]
 [0.697]
 [0.864]
 [0.864]] [[0.441]
 [0.441]
 [0.559]
 [0.441]
 [0.441]] [[1.495]
 [1.495]
 [1.279]
 [1.495]
 [1.495]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
siam score:  -0.8780133
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]] [[6.104]
 [6.104]
 [6.104]
 [6.104]
 [6.104]] [[1.488]
 [1.488]
 [1.488]
 [1.488]
 [1.488]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
line 256 mcts: sample exp_bonus 4.4479943482566355
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.055]
 [1.055]
 [1.055]
 [1.055]
 [1.055]] [[2.018]
 [2.018]
 [2.018]
 [2.018]
 [2.018]] [[2.228]
 [2.228]
 [2.228]
 [2.228]
 [2.228]]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.675]
 [0.717]
 [0.675]
 [0.656]] [[1.342]
 [1.342]
 [1.704]
 [1.342]
 [1.605]] [[0.675]
 [0.675]
 [0.717]
 [0.675]
 [0.656]]
Printing some Q and Qe and total Qs values:  [[0.859]
 [0.883]
 [0.907]
 [0.859]
 [0.827]] [[2.169]
 [1.304]
 [1.277]
 [2.169]
 [2.323]] [[0.859]
 [0.883]
 [0.907]
 [0.859]
 [0.827]]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.887]] [[2.938]
 [2.938]
 [2.938]
 [2.938]
 [3.716]] [[1.285]
 [1.285]
 [1.285]
 [1.285]
 [1.765]]
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.382]] [[4.256]
 [4.256]
 [4.256]
 [4.256]
 [4.726]] [[1.333]
 [1.333]
 [1.333]
 [1.333]
 [1.519]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
2326 9402
siam score:  -0.86893386
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.285]
 [1.409]
 [1.454]
 [1.335]
 [1.278]] [[2.518]
 [2.494]
 [2.196]
 [1.977]
 [2.678]] [[2.149]
 [2.269]
 [2.036]
 [1.69 ]
 [2.295]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.87366426
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
start point for exploration sampling:  20019
Printing some Q and Qe and total Qs values:  [[1.158]
 [1.158]
 [1.158]
 [1.158]
 [1.158]] [[3.513]
 [3.513]
 [3.513]
 [3.513]
 [3.513]] [[2.479]
 [2.479]
 [2.479]
 [2.479]
 [2.479]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
Printing some Q and Qe and total Qs values:  [[0.848]
 [1.099]
 [1.411]
 [0.923]
 [1.226]] [[1.877]
 [1.167]
 [0.943]
 [1.806]
 [1.322]] [[1.903]
 [1.557]
 [1.81 ]
 [1.946]
 [1.915]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.34 ]
 [1.209]
 [1.209]
 [1.209]
 [1.209]] [[1.604]
 [0.746]
 [0.746]
 [0.746]
 [0.746]] [[2.545]
 [1.627]
 [1.627]
 [1.627]
 [1.627]]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.86868864
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.87374854
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.709]] [[2.558]
 [2.558]
 [2.558]
 [2.558]
 [3.086]] [[1.44 ]
 [1.44 ]
 [1.44 ]
 [1.44 ]
 [1.771]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.058]
 [1.058]
 [1.058]
 [1.058]
 [1.039]] [[2.538]
 [2.538]
 [2.538]
 [2.538]
 [3.001]] [[1.972]
 [1.972]
 [1.972]
 [1.972]
 [2.263]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.815]
 [1.293]
 [0.815]
 [0.815]
 [1.183]] [[2.955]
 [2.891]
 [2.955]
 [2.955]
 [2.574]] [[2.024]
 [2.425]
 [2.024]
 [2.024]
 [2.013]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.944]
 [0.989]
 [0.448]
 [0.539]] [[5.05 ]
 [1.947]
 [1.465]
 [1.602]
 [4.291]] [[1.478]
 [0.49 ]
 [0.318]
 [0.114]
 [1.234]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]] [[4.471]
 [4.471]
 [4.471]
 [4.471]
 [4.471]] [[5.332]
 [5.332]
 [5.332]
 [5.332]
 [5.332]]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4201 1.0 1.0
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20019
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 1.173228084837404
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.392]
 [0.392]
 [0.392]
 [0.428]] [[5.73 ]
 [3.732]
 [3.732]
 [3.732]
 [4.323]] [[2.089]
 [1.214]
 [1.214]
 [1.214]
 [1.491]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.87303877
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.198]
 [1.198]
 [1.198]
 [1.198]
 [1.198]] [[2.326]
 [2.326]
 [2.326]
 [2.326]
 [2.326]] [[2.874]
 [2.874]
 [2.874]
 [2.874]
 [2.874]]
Printing some Q and Qe and total Qs values:  [[1.193]
 [1.214]
 [1.193]
 [1.094]
 [1.159]] [[1.501]
 [1.83 ]
 [1.501]
 [2.156]
 [2.12 ]] [[1.59 ]
 [1.9  ]
 [1.59 ]
 [1.972]
 [2.05 ]]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.4281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20019
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
maxi score, test score, baseline:  0.4281 1.0 1.0
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8647858
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4301 1.0 1.0
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4301 1.0 1.0
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4301 1.0 1.0
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.98 ]
 [0.98 ]
 [1.063]
 [0.98 ]
 [0.98 ]] [[0.426]
 [0.426]
 [0.641]
 [0.426]
 [0.426]] [[1.535]
 [1.535]
 [1.846]
 [1.535]
 [1.535]]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
in main func line 156:  2400
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8511206
Printing some Q and Qe and total Qs values:  [[1.26]
 [1.26]
 [1.26]
 [1.26]
 [1.26]] [[2.535]
 [2.535]
 [2.535]
 [2.535]
 [2.535]] [[2.395]
 [2.395]
 [2.395]
 [2.395]
 [2.395]]
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.3248407888489337
maxi score, test score, baseline:  0.4361 1.0 1.0
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.319]
 [0.319]
 [0.319]
 [0.338]] [[3.693]
 [3.693]
 [3.693]
 [3.693]
 [5.166]] [[0.641]
 [0.641]
 [0.641]
 [0.641]
 [1.053]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4381 1.0 1.0
maxi score, test score, baseline:  0.4381 1.0 1.0
siam score:  -0.8604132
Printing some Q and Qe and total Qs values:  [[0.418]
 [1.158]
 [1.158]
 [1.158]
 [1.285]] [[1.945]
 [0.785]
 [0.785]
 [0.785]
 [0.933]] [[1.296]
 [2.003]
 [2.003]
 [2.003]
 [2.356]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4401 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.57 ]
 [0.629]
 [0.556]
 [0.632]] [[1.533]
 [1.533]
 [1.982]
 [1.5  ]
 [1.771]] [[0.57 ]
 [0.57 ]
 [0.629]
 [0.556]
 [0.632]]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4421 1.0 1.0
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.058]
 [1.058]
 [1.058]
 [1.058]
 [1.058]] [[0.493]
 [0.493]
 [0.87 ]
 [0.493]
 [0.493]] [[1.93 ]
 [1.93 ]
 [2.055]
 [1.93 ]
 [1.93 ]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.87344724
first move QE:  0.32545099136439976
maxi score, test score, baseline:  0.4421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
2421 9437
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4461 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4481 1.0 1.0
siam score:  -0.8586922
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.682]] [[4.294]
 [4.294]
 [4.294]
 [4.294]
 [4.446]] [[1.413]
 [1.413]
 [1.413]
 [1.413]
 [1.488]]
maxi score, test score, baseline:  0.4561 1.0 1.0
maxi score, test score, baseline:  0.4561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8632382
maxi score, test score, baseline:  0.4561 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.328]
 [0.46 ]
 [0.242]
 [0.383]] [[1.237]
 [4.057]
 [4.175]
 [3.539]
 [5.324]] [[0.019]
 [1.202]
 [1.32 ]
 [0.96 ]
 [1.707]]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
UNIT TEST: sample policy line 217 mcts : [0.    0.417 0.    0.25  0.333]
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.05 ]
 [0.382]
 [0.401]
 [0.316]
 [0.394]] [[-0.241]
 [ 0.339]
 [ 0.14 ]
 [ 0.   ]
 [ 0.192]] [[0.05 ]
 [0.382]
 [0.401]
 [0.316]
 [0.394]]
Printing some Q and Qe and total Qs values:  [[0.847]
 [0.9  ]
 [0.834]
 [0.847]
 [0.849]] [[ 0.511]
 [-0.076]
 [ 0.964]
 [ 0.511]
 [ 0.929]] [[0.847]
 [0.9  ]
 [0.834]
 [0.847]
 [0.849]]
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.938]
 [0.945]
 [0.905]
 [0.936]] [[0.063]
 [0.435]
 [0.396]
 [0.299]
 [0.667]] [[0.51 ]
 [0.938]
 [0.945]
 [0.905]
 [0.936]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5101 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5101 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.41 ]
 [1.432]
 [1.428]
 [1.41 ]
 [1.4  ]] [[0.205]
 [0.161]
 [0.25 ]
 [0.205]
 [0.662]] [[2.162]
 [2.175]
 [2.228]
 [2.162]
 [2.446]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8705741
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
first move QE:  0.32462812526163837
maxi score, test score, baseline:  0.5141 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5161 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.7207808601573359
Printing some Q and Qe and total Qs values:  [[1.096]
 [0.092]
 [1.173]
 [1.096]
 [1.096]] [[0.749]
 [0.185]
 [1.22 ]
 [0.749]
 [0.749]] [[1.112]
 [0.065]
 [1.444]
 [1.112]
 [1.112]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.5161 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.8629891
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.07]
 [1.07]
 [1.07]
 [1.07]
 [1.05]] [[2.34 ]
 [2.34 ]
 [2.34 ]
 [2.34 ]
 [2.395]] [[2.148]
 [2.148]
 [2.148]
 [2.148]
 [2.178]]
first move QE:  0.32445734939488474
maxi score, test score, baseline:  0.5181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5181 1.0 1.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
first move QE:  0.3244678391811572
maxi score, test score, baseline:  0.5201 1.0 1.0
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.103]
 [1.103]
 [1.183]
 [1.103]
 [1.103]] [[0.607]
 [0.607]
 [0.446]
 [0.607]
 [0.607]] [[2.185]
 [2.185]
 [2.292]
 [2.185]
 [2.185]]
Printing some Q and Qe and total Qs values:  [[0.57]
 [0.57]
 [0.57]
 [0.57]
 [0.57]] [[2.442]
 [2.442]
 [2.442]
 [2.442]
 [2.442]] [[1.368]
 [1.368]
 [1.368]
 [1.368]
 [1.368]]
line 256 mcts: sample exp_bonus 2.842873994112164
maxi score, test score, baseline:  0.5201 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.249]
 [1.242]
 [1.19 ]
 [1.249]
 [1.247]] [[2.685]
 [2.739]
 [2.434]
 [2.685]
 [2.801]] [[2.308]
 [2.342]
 [2.073]
 [2.308]
 [2.392]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8590578
maxi score, test score, baseline:  0.5221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.022]
 [1.018]
 [1.018]
 [1.018]
 [1.018]] [[2.627]
 [2.423]
 [2.423]
 [2.423]
 [2.423]] [[2.17 ]
 [2.058]
 [2.058]
 [2.058]
 [2.058]]
line 256 mcts: sample exp_bonus 0.6610390253391193
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.303573853904759
Printing some Q and Qe and total Qs values:  [[1.067]
 [1.067]
 [1.067]
 [1.067]
 [1.229]] [[1.873]
 [1.873]
 [1.873]
 [1.873]
 [2.339]] [[1.538]
 [1.538]
 [1.538]
 [1.538]
 [2.251]]
Printing some Q and Qe and total Qs values:  [[1.127]
 [1.127]
 [1.127]
 [1.127]
 [1.127]] [[1.855]
 [1.855]
 [1.855]
 [1.855]
 [1.855]] [[5.964]
 [5.964]
 [5.964]
 [5.964]
 [5.964]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8575879
maxi score, test score, baseline:  0.5241 1.0 1.0
maxi score, test score, baseline:  0.5241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.179]
 [1.19 ]
 [1.179]
 [1.179]
 [1.084]] [[2.319]
 [1.763]
 [2.319]
 [2.319]
 [2.272]] [[2.602]
 [1.934]
 [2.602]
 [2.602]
 [2.429]]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.042 0.25  0.208 0.125 0.375]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.221]
 [1.325]
 [1.221]
 [1.221]
 [1.198]] [[0.031]
 [0.04 ]
 [0.031]
 [0.031]
 [0.453]] [[1.558]
 [1.769]
 [1.558]
 [1.558]
 [1.653]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20019
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.52 ]] [[3.008]
 [3.008]
 [3.008]
 [3.008]
 [4.019]] [[1.05 ]
 [1.05 ]
 [1.05 ]
 [1.05 ]
 [1.903]]
maxi score, test score, baseline:  0.5241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5241 1.0 1.0
maxi score, test score, baseline:  0.5241 1.0 1.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.171]
 [1.227]
 [1.126]
 [1.096]
 [1.115]] [[0.764]
 [0.461]
 [0.788]
 [1.061]
 [1.109]] [[1.954]
 [1.863]
 [1.879]
 [2.001]
 [2.072]]
2489 9461
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8624091
siam score:  -0.8629256
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5261 1.0 1.0
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.88 ]] [[3.965]
 [3.965]
 [3.965]
 [3.965]
 [3.961]] [[2.057]
 [2.057]
 [2.057]
 [2.057]
 [2.031]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5301 1.0 1.0
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5301 1.0 1.0
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.136]
 [1.136]
 [1.136]
 [1.136]
 [1.136]] [[0.882]
 [0.882]
 [0.882]
 [0.882]
 [0.882]] [[2.016]
 [2.016]
 [2.016]
 [2.016]
 [2.016]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5321 1.0 1.0
maxi score, test score, baseline:  0.5321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.803]
 [0.933]
 [0.798]
 [0.54 ]
 [0.732]] [[2.059]
 [2.868]
 [3.776]
 [8.572]
 [8.405]] [[0.475]
 [0.756]
 [0.874]
 [1.815]
 [1.911]]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.5321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.32324080039399034
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5401 1.0 1.0
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.155]
 [1.155]
 [1.198]
 [1.155]
 [1.155]] [[0.992]
 [0.992]
 [1.544]
 [0.992]
 [0.992]] [[2.035]
 [2.035]
 [2.449]
 [2.035]
 [2.035]]
start point for exploration sampling:  20019
maxi score, test score, baseline:  0.5441 1.0 1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.25 ]
 [0.053]
 [0.211]
 [0.441]
 [0.509]] [[0.218]
 [0.611]
 [1.428]
 [1.248]
 [1.539]] [[0.224]
 [0.393]
 [1.229]
 [1.272]
 [1.58 ]]
Printing some Q and Qe and total Qs values:  [[1.093]
 [1.127]
 [1.098]
 [0.999]
 [0.999]] [[1.775]
 [1.589]
 [1.857]
 [1.691]
 [1.691]] [[2.576]
 [2.429]
 [2.675]
 [2.332]
 [2.332]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.404]
 [1.404]
 [1.436]
 [1.404]
 [1.367]] [[0.234]
 [0.234]
 [0.264]
 [0.234]
 [0.203]] [[2.   ]
 [2.   ]
 [2.082]
 [2.   ]
 [1.905]]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
2525 9489
maxi score, test score, baseline:  0.5481 1.0 1.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  91 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5501 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5521 1.0 1.0
maxi score, test score, baseline:  0.5521 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.508]
 [0.337]
 [0.508]
 [0.328]] [[3.62 ]
 [3.62 ]
 [3.206]
 [3.62 ]
 [4.226]] [[1.743]
 [1.743]
 [1.381]
 [1.743]
 [1.924]]
maxi score, test score, baseline:  0.5521 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5541 1.0 1.0
siam score:  -0.8481508
maxi score, test score, baseline:  0.5541 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.771]
 [0.726]
 [0.534]
 [0.589]
 [0.633]] [[1.362]
 [3.591]
 [3.427]
 [6.071]
 [6.258]] [[0.408]
 [1.031]
 [0.81 ]
 [1.647]
 [1.742]]
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.84477997
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5561 1.0 1.0
