dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 25}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
res_block_channels:[32, 64, 64]
res_block_ds:[False, False, False]
reward_support:[-1, 1, 41]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64, 64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 41]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:2
max_workers:32
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
resampling:False
resampling_use_max:False
resampling_assess_best_child:False
rs_start:1000
ep_to_batch_ratio:[11, 12]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
env:<class 'game_play.frozen_lakeGym_Image.gridWorld'>
same_env_each_time:True
channels:3
env_size:[8, 8]
observable_size:[8, 8]
game_modes:1
env_map:[['S' 'H' 'F' 'F' 'F' 'H' 'F' 'F']
 ['F' 'F' 'F' 'H' 'F' 'H' 'H' 'F']
 ['H' 'F' 'F' 'H' 'F' 'F' 'F' 'H']
 ['H' 'F' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'H' 'H' 'F' 'H' 'F']
 ['H' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'G']]
max_steps:100
actions_size:5
optimal_score:1
total_frames:205000
exp_gamma:0.95
atari_env:False
reward_clipping:False
memory_size:100
image_size:[48, 48]
timesteps_in_obs:2
store_prev_actions:True
running_reward_in_obs:False
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 1, 6]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
state_size:[6, 6]
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 64)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:25
detach_expV_calc:True
use_new_episode_expV:True
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['S' 'H' 'F' 'F' 'F' 'H' 'F' 'F']
 ['F' 'F' 'F' 'H' 'F' 'H' 'H' 'F']
 ['H' 'F' 'F' 'H' 'F' 'F' 'F' 'H']
 ['H' 'F' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'H' 'H' 'F' 'H' 'F']
 ['H' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'G']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
0 69
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1 115
1 149
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  0.007688954239711165
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.04954558946025957
maxi score, test score, baseline:  0.0001 0.0 0.0001
deleting a thread, now have 2 threads
Frames:  1121 train batches done:  32 episodes:  192
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.21457022
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
siam score:  -0.4171015
siam score:  -0.4206904
maxi score, test score, baseline:  0.0001 0.0 0.0001
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
2 226
2 235
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
2 266
3 270
deleting a thread, now have 2 threads
Frames:  1721 train batches done:  110 episodes:  282
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
3 296
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.38493878
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
6 357
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]] [[0]
 [0]
 [0]
 [0]
 [0]] [[3.]
 [3.]
 [3.]
 [3.]
 [3.]]
8 403
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.31976468645840506
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.004]
 [0.002]
 [0.001]] [[-0.213]
 [ 0.   ]
 [-0.064]
 [ 0.   ]
 [-0.072]] [[0.001]
 [0.002]
 [0.004]
 [0.002]
 [0.001]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
8 476
9 486
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
9 549
9 570
10 570
10 576
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.13822183
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
11 640
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.2 0.2 0.2 0.2 0.2]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.31837878
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
in main func line 156:  12
12 689
12 691
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[0.07]
 [0.07]
 [0.07]
 [0.07]
 [0.07]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.27114782
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.16009285
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.1808773
siam score:  -0.24369152
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
13 840
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
13 863
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.3395858
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
15 913
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
15 930
15 933
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
15 956
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.41696918
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
18 1022
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
19 1030
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[-0.]
 [ 0.]
 [-0.]
 [-0.]
 [-0.]] [[ 0.302]
 [ 0.011]
 [-0.001]
 [-0.006]
 [-0.002]] [[0.771]
 [0.287]
 [0.266]
 [0.258]
 [0.265]]
19 1039
siam score:  -0.46994746
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
siam score:  -0.43854752
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
21 1089
21 1097
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
21 1136
21 1152
21 1164
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.43896598
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
21 1186
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]] [[0.671]
 [0.671]
 [0.671]
 [0.949]
 [0.671]] [[-0.084]
 [-0.084]
 [-0.084]
 [ 0.471]
 [-0.084]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.125 0.333 0.25  0.208 0.083]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
21 1250
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.42531937
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
22 1309
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.117]
 [-0.117]
 [-0.122]
 [-0.156]
 [-0.155]] [[0.151]
 [0.151]
 [0.147]
 [0.125]
 [0.125]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
23 1409
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
25 1449
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
26 1465
maxi score, test score, baseline:  0.0001 0.0 0.0001
26 1484
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus -0.021145922725544608
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
26 1519
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
26 1529
Printing some Q and Qe and total Qs values:  [[ 0.]
 [ 0.]
 [-0.]
 [ 0.]
 [ 0.]] [[-0.033]
 [-0.277]
 [-0.006]
 [-0.071]
 [-0.034]] [[ 0.]
 [ 0.]
 [-0.]
 [ 0.]
 [ 0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus -0.13597312362970998
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
26 1549
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.14631249751430436
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.25  0.167 0.292 0.083 0.208]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.475]
 [0.004]
 [0.037]
 [0.07 ]
 [0.288]] [[2.   ]
 [0.004]
 [0.048]
 [0.094]
 [0.389]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.205]
 [-0.036]
 [ 0.067]
 [-0.036]
 [ 0.136]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.111]
 [0.111]
 [0.111]
 [0.239]
 [0.111]] [[0.285]
 [0.285]
 [0.285]
 [0.37 ]
 [0.285]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.14098268354135912
27 1611
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]] [[0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.013]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.024]
 [-0.024]
 [-0.024]
 [-0.049]
 [-0.024]] [[0.125]
 [0.125]
 [0.125]
 [0.075]
 [0.125]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
31 1690
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.001]
 [ 0.001]
 [ 0.001]
 [ 0.001]
 [-0.   ]] [[0.08 ]
 [0.08 ]
 [0.08 ]
 [0.08 ]
 [0.077]]
first move QE:  -0.13265251828993796
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.004]
 [-0.016]
 [-0.06 ]
 [-0.008]
 [-0.008]] [[0.189]
 [0.17 ]
 [0.096]
 [0.183]
 [0.182]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
33 1741
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.005]
 [ 0.005]
 [-0.01 ]
 [ 0.005]
 [-0.013]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
33 1743
maxi score, test score, baseline:  0.0001 0.0 0.0001
35 1759
using explorer policy with actor:  1
36 1775
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.44851086
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.39788547
36 1787
36 1790
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.004]
 [-0.04 ]
 [-0.009]
 [-0.005]
 [-0.007]] [[0.06 ]
 [0.025]
 [0.055]
 [0.06 ]
 [0.058]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.12552907043950964
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.017241404511661913
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.35885262
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.001]
 [-0.056]
 [-0.   ]
 [-0.001]
 [ 0.   ]] [[0.118]
 [0.062]
 [0.118]
 [0.117]
 [0.119]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.   ]
 [-0.002]
 [-0.001]
 [-0.001]
 [-0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.011]
 [0.011]
 [0.016]
 [0.011]] [[0.26 ]
 [0.26 ]
 [0.26 ]
 [0.271]
 [0.26 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
in main func line 156:  39
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
40 1921
siam score:  -0.48734036
siam score:  -0.49201584
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
41 1940
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.012]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.017]] [[0.188]
 [0.188]
 [0.188]
 [0.188]
 [0.178]]
41 1961
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.036]
 [-0.033]
 [-0.041]
 [-0.036]
 [-0.035]] [[0.004]
 [0.005]
 [0.003]
 [0.004]
 [0.005]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.3595713
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
50 2102
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.028]
 [-0.012]
 [-0.015]
 [-0.013]
 [-0.011]] [[0.104]
 [0.125]
 [0.121]
 [0.124]
 [0.126]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
52 2131
52 2132
using explorer policy with actor:  1
53 2146
54 2147
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.006]
 [ 0.006]
 [ 0.006]
 [-0.005]
 [-0.009]] [[0.038]
 [0.038]
 [0.038]
 [0.034]
 [0.032]]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.003]
 [-0.099]
 [-0.103]
 [-0.101]
 [-0.098]] [[0.033]
 [0.001]
 [0.   ]
 [0.001]
 [0.002]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -0.00739478074548155
55 2181
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.4653586
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
57 2229
57 2232
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
57 2241
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.004]
 [-0.003]
 [ 0.012]
 [ 0.011]
 [ 0.012]] [[0.   ]
 [0.002]
 [0.028]
 [0.025]
 [0.027]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.3108889
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.077]
 [-0.095]
 [-0.093]
 [-0.089]
 [-0.089]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
60 2346
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.35904548
UNIT TEST: sample policy line 217 mcts : [0.167 0.083 0.458 0.167 0.125]
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.09667164105762391
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
61 2450
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.   ]
 [0.004]
 [0.004]
 [0.005]] [[ 0.001]
 [-0.001]
 [ 0.001]
 [ 0.001]
 [ 0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
62 2487
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
67 2557
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [-0.004]
 [ 0.   ]
 [ 0.   ]] [[0.007]
 [0.007]
 [0.   ]
 [0.007]
 [0.007]]
using explorer policy with actor:  1
67 2582
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.39242688
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus -0.03141218422934693
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.089]
 [-0.088]
 [-0.089]
 [-0.087]
 [-0.089]] [[0.   ]
 [0.001]
 [0.   ]
 [0.002]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.08770081961732262
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.36018586
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
77 2726
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
UNIT TEST: sample policy line 217 mcts : [0.125 0.208 0.167 0.167 0.333]
79 2777
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.057]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.057]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
81 2806
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.08403441767221256
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.001]
 [-0.001]
 [-0.039]
 [-0.001]
 [-0.001]] [[0.082]
 [0.081]
 [0.043]
 [0.081]
 [0.081]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.44111097
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
86 2919
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[0.1]
 [0.1]
 [0.1]
 [0.1]
 [0.1]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
86 2945
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
86 2973
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
STARTED EXPV TRAINING ON FRAME NO.  20015
Starting evaluation
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
89 2984
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.206]
 [0.206]
 [0.206]
 [0.223]
 [0.206]] [[0.167]
 [0.167]
 [0.167]
 [0.195]
 [0.167]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
89 3038
89 3043
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.51529664
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.124]
 [0.124]
 [0.124]
 [0.053]
 [0.241]] [[0.36 ]
 [0.36 ]
 [0.36 ]
 [0.242]
 [0.555]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
92 3081
92 3083
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.5655626
95 3139
first move QE:  -0.09488491859008179
96 3154
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.4148382
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.042 0.042 0.833]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.369]] [[0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.921]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20015
110 3213
UNIT TEST: sample policy line 217 mcts : [0.292 0.25  0.167 0.125 0.167]
111 3217
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.006966926589348624
114 3232
using explorer policy with actor:  1
start point for exploration sampling:  20015
using explorer policy with actor:  1
siam score:  -0.494384
siam score:  -0.49735653
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.353]
 [1.553]
 [1.553]
 [1.176]
 [1.873]] [[1.107]
 [1.274]
 [1.274]
 [0.959]
 [1.542]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
116 3250
UNIT TEST: sample policy line 217 mcts : [0.042 0.417 0.083 0.208 0.25 ]
siam score:  -0.5506695
121 3271
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
122 3287
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.632]
 [1.765]
 [1.733]
 [1.638]
 [1.747]] [[0.788]
 [0.865]
 [0.846]
 [0.791]
 [0.854]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.851]] [[0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
first move QE:  -0.09928961849788127
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
123 3307
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.675]
 [-0.675]
 [-0.675]
 [ 1.157]
 [-0.675]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.612]
 [0.   ]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
125 3377
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20015
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.042 0.25  0.625]
126 3411
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.10906026379248054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.10900611705346042
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.074]] [[0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.548]]
137 3478
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.5165236
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.572]
 [ 0.05 ]
 [-0.01 ]
 [-0.29 ]
 [-0.149]] [[0.   ]
 [0.643]
 [0.58 ]
 [0.291]
 [0.437]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.124]
 [ 0.194]
 [ 0.064]
 [-0.052]
 [ 1.663]] [[0.082]
 [0.367]
 [0.25 ]
 [0.147]
 [1.679]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
in main func line 156:  148
maxi score, test score, baseline:  0.0001 0.0 0.0001
149 3545
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.52767074
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.46732727
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.229]
 [2.229]
 [2.229]
 [2.229]
 [2.229]] [[1.611]
 [1.611]
 [1.611]
 [1.611]
 [1.611]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.291]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.209]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20015
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.428]
 [1.172]
 [0.683]
 [1.7  ]
 [1.532]] [[0.188]
 [0.437]
 [0.273]
 [0.613]
 [0.557]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.645]
 [1.645]
 [1.645]
 [1.645]
 [2.235]] [[0.864]
 [0.864]
 [0.864]
 [0.864]
 [1.454]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.421]
 [0.702]
 [0.408]
 [1.766]
 [1.661]] [[0.012]
 [0.288]
 [0.   ]
 [1.328]
 [1.226]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.348]
 [1.503]
 [1.451]
 [0.767]
 [1.416]] [[1.515]
 [1.696]
 [1.635]
 [0.84 ]
 [1.595]]
start point for exploration sampling:  20015
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.54823285
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.0145716064788055
siam score:  -0.5523532
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.215]
 [1.989]
 [3.215]
 [1.943]
 [2.989]] [[1.415]
 [0.611]
 [1.415]
 [0.581]
 [1.267]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 1.9039764695358146
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.818]
 [2.302]
 [0.83 ]
 [2.456]
 [2.289]] [[0.926]
 [1.258]
 [0.249]
 [1.364]
 [1.249]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.683]
 [2.683]
 [2.683]
 [2.683]
 [2.683]] [[1.431]
 [1.431]
 [1.431]
 [1.431]
 [1.431]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.25  0.125 0.167 0.125 0.333]
line 256 mcts: sample exp_bonus 1.3367783181780333
188 3641
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.658]
 [2.658]
 [2.658]
 [2.658]
 [2.658]] [[1.313]
 [1.313]
 [1.313]
 [1.313]
 [1.313]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.021]
 [ 0.567]
 [ 0.567]
 [ 1.649]
 [ 0.567]] [[0.   ]
 [0.392]
 [0.392]
 [1.112]
 [0.392]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
198 3666
siam score:  -0.42740428
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.109]
 [0.048]
 [0.305]
 [1.745]
 [0.064]] [[0.09 ]
 [0.07 ]
 [0.156]
 [0.637]
 [0.075]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20015
start point for exploration sampling:  20015
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.5979236
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
221 3708
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.793]
 [4.793]
 [4.793]
 [4.793]
 [4.793]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
line 256 mcts: sample exp_bonus 7.354971874794395
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000]], dtype=torch.float64)
0.0 0.0
0.0 5.70847472576362e-11
0.0 0.0
0.0 0.0
0.0 0.0
0.0 -6.4004110268675775e-12
0.0 0.0
0.0 0.0
0.0 1.8682281019100824e-11
0.0 0.0
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.143]
 [0.143]
 [0.143]
 [0.143]
 [0.143]] [[0.04]
 [0.04]
 [0.04]
 [0.04]
 [0.04]]
using explorer policy with actor:  1
229 3729
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Starting evaluation
siam score:  -0.56669205
line 256 mcts: sample exp_bonus 7.194494779175087
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.874]
 [2.842]
 [0.52 ]
 [1.538]
 [2.561]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
line 256 mcts: sample exp_bonus 2.6200648685335226
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.023]
 [2.023]
 [2.023]
 [2.023]
 [2.023]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.623]
 [3.045]
 [4.715]
 [2.763]
 [3.226]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.695]
 [3.103]
 [2.157]
 [2.925]
 [3.122]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  0
rdn probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
using another actor
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.834]
 [2.834]
 [0.107]
 [3.161]
 [3.69 ]] [[0.886]
 [1.398]
 [0.   ]
 [1.566]
 [1.837]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.2232141574222353
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.311]
 [3.641]
 [3.641]
 [3.641]
 [3.582]] [[1.746]
 [1.344]
 [1.344]
 [1.344]
 [1.309]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
249 3759
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.45166776244702134
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.86 ]
 [2.409]
 [3.86 ]
 [3.17 ]
 [3.439]] [[1.967]
 [1.151]
 [1.967]
 [1.579]
 [1.73 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
siam score:  -0.5038327
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.51119345
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
in main func line 156:  257
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
start point for exploration sampling:  20015
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.409]
 [ 0.829]
 [ 0.103]
 [ 0.262]
 [ 1.941]] [[0.   ]
 [0.75 ]
 [0.31 ]
 [0.406]
 [1.423]]
in main func line 156:  265
using another actor
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.04 ]
 [3.04 ]
 [3.04 ]
 [3.04 ]
 [2.883]] [[1.929]
 [1.929]
 [1.929]
 [1.929]
 [1.81 ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.896]
 [1.896]
 [1.896]
 [2.563]
 [1.896]] [[0.935]
 [0.935]
 [0.935]
 [1.74 ]
 [0.935]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.305]
 [1.645]
 [3.305]
 [2.715]
 [2.979]] [[2.   ]
 [0.409]
 [2.   ]
 [1.435]
 [1.687]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
using explorer policy with actor:  1
271 3781
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.207]
 [2.206]
 [2.207]
 [2.395]
 [2.53 ]] [[1.037]
 [1.036]
 [1.037]
 [1.176]
 [1.276]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.435]
 [2.009]
 [0.594]
 [1.435]
 [1.875]] [[0.863]
 [1.263]
 [0.275]
 [0.863]
 [1.17 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.209]
 [1.209]
 [1.209]
 [1.531]
 [1.905]] [[0.551]
 [0.551]
 [0.551]
 [0.98 ]
 [1.479]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.081]
 [ 0.081]
 [-0.296]
 [ 0.081]
 [ 0.901]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using another actor
using another actor
first move QE:  -0.034188803252533366
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.836]
 [5.836]
 [5.836]
 [5.836]
 [5.974]] [[1.932]
 [1.932]
 [1.932]
 [1.932]
 [2.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.7096536638125498
using explorer policy with actor:  1
first move QE:  -0.03341025231058841
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.321]
 [0.321]
 [0.321]
 [0.38 ]
 [0.321]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.56267244
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
using explorer policy with actor:  1
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.097]
 [-0.229]
 [ 0.843]
 [ 0.156]
 [ 0.036]] [[0.358]
 [0.   ]
 [1.176]
 [0.422]
 [0.291]]
using another actor
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.4281011478905223
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.517]
 [1.4  ]
 [1.517]
 [1.517]
 [2.148]] [[0.702]
 [0.618]
 [0.702]
 [0.702]
 [1.149]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.924]
 [2.924]
 [2.924]
 [2.924]
 [2.924]] [[1.701]
 [1.701]
 [1.701]
 [1.701]
 [1.701]]
siam score:  -0.57911146
using explorer policy with actor:  1
siam score:  -0.5721853
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.501]
 [2.699]
 [3.501]
 [2.774]
 [3.332]] [[1.814]
 [1.218]
 [1.814]
 [1.274]
 [1.689]]
first move QE:  -0.029123916227097466
siam score:  -0.5148199
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.004]
 [0.003]
 [0.003]] [[-0.073]
 [ 0.083]
 [ 0.1  ]
 [ 0.5  ]
 [ 1.579]] [[0.001]
 [0.002]
 [0.004]
 [0.003]
 [0.003]]
using another actor
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.001]] [[1.139]
 [1.047]
 [0.616]
 [1.333]
 [1.348]] [[1.097]
 [0.991]
 [0.5  ]
 [1.318]
 [1.335]]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.001]
 [0.002]
 [0.001]] [[0.38 ]
 [1.148]
 [1.396]
 [1.602]
 [1.524]] [[0.   ]
 [0.568]
 [0.752]
 [0.905]
 [0.846]]
siam score:  -0.46495727
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.613]
 [1.613]
 [1.613]
 [1.613]
 [1.387]] [[1.104]
 [1.104]
 [1.104]
 [1.104]
 [0.934]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.558]
 [1.558]
 [1.558]
 [1.558]
 [1.558]] [[1.812]
 [1.812]
 [1.812]
 [1.812]
 [1.812]]
using explorer policy with actor:  1
315 3857
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]] [[2.   ]
 [2.302]
 [2.246]
 [2.942]
 [4.54 ]] [[0.548]
 [0.694]
 [0.667]
 [1.003]
 [1.774]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
316 3862
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
in main func line 156:  321
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
line 256 mcts: sample exp_bonus 2.494877320794755
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.001]] [[-0.204]
 [ 0.265]
 [ 0.248]
 [ 0.265]
 [ 0.348]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.001]]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.068]
 [2.068]
 [2.068]
 [2.068]
 [2.739]] [[0.637]
 [0.637]
 [0.637]
 [0.637]
 [1.218]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.5549821
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[3.662]
 [3.863]
 [3.662]
 [3.662]
 [4.325]] [[1.125]
 [1.233]
 [1.125]
 [1.125]
 [1.484]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[5.453]
 [5.86 ]
 [5.91 ]
 [6.361]
 [6.157]] [[1.24 ]
 [1.451]
 [1.477]
 [1.712]
 [1.606]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[3.391]
 [3.391]
 [3.391]
 [3.391]
 [4.075]] [[1.464]
 [1.464]
 [1.464]
 [1.464]
 [1.746]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
using explorer policy with actor:  1
siam score:  -0.5839209
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[1.645]
 [2.207]
 [2.866]
 [2.311]
 [2.788]] [[0.431]
 [0.824]
 [1.286]
 [0.897]
 [1.232]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[0.874]
 [1.132]
 [0.874]
 [0.874]
 [1.7  ]] [[0.474]
 [0.828]
 [0.474]
 [0.474]
 [1.611]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.884]
 [1.119]
 [2.007]
 [1.891]
 [1.756]] [[0.11 ]
 [0.266]
 [0.858]
 [0.781]
 [0.691]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]] [[0.809]
 [1.273]
 [1.135]
 [1.834]
 [1.634]] [[0.198]
 [0.564]
 [0.454]
 [1.005]
 [0.847]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
siam score:  -0.60086226
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.023904312868625422
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.    0.208 0.    0.125 0.667]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.16]
 [0.16]
 [0.16]
 [0.16]
 [0.16]] [[0.13]
 [0.13]
 [0.13]
 [0.13]
 [0.13]]
351 3947
siam score:  -0.61951065
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20015
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
351 3954
using another actor
siam score:  -0.6171303
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.001]
 [0.002]] [[3.409]
 [3.409]
 [3.409]
 [1.289]
 [3.409]] [[1.866]
 [1.866]
 [1.866]
 [0.452]
 [1.866]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.269]
 [1.269]
 [1.269]
 [1.269]
 [1.775]] [[0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.885]]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[2.771]
 [3.871]
 [3.829]
 [3.798]
 [4.024]] [[0.372]
 [0.967]
 [0.945]
 [0.928]
 [1.05 ]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.   ]] [[2.973]
 [4.4  ]
 [2.973]
 [2.973]
 [3.297]] [[0.481]
 [1.253]
 [0.481]
 [0.481]
 [0.656]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
first move QE:  -0.021411983243523256
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.951]
 [2.951]
 [2.951]
 [2.951]
 [3.292]] [[1.321]
 [1.321]
 [1.321]
 [1.321]
 [1.548]]
using explorer policy with actor:  1
using another actor
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.904]
 [1.904]
 [1.904]
 [1.904]
 [2.072]] [[1.524]
 [1.524]
 [1.524]
 [1.524]
 [1.692]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.894]
 [3.894]
 [3.894]
 [3.894]
 [3.894]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.819]
 [0.819]
 [0.819]
 [0.819]
 [2.256]] [[0.343]
 [0.343]
 [0.343]
 [0.343]
 [1.301]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.521]
 [5.521]
 [5.521]
 [5.521]
 [5.521]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
using another actor
UNIT TEST: sample policy line 217 mcts : [0.042 0.292 0.042 0.125 0.5  ]
369 4005
first move QE:  -0.017990859304020043
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
using explorer policy with actor:  1
start point for exploration sampling:  20015
siam score:  -0.5067286
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.082]
 [1.082]
 [1.082]
 [1.082]
 [1.652]] [[0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.831]]
start point for exploration sampling:  20015
UNIT TEST: sample policy line 217 mcts : [0.042 0.667 0.042 0.042 0.208]
using explorer policy with actor:  1
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.001]] [[7.738]
 [7.738]
 [7.738]
 [7.738]
 [9.347]] [[1.358]
 [1.358]
 [1.358]
 [1.358]
 [1.99 ]]
in main func line 156:  379
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
siam score:  -0.5560921
first move QE:  -0.016891652582322125
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.001]
 [0.001]
 [0.   ]] [[0.855]
 [1.47 ]
 [1.319]
 [1.097]
 [2.411]] [[0.   ]
 [0.404]
 [0.305]
 [0.159]
 [1.021]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.801]
 [0.943]
 [0.801]
 [0.801]
 [1.036]] [[0.279]
 [0.515]
 [0.279]
 [0.279]
 [0.67 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[4.265]
 [4.265]
 [4.265]
 [4.265]
 [3.947]] [[0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.713]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.797]
 [2.842]
 [1.892]
 [2.099]
 [3.813]] [[0.001]
 [0.351]
 [0.033]
 [0.102]
 [0.675]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
389 4037
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.395]
 [2.395]
 [2.395]
 [2.395]
 [2.395]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
using another actor
using explorer policy with actor:  1
start point for exploration sampling:  20015
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.199768602578912
using explorer policy with actor:  1
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.159]
 [1.159]
 [1.159]
 [1.159]
 [1.452]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  0
rdn probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
397 4058
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.886]
 [1.886]
 [1.886]
 [1.886]
 [2.257]] [[1.254]
 [1.254]
 [1.254]
 [1.254]
 [1.724]]
siam score:  -0.593199
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
400 4059
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.253]
 [1.253]
 [1.253]
 [1.253]
 [1.003]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
400 4063
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
first move QE:  -0.011542131640368934
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.226]
 [1.302]
 [1.441]
 [1.141]
 [0.812]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.629]
 [1.629]
 [1.629]
 [1.629]
 [1.779]] [[1.513]
 [1.513]
 [1.513]
 [1.513]
 [1.701]]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
using another actor
407 4081
409 4085
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[5.678]
 [5.678]
 [5.678]
 [3.543]
 [5.651]] [[1.864]
 [1.864]
 [1.864]
 [0.529]
 [1.847]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.203]
 [1.276]
 [1.276]
 [1.276]
 [1.474]] [[0.1  ]
 [1.106]
 [1.106]
 [1.106]
 [1.291]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.763]
 [4.737]
 [4.737]
 [4.737]
 [5.495]] [[0.789]
 [1.177]
 [1.177]
 [1.177]
 [1.48 ]]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.915]
 [4.915]
 [4.915]
 [4.915]
 [4.945]] [[1.988]
 [1.988]
 [1.988]
 [1.988]
 [2.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.006]
 [1.187]
 [1.993]
 [1.006]
 [1.21 ]] [[0.336]
 [0.455]
 [0.982]
 [0.336]
 [0.47 ]]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[3.327]
 [5.073]
 [3.51 ]
 [5.304]
 [5.979]] [[0.095]
 [1.092]
 [0.199]
 [1.225]
 [1.611]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6219016
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.62211347
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
using explorer policy with actor:  1
siam score:  -0.61219496
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.553]
 [2.907]
 [4.647]
 [2.414]
 [4.   ]] [[1.77 ]
 [0.879]
 [1.821]
 [0.612]
 [1.471]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6073692
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.544]
 [4.544]
 [4.544]
 [4.544]
 [5.819]] [[1.188]
 [1.188]
 [1.188]
 [1.188]
 [1.802]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.87 ]
 [3.87 ]
 [3.87 ]
 [4.645]
 [3.87 ]] [[1.364]
 [1.364]
 [1.364]
 [2.   ]
 [1.364]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.288]
 [0.288]
 [0.288]
 [0.288]
 [0.288]] [[0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
449 4135
from probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
first move QE:  -0.001923064640876289
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.529]
 [4.597]
 [4.639]
 [4.529]
 [4.547]] [[1.483]
 [1.529]
 [1.558]
 [1.483]
 [1.495]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.813]
 [0.813]
 [0.813]
 [0.813]
 [1.026]] [[1.613]
 [1.613]
 [1.613]
 [1.613]
 [1.912]]
line 256 mcts: sample exp_bonus 4.8746379437805505
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.03846745623582508, 0.8076627188208746]
siam score:  -0.6152925
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 2.79120119642677
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.021742911621648014, 0.021742911621648014, 0.456514176756704, 0.021742911621648014, 0.021742911621648014, 0.456514176756704]
using another actor
from probs:  [0.021742911621648014, 0.021742911621648014, 0.456514176756704, 0.021742911621648014, 0.021742911621648014, 0.456514176756704]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.001]
 [0.001]
 [0.002]
 [0.002]] [[4.776]
 [4.054]
 [4.051]
 [5.099]
 [5.154]] [[1.308]
 [0.836]
 [0.834]
 [1.518]
 [1.555]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[4.022]
 [4.022]
 [4.022]
 [4.022]
 [3.813]] [[1.261]
 [1.261]
 [1.261]
 [1.261]
 [1.154]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.021742911621648014, 0.021742911621648014, 0.456514176756704, 0.021742911621648014, 0.021742911621648014, 0.456514176756704]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.882]
 [3.882]
 [3.882]
 [3.882]
 [4.8  ]] [[1.103]
 [1.103]
 [1.103]
 [1.103]
 [1.655]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.021742911621648014, 0.021742911621648014, 0.456514176756704, 0.021742911621648014, 0.021742911621648014, 0.456514176756704]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.021742911621648014, 0.021742911621648014, 0.456514176756704, 0.021742911621648014, 0.021742911621648014, 0.456514176756704]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.307]
 [2.44 ]
 [2.307]
 [2.307]
 [2.527]] [[0.97 ]
 [1.104]
 [0.97 ]
 [0.97 ]
 [1.191]]
from probs:  [0.021742911621648014, 0.021742911621648014, 0.456514176756704, 0.021742911621648014, 0.021742911621648014, 0.456514176756704]
from probs:  [0.021742911621648014, 0.021742911621648014, 0.456514176756704, 0.021742911621648014, 0.021742911621648014, 0.456514176756704]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.38]
 [2.38]
 [2.38]
 [2.38]
 [2.38]] [[1.283]
 [1.283]
 [1.283]
 [1.283]
 [1.283]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.021742911621648014, 0.021742911621648014, 0.456514176756704, 0.021742911621648014, 0.021742911621648014, 0.456514176756704]
UNIT TEST: sample policy line 217 mcts : [0.167 0.375 0.125 0.042 0.292]
start point for exploration sampling:  20015
471 4175
siam score:  -0.6035058
from probs:  [0.021742911621648014, 0.021742911621648014, 0.456514176756704, 0.021742911621648014, 0.021742911621648014, 0.456514176756704]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.298]
 [2.298]
 [2.624]
 [2.298]
 [2.21 ]] [[0.913]
 [0.913]
 [1.13 ]
 [0.913]
 [0.855]]
siam score:  -0.60545945
siam score:  -0.60414624
using explorer policy with actor:  1
using another actor
start point for exploration sampling:  20015
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.021742911621648014, 0.021742911621648014, 0.456514176756704, 0.021742911621648014, 0.021742911621648014, 0.456514176756704]
from probs:  [0.021742911621648014, 0.021742911621648014, 0.456514176756704, 0.021742911621648014, 0.021742911621648014, 0.456514176756704]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.021742911621648014, 0.021742911621648014, 0.456514176756704, 0.021742911621648014, 0.021742911621648014, 0.456514176756704]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.001]] [[2.808]
 [4.234]
 [3.658]
 [3.515]
 [3.896]] [[0.579]
 [1.227]
 [0.966]
 [0.9  ]
 [1.073]]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.004]
 [0.005]
 [0.005]
 [0.006]] [[5.806]
 [3.904]
 [4.186]
 [5.806]
 [6.418]] [[1.459]
 [0.758]
 [0.862]
 [1.459]
 [1.685]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.021742911621648014, 0.021742911621648014, 0.456514176756704, 0.021742911621648014, 0.021742911621648014, 0.456514176756704]
using explorer policy with actor:  1
from probs:  [0.021742911621648014, 0.021742911621648014, 0.456514176756704, 0.021742911621648014, 0.021742911621648014, 0.456514176756704]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.522]
 [2.522]
 [2.522]
 [2.522]
 [2.522]] [[0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.021742911621648014, 0.021742911621648014, 0.456514176756704, 0.021742911621648014, 0.021742911621648014, 0.456514176756704]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.021742911621648014, 0.021742911621648014, 0.456514176756704, 0.021742911621648014, 0.021742911621648014, 0.456514176756704]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.021742911621648014, 0.021742911621648014, 0.456514176756704, 0.021742911621648014, 0.021742911621648014, 0.456514176756704]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.021742911621648014, 0.021742911621648014, 0.456514176756704, 0.021742911621648014, 0.021742911621648014, 0.456514176756704]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.021742911621648014, 0.021742911621648014, 0.456514176756704, 0.021742911621648014, 0.021742911621648014, 0.456514176756704]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[3.524]
 [3.675]
 [3.071]
 [3.524]
 [4.183]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.021742911621648014, 0.021742911621648014, 0.456514176756704, 0.021742911621648014, 0.021742911621648014, 0.456514176756704]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.021742911621648014, 0.021742911621648014, 0.456514176756704, 0.021742911621648014, 0.021742911621648014, 0.456514176756704]
first move QE:  0.014207445022423494
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[4.194]
 [4.194]
 [4.194]
 [4.194]
 [4.279]] [[1.948]
 [1.948]
 [1.948]
 [1.948]
 [2.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.021742911621648014, 0.021742911621648014, 0.456514176756704, 0.021742911621648014, 0.021742911621648014, 0.456514176756704]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.021742911621648014, 0.021742911621648014, 0.456514176756704, 0.021742911621648014, 0.021742911621648014, 0.456514176756704]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.002]] [[3.494]
 [3.494]
 [3.494]
 [3.494]
 [6.779]] [[0.786]
 [0.786]
 [0.786]
 [0.786]
 [1.871]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.213]
 [1.213]
 [1.213]
 [1.213]
 [1.189]] [[1.369]
 [1.369]
 [1.369]
 [1.369]
 [1.337]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[2.129]
 [2.419]
 [2.419]
 [2.419]
 [2.438]] [[1.298]
 [1.685]
 [1.685]
 [1.685]
 [1.711]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.32564932074182434
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
first move QE:  0.0157515472846736
from probs:  [0.015154270335628264, 0.015154270335628264, 0.3181790629977051, 0.015154270335628264, 0.3181790629977051, 0.3181790629977051]
from probs:  [0.015154270335628264, 0.015154270335628264, 0.3181790629977051, 0.015154270335628264, 0.3181790629977051, 0.3181790629977051]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.002]] [[2.455]
 [3.443]
 [2.455]
 [2.455]
 [2.566]] [[0.803]
 [1.443]
 [0.803]
 [0.803]
 [0.874]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.002]
 [0.001]
 [0.002]] [[1.309]
 [1.309]
 [2.198]
 [1.309]
 [1.304]] [[0.601]
 [0.601]
 [1.379]
 [0.601]
 [0.598]]
siam score:  -0.6386447
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.011630070602623086, 0.011630070602623086, 0.24418496469868844, 0.24418496469868844, 0.24418496469868844, 0.24418496469868844]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.011630070602623086, 0.011630070602623086, 0.24418496469868844, 0.24418496469868844, 0.24418496469868844, 0.24418496469868844]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.011630070602623086, 0.011630070602623086, 0.24418496469868844, 0.24418496469868844, 0.24418496469868844, 0.24418496469868844]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
534 4266
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.013]
 [0.023]
 [0.013]
 [0.015]] [[2.769]
 [3.719]
 [3.742]
 [3.719]
 [4.159]] [[0.662]
 [1.282]
 [1.305]
 [1.282]
 [1.573]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.007938019864691498, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.32539531346864187, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.006850628822028332, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326, 0.4178061547082413, 0.1438358041174326]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.006850628822028332, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326, 0.4178061547082413, 0.1438358041174326]
from probs:  [0.006850628822028332, 0.1438358041174326, 0.1438358041174326, 0.1438358041174326, 0.4178061547082413, 0.1438358041174326]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.065]
 [1.065]
 [1.065]
 [1.065]
 [1.065]] [[1.77]
 [1.77]
 [1.77]
 [1.77]
 [1.77]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.6]
 [2.6]
 [2.6]
 [2.6]
 [2.6]] [[2.001]
 [2.001]
 [2.001]
 [2.001]
 [2.001]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.643]
 [-0.274]
 [-0.569]
 [-0.496]
 [-0.772]] [[0.086]
 [0.332]
 [0.134]
 [0.184]
 [0.   ]]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[1.528]
 [0.851]
 [0.851]
 [0.851]
 [0.951]] [[1.974]
 [0.789]
 [0.789]
 [0.789]
 [0.964]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.003]] [[0.605]
 [0.605]
 [1.039]
 [0.605]
 [0.811]] [[0.601]
 [0.601]
 [1.357]
 [0.601]
 [0.964]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.006025257822951336, 0.12650631445573782, 0.12650631445573782, 0.24698737108852434, 0.3674684277213109, 0.12650631445573782]
using another actor
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.549]
 [-0.249]
 [-0.043]
 [-0.459]
 [-0.325]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.006025257822951336, 0.12650631445573782, 0.12650631445573782, 0.24698737108852434, 0.3674684277213109, 0.12650631445573782]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.006025257822951336, 0.12650631445573782, 0.12650631445573782, 0.24698737108852434, 0.3674684277213109, 0.12650631445573782]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.006025257822951336, 0.12650631445573782, 0.12650631445573782, 0.24698737108852434, 0.3674684277213109, 0.12650631445573782]
first move QE:  0.010429959959321883
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.06648053125725013
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
from probs:  [0.006025257822951336, 0.12650631445573782, 0.12650631445573782, 0.24698737108852434, 0.3674684277213109, 0.12650631445573782]
from probs:  [0.006025257822951336, 0.12650631445573782, 0.12650631445573782, 0.24698737108852434, 0.3674684277213109, 0.12650631445573782]
from probs:  [0.006025257822951336, 0.12650631445573782, 0.12650631445573782, 0.24698737108852434, 0.3674684277213109, 0.12650631445573782]
from probs:  [0.006025257822951336, 0.12650631445573782, 0.12650631445573782, 0.24698737108852434, 0.3674684277213109, 0.12650631445573782]
548 4341
start point for exploration sampling:  20015
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.006025257822951336, 0.12650631445573782, 0.12650631445573782, 0.24698737108852434, 0.3674684277213109, 0.12650631445573782]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.007]
 [0.006]
 [0.004]
 [0.003]] [[0.651]
 [1.309]
 [0.825]
 [0.368]
 [0.851]] [[0.425]
 [1.41 ]
 [0.687]
 [0.002]
 [0.722]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.006096302887667094
actor:  1 policy actor:  1  step number:  95 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.005377384818140648, 0.112903572717158, 0.112903572717158, 0.22042976061617536, 0.3279559485151928, 0.22042976061617536]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.005377384818140648, 0.112903572717158, 0.112903572717158, 0.22042976061617536, 0.3279559485151928, 0.22042976061617536]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.002]
 [0.002]
 [0.003]
 [0.002]] [[0.77 ]
 [0.804]
 [0.907]
 [0.77 ]
 [0.728]] [[1.058]
 [1.102]
 [1.24 ]
 [1.058]
 [1.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.113]
 [0.129]
 [0.052]
 [0.053]] [[1.706]
 [1.931]
 [2.418]
 [2.179]
 [2.885]] [[0.152]
 [0.453]
 [0.897]
 [0.59 ]
 [1.206]]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004855311663863744, 0.10194212466554549, 0.10194212466554549, 0.29611575066890905, 0.29611575066890905, 0.19902893766722726]
siam score:  -0.6903594
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
from probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
559 4394
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.003]
 [0.001]
 [0.001]] [[ 0.   ]
 [ 0.   ]
 [ 0.049]
 [ 0.093]
 [-0.156]] [[0.002]
 [0.002]
 [0.003]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[ 0.479]
 [ 0.479]
 [ 0.479]
 [ 0.479]
 [-0.778]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.026]
 [0.018]
 [0.011]
 [0.015]] [[2.028]
 [0.72 ]
 [0.716]
 [0.64 ]
 [1.239]] [[2.003]
 [0.32 ]
 [0.298]
 [0.184]
 [0.98 ]]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.003]
 [0.001]
 [0.001]] [[-0.414]
 [ 0.031]
 [-0.077]
 [-0.   ]
 [-0.536]] [[0.001]
 [0.002]
 [0.003]
 [0.001]
 [0.001]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.001]
 [0.001]] [[ 0.452]
 [ 0.452]
 [ 0.452]
 [ 0.211]
 [-0.509]] [[0.002]
 [0.002]
 [0.002]
 [0.001]
 [0.001]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.004]
 [0.001]
 [0.001]] [[0.   ]
 [0.   ]
 [0.154]
 [0.   ]
 [0.313]] [[0.001]
 [0.001]
 [0.004]
 [0.001]
 [0.001]]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]] [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]]
using another actor
from probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
using explorer policy with actor:  0
using another actor
from probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
rdn probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
from probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
siam score:  -0.7732872
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[-0.054]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.054]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
using another actor
UNIT TEST: sample policy line 217 mcts : [0.792 0.042 0.042 0.042 0.083]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
from probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
Printing some Q and Qe and total Qs values:  [[0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.052]] [[1.604]
 [1.604]
 [1.604]
 [1.604]
 [2.142]] [[0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.838]]
Printing some Q and Qe and total Qs values:  [[0.027]
 [0.027]
 [0.027]
 [0.027]
 [0.031]] [[0.206]
 [0.206]
 [0.206]
 [0.206]
 [5.46 ]] [[0.047]
 [0.047]
 [0.047]
 [0.047]
 [2.004]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
from probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
using another actor
from probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
from probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
from probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.004]
 [0.004]] [[-0.915]
 [-0.915]
 [-0.915]
 [-1.042]
 [-1.051]] [[0.003]
 [0.003]
 [0.003]
 [0.004]
 [0.004]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.0033202854111791455
siam score:  -0.7832695
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
from probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
from probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
566 4556
using another actor
566 4566
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0]
 [0]
 [0]
 [0]
 [0]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
566 4611
from probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
using another actor
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.012]
 [0.01 ]
 [0.008]
 [0.01 ]] [[1.2  ]
 [1.486]
 [1.484]
 [1.2  ]
 [1.303]] [[0.989]
 [1.282]
 [1.276]
 [0.989]
 [1.095]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
from probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
from probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
571 4652
using another actor
start point for exploration sampling:  20015
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.025]
 [0.029]
 [0.006]
 [0.008]] [[0.491]
 [0.294]
 [1.123]
 [0.306]
 [0.448]] [[0.458]
 [0.169]
 [1.525]
 [0.157]
 [0.392]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
using another actor
from probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
572 4700
from probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
using another actor
from probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
from probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
siam score:  -0.79079455
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.624529590522919
from probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
siam score:  -0.7967285
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
using another actor
from probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
maxi score, test score, baseline:  0.0001 0.0 0.0001
579 4768
from probs:  [0.004425640347074613, 0.09292074561230663, 0.09292074561230663, 0.2699109561427707, 0.35840606140800274, 0.1814158508775387]
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using another actor
first move QE:  -0.015206048036600079
using explorer policy with actor:  1
581 4802
siam score:  -0.82407546
582 4808
583 4811
maxi score, test score, baseline:  0.0021 0.0 0.0021
using another actor
using another actor
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
first move QE:  -0.01817540929292412
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.001]
 [0.015]
 [0.007]
 [0.013]] [[-0.683]
 [-0.188]
 [-0.43 ]
 [-0.753]
 [-0.518]] [[0.071]
 [0.557]
 [0.343]
 [0.005]
 [0.25 ]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]] [[0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.746]] [[0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.001]
 [0.018]
 [0.015]
 [0.016]] [[-0.631]
 [-0.284]
 [-0.593]
 [-0.938]
 [-0.695]] [[0.615]
 [1.279]
 [0.699]
 [0.01 ]
 [0.494]]
first move QE:  -0.020181890548624876
585 4899
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
maxi score, test score, baseline:  0.0021 0.0 0.0021
using explorer policy with actor:  1
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
maxi score, test score, baseline:  0.0021 0.0 0.0021
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.005]
 [0.013]
 [0.011]
 [0.011]] [[ 0.248]
 [ 0.154]
 [-0.087]
 [ 0.248]
 [ 0.599]] [[1.055]
 [0.912]
 [0.583]
 [1.055]
 [1.554]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
using another actor
using another actor
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
maxi score, test score, baseline:  0.0021 0.0 0.0021
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
start point for exploration sampling:  20015
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
siam score:  -0.85605764
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
maxi score, test score, baseline:  0.0021 0.0 0.0021
using another actor
598 4975
using another actor
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
rdn beta is 0 so we're just using the maxi policy
599 5005
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
siam score:  -0.8505983
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
siam score:  -0.84767973
maxi score, test score, baseline:  0.0021 0.0 0.0021
601 5052
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
maxi score, test score, baseline:  0.0021 0.0 0.0021
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.   ]] [[-0.116]
 [-0.116]
 [-0.116]
 [-0.116]
 [ 0.001]] [[0.969]
 [0.969]
 [0.969]
 [0.969]
 [1.197]]
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
siam score:  -0.85479903
siam score:  -0.85573465
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]] [[0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]]
line 256 mcts: sample exp_bonus 1.7863084855090094
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
using another actor
609 5156
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
siam score:  -0.85653216
610 5184
using another actor
using another actor
siam score:  -0.8592785
using another actor
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.3308],
        [0.0949],
        [0.0174],
        [0.1730],
        [0.1609],
        [0.5544],
        [0.0017],
        [0.0083],
        [0.3181],
        [0.0020]], dtype=torch.float64)
0.0 0.3308413185364505
0.0 0.09487883523684276
0.0 0.017363661610296736
0.0 0.17302007929759372
0.0 0.16087559949973096
0.0 0.5544226490140507
0.0 0.0017478482413945226
0.0 0.008294667726073256
0.0 0.31813390661707064
0.0 0.0020189430504900935
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.001]
 [0.001]] [[-0.108]
 [-0.108]
 [-0.082]
 [-0.447]
 [-0.302]] [[0.454]
 [0.454]
 [0.491]
 [0.001]
 [0.195]]
615 5211
using another actor
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
maxi score, test score, baseline:  0.0021 0.0 0.0021
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.09187364086110154
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
first move QE:  -0.014675277700566957
using another actor
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
using another actor
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
using another actor
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.004]
 [0.006]
 [0.007]
 [0.007]] [[1.57 ]
 [1.526]
 [1.075]
 [0.068]
 [1.638]] [[0.983]
 [0.954]
 [0.665]
 [0.019]
 [1.031]]
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
627 5281
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.001]
 [0.01 ]
 [0.005]
 [0.006]] [[-0.311]
 [-0.264]
 [-0.577]
 [-0.721]
 [-0.284]] [[0.551]
 [0.608]
 [0.208]
 [0.008]
 [0.592]]
using another actor
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
using another actor
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.007]
 [0.005]
 [0.006]] [[-0.206]
 [-0.23 ]
 [-0.52 ]
 [-0.601]
 [-0.251]] [[0.562]
 [0.528]
 [0.145]
 [0.034]
 [0.503]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
maxi score, test score, baseline:  0.0021 0.0 0.0021
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.011]
 [0.014]
 [0.015]] [[-0.038]
 [ 0.052]
 [-0.309]
 [-0.454]
 [-0.223]] [[0.567]
 [0.687]
 [0.205]
 [0.017]
 [0.329]]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.015]
 [0.015]
 [0.015]
 [0.015]] [[-0.165]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]] [[0.268]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]]
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.032]
 [0.032]
 [0.032]
 [0.034]] [[3.12 ]
 [3.12 ]
 [3.12 ]
 [3.12 ]
 [2.843]] [[1.034]
 [1.034]
 [1.034]
 [1.034]
 [0.912]]
first move QE:  -0.015286972713725167
using another actor
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
maxi score, test score, baseline:  0.0021 0.0 0.0021
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.025]
 [0.013]
 [0.014]] [[-0.054]
 [-0.054]
 [ 0.489]
 [-0.054]
 [ 0.035]] [[0.477]
 [0.477]
 [1.044]
 [0.477]
 [0.568]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
using another actor
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.014]
 [0.002]
 [0.008]
 [0.008]] [[-0.107]
 [-0.001]
 [ 0.039]
 [-0.071]
 [-0.252]] [[0.055]
 [0.11 ]
 [0.098]
 [0.074]
 [0.013]]
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.002]
 [0.007]
 [0.006]
 [0.006]] [[-0.149]
 [ 0.073]
 [-0.29 ]
 [-0.458]
 [-0.19 ]] [[0.005]
 [0.002]
 [0.007]
 [0.006]
 [0.006]]
siam score:  -0.86917114
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.007]] [[-0.045]
 [ 0.153]
 [-0.045]
 [-0.045]
 [ 0.017]] [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.007]]
using explorer policy with actor:  1
using another actor
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
644 5445
using another actor
maxi score, test score, baseline:  0.0021 0.0 0.0021
644 5455
maxi score, test score, baseline:  0.0021 0.0 0.0021
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
644 5462
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
maxi score, test score, baseline:  0.0021 0.0 0.0021
siam score:  -0.8618346
using explorer policy with actor:  1
645 5486
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
645 5509
using another actor
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
UNIT TEST: sample policy line 217 mcts : [0.042 0.583 0.125 0.042 0.208]
using another actor
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
649 5546
649 5560
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]] [[0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]] [[1.067]
 [1.067]
 [1.067]
 [1.067]
 [1.067]]
line 256 mcts: sample exp_bonus 0.09450501263612064
line 256 mcts: sample exp_bonus 0.36505834824153627
651 5564
651 5566
from probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.004442924565367195, 0.0929286020751669, 0.0929286020751669, 0.26989995709476633, 0.358385634604566, 0.18141427958496664]
maxi score, test score, baseline:  0.0021 0.0 0.0021
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0040817483014857535, 0.0853742074840762, 0.0853742074840762, 0.24795912584925714, 0.32925158503184754, 0.24795912584925714]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0040817483014857535, 0.0853742074840762, 0.0853742074840762, 0.24795912584925714, 0.32925158503184754, 0.24795912584925714]
using explorer policy with actor:  1
from probs:  [0.0040817483014857535, 0.0853742074840762, 0.0853742074840762, 0.24795912584925714, 0.32925158503184754, 0.24795912584925714]
654 5599
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0040817483014857535, 0.0853742074840762, 0.0853742074840762, 0.24795912584925714, 0.32925158503184754, 0.24795912584925714]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0040817483014857535, 0.0853742074840762, 0.0853742074840762, 0.24795912584925714, 0.32925158503184754, 0.24795912584925714]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0040817483014857535, 0.0853742074840762, 0.0853742074840762, 0.24795912584925714, 0.32925158503184754, 0.24795912584925714]
656 5613
Printing some Q and Qe and total Qs values:  [[0.051]
 [0.234]
 [0.127]
 [0.082]
 [0.124]] [[2.24 ]
 [1.946]
 [1.887]
 [1.695]
 [2.255]] [[1.241]
 [1.15 ]
 [0.92 ]
 [0.619]
 [1.368]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.064]
 [0.153]
 [0.017]
 [0.032]] [[1.836]
 [1.221]
 [1.951]
 [1.962]
 [3.799]] [[0.453]
 [0.02 ]
 [0.676]
 [0.557]
 [2.01 ]]
first move QE:  -0.02305348769590433
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]] [[3.144]
 [3.144]
 [3.144]
 [3.144]
 [3.443]] [[1.621]
 [1.621]
 [1.621]
 [1.621]
 [1.81 ]]
Starting evaluation
using explorer policy with actor:  0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.22519543538451484
line 256 mcts: sample exp_bonus -0.16124140483688293
siam score:  -0.8295912
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.007]
 [0.006]
 [0.009]
 [0.01 ]] [[1.31 ]
 [0.809]
 [1.252]
 [0.082]
 [0.416]] [[0.008]
 [0.007]
 [0.006]
 [0.009]
 [0.01 ]]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.01 ]
 [0.01 ]
 [0.01 ]
 [0.009]] [[1.109]
 [1.109]
 [1.109]
 [1.109]
 [1.238]] [[0.01 ]
 [0.01 ]
 [0.01 ]
 [0.01 ]
 [0.009]]
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.025]] [[3.144]
 [3.144]
 [3.144]
 [3.144]
 [3.102]] [[2.008]
 [2.008]
 [2.008]
 [2.008]
 [1.977]]
using explorer policy with actor:  0
rdn probs:  [0.0040817483014857535, 0.0853742074840762, 0.0853742074840762, 0.24795912584925714, 0.32925158503184754, 0.24795912584925714]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0040817483014857535, 0.0853742074840762, 0.0853742074840762, 0.24795912584925714, 0.32925158503184754, 0.24795912584925714]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0037748790966057187, 0.15413652916127735, 0.07895570412894153, 0.22931735419361315, 0.30449817922594896, 0.22931735419361315]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0037748790966057187, 0.15413652916127735, 0.07895570412894153, 0.22931735419361315, 0.30449817922594896, 0.22931735419361315]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]] [[2.336]
 [2.336]
 [2.336]
 [2.336]
 [2.336]] [[2.006]
 [2.006]
 [2.006]
 [2.006]
 [2.006]]
siam score:  -0.81968015
maxi score, test score, baseline:  0.0021 0.0 0.0021
siam score:  -0.8178038
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0035109248683747596, 0.14335870355262498, 0.07343481421049985, 0.21328259289475007, 0.2832064822368751, 0.2832064822368751]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0035109248683747596, 0.14335870355262498, 0.07343481421049985, 0.21328259289475007, 0.2832064822368751, 0.2832064822368751]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.006]] [[0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.778]] [[0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.377]]
first move QE:  -0.021993053549633313
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.003080169943768713, 0.12577004248594217, 0.12577004248594217, 0.1871149787570289, 0.2484599150281156, 0.3098048512992024]
maxi score, test score, baseline:  0.0021 0.0 0.0021
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.029]
 [0.021]
 [0.021]
 [0.024]] [[2.733]
 [2.64 ]
 [2.733]
 [2.733]
 [2.688]] [[1.732]
 [1.66 ]
 [1.732]
 [1.732]
 [1.695]]
from probs:  [0.002743562598274893, 0.11202563197720275, 0.16666666666666669, 0.2213077013561306, 0.2213077013561306, 0.27594873604559456]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[ 0.083]
 [ 0.083]
 [ 0.083]
 [ 0.083]
 [-0.088]] [[0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.332]]
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.038]
 [0.038]
 [0.038]
 [0.039]] [[2.258]
 [2.913]
 [2.258]
 [2.258]
 [2.14 ]] [[0.618]
 [1.03 ]
 [0.618]
 [0.618]
 [0.545]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using another actor
675 5681
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0021 0.0 0.0021
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
678 5693
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 0.3385047904812679
maxi score, test score, baseline:  0.0021 0.0 0.0021
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0021 0.0 0.0021
using explorer policy with actor:  1
using another actor
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.033]
 [0.033]
 [0.033]
 [0.033]] [[1.3]
 [1.3]
 [1.3]
 [1.3]
 [1.3]] [[0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]]
start point for exploration sampling:  20015
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
687 5718
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0019090589484590378, 0.1159720181379874, 0.2680559637240252, 0.1920139909310063, 0.1920139909310063, 0.2300349773275157]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0019090589484590378, 0.1159720181379874, 0.2680559637240252, 0.1920139909310063, 0.1920139909310063, 0.2300349773275157]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0019090589484590378, 0.1159720181379874, 0.2680559637240252, 0.1920139909310063, 0.1920139909310063, 0.2300349773275157]
from probs:  [0.0019090589484590378, 0.1159720181379874, 0.2680559637240252, 0.1920139909310063, 0.1920139909310063, 0.2300349773275157]
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.033]
 [0.033]
 [0.033]
 [0.033]] [[4.383]
 [4.383]
 [4.383]
 [4.383]
 [5.075]] [[1.394]
 [1.394]
 [1.394]
 [1.394]
 [1.727]]
first move QE:  -0.022433042822342614
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0019090589484590378, 0.1159720181379874, 0.2680559637240252, 0.1920139909310063, 0.1920139909310063, 0.2300349773275157]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.03710826004428472, 0.14311059273532448, 0.24911292542636426, 0.17844470363233775, 0.17844470363233775, 0.213778814529351]
from probs:  [0.03710826004428472, 0.14311059273532448, 0.24911292542636426, 0.17844470363233775, 0.17844470363233775, 0.213778814529351]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]] [[3.553]
 [3.553]
 [3.553]
 [3.553]
 [3.816]] [[1.45 ]
 [1.45 ]
 [1.45 ]
 [1.45 ]
 [1.596]]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.061]] [[2.24 ]
 [2.24 ]
 [2.24 ]
 [2.24 ]
 [2.028]] [[1.506]
 [1.506]
 [1.506]
 [1.506]
 [1.295]]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.78955954
using another actor
from probs:  [0.03465897211566213, 0.1336647430289155, 0.23267051394216892, 0.19966859030441778, 0.16666666666666666, 0.23267051394216892]
from probs:  [0.03465897211566213, 0.1336647430289155, 0.23267051394216892, 0.19966859030441778, 0.16666666666666666, 0.23267051394216892]
maxi score, test score, baseline:  0.0021 0.0 0.0021
start point for exploration sampling:  20015
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.03465897211566213, 0.1336647430289155, 0.23267051394216892, 0.19966859030441778, 0.16666666666666666, 0.23267051394216892]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.03465897211566213, 0.1336647430289155, 0.23267051394216892, 0.19966859030441778, 0.16666666666666666, 0.23267051394216892]
siam score:  -0.80136406
siam score:  -0.8015042
from probs:  [0.03465897211566213, 0.1336647430289155, 0.23267051394216892, 0.19966859030441778, 0.16666666666666666, 0.23267051394216892]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.03465897211566213, 0.1336647430289155, 0.23267051394216892, 0.19966859030441778, 0.16666666666666666, 0.23267051394216892]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[-0.417]
 [-0.417]
 [-0.417]
 [-0.417]
 [-0.417]] [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
using another actor
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.033551701427243605, 0.16134206805708973, 0.22523725137201278, 0.19328965971455125, 0.16134206805708973, 0.22523725137201278]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.033551701427243605, 0.16134206805708973, 0.22523725137201278, 0.19328965971455125, 0.16134206805708973, 0.22523725137201278]
from probs:  [0.033551701427243605, 0.16134206805708973, 0.22523725137201278, 0.19328965971455125, 0.16134206805708973, 0.22523725137201278]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.006]
 [0.009]
 [0.006]
 [0.009]] [[2.48 ]
 [1.756]
 [0.962]
 [0.301]
 [0.692]] [[0.002]
 [0.006]
 [0.009]
 [0.006]
 [0.009]]
from probs:  [0.033551701427243605, 0.16134206805708973, 0.22523725137201278, 0.19328965971455125, 0.16134206805708973, 0.22523725137201278]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.033551701427243605, 0.16134206805708973, 0.22523725137201278, 0.19328965971455125, 0.16134206805708973, 0.22523725137201278]
first move QE:  -0.026575659712041632
using another actor
maxi score, test score, baseline:  0.0021 0.0 0.0021
710 5946
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20015
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
UNIT TEST: sample policy line 217 mcts : [0.208 0.417 0.292 0.042 0.042]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.059770703043012484, 0.17638448154154432, 0.23469137079081023, 0.17638448154154432, 0.14723103691691136, 0.2055379261661773]
siam score:  -0.85485846
using explorer policy with actor:  1
siam score:  -0.85524714
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.557]
 [1.046]
 [0.557]
 [0.462]] [[0.573]
 [0.573]
 [0.997]
 [0.573]
 [0.479]] [[1.227]
 [1.227]
 [2.272]
 [1.227]
 [1.021]]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.08402491618544201, 0.16666666666666663, 0.2493084171478913, 0.16666666666666663, 0.13911941650625845, 0.19421391682707487]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using another actor
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.143]
 [0.143]
 [0.143]
 [0.143]] [[2.554]
 [1.606]
 [1.606]
 [1.606]
 [1.606]] [[0.979]
 [0.608]
 [0.608]
 [0.608]
 [0.608]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
723 6032
siam score:  -0.8518236
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.08177231380097125, 0.16219854283163004, 0.26943351487250844, 0.16219854283163004, 0.13538979982141044, 0.18900728584184964]
first move QE:  -0.03212219260246238
siam score:  -0.8441574
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.011]
 [0.027]
 [0.022]
 [0.026]] [[-0.133]
 [ 0.025]
 [-0.226]
 [-0.26 ]
 [-0.091]] [[0.192]
 [0.372]
 [0.07 ]
 [0.015]
 [0.248]]
727 6036
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.033]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.033]] [[0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]]
727 6052
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.131]
 [0.107]
 [0.036]
 [0.067]] [[0.544]
 [2.056]
 [1.215]
 [0.583]
 [1.204]] [[0.023]
 [0.721]
 [0.394]
 [0.041]
 [0.31 ]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.1030554809993771, 0.17938890380012457, 0.255722326600872, 0.15394442953320875, 0.12849995526629293, 0.17938890380012457]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.971185765864811
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.042 0.042 0.833]
from probs:  [0.10049835323658149, 0.17493770584542728, 0.27419017599055506, 0.15012458830914535, 0.12531147077286342, 0.17493770584542728]
maxi score, test score, baseline:  0.0021 0.0 0.0021
start point for exploration sampling:  20015
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0021 0.0 0.0021
733 6092
using another actor
from probs:  [0.1193867555760935, 0.19030662221195324, 0.261226488847813, 0.14302671112138007, 0.1193867555760935, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
734 6107
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.11399698811349436, 0.20428786563321832, 0.27200602377301125, 0.13656970749342534, 0.11399698811349436, 0.15914242687335634]
from probs:  [0.11399698811349436, 0.20428786563321832, 0.27200602377301125, 0.13656970749342534, 0.11399698811349436, 0.15914242687335634]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.068]
 [1.068]
 [1.068]
 [1.068]
 [1.219]] [[1.272]
 [1.272]
 [1.272]
 [1.272]
 [0.343]] [[2.146]
 [2.146]
 [2.146]
 [2.146]
 [1.828]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.0021 0.0 0.0021
using another actor
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[-0.062]
 [-0.085]
 [-0.085]
 [-0.085]
 [-0.085]] [[0.342]
 [0.313]
 [0.313]
 [0.313]
 [0.313]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.12027949567520868, 0.21968057637119007, 0.27932122478877885, 0.12027949567520868, 0.12027949567520868, 0.14015971181440495]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.82693255
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[-0.321]
 [-0.321]
 [-0.321]
 [-0.321]
 [-0.321]] [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using another actor
from probs:  [0.11793492389776153, 0.23489110654313383, 0.273876500758258, 0.11793492389776153, 0.11793492389776153, 0.13742762100532357]
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.023]] [[3.17 ]
 [3.17 ]
 [3.17 ]
 [3.17 ]
 [3.137]] [[1.52 ]
 [1.52 ]
 [1.52 ]
 [1.52 ]
 [1.499]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.0290175988552335
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0021 0.0 0.0021
from probs:  [0.12303761162756024, 0.24519896573705818, 0.2975538317839859, 0.10558598961191769, 0.10558598961191769, 0.12303761162756024]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.039]
 [0.045]
 [0.039]
 [0.039]] [[0.799]
 [0.799]
 [1.287]
 [0.799]
 [0.799]] [[0.923]
 [0.923]
 [1.525]
 [0.923]
 [0.923]]
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.028]
 [0.039]
 [0.022]
 [0.03 ]] [[1.274]
 [1.33 ]
 [1.318]
 [0.881]
 [1.053]] [[0.026]
 [0.028]
 [0.039]
 [0.022]
 [0.03 ]]
actor:  0 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.037]
 [0.057]
 [0.037]
 [0.037]] [[1.071]
 [1.071]
 [1.657]
 [1.071]
 [1.071]] [[0.896]
 [0.896]
 [1.521]
 [0.896]
 [0.896]]
using another actor
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12092818546215332, 0.24099169862400088, 0.30959942043077093, 0.10377625501046084, 0.10377625501046084, 0.12092818546215334]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]] [[0.916]
 [0.916]
 [0.916]
 [0.916]
 [4.262]] [[0.226]
 [0.226]
 [0.226]
 [0.226]
 [1.734]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.361]
 [0.361]
 [0.361]
 [0.357]] [[2.848]
 [2.848]
 [2.848]
 [2.848]
 [2.859]] [[2.105]
 [2.105]
 [2.105]
 [2.105]
 [2.112]]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.029]
 [0.027]
 [0.023]
 [0.025]] [[3.738]
 [4.748]
 [1.807]
 [3.738]
 [3.852]] [[1.177]
 [1.796]
 [0.009]
 [1.177]
 [1.248]]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
from probs:  [0.10796547163721364, 0.24578566866288595, 0.2917257343381101, 0.09265211641213894, 0.10796547163721364, 0.15390553731243775]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12141948712470142, 0.2420786325699421, 0.28732581211190744, 0.09125470076339125, 0.10633709394404633, 0.15158427348601158]
maxi score, test score, baseline:  0.0041 0.0 0.0041
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.11961540062205796, 0.2533400514856827, 0.28305664056648827, 0.08989881154125245, 0.10475710608165521, 0.14933198970286346]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.11961540062205796, 0.2533400514856827, 0.28305664056648827, 0.08989881154125245, 0.10475710608165521, 0.14933198970286346]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.11961540062205796, 0.2533400514856827, 0.28305664056648827, 0.08989881154125245, 0.10475710608165521, 0.14933198970286346]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
773 6202
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12692985072999802, 0.2531526778229454, 0.2671774363888285, 0.112905092164115, 0.09888033359823195, 0.14095460929588108]
siam score:  -0.79482186
779 6215
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12692985072999802, 0.2531526778229454, 0.2671774363888285, 0.112905092164115, 0.09888033359823195, 0.14095460929588108]
Printing some Q and Qe and total Qs values:  [[0.72 ]
 [0.81 ]
 [0.72 ]
 [0.176]
 [0.212]] [[1.189]
 [1.586]
 [1.189]
 [1.481]
 [2.591]] [[1.496]
 [1.901]
 [1.496]
 [0.685]
 [1.422]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
782 6219
782 6220
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.022]
 [0.052]
 [0.048]
 [0.046]] [[1.143]
 [0.775]
 [1.086]
 [1.143]
 [1.097]] [[1.262]
 [0.473]
 [1.154]
 [1.262]
 [1.164]]
using another actor
maxi score, test score, baseline:  0.0041 0.0 0.0041
using another actor
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.035]] [[3.747]
 [3.747]
 [3.747]
 [3.747]
 [3.747]] [[1.538]
 [1.538]
 [1.538]
 [1.538]
 [1.538]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.1251743112362588, 0.26348216267095165, 0.26348216267095165, 0.11134352609278951, 0.09751274094932023, 0.13900509637972808]
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.068]
 [0.068]
 [0.068]
 [0.056]] [[2.222]
 [2.222]
 [2.222]
 [2.222]
 [2.383]] [[1.888]
 [1.888]
 [1.888]
 [1.888]
 [2.03 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.175]
 [0.184]
 [0.137]
 [0.203]] [[2.211]
 [1.897]
 [2.182]
 [0.535]
 [2.164]] [[1.488]
 [1.201]
 [1.4  ]
 [0.252]
 [1.401]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.1251743112362588, 0.26348216267095165, 0.26348216267095165, 0.11134352609278951, 0.09751274094932023, 0.13900509637972808]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.1218049938379113, 0.2563900123241774, 0.2563900123241774, 0.1218049938379113, 0.09488799014065809, 0.14872199753516452]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.1218049938379113, 0.2563900123241774, 0.2563900123241774, 0.1218049938379113, 0.09488799014065809, 0.14872199753516452]
maxi score, test score, baseline:  0.0041 0.0 0.0041
siam score:  -0.8028965
maxi score, test score, baseline:  0.0041 0.0 0.0041
from probs:  [0.1218049938379113, 0.2563900123241774, 0.2563900123241774, 0.1218049938379113, 0.09488799014065809, 0.14872199753516452]
from probs:  [0.1218049938379113, 0.2563900123241774, 0.2563900123241774, 0.1218049938379113, 0.09488799014065809, 0.14872199753516452]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.081]
 [0.081]
 [0.08 ]
 [0.081]
 [0.081]] [[2.287]
 [2.287]
 [2.943]
 [2.287]
 [2.287]] [[1.277]
 [1.277]
 [1.947]
 [1.277]
 [1.277]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.031]
 [0.031]
 [0.031]
 [0.041]] [[1.786]
 [2.959]
 [2.959]
 [2.959]
 [3.045]] [[0.215]
 [1.007]
 [1.007]
 [1.007]
 [1.084]]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
Printing some Q and Qe and total Qs values:  [[0.153]
 [0.153]
 [0.153]
 [0.153]
 [0.153]] [[2.28]
 [2.28]
 [2.28]
 [2.28]
 [2.28]] [[3.349]
 [3.349]
 [3.349]
 [3.349]
 [3.349]]
Printing some Q and Qe and total Qs values:  [[0.14]
 [0.32]
 [0.14]
 [0.14]
 [0.14]] [[2.39 ]
 [2.504]
 [2.39 ]
 [2.39 ]
 [2.39 ]] [[1.514]
 [1.951]
 [1.514]
 [1.514]
 [1.514]]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8024867
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
798 6255
line 256 mcts: sample exp_bonus 0.8454178073345431
maxi score, test score, baseline:  0.0041 0.0 0.0041
using explorer policy with actor:  1
siam score:  -0.80384576
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.11131779828032565, 0.2712145291741997, 0.25891478064390167, 0.12361754681062367, 0.08671830121972965, 0.14821704387121964]
siam score:  -0.80188805
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
from probs:  [0.10996525331745048, 0.2679191905045527, 0.2679191905045527, 0.12211555617799683, 0.08566464759635785, 0.14641616189908946]
803 6266
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0041 0.0 0.0041
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  0
807 6285
maxi score, test score, baseline:  0.0041 0.0 0.0041
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8226338
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]] [[2.778]
 [2.778]
 [2.778]
 [2.778]
 [2.778]] [[2.161]
 [2.161]
 [2.161]
 [2.161]
 [2.161]]
siam score:  -0.8251351
Printing some Q and Qe and total Qs values:  [[0.164]
 [0.164]
 [0.165]
 [0.164]
 [0.116]] [[1.436]
 [1.436]
 [2.202]
 [1.436]
 [1.081]] [[0.799]
 [0.799]
 [1.31 ]
 [0.799]
 [0.466]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.13055351449309158, 0.2605608623179619, 0.2605608623179619, 0.1305535144930916, 0.08721773188480152, 0.13055351449309158]
maxi score, test score, baseline:  0.0041 0.0 0.0041
start point for exploration sampling:  20015
Printing some Q and Qe and total Qs values:  [[0.098]
 [0.116]
 [0.098]
 [0.105]
 [0.093]] [[2.302]
 [2.533]
 [2.59 ]
 [2.415]
 [2.511]] [[1.567]
 [1.846]
 [1.89 ]
 [1.702]
 [1.796]]
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
821 6296
Printing some Q and Qe and total Qs values:  [[0.15 ]
 [0.15 ]
 [0.15 ]
 [0.15 ]
 [0.137]] [[1.512]
 [1.512]
 [1.512]
 [1.512]
 [1.551]] [[0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.425]]
using another actor
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  -0.03369205142244131
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0041 0.0 0.0041
first move QE:  -0.033243742566588144
maxi score, test score, baseline:  0.0041 0.0 0.0041
from probs:  [0.15792257758106376, 0.2523587397055747, 0.2523587397055747, 0.1264438568728935, 0.08447222926199975, 0.12644385687289347]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.15792257758106376, 0.2523587397055747, 0.2523587397055747, 0.1264438568728935, 0.08447222926199975, 0.12644385687289347]
first move QE:  -0.03337921135231089
maxi score, test score, baseline:  0.0041 0.0 0.0041
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.1579225775810638, 0.2523587397055747, 0.2523587397055747, 0.1264438568728935, 0.08447222926199975, 0.12644385687289347]
siam score:  -0.8266727
maxi score, test score, baseline:  0.0041 0.0 0.0041
828 6303
from probs:  [0.1579225775810638, 0.2523587397055747, 0.2523587397055747, 0.1264438568728935, 0.08447222926199975, 0.12644385687289347]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.1579225775810638, 0.2523587397055747, 0.2523587397055747, 0.1264438568728935, 0.08447222926199975, 0.12644385687289347]
start point for exploration sampling:  20015
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.1579225775810638, 0.2523587397055747, 0.2523587397055747, 0.1264438568728935, 0.08447222926199975, 0.12644385687289347]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.1579225775810638, 0.2523587397055747, 0.2523587397055747, 0.1264438568728935, 0.08447222926199975, 0.12644385687289347]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.1579225775810638, 0.2523587397055747, 0.2523587397055747, 0.1264438568728935, 0.08447222926199975, 0.12644385687289347]
using another actor
Starting evaluation
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.   ]
 [0.049]
 [0.03 ]
 [0.03 ]] [[-0.888]
 [-0.56 ]
 [-0.842]
 [-1.141]
 [-1.063]] [[0.472]
 [1.034]
 [0.605]
 [0.019]
 [0.163]]
Printing some Q and Qe and total Qs values:  [[0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.134]] [[2.024]
 [2.024]
 [2.024]
 [2.024]
 [2.024]] [[0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.134]]
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  0
from probs:  [0.1546765640087107, 0.2677261033551525, 0.2471716416557995, 0.12384487145968114, 0.08273594806097502, 0.12384487145968111]
actor:  0 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8254008
actor:  0 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn probs:  [0.1546781071835233, 0.26771309659601783, 0.24716128033920062, 0.12385038279829753, 0.0827467502846632, 0.12385038279829755]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.1716611992788959, 0.26156278629902174, 0.24158465585010488, 0.1217158731566037, 0.08175961225876997, 0.1217158731566037]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using another actor
from probs:  [0.1716611992788959, 0.26156278629902174, 0.24158465585010488, 0.1217158731566037, 0.08175961225876997, 0.1217158731566037]
maxi score, test score, baseline:  0.0281 0.48 0.48
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.1682989018630661, 0.2662330136470316, 0.2466461912902385, 0.11933184597108337, 0.08015820125749719, 0.11933184597108336]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0281 0.48 0.48
probs:  [0.16195455179144014, 0.27504530879687716, 0.25619684929597103, 0.11483340303917469, 0.07713648403736233, 0.11483340303917468]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using another actor
from probs:  [0.16195455179144014, 0.27504530879687716, 0.25619684929597103, 0.11483340303917469, 0.07713648403736233, 0.11483340303917468]
UNIT TEST: sample policy line 217 mcts : [0.292 0.208 0.083 0.042 0.375]
from probs:  [0.1604425047647985, 0.28181366185122775, 0.25380493329282106, 0.11376129050078722, 0.0764163190895782, 0.1137612905007872]
using another actor
siam score:  -0.8250757
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.1604425047647985, 0.28181366185122775, 0.253804933292821, 0.11376129050078722, 0.0764163190895782, 0.1137612905007872]
maxi score, test score, baseline:  0.0301 0.48 0.48
maxi score, test score, baseline:  0.0301 0.48 0.48
maxi score, test score, baseline:  0.0301 0.48 0.48
maxi score, test score, baseline:  0.0301 0.48 0.48
probs:  [0.16666666666666666, 0.27664795683180127, 0.2583177418042788, 0.11167602158409934, 0.07501559152905446, 0.11167602158409932]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.475]
 [0.475]
 [0.59 ]
 [0.396]] [[2.206]
 [2.183]
 [2.183]
 [2.534]
 [2.314]] [[1.698]
 [1.705]
 [1.705]
 [2.263]
 [1.679]]
maxi score, test score, baseline:  0.0301 0.48 0.48
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.915]
 [1.324]
 [0.915]
 [0.949]] [[0.619]
 [1.376]
 [0.553]
 [1.376]
 [1.53 ]] [[1.293]
 [2.296]
 [2.838]
 [2.296]
 [2.415]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0301 0.48 0.48
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.001]
 [0.027]
 [0.013]
 [0.012]] [[-0.353]
 [-0.263]
 [-0.522]
 [-0.689]
 [-0.494]] [[0.462]
 [0.556]
 [0.265]
 [0.013]
 [0.272]]
in main func line 156:  846
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.17112659330219304, 0.2692449792837727, 0.2692449792837727, 0.1086876204048241, 0.0730082073206133, 0.10868762040482409]
849 6412
849 6413
maxi score, test score, baseline:  0.0301 0.48 0.48
probs:  [0.17112659330219304, 0.2692449792837727, 0.2692449792837727, 0.1086876204048241, 0.0730082073206133, 0.10868762040482409]
maxi score, test score, baseline:  0.0301 0.48 0.48
siam score:  -0.8258713
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]] [[1.986]
 [1.986]
 [1.986]
 [1.986]
 [1.986]] [[1.674]
 [1.674]
 [1.674]
 [1.674]
 [1.674]]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.17689076719645314, 0.2645259145946226, 0.2645259145946226, 0.11554616401773446, 0.07172859031864968, 0.10678264927791747]
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.937]
 [0.421]
 [0.421]
 [0.421]] [[1.55 ]
 [1.135]
 [1.55 ]
 [1.55 ]
 [1.55 ]] [[1.146]
 [2.039]
 [1.146]
 [1.146]
 [1.146]]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
857 6432
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
857 6434
siam score:  -0.84828293
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.032100000000000004 0.48 0.48
probs:  [0.1980592639255688, 0.24719550311341554, 0.25538487631139, 0.11616553194582416, 0.08340803915392629, 0.09978678554987522]
maxi score, test score, baseline:  0.032100000000000004 0.48 0.48
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.077]] [[5.417]
 [5.417]
 [5.417]
 [5.417]
 [5.491]] [[1.986]
 [1.986]
 [1.986]
 [1.986]
 [2.02 ]]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.032100000000000004 0.48 0.48
probs:  [0.20292498033326184, 0.2512693985553887, 0.2512693985553887, 0.11429354692602924, 0.08206393477794464, 0.09817874085198694]
actor:  0 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.86582506
siam score:  -0.86430895
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.034100000000000005 0.48 0.48
probs:  [0.20600283965272442, 0.26107348183320533, 0.2453390126387822, 0.11159602448618577, 0.08012708609733957, 0.09586155529176267]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8614935
from probs:  [0.2043948176716473, 0.26684141243851184, 0.24342393940093762, 0.11072492552135055, 0.0795016281379183, 0.09511327682963441]
maxi score, test score, baseline:  0.034100000000000005 0.48 0.48
from probs:  [0.2043948176716473, 0.26684141243851184, 0.24342393940093762, 0.11072492552135055, 0.0795016281379183, 0.09511327682963441]
start point for exploration sampling:  20015
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
870 6480
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.8392833042228387
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0361 0.48 0.48
probs:  [0.21174234846969395, 0.2643306439065591, 0.24930541663888336, 0.10656575759596364, 0.07651530306061213, 0.09154053032828788]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0361 0.48 0.48
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0361 0.48 0.48
probs:  [0.208607966365209, 0.26041780716929075, 0.2604178071692907, 0.10498828475704557, 0.07538266144042745, 0.09018547309873651]
start point for exploration sampling:  20015
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0361 0.48 0.48
probs:  [0.20707531790022826, 0.2658515378763178, 0.25850451037930655, 0.10421693294207152, 0.07482882295402675, 0.08952287794804913]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0361 0.48 0.48
probs:  [0.20407660504273686, 0.26200167091536175, 0.2692423041494398, 0.1027077397656434, 0.07374520682933099, 0.08822647329748717]
line 256 mcts: sample exp_bonus 1.1329033874335552
from probs:  [0.20407660504273686, 0.26200167091536175, 0.2692423041494398, 0.1027077397656434, 0.07374520682933099, 0.08822647329748717]
maxi score, test score, baseline:  0.0361 0.48 0.48
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0361 0.48 0.48
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
880 6518
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]] [[-0.066]
 [-0.066]
 [-0.066]
 [-0.066]
 [-0.066]] [[0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]]
880 6519
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0361 0.48 0.48
from probs:  [0.21543805562007728, 0.25826171421331595, 0.2653989906455223, 0.10124163270477438, 0.07269252697594866, 0.0869670798403615]
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0381 0.48 0.48
probs:  [0.21543805562007728, 0.25826171421331595, 0.2653989906455223, 0.10124163270477438, 0.07269252697594866, 0.0869670798403615]
maxi score, test score, baseline:  0.0381 0.48 0.48
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]] [[1.519]
 [1.519]
 [1.519]
 [1.519]
 [1.519]] [[1.415]
 [1.415]
 [1.415]
 [1.415]
 [1.415]]
from probs:  [0.22099800817695772, 0.2564314917706259, 0.26351818848935943, 0.1005241639584862, 0.07217737708355174, 0.08635077052101897]
883 6534
using another actor
start point for exploration sampling:  20015
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.84 ]
 [0.437]
 [0.437]
 [0.337]] [[1.158]
 [1.392]
 [1.165]
 [1.165]
 [1.163]] [[0.794]
 [1.488]
 [0.848]
 [0.848]
 [0.732]]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.667 0.042 0.208]
maxi score, test score, baseline:  0.0381 0.48 0.48
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus -1.5958835078440008
Printing some Q and Qe and total Qs values:  [[0.798]
 [0.809]
 [0.864]
 [0.75 ]
 [0.73 ]] [[1.422]
 [0.494]
 [0.756]
 [1.077]
 [1.881]] [[0.798]
 [0.809]
 [0.864]
 [0.75 ]
 [0.73 ]]
actor:  0 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.040100000000000004 0.48 0.48
maxi score, test score, baseline:  0.040100000000000004 0.48 0.48
probs:  [0.22647971186475965, 0.2546270272520975, 0.261663856098932, 0.09981679262173922, 0.07166947723440134, 0.08574313492807029]
maxi score, test score, baseline:  0.040100000000000004 0.48 0.48
probs:  [0.2264797118647596, 0.2546270272520975, 0.261663856098932, 0.09981679262173922, 0.07166947723440134, 0.08574313492807029]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 2.2146367980903783
Printing some Q and Qe and total Qs values:  [[0.794]
 [1.25 ]
 [1.089]
 [1.089]
 [1.089]] [[0.93 ]
 [0.438]
 [0.41 ]
 [0.41 ]
 [0.41 ]] [[1.227]
 [1.976]
 [1.643]
 [1.643]
 [1.643]]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0441 0.48 0.48
probs:  [0.22868880869370195, 0.2562542051501621, 0.26314555426427716, 0.09775317552551635, 0.07018777906905621, 0.08397047729728628]
maxi score, test score, baseline:  0.0441 0.48 0.48
maxi score, test score, baseline:  0.0441 0.48 0.48
maxi score, test score, baseline:  0.0441 0.48 0.48
probs:  [0.22868880869370195, 0.2562542051501621, 0.26314555426427716, 0.09775317552551635, 0.07018777906905622, 0.08397047729728628]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0441 0.48 0.48
probs:  [0.23917502966434045, 0.25277034772640433, 0.2595680067574362, 0.09642419001267019, 0.06923355388854253, 0.08282887195060636]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.01 ]
 [0.018]
 [0.013]
 [0.002]] [[ 0.418]
 [-0.031]
 [-0.052]
 [-0.136]
 [-0.048]] [[0.17 ]
 [0.039]
 [0.049]
 [0.01 ]
 [0.018]]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0441 0.48 0.48
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.339]
 [1.339]
 [1.467]
 [1.339]
 [1.339]] [[0.256]
 [0.256]
 [0.153]
 [0.256]
 [0.256]] [[1.876]
 [1.876]
 [2.098]
 [1.876]
 [1.876]]
siam score:  -0.905531
maxi score, test score, baseline:  0.0441 0.48 0.48
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.078]
 [0.123]
 [0.123]
 [0.205]
 [0.136]] [[2.901]
 [3.059]
 [3.059]
 [1.072]
 [3.972]] [[0.98 ]
 [1.154]
 [1.154]
 [0.124]
 [1.717]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.24446151751502537, 0.26418415575827126, 0.251035730262774, 0.09325462431680702, 0.06695777332581254, 0.08010619882130977]
in main func line 156:  909
maxi score, test score, baseline:  0.0441 0.48 0.48
probs:  [0.24286487217638303, 0.2624586964503101, 0.25592742169233446, 0.09264555274294214, 0.0665204537110394, 0.07958300322699076]
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0461 0.48 0.48
probs:  [0.24286487217638303, 0.2624586964503101, 0.25592742169233446, 0.09264555274294214, 0.0665204537110394, 0.07958300322699076]
from probs:  [0.24286487217638303, 0.26245869645031017, 0.25592742169233446, 0.09264555274294216, 0.0665204537110394, 0.07958300322699077]
912 6590
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]] [[1.516]
 [1.516]
 [1.516]
 [1.516]
 [1.516]] [[1.751]
 [1.751]
 [1.751]
 [1.751]
 [1.751]]
maxi score, test score, baseline:  0.0461 0.48 0.48
probs:  [0.24777784177081533, 0.2607556297874791, 0.25426673577914727, 0.0920443855708499, 0.06608880953752233, 0.0790665975541861]
Printing some Q and Qe and total Qs values:  [[1.135]
 [1.135]
 [1.135]
 [1.135]
 [1.14 ]] [[3.255]
 [3.255]
 [3.255]
 [3.255]
 [3.124]] [[2.554]
 [2.554]
 [2.554]
 [2.554]
 [2.497]]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.131432963562143
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
in main func line 156:  916
maxi score, test score, baseline:  0.048100000000000004 0.48 0.48
probs:  [0.24618040322734466, 0.26552158239075285, 0.2526274629484807, 0.09145096992007934, 0.06566273103553513, 0.07855685047780724]
siam score:  -0.89928174
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.048100000000000004 0.48 0.48
probs:  [0.24460343030417891, 0.263820714488771, 0.25100919169904296, 0.09727091822230645, 0.06524211124798636, 0.0780536340377144]
siam score:  -0.89450264
maxi score, test score, baseline:  0.048100000000000004 0.48 0.48
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.259]
 [0.259]
 [0.391]
 [0.259]
 [0.208]] [[-0.065]
 [-0.065]
 [ 0.056]
 [-0.065]
 [-0.183]] [[0.739]
 [0.739]
 [1.055]
 [0.739]
 [0.559]]
maxi score, test score, baseline:  0.050100000000000006 0.48 0.48
probs:  [0.24460343030417891, 0.263820714488771, 0.25100919169904296, 0.09727091822230645, 0.06524211124798636, 0.0780536340377144]
using another actor
from probs:  [0.24460343030417891, 0.263820714488771, 0.25100919169904296, 0.09727091822230645, 0.06524211124798636, 0.0780536340377144]
Printing some Q and Qe and total Qs values:  [[1.078]
 [1.078]
 [1.078]
 [1.078]
 [1.078]] [[0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.673]] [[2.286]
 [2.286]
 [2.286]
 [2.286]
 [2.286]]
siam score:  -0.9012772
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
925 6608
actor:  0 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.213]
 [0.313]
 [0.192]
 [0.241]] [[2.283]
 [2.279]
 [2.285]
 [1.418]
 [2.095]] [[1.839]
 [2.002]
 [2.193]
 [0.665]
 [1.777]]
maxi score, test score, baseline:  0.0521 0.48 0.48
probs:  [0.24150932804401118, 0.26680825583353607, 0.25415879193877366, 0.096040493254243, 0.06441683351733689, 0.07706629741209932]
actor:  0 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
929 6608
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0541 0.48 0.48
probs:  [0.23999144647539003, 0.2714163521077001, 0.25256140872831406, 0.09543688056676397, 0.06401197493445399, 0.07658193718737796]
in main func line 156:  932
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.2384925254541087, 0.27596688656060026, 0.2509839791562726, 0.09484080787922465, 0.06361217362381508, 0.07610362732597889]
Printing some Q and Qe and total Qs values:  [[0.09 ]
 [0.131]
 [0.131]
 [0.131]
 [0.185]] [[2.094]
 [3.022]
 [3.022]
 [3.022]
 [1.471]] [[1.147]
 [2.062]
 [2.062]
 [2.062]
 [0.702]]
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.474]
 [0.64 ]
 [0.56 ]
 [0.444]] [[1.004]
 [1.262]
 [1.467]
 [1.295]
 [1.164]] [[0.802]
 [1.195]
 [1.623]
 [1.358]
 [1.068]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
936 6612
UNIT TEST: sample policy line 217 mcts : [0.042 0.    0.625 0.042 0.292]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.9024735
maxi score, test score, baseline:  0.0541 0.48 0.48
probs:  [0.23267951468387754, 0.26924047666264045, 0.250959995673259, 0.10471614775820728, 0.062061692116317235, 0.08034217310569867]
maxi score, test score, baseline:  0.0541 0.48 0.48
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.417]
 [0.58 ]
 [0.121]
 [0.128]] [[ 0.   ]
 [ 0.   ]
 [ 0.767]
 [-0.619]
 [-0.378]] [[0.613]
 [0.613]
 [1.189]
 [0.03 ]
 [0.179]]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.24498864856303132, 0.2687826683796484, 0.24498864856303132, 0.10222452966332869, 0.06058499498424879, 0.0784305098467116]
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]] [[1.443]
 [1.443]
 [1.443]
 [1.443]
 [1.443]] [[0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]]
maxi score, test score, baseline:  0.0541 0.48 0.48
maxi score, test score, baseline:  0.0541 0.48 0.48
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  0.0541 0.48 0.48
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.72 ]
 [0.72 ]
 [0.786]
 [0.72 ]
 [0.72 ]] [[0.426]
 [0.426]
 [0.957]
 [0.426]
 [0.426]] [[0.913]
 [0.913]
 [1.398]
 [0.913]
 [0.913]]
first move QE:  -0.04221613271772258
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.056100000000000004 0.48 0.48
siam score:  -0.9115115
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.056100000000000004 0.48 0.48
maxi score, test score, baseline:  0.056100000000000004 0.48 0.48
maxi score, test score, baseline:  0.056100000000000004 0.48 0.48
probs:  [0.2325392980639126, 0.27206287690226016, 0.2551242002572541, 0.10267611045219918, 0.057506306065516266, 0.08009120825885771]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.22 ]
 [1.274]
 [1.274]
 [0.961]
 [1.083]] [[0.014]
 [0.095]
 [0.095]
 [0.209]
 [0.16 ]] [[1.777]
 [1.913]
 [1.913]
 [1.325]
 [1.553]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.24110903714960605, 0.26902492608070827, 0.2522753927220469, 0.1015295924940947, 0.05686417020433109, 0.07919688134921289]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.056100000000000004 0.48 0.48
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.056100000000000004 0.48 0.48
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.24130325895526006, 0.2631481152348483, 0.2631481152348483, 0.09931169313793604, 0.05562198057875944, 0.07746683685834774]
UNIT TEST: sample policy line 217 mcts : [0.292 0.25  0.125 0.042 0.292]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.058100000000000006 0.48 0.48
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.058100000000000006 0.48 0.48
maxi score, test score, baseline:  0.058100000000000006 0.48 0.48
probs:  [0.2386961177617594, 0.26030495309028717, 0.26570716192241917, 0.10364089695846054, 0.055021017469272945, 0.07662985279780075]
line 256 mcts: sample exp_bonus 1.810214227296711
maxi score, test score, baseline:  0.058100000000000006 0.48 0.48
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.285]
 [0.285]
 [0.285]
 [0.288]] [[4.401]
 [4.401]
 [4.401]
 [4.401]
 [4.886]] [[1.7  ]
 [1.7  ]
 [1.7  ]
 [1.7  ]
 [1.917]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
963 6644
maxi score, test score, baseline:  0.056100000000000004 0.48 0.48
UNIT TEST: sample policy line 217 mcts : [0.208 0.083 0.5   0.083 0.125]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
maxi score, test score, baseline:  0.056100000000000004 0.48 0.48
probs:  [0.24683364166785257, 0.2575225716680107, 0.26286703666808975, 0.10253308666571793, 0.05443290166500641, 0.07581076166532263]
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.769]
 [0.903]
 [0.641]
 [0.573]
 [0.674]] [[0.884]
 [0.693]
 [2.03 ]
 [1.964]
 [2.661]] [[0.455]
 [0.52 ]
 [1.193]
 [1.026]
 [1.771]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0601 0.48 0.48
from probs:  [0.24951109998591972, 0.2547990425382124, 0.2653749276427979, 0.10144870852172282, 0.05385722555108811, 0.07500899576025909]
maxi score, test score, baseline:  0.0601 0.48 0.48
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
969 6652
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
maxi score, test score, baseline:  0.0601 0.48 0.48
maxi score, test score, baseline:  0.0601 0.48 0.48
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.335]] [[3.588]
 [3.588]
 [3.588]
 [3.588]
 [3.476]] [[1.74 ]
 [1.74 ]
 [1.74 ]
 [1.74 ]
 [1.642]]
actor:  0 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0621 0.48 0.48
siam score:  -0.91292405
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.042 0.167 0.042 0.167 0.583]
Printing some Q and Qe and total Qs values:  [[0.2  ]
 [0.455]
 [0.976]
 [0.455]
 [0.502]] [[1.986]
 [2.608]
 [0.941]
 [2.608]
 [1.934]] [[1.089]
 [1.965]
 [0.821]
 [1.965]
 [1.339]]
maxi score, test score, baseline:  0.0621 0.48 0.48
probs:  [0.2534587671382106, 0.2534587671382106, 0.263979021740822, 0.10091507540034549, 0.05357392968859424, 0.07461443889381701]
maxi score, test score, baseline:  0.0621 0.48 0.48
siam score:  -0.9092589
maxi score, test score, baseline:  0.0621 0.48 0.48
probs:  [0.2534587671382106, 0.2534587671382106, 0.263979021740822, 0.10091507540034549, 0.05357392968859424, 0.07461443889381701]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
siam score:  -0.9090638
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.14]
 [0.14]
 [0.14]
 [0.14]
 [0.14]] [[0.027]
 [0.027]
 [0.027]
 [0.027]
 [0.027]] [[0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]]
maxi score, test score, baseline:  0.0641 0.48 0.48
probs:  [0.252132517996749, 0.25736512113940707, 0.26259772428206524, 0.10038702685966407, 0.053293598575741175, 0.07422401114637357]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
in main func line 156:  982
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0641 0.48 0.48
maxi score, test score, baseline:  0.0641 0.48 0.48
probs:  [0.2546996368984695, 0.2598780469121049, 0.2598780469121049, 0.0993473364894057, 0.052741646366686594, 0.07345528642122842]
siam score:  -0.8987309
maxi score, test score, baseline:  0.0641 0.48 0.48
probs:  [0.2546996368984695, 0.2598780469121049, 0.2598780469121049, 0.0993473364894057, 0.052741646366686594, 0.07345528642122842]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0641 0.48 0.48
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.051]
 [0.264]
 [0.078]
 [0.117]] [[ 2.092]
 [ 0.149]
 [-0.415]
 [ 1.701]
 [-0.116]] [[1.44 ]
 [0.246]
 [0.064]
 [1.183]
 [0.136]]
maxi score, test score, baseline:  0.0641 0.48 0.48
probs:  [0.2546996368984695, 0.2598780469121049, 0.2598780469121049, 0.0993473364894057, 0.052741646366686594, 0.07345528642122842]
from probs:  [0.2546996368984695, 0.2598780469121049, 0.2598780469121049, 0.0993473364894057, 0.052741646366686594, 0.07345528642122842]
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0661 0.48 0.48
probs:  [0.2546996368984695, 0.2598780469121049, 0.2598780469121049, 0.0993473364894057, 0.052741646366686594, 0.07345528642122842]
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.541]
 [0.23 ]
 [0.396]
 [0.315]] [[2.055]
 [1.637]
 [2.559]
 [2.214]
 [2.825]] [[1.039]
 [1.059]
 [1.052]
 [1.142]
 [1.357]]
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.172]
 [0.263]
 [0.112]
 [0.143]] [[0.705]
 [0.268]
 [1.714]
 [3.085]
 [2.374]] [[0.137]
 [0.081]
 [0.849]
 [1.259]
 [0.983]]
maxi score, test score, baseline:  0.0661 0.48 0.48
probs:  [0.2546996368984695, 0.25987804691210487, 0.2598780469121049, 0.0993473364894057, 0.052741646366686594, 0.07345528642122842]
maxi score, test score, baseline:  0.0661 0.48 0.48
maxi score, test score, baseline:  0.0661 0.48 0.48
probs:  [0.2546996368984695, 0.25987804691210487, 0.2598780469121049, 0.0993473364894057, 0.052741646366686594, 0.07345528642122842]
Printing some Q and Qe and total Qs values:  [[1.166]
 [1.05 ]
 [1.461]
 [1.478]
 [1.222]] [[ 0.425]
 [-0.038]
 [ 0.26 ]
 [ 0.16 ]
 [ 0.597]] [[2.307]
 [1.506]
 [2.559]
 [2.452]
 [2.625]]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]] [[3.389]
 [3.389]
 [3.389]
 [3.389]
 [3.389]] [[2.175]
 [2.175]
 [2.175]
 [2.175]
 [2.175]]
maxi score, test score, baseline:  0.0661 0.48 0.48
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.7669981957134755
maxi score, test score, baseline:  0.0661 0.48 0.48
probs:  [0.2520887985806784, 0.2572141264955191, 0.26233945441035983, 0.09832896113545725, 0.05732633781673162, 0.07270232156125374]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.299248596161134
in main func line 156:  1001
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0661 0.48 0.48
probs:  [0.25590254205325486, 0.25590254205325486, 0.26100173493248846, 0.09782756279701288, 0.05703401976314399, 0.07233159840084483]
maxi score, test score, baseline:  0.0661 0.48 0.48
probs:  [0.25590254205325486, 0.25590254205325486, 0.26100173493248846, 0.09782756279701288, 0.05703401976314399, 0.07233159840084483]
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.503]] [[3.295]
 [3.295]
 [3.295]
 [3.295]
 [3.664]] [[2.006]
 [2.006]
 [2.006]
 [2.006]
 [2.268]]
maxi score, test score, baseline:  0.0661 0.48 0.48
probs:  [0.25590254205325486, 0.25590254205325486, 0.26100173493248846, 0.09782756279701288, 0.05703401976314399, 0.07233159840084483]
actor:  1 policy actor:  1  step number:  88 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
1004 6682
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0661 0.48 0.48
probs:  [0.2533190961903197, 0.2583668105315034, 0.2634145248726871, 0.09683995161362582, 0.056458236884156444, 0.07160137990770747]
from probs:  [0.2533190961903197, 0.2583668105315034, 0.2634145248726871, 0.09683995161362582, 0.056458236884156444, 0.07160137990770747]
maxi score, test score, baseline:  0.0661 0.48 0.48
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.484]
 [0.653]
 [0.265]
 [0.484]] [[ 0.364]
 [ 0.364]
 [ 1.135]
 [-0.025]
 [ 0.364]] [[0.59 ]
 [0.59 ]
 [1.309]
 [0.126]
 [0.59 ]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1009 6688
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.2557845557904697, 0.26078182060115024, 0.26078182060115024, 0.09587208184869227, 0.05589396336324792, 0.07088575779528955]
1013 6696
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.25451268848841485, 0.26445752114748067, 0.25948510481794773, 0.09539536594336152, 0.055616035307098195, 0.07053328429569694]
from probs:  [0.25451268848841485, 0.26445752114748067, 0.25948510481794773, 0.09539536594336152, 0.055616035307098195, 0.07053328429569694]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
1014 6704
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.07010000000000001 0.48 0.48
probs:  [0.2532534071113844, 0.2680968483304788, 0.2582012208510825, 0.09492336744104343, 0.0553408575234582, 0.07018429874255266]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.90534854
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
maxi score, test score, baseline:  0.0721 0.48 0.48
probs:  [0.25679613730840106, 0.27128123080439404, 0.25679613730840106, 0.0926317443538134, 0.05400482836449866, 0.06848992186049169]
maxi score, test score, baseline:  0.0721 0.48 0.48
probs:  [0.25679613730840106, 0.27128123080439404, 0.25679613730840106, 0.0926317443538134, 0.05400482836449866, 0.06848992186049169]
Printing some Q and Qe and total Qs values:  [[0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]] [[3.738]
 [3.738]
 [3.738]
 [3.738]
 [3.738]] [[1.707]
 [1.707]
 [1.707]
 [1.707]
 [1.707]]
maxi score, test score, baseline:  0.0721 0.48 0.48
probs:  [0.25679613730840106, 0.27128123080439404, 0.25679613730840106, 0.0926317443538134, 0.05400482836449866, 0.06848992186049169]
from probs:  [0.25679613730840106, 0.27128123080439404, 0.25679613730840106, 0.0926317443538134, 0.05400482836449866, 0.06848992186049169]
using another actor
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.864]
 [0.864]
 [0.864]
 [0.864]
 [0.864]] [[1.078]
 [1.078]
 [1.078]
 [1.078]
 [1.078]] [[0.864]
 [0.864]
 [0.864]
 [0.864]
 [0.864]]
maxi score, test score, baseline:  0.0721 0.48 0.48
probs:  [0.25679613730840106, 0.27128123080439404, 0.25679613730840106, 0.0926317443538134, 0.05400482836449866, 0.06848992186049169]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn probs:  [0.25679613730840106, 0.27128123080439404, 0.25679613730840106, 0.0926317443538134, 0.05400482836449866, 0.06848992186049169]
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.759]
 [1.345]
 [1.094]
 [1.094]
 [1.29 ]] [[0.969]
 [0.466]
 [1.064]
 [1.064]
 [0.959]] [[1.162]
 [2.165]
 [1.862]
 [1.862]
 [2.22 ]]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.419]
 [0.491]
 [0.419]
 [0.419]] [[0.022]
 [0.022]
 [0.205]
 [0.022]
 [0.022]] [[1.23 ]
 [1.23 ]
 [1.639]
 [1.23 ]
 [1.23 ]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.5641852730527934
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1261 1.0 1.0
maxi score, test score, baseline:  0.1261 1.0 1.0
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.948]
 [0.948]
 [0.948]
 [0.948]
 [0.948]] [[1.091]
 [1.091]
 [1.091]
 [1.091]
 [1.091]] [[2.988]
 [2.988]
 [2.988]
 [2.988]
 [2.988]]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus -0.6131820956861949
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.9324197
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.93314415
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.93311113
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1301 1.0 1.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1045 6730
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.952]
 [0.952]
 [1.024]
 [0.952]
 [0.952]] [[0.799]
 [0.799]
 [1.371]
 [0.799]
 [0.799]] [[1.557]
 [1.557]
 [2.081]
 [1.557]
 [1.557]]
Printing some Q and Qe and total Qs values:  [[0.98]
 [0.98]
 [0.98]
 [0.98]
 [0.98]] [[1.238]
 [1.238]
 [1.238]
 [1.238]
 [1.238]] [[2.614]
 [2.614]
 [2.614]
 [2.614]
 [2.614]]
siam score:  -0.93540245
maxi score, test score, baseline:  0.1341 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.226]
 [0.338]
 [0.338]
 [0.338]
 [0.338]] [[2.963]
 [2.458]
 [2.458]
 [2.458]
 [2.458]] [[1.406]
 [1.049]
 [1.049]
 [1.049]
 [1.049]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.166866066721274
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.93605286
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.93573534
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1050 6735
maxi score, test score, baseline:  0.1361 1.0 1.0
siam score:  -0.938143
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
1059 6742
Printing some Q and Qe and total Qs values:  [[0.95 ]
 [1.12 ]
 [0.971]
 [0.926]
 [0.917]] [[1.783]
 [2.041]
 [2.073]
 [1.873]
 [1.795]] [[1.493]
 [1.852]
 [1.753]
 [1.548]
 [1.475]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1060 6746
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.9388759
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.187]
 [0.233]
 [0.233]
 [0.233]
 [0.233]] [[4.746]
 [4.726]
 [4.726]
 [4.726]
 [4.726]] [[1.801]
 [1.837]
 [1.837]
 [1.837]
 [1.837]]
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]] [[3.774]
 [3.774]
 [3.774]
 [3.774]
 [3.774]] [[5.523]
 [5.523]
 [5.523]
 [5.523]
 [5.523]]
Printing some Q and Qe and total Qs values:  [[0.832]
 [0.64 ]
 [0.832]
 [0.28 ]
 [0.832]] [[ 0.   ]
 [-0.532]
 [ 0.   ]
 [-1.13 ]
 [ 0.   ]] [[2.155]
 [1.415]
 [2.155]
 [0.298]
 [2.155]]
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
start point for exploration sampling:  20015
siam score:  -0.93665665
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.147]
 [1.147]
 [1.082]
 [1.147]
 [1.147]] [[0.777]
 [0.777]
 [0.685]
 [0.777]
 [0.777]] [[1.898]
 [1.898]
 [1.707]
 [1.898]
 [1.898]]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 1.295506955960361
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.472]] [[5.244]
 [5.244]
 [5.244]
 [5.244]
 [5.551]] [[1.791]
 [1.791]
 [1.791]
 [1.791]
 [1.903]]
UNIT TEST: sample policy line 217 mcts : [0.042 0.    0.083 0.25  0.625]
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.9311392
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.768]
 [0.768]
 [0.935]
 [0.768]
 [0.768]] [[0.402]
 [0.402]
 [1.225]
 [0.402]
 [0.402]] [[0.594]
 [0.594]
 [1.323]
 [0.594]
 [0.594]]
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.316]
 [1.316]
 [1.316]
 [1.316]
 [1.316]] [[2.728]
 [2.728]
 [2.728]
 [2.728]
 [2.728]] [[1.788]
 [1.788]
 [1.788]
 [1.788]
 [1.788]]
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
first move QE:  -0.04642409404449441
line 256 mcts: sample exp_bonus 2.3431371770604534
Printing some Q and Qe and total Qs values:  [[0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.773]] [[4.328]
 [4.328]
 [4.328]
 [4.328]
 [4.317]] [[2.066]
 [2.066]
 [2.066]
 [2.066]
 [2.048]]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.647]
 [1.096]
 [0.741]
 [0.601]
 [0.586]] [[2.125]
 [2.273]
 [2.81 ]
 [2.887]
 [2.352]] [[1.129]
 [1.704]
 [1.798]
 [1.721]
 [1.258]]
siam score:  -0.93135524
Printing some Q and Qe and total Qs values:  [[0.844]
 [1.125]
 [0.844]
 [0.844]
 [0.814]] [[3.037]
 [2.219]
 [3.037]
 [3.037]
 [2.783]] [[2.556]
 [2.341]
 [2.556]
 [2.556]
 [2.292]]
1101 6770
siam score:  -0.9297542
in main func line 156:  1102
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.92617387
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.92553574
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.232]
 [1.232]
 [1.232]
 [1.232]
 [1.232]] [[2.598]
 [2.598]
 [2.598]
 [2.598]
 [2.598]] [[2.065]
 [2.065]
 [2.065]
 [2.065]
 [2.065]]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.3964872728422852
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.121]
 [1.45 ]
 [1.397]
 [1.397]
 [1.271]] [[2.797]
 [0.939]
 [1.979]
 [1.979]
 [2.197]] [[2.232]
 [1.391]
 [2.046]
 [2.046]
 [2.022]]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1541 1.0 1.0
siam score:  -0.9274418
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
in main func line 156:  1119
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1561 1.0 1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.117]
 [1.117]
 [1.117]
 [1.117]
 [0.978]] [[2.78 ]
 [2.78 ]
 [2.78 ]
 [2.78 ]
 [3.387]] [[1.831]
 [1.831]
 [1.831]
 [1.831]
 [2.074]]
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1581 1.0 1.0
1130 6778
line 256 mcts: sample exp_bonus 2.9548982687596985
Printing some Q and Qe and total Qs values:  [[0.645]
 [0.612]
 [0.612]
 [0.612]
 [0.597]] [[2.489]
 [1.864]
 [1.864]
 [1.864]
 [1.777]] [[1.651]
 [1.109]
 [1.109]
 [1.109]
 [1.024]]
Printing some Q and Qe and total Qs values:  [[0.774]
 [0.774]
 [0.774]
 [0.695]
 [0.824]] [[1.939]
 [1.939]
 [1.939]
 [1.458]
 [2.479]] [[1.33 ]
 [1.33 ]
 [1.33 ]
 [0.916]
 [1.766]]
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]] [[2.624]
 [2.624]
 [2.624]
 [2.624]
 [2.624]] [[1.56]
 [1.56]
 [1.56]
 [1.56]
 [1.56]]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.92616904
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1581 1.0 1.0
siam score:  -0.9229485
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.1505710683454426
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.233]
 [0.242]
 [0.242]
 [0.242]
 [0.242]] [[4.957]
 [4.211]
 [4.211]
 [4.211]
 [4.211]] [[1.831]
 [1.374]
 [1.374]
 [1.374]
 [1.374]]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1601 1.0 1.0
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.91448814
Printing some Q and Qe and total Qs values:  [[0.889]
 [0.873]
 [0.926]
 [0.771]
 [0.871]] [[0.047]
 [0.251]
 [0.246]
 [0.27 ]
 [0.064]] [[0.889]
 [0.873]
 [0.926]
 [0.771]
 [0.871]]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1621 1.0 1.0
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1621 1.0 1.0
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1621 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.928]
 [0.428]
 [0.405]
 [0.452]] [[3.268]
 [2.586]
 [2.727]
 [2.788]
 [2.695]] [[2.212]
 [2.095]
 [1.611]
 [1.645]
 [1.608]]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.91620755
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
UNIT TEST: sample policy line 217 mcts : [0.    0.667 0.042 0.    0.292]
UNIT TEST: sample policy line 217 mcts : [0.417 0.042 0.042 0.042 0.458]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
1158 6787
start point for exploration sampling:  20015
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.513]
 [0.435]
 [0.238]
 [0.297]] [[1.269]
 [2.014]
 [2.023]
 [0.58 ]
 [2.081]] [[0.734]
 [1.412]
 [1.358]
 [0.065]
 [1.295]]
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  91 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
siam score:  -0.9124733
1164 6793
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1661 1.0 1.0
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  20015
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.571]
 [0.691]
 [0.461]
 [0.522]] [[1.395]
 [1.395]
 [2.128]
 [0.834]
 [1.557]] [[1.059]
 [1.059]
 [1.695]
 [0.557]
 [1.143]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.90989065
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.465]] [[2.321]
 [2.321]
 [2.321]
 [2.321]
 [2.558]] [[2.144]
 [2.144]
 [2.144]
 [2.144]
 [2.42 ]]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1175 6798
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.738]
 [1.062]
 [0.738]
 [0.738]] [[2.272]
 [2.272]
 [2.361]
 [2.272]
 [2.272]] [[1.854]
 [1.854]
 [2.114]
 [1.854]
 [1.854]]
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.231]
 [0.171]
 [0.178]
 [0.18 ]] [[3.989]
 [3.703]
 [4.49 ]
 [3.989]
 [4.826]] [[1.337]
 [1.243]
 [1.559]
 [1.337]
 [1.718]]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1681 1.0 1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1701 1.0 1.0
maxi score, test score, baseline:  0.1701 1.0 1.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
siam score:  -0.91146606
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
1182 6809
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.1758002247372397
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.71]
 [0.71]
 [1.09]
 [0.71]
 [0.71]] [[1.863]
 [1.863]
 [1.867]
 [1.863]
 [1.863]] [[1.534]
 [1.534]
 [1.969]
 [1.534]
 [1.534]]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.8  ]
 [0.838]
 [0.8  ]
 [0.81 ]] [[1.483]
 [1.597]
 [1.952]
 [1.597]
 [1.842]] [[0.881]
 [1.376]
 [1.68 ]
 [1.376]
 [1.563]]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
start point for exploration sampling:  20015
siam score:  -0.9096784
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1197 6823
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.012]
 [1.153]
 [0.906]
 [0.852]
 [1.052]] [[2.378]
 [1.948]
 [2.522]
 [1.997]
 [2.701]] [[2.312]
 [2.309]
 [2.197]
 [1.74 ]
 [2.608]]
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.19 ]] [[0.765]
 [0.765]
 [0.765]
 [0.765]
 [1.153]] [[0.961]
 [0.961]
 [0.961]
 [0.961]
 [0.967]]
1199 6823
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
siam score:  -0.91668135
siam score:  -0.91696626
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
UNIT TEST: sample policy line 217 mcts : [0.    0.542 0.292 0.    0.167]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.9256075
Printing some Q and Qe and total Qs values:  [[0.806]
 [0.806]
 [0.887]
 [0.806]
 [0.806]] [[1.477]
 [1.477]
 [1.444]
 [1.477]
 [1.477]] [[1.573]
 [1.573]
 [1.724]
 [1.573]
 [1.573]]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.9251767
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.399]
 [0.523]
 [0.455]
 [0.5  ]] [[1.67 ]
 [0.705]
 [1.072]
 [0.699]
 [0.21 ]] [[0.535]
 [0.399]
 [0.523]
 [0.455]
 [0.5  ]]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2161 1.0 1.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2341 1.0 1.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.09 ]
 [0.09 ]
 [0.1  ]
 [0.09 ]
 [0.093]] [[1.072]
 [1.072]
 [0.834]
 [1.072]
 [1.414]] [[0.09 ]
 [0.09 ]
 [0.1  ]
 [0.09 ]
 [0.093]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.23609999999999998 0.96 0.96
maxi score, test score, baseline:  0.23609999999999998 0.96 0.96
from probs:  [0.24689265536723168, 0.22203389830508477, 0.20621468926553677, 0.12259887005649715, 0.09774011299435025, 0.10451977401129937]
maxi score, test score, baseline:  0.23609999999999998 0.96 0.96
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.746]
 [0.614]
 [0.614]
 [0.683]] [[1.93 ]
 [2.407]
 [1.93 ]
 [1.93 ]
 [2.775]] [[0.916]
 [1.567]
 [0.916]
 [0.916]
 [1.831]]
siam score:  -0.92649454
maxi score, test score, baseline:  0.23609999999999998 0.96 0.96
probs:  [0.25196408529741865, 0.22053872053872053, 0.2048260381593715, 0.12177328843995507, 0.0970819304152637, 0.10381593714927041]
maxi score, test score, baseline:  0.23609999999999998 0.96 0.96
probs:  [0.25196408529741865, 0.22053872053872053, 0.2048260381593715, 0.12177328843995507, 0.0970819304152637, 0.10381593714927041]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.23609999999999998 0.96 0.96
probs:  [0.25196408529741865, 0.22053872053872053, 0.2048260381593715, 0.12177328843995507, 0.0970819304152637, 0.10381593714927041]
siam score:  -0.92588025
maxi score, test score, baseline:  0.23609999999999998 0.96 0.96
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.4  ]
 [0.4  ]
 [0.4  ]
 [0.4  ]] [[2.7  ]
 [1.393]
 [1.393]
 [1.393]
 [1.393]] [[1.433]
 [0.907]
 [0.907]
 [0.907]
 [0.907]]
maxi score, test score, baseline:  0.23609999999999998 0.96 0.96
probs:  [0.2519640852974187, 0.22053872053872053, 0.2048260381593715, 0.12177328843995507, 0.0970819304152637, 0.10381593714927041]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.951]
 [0.951]
 [1.058]
 [0.951]
 [0.951]] [[1.522]
 [1.522]
 [1.779]
 [1.522]
 [1.522]] [[1.971]
 [1.971]
 [2.231]
 [1.971]
 [1.971]]
maxi score, test score, baseline:  0.23809999999999998 0.96 0.96
probs:  [0.25363941769316917, 0.22004479283314674, 0.20436730123180294, 0.12150055991041431, 0.0968645016797312, 0.10358342665173566]
maxi score, test score, baseline:  0.23809999999999998 0.96 0.96
probs:  [0.25363941769316917, 0.22004479283314674, 0.20436730123180294, 0.12150055991041431, 0.0968645016797312, 0.10358342665173566]
maxi score, test score, baseline:  0.23809999999999998 0.96 0.96
probs:  [0.25363941769316917, 0.22004479283314674, 0.20436730123180294, 0.12150055991041431, 0.0968645016797312, 0.10358342665173566]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.051]
 [1.202]
 [0.847]
 [0.752]
 [1.022]] [[2.549]
 [2.25 ]
 [2.605]
 [2.456]
 [2.595]] [[1.799]
 [1.743]
 [1.606]
 [1.384]
 [1.801]]
using another actor
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
using explorer policy with actor:  1
start point for exploration sampling:  20015
maxi score, test score, baseline:  0.24009999999999998 0.96 0.96
probs:  [0.25530726256983244, 0.21955307262569834, 0.2039106145251397, 0.12122905027932958, 0.09664804469273738, 0.1033519553072625]
maxi score, test score, baseline:  0.24009999999999998 0.96 0.96
probs:  [0.25530726256983244, 0.21955307262569834, 0.2039106145251397, 0.12122905027932958, 0.09664804469273738, 0.1033519553072625]
siam score:  -0.9211632
using another actor
from probs:  [0.25530726256983244, 0.21955307262569834, 0.2039106145251397, 0.12122905027932958, 0.09664804469273738, 0.1033519553072625]
Printing some Q and Qe and total Qs values:  [[1.34 ]
 [1.437]
 [1.396]
 [1.396]
 [1.375]] [[1.358]
 [0.358]
 [0.527]
 [1.156]
 [1.4  ]] [[2.097]
 [1.958]
 [1.931]
 [2.14 ]
 [2.181]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.118]
 [1.118]
 [1.118]
 [1.118]
 [1.118]] [[2.909]
 [2.909]
 [2.909]
 [2.909]
 [2.909]] [[2.797]
 [2.797]
 [2.797]
 [2.797]
 [2.797]]
maxi score, test score, baseline:  0.24009999999999998 0.96 0.96
probs:  [0.25696767001114834, 0.21906354515050172, 0.2034559643255296, 0.12095875139353399, 0.09643255295429204, 0.10312151616499438]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.24009999999999998 0.96 0.96
probs:  [0.2586206896551725, 0.21857619577308124, 0.2030033370411569, 0.12068965517241377, 0.0962180200222469, 0.10289210233592874]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.24009999999999998 0.96 0.96
probs:  [0.26026637069922315, 0.21809100998890124, 0.20255271920088794, 0.1204217536071032, 0.09600443951165366, 0.10266370699223079]
maxi score, test score, baseline:  0.24009999999999998 0.96 0.96
probs:  [0.26026637069922315, 0.21809100998890124, 0.20255271920088794, 0.1204217536071032, 0.09600443951165366, 0.10266370699223079]
maxi score, test score, baseline:  0.24009999999999998 0.96 0.96
using another actor
maxi score, test score, baseline:  0.24009999999999998 0.96 0.96
using explorer policy with actor:  0
maxi score, test score, baseline:  0.24009999999999998 0.96 0.96
maxi score, test score, baseline:  0.24009999999999998 0.96 0.96
UNIT TEST: sample policy line 217 mcts : [0.042 0.375 0.125 0.    0.458]
Printing some Q and Qe and total Qs values:  [[0.864]
 [1.15 ]
 [0.864]
 [0.864]
 [0.864]] [[2.512]
 [2.246]
 [2.512]
 [2.512]
 [2.512]] [[1.717]
 [2.2  ]
 [1.717]
 [1.717]
 [1.717]]
actor:  0 policy actor:  1  step number:  94 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.24209999999999998 0.96 0.96
probs:  [0.26026637069922315, 0.21809100998890124, 0.20255271920088794, 0.1204217536071032, 0.09600443951165366, 0.10266370699223079]
actor:  1 policy actor:  1  step number:  94 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
maxi score, test score, baseline:  0.24209999999999998 0.96 0.96
maxi score, test score, baseline:  0.24209999999999998 0.96 0.96
probs:  [0.261904761904762, 0.21760797342192692, 0.20210409745293467, 0.12015503875968989, 0.09579180509413061, 0.10243632336655585]
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.764]
 [0.764]
 [0.926]
 [0.764]
 [0.764]] [[1.965]
 [1.965]
 [3.049]
 [1.965]
 [1.965]] [[0.636]
 [0.636]
 [1.482]
 [0.636]
 [0.636]]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.24609999999999999 0.96 0.96
probs:  [0.261904761904762, 0.21760797342192692, 0.20210409745293467, 0.12015503875968989, 0.09579180509413061, 0.10243632336655585]
maxi score, test score, baseline:  0.24609999999999999 0.96 0.96
maxi score, test score, baseline:  0.24609999999999999 0.96 0.96
probs:  [0.261904761904762, 0.21760797342192692, 0.20210409745293467, 0.12015503875968989, 0.09579180509413061, 0.10243632336655585]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
1263 6863
maxi score, test score, baseline:  0.2481 0.96 0.96
probs:  [0.261904761904762, 0.21760797342192692, 0.20210409745293467, 0.12015503875968989, 0.09579180509413061, 0.10243632336655585]
line 256 mcts: sample exp_bonus 6.672144936055876
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2481 0.96 0.96
using another actor
from probs:  [0.261904761904762, 0.21760797342192692, 0.20210409745293467, 0.12015503875968989, 0.09579180509413061, 0.10243632336655585]
maxi score, test score, baseline:  0.2481 0.96 0.96
probs:  [0.261904761904762, 0.21760797342192692, 0.20210409745293467, 0.12015503875968989, 0.09579180509413061, 0.10243632336655585]
actor:  0 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2501 0.96 0.96
probs:  [0.261904761904762, 0.21760797342192692, 0.20210409745293467, 0.12015503875968989, 0.09579180509413061, 0.10243632336655585]
maxi score, test score, baseline:  0.2501 0.96 0.96
probs:  [0.261904761904762, 0.21760797342192692, 0.20210409745293467, 0.12015503875968989, 0.09579180509413061, 0.10243632336655585]
actor:  1 policy actor:  1  step number:  92 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2501 0.96 0.96
maxi score, test score, baseline:  0.2501 0.96 0.96
maxi score, test score, baseline:  0.2501 0.96 0.96
probs:  [0.26353591160221, 0.21712707182320445, 0.20165745856353595, 0.11988950276243092, 0.09558011049723752, 0.10220994475138115]
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2501 0.96 0.96
probs:  [0.26515986769570016, 0.2166482910694598, 0.20121278941565607, 0.11962513781697903, 0.09536934950385884, 0.10198456449834614]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2501 0.96 0.96
maxi score, test score, baseline:  0.2501 0.96 0.96
probs:  [0.26677667766776686, 0.2161716171617162, 0.20077007700770083, 0.11936193619361934, 0.09515951595159512, 0.1017601760176017]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.273]
 [0.273]
 [0.216]
 [0.282]] [[1.877]
 [6.453]
 [0.717]
 [3.685]
 [6.146]] [[0.511]
 [1.998]
 [0.098]
 [1.024]
 [1.905]]
maxi score, test score, baseline:  0.2501 0.96 0.96
maxi score, test score, baseline:  0.2501 0.96 0.96
maxi score, test score, baseline:  0.2501 0.96 0.96
probs:  [0.26677667766776686, 0.2161716171617162, 0.20077007700770083, 0.11936193619361934, 0.09515951595159512, 0.1017601760176017]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
from probs:  [0.26677667766776686, 0.2161716171617162, 0.20077007700770083, 0.11936193619361934, 0.09515951595159512, 0.1017601760176017]
maxi score, test score, baseline:  0.2501 0.96 0.96
probs:  [0.26677667766776686, 0.2161716171617162, 0.20077007700770083, 0.11936193619361934, 0.09515951595159512, 0.1017601760176017]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.26677667766776686, 0.2161716171617162, 0.20077007700770083, 0.11936193619361934, 0.09515951595159512, 0.1017601760176017]
Printing some Q and Qe and total Qs values:  [[0.903]
 [0.807]
 [0.96 ]
 [0.903]
 [0.903]] [[3.771]
 [3.373]
 [3.991]
 [3.771]
 [3.771]] [[1.747]
 [1.583]
 [1.839]
 [1.747]
 [1.747]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2521 0.96 0.96
probs:  [0.26677667766776686, 0.2161716171617162, 0.20077007700770083, 0.11936193619361934, 0.09515951595159512, 0.1017601760176017]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.89688677
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  99 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2541 0.96 0.96
probs:  [0.26838638858397373, 0.21569703622392977, 0.2003293084522503, 0.1190998902305159, 0.09495060373216241, 0.10153677277716788]
maxi score, test score, baseline:  0.2541 0.96 0.96
maxi score, test score, baseline:  0.2541 0.96 0.96
probs:  [0.26838638858397373, 0.21569703622392977, 0.2003293084522503, 0.1190998902305159, 0.09495060373216241, 0.10153677277716788]
maxi score, test score, baseline:  0.2541 0.96 0.96
probs:  [0.26838638858397373, 0.21569703622392977, 0.2003293084522503, 0.1190998902305159, 0.09495060373216241, 0.10153677277716788]
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
siam score:  -0.9047972
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.285]
 [0.223]
 [0.011]
 [0.136]] [[1.123]
 [0.826]
 [0.599]
 [0.325]
 [1.049]] [[0.186]
 [0.285]
 [0.223]
 [0.011]
 [0.136]]
maxi score, test score, baseline:  0.2561 0.96 0.96
maxi score, test score, baseline:  0.2561 0.96 0.96
probs:  [0.2699890470974809, 0.21522453450164294, 0.19989047097480836, 0.1188389923329682, 0.09474260679079952, 0.10131434830230004]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.681]
 [0.681]
 [0.681]
 [0.846]] [[3.091]
 [3.091]
 [3.091]
 [3.091]
 [3.531]] [[1.647]
 [1.647]
 [1.647]
 [1.647]
 [2.152]]
Printing some Q and Qe and total Qs values:  [[0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.116]] [[4.014]
 [4.014]
 [4.014]
 [4.014]
 [5.709]] [[0.612]
 [0.612]
 [0.612]
 [0.612]
 [1.53 ]]
maxi score, test score, baseline:  0.2561 0.96 0.96
probs:  [0.2699890470974809, 0.21522453450164294, 0.19989047097480836, 0.1188389923329682, 0.09474260679079952, 0.10131434830230004]
Printing some Q and Qe and total Qs values:  [[0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.093]] [[3.982]
 [3.982]
 [3.982]
 [3.982]
 [5.122]] [[0.728]
 [0.728]
 [0.728]
 [0.728]
 [1.235]]
Printing some Q and Qe and total Qs values:  [[0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.212]] [[3.16 ]
 [3.16 ]
 [3.16 ]
 [3.16 ]
 [3.309]] [[1.939]
 [1.939]
 [1.939]
 [1.939]
 [2.064]]
maxi score, test score, baseline:  0.2561 0.96 0.96
probs:  [0.2699890470974809, 0.21522453450164294, 0.19989047097480836, 0.1188389923329682, 0.09474260679079952, 0.10131434830230004]
maxi score, test score, baseline:  0.2561 0.96 0.96
maxi score, test score, baseline:  0.2561 0.96 0.96
maxi score, test score, baseline:  0.2561 0.96 0.96
maxi score, test score, baseline:  0.2561 0.96 0.96
probs:  [0.26998904709748084, 0.21522453450164294, 0.19989047097480836, 0.1188389923329682, 0.09474260679079952, 0.10131434830230004]
maxi score, test score, baseline:  0.2561 0.96 0.96
probs:  [0.2699890470974809, 0.21522453450164294, 0.19989047097480836, 0.1188389923329682, 0.09474260679079952, 0.10131434830230004]
first move QE:  -0.025978246277202095
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
actor:  0 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.092]
 [1.092]
 [1.092]
 [1.092]
 [1.079]] [[3.659]
 [3.659]
 [3.659]
 [3.659]
 [3.429]] [[2.613]
 [2.613]
 [2.613]
 [2.613]
 [2.468]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2581 0.96 0.96
probs:  [0.27158469945355196, 0.21475409836065576, 0.19945355191256836, 0.11857923497267757, 0.09453551912568302, 0.10109289617486333]
maxi score, test score, baseline:  0.2581 0.96 0.96
probs:  [0.27158469945355196, 0.21475409836065576, 0.19945355191256836, 0.11857923497267757, 0.09453551912568302, 0.10109289617486333]
from probs:  [0.27158469945355196, 0.21475409836065576, 0.19945355191256836, 0.11857923497267757, 0.09453551912568302, 0.10109289617486333]
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
1317 6887
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.721]
 [0.907]
 [0.721]
 [0.721]] [[1.483]
 [1.483]
 [1.668]
 [1.483]
 [1.483]] [[1.503]
 [1.503]
 [1.746]
 [1.503]
 [1.503]]
maxi score, test score, baseline:  0.2601 0.96 0.96
probs:  [0.27158469945355196, 0.21475409836065576, 0.19945355191256836, 0.11857923497267757, 0.09453551912568302, 0.10109289617486333]
maxi score, test score, baseline:  0.2601 0.96 0.96
probs:  [0.27158469945355196, 0.21475409836065576, 0.19945355191256836, 0.11857923497267757, 0.09453551912568302, 0.10109289617486333]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2601 0.96 0.96
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
siam score:  -0.8958366
maxi score, test score, baseline:  0.2601 0.96 0.96
probs:  [0.27317339149400227, 0.2142857142857143, 0.19901853871319525, 0.11832061068702288, 0.09432933478735, 0.10087241003271531]
siam score:  -0.89857423
maxi score, test score, baseline:  0.2601 0.96 0.96
probs:  [0.27317339149400227, 0.2142857142857143, 0.19901853871319525, 0.11832061068702288, 0.09432933478735, 0.10087241003271531]
maxi score, test score, baseline:  0.2601 0.96 0.96
probs:  [0.27317339149400227, 0.2142857142857143, 0.19901853871319525, 0.11832061068702288, 0.09432933478735, 0.10087241003271531]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.228]
 [1.339]
 [1.228]
 [1.228]
 [1.228]] [[0.299]
 [0.328]
 [0.299]
 [0.299]
 [0.299]] [[1.746]
 [1.978]
 [1.746]
 [1.746]
 [1.746]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
siam score:  -0.8936661
maxi score, test score, baseline:  0.2621 0.96 0.96
probs:  [0.278856526429342, 0.21413160733549083, 0.19687162891046386, 0.11704422869471409, 0.09331175836030199, 0.0997842502696871]
maxi score, test score, baseline:  0.2621 0.96 0.96
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.89977807
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2661 0.96 0.96
maxi score, test score, baseline:  0.2661 0.96 0.96
maxi score, test score, baseline:  0.2661 0.96 0.96
probs:  [0.278856526429342, 0.2141316073354909, 0.19687162891046392, 0.1170442286947141, 0.093311758360302, 0.09978425026968711]
maxi score, test score, baseline:  0.2661 0.96 0.96
probs:  [0.278856526429342, 0.2141316073354909, 0.19687162891046392, 0.1170442286947141, 0.093311758360302, 0.09978425026968711]
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2681 0.96 0.96
probs:  [0.278856526429342, 0.2141316073354909, 0.19687162891046392, 0.1170442286947141, 0.093311758360302, 0.09978425026968711]
maxi score, test score, baseline:  0.2681 0.96 0.96
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.9079807
maxi score, test score, baseline:  0.2701 0.96 0.96
siam score:  -0.90239364
from probs:  [0.278856526429342, 0.2141316073354909, 0.19687162891046392, 0.1170442286947141, 0.093311758360302, 0.09978425026968711]
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.523]
 [0.579]
 [0.601]
 [0.529]] [[0.107]
 [0.312]
 [0.706]
 [0.107]
 [0.122]] [[0.601]
 [0.523]
 [0.579]
 [0.601]
 [0.529]]
actor:  0 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2721 0.96 0.96
probs:  [0.2780149413020278, 0.21824973319103524, 0.19690501600853794, 0.115795090715048, 0.09231590181430091, 0.09871931696905009]
maxi score, test score, baseline:  0.2721 0.96 0.96
probs:  [0.2780149413020278, 0.21824973319103524, 0.19690501600853794, 0.115795090715048, 0.09231590181430091, 0.09871931696905009]
from probs:  [0.2780149413020278, 0.21824973319103524, 0.19690501600853794, 0.115795090715048, 0.09231590181430091, 0.09871931696905009]
actor:  0 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2741 0.96 0.96
probs:  [0.2810839532412328, 0.21732199787460152, 0.1960680127523911, 0.11530286928799147, 0.09192348565356, 0.0982996811902231]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.28412698412698423, 0.21640211640211646, 0.19523809523809527, 0.1148148148148148, 0.0915343915343915, 0.09788359788359782]
Printing some Q and Qe and total Qs values:  [[1.215]
 [1.208]
 [1.215]
 [1.215]
 [1.215]] [[1.61 ]
 [1.285]
 [1.61 ]
 [1.61 ]
 [1.61 ]] [[2.711]
 [2.373]
 [2.711]
 [2.711]
 [2.711]]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.773]
 [1.04 ]
 [0.299]
 [0.873]] [[1.379]
 [1.293]
 [1.011]
 [0.554]
 [1.426]] [[1.189]
 [1.177]
 [1.263]
 [0.112]
 [1.384]]
maxi score, test score, baseline:  0.2781 0.96 0.96
probs:  [0.2835269271383316, 0.21805702217529044, 0.1948257655755016, 0.11457233368532205, 0.09134107708553321, 0.09767687434002106]
Printing some Q and Qe and total Qs values:  [[0.958]
 [0.958]
 [0.958]
 [0.958]
 [0.958]] [[1.946]
 [1.946]
 [1.946]
 [1.946]
 [1.946]] [[2.177]
 [2.177]
 [2.177]
 [2.177]
 [2.177]]
maxi score, test score, baseline:  0.2781 0.96 0.96
probs:  [0.2835269271383316, 0.21805702217529044, 0.1948257655755016, 0.11457233368532205, 0.09134107708553321, 0.09767687434002106]
maxi score, test score, baseline:  0.2781 0.96 0.96
maxi score, test score, baseline:  0.2781 0.96 0.96
probs:  [0.2835269271383316, 0.21805702217529044, 0.19482576557550163, 0.11457233368532205, 0.09134107708553321, 0.09767687434002106]
actor:  1 policy actor:  1  step number:  96 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2781 0.96 0.96
probs:  [0.2829293993677556, 0.21970495258166495, 0.1944151738672287, 0.11433087460484717, 0.09114857744994727, 0.09747102212855631]
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.2829293993677556, 0.21970495258166495, 0.1944151738672287, 0.11433087460484717, 0.09114857744994727, 0.09747102212855631]
Printing some Q and Qe and total Qs values:  [[1.241]
 [1.258]
 [1.362]
 [1.241]
 [1.241]] [[0.321]
 [0.853]
 [0.103]
 [0.321]
 [0.321]] [[1.696]
 [1.907]
 [1.866]
 [1.696]
 [1.696]]
maxi score, test score, baseline:  0.2781 0.96 0.96
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.90543365
maxi score, test score, baseline:  0.2781 0.96 0.96
maxi score, test score, baseline:  0.2781 0.96 0.96
probs:  [0.2838405036726129, 0.22088142707240296, 0.19359916054564538, 0.11385099685204614, 0.09076600209863583, 0.0970619097586568]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]] [[0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]]
maxi score, test score, baseline:  0.2801 0.96 0.96
probs:  [0.2838405036726129, 0.22088142707240296, 0.19359916054564538, 0.11385099685204614, 0.09076600209863582, 0.0970619097586568]
maxi score, test score, baseline:  0.2801 0.96 0.96
probs:  [0.2838405036726129, 0.22088142707240296, 0.19359916054564538, 0.11385099685204614, 0.09076600209863582, 0.0970619097586568]
first move QE:  -0.027810788829182812
Printing some Q and Qe and total Qs values:  [[1.385]
 [1.4  ]
 [1.385]
 [1.385]
 [1.385]] [[1.073]
 [0.652]
 [1.073]
 [1.073]
 [1.073]] [[2.442]
 [2.217]
 [2.442]
 [2.442]
 [2.442]]
maxi score, test score, baseline:  0.2801 0.96 0.96
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.084]
 [0.084]
 [0.084]
 [0.084]] [[-0.505]
 [-0.505]
 [-0.505]
 [-0.505]
 [-0.505]] [[0.084]
 [0.084]
 [0.084]
 [0.084]
 [0.084]]
siam score:  -0.9025603
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.28324607329842943, 0.22251308900523567, 0.19319371727748694, 0.11361256544502617, 0.09057591623036644, 0.09685863874345545]
Printing some Q and Qe and total Qs values:  [[0.95 ]
 [0.825]
 [0.825]
 [0.825]
 [0.825]] [[2.145]
 [2.129]
 [2.129]
 [2.129]
 [2.129]] [[2.195]
 [2.024]
 [2.024]
 [2.024]
 [2.024]]
maxi score, test score, baseline:  0.3261 0.92 0.92
probs:  [0.3328991440267957, 0.24655749906959432, 0.20189802754000746, 0.09173799776702644, 0.05898771864532939, 0.06791961295124677]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
maxi score, test score, baseline:  0.3261 0.92 0.92
probs:  [0.3328991440267957, 0.24655749906959432, 0.20189802754000746, 0.09173799776702644, 0.05898771864532939, 0.06791961295124677]
1385 6931
Printing some Q and Qe and total Qs values:  [[0.704]
 [0.704]
 [0.802]
 [0.704]
 [0.704]] [[0.131]
 [0.131]
 [0.305]
 [0.131]
 [0.131]] [[1.508]
 [1.508]
 [1.762]
 [1.508]
 [1.508]]
maxi score, test score, baseline:  0.3261 0.92 0.92
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]] [[2.595]
 [2.595]
 [2.595]
 [2.595]
 [2.595]] [[4.971]
 [4.971]
 [4.971]
 [4.971]
 [4.971]]
Printing some Q and Qe and total Qs values:  [[0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]] [[5.602]
 [5.602]
 [5.602]
 [5.602]
 [5.602]] [[1.966]
 [1.966]
 [1.966]
 [1.966]
 [1.966]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3261 0.92 0.92
maxi score, test score, baseline:  0.3261 0.92 0.92
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.9039196
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3281 0.92 0.92
probs:  [0.33784786641929504, 0.24582560296846007, 0.2012987012987013, 0.08849721706864565, 0.058812615955473116, 0.06771799628942488]
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.74 ]
 [0.715]
 [0.715]
 [0.47 ]] [[0.642]
 [0.398]
 [0.642]
 [0.642]
 [1.538]] [[0.715]
 [0.74 ]
 [0.715]
 [0.715]
 [0.47 ]]
actor:  0 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3301 0.92 0.92
maxi score, test score, baseline:  0.3301 0.92 0.92
1394 6935
maxi score, test score, baseline:  0.3301 0.92 0.92
probs:  [0.33784786641929504, 0.2458256029684601, 0.2012987012987013, 0.08849721706864565, 0.058812615955473116, 0.06771799628942488]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.945]
 [0.945]
 [0.945]
 [0.945]
 [0.945]] [[1.878]
 [1.878]
 [1.878]
 [1.878]
 [1.878]] [[2.528]
 [2.528]
 [2.528]
 [2.528]
 [2.528]]
maxi score, test score, baseline:  0.3341 0.92 0.92
probs:  [0.33784786641929504, 0.2458256029684601, 0.2012987012987013, 0.08849721706864565, 0.058812615955473116, 0.06771799628942488]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3341 0.92 0.92
Printing some Q and Qe and total Qs values:  [[1.385]
 [1.385]
 [1.385]
 [1.385]
 [1.385]] [[1.375]
 [1.375]
 [1.375]
 [1.375]
 [1.375]] [[2.563]
 [2.563]
 [2.563]
 [2.563]
 [2.563]]
Printing some Q and Qe and total Qs values:  [[0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]] [[2.947]
 [2.947]
 [2.947]
 [2.947]
 [2.947]] [[3.473]
 [3.473]
 [3.473]
 [3.473]
 [3.473]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3341 0.92 0.92
maxi score, test score, baseline:  0.3341 0.92 0.92
siam score:  -0.8940852
using explorer policy with actor:  0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.651]
 [0.403]
 [0.403]
 [0.412]] [[3.195]
 [0.64 ]
 [3.195]
 [3.195]
 [1.818]] [[0.403]
 [0.651]
 [0.403]
 [0.403]
 [0.412]]
actor:  0 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.155]
 [1.247]
 [1.155]
 [1.155]
 [1.155]] [[0.76 ]
 [0.906]
 [0.76 ]
 [0.76 ]
 [0.76 ]] [[2.469]
 [2.702]
 [2.469]
 [2.469]
 [2.469]]
from probs:  [0.33784786641929504, 0.24582560296846007, 0.20129870129870134, 0.08849721706864565, 0.058812615955473116, 0.06771799628942488]
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3381 0.92 0.92
maxi score, test score, baseline:  0.3381 0.92 0.92
probs:  [0.33980762116167224, 0.2450980392156862, 0.20070292267850537, 0.08823529411764706, 0.05863854975952647, 0.06751757306696264]
maxi score, test score, baseline:  0.3381 0.92 0.92
probs:  [0.33980762116167224, 0.24509803921568626, 0.20070292267850537, 0.08823529411764706, 0.05863854975952647, 0.06751757306696264]
maxi score, test score, baseline:  0.3381 0.92 0.92
1415 6945
from probs:  [0.33980762116167224, 0.24509803921568626, 0.20070292267850537, 0.08823529411764706, 0.05863854975952647, 0.06751757306696264]
Printing some Q and Qe and total Qs values:  [[1.193]
 [0.726]
 [0.725]
 [0.711]
 [0.854]] [[1.31 ]
 [1.12 ]
 [1.353]
 [2.335]
 [3.614]] [[1.736]
 [0.72 ]
 [0.867]
 [1.466]
 [2.557]]
UNIT TEST: sample policy line 217 mcts : [0.125 0.458 0.167 0.042 0.208]
actor:  0 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8990818
maxi score, test score, baseline:  0.3421 0.92 0.92
maxi score, test score, baseline:  0.3421 0.92 0.92
maxi score, test score, baseline:  0.3421 0.92 0.92
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.5270972844100025
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.449]] [[2.427]
 [2.427]
 [2.427]
 [2.427]
 [4.635]] [[1.025]
 [1.025]
 [1.025]
 [1.025]
 [2.095]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 1.8885822602014577
maxi score, test score, baseline:  0.3421 0.92 0.92
using another actor
actor:  0 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.3436925340198603, 0.24365575579257073, 0.19952188304523724, 0.0877160720853255, 0.058293490253769786, 0.0671202648032365]
actor:  0 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.3436925340198602, 0.24365575579257073, 0.19952188304523724, 0.0877160720853255, 0.058293490253769786, 0.0671202648032365]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.34809999999999997 0.92 0.92
probs:  [0.3436925340198602, 0.24365575579257073, 0.19952188304523724, 0.0877160720853255, 0.058293490253769786, 0.0671202648032365]
using explorer policy with actor:  1
from probs:  [0.3436925340198602, 0.24365575579257073, 0.19952188304523724, 0.0877160720853255, 0.058293490253769786, 0.0671202648032365]
actor:  0 policy actor:  0  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.35009999999999997 0.92 0.92
probs:  [0.3436925340198602, 0.24365575579257073, 0.19952188304523724, 0.0877160720853255, 0.058293490253769786, 0.0671202648032365]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.35209999999999997 0.92 0.92
actor:  0 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20015
UNIT TEST: sample policy line 217 mcts : [0.042 0.    0.125 0.25  0.583]
line 256 mcts: sample exp_bonus 0.8866444393173659
actor:  0 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3581 0.92 0.92
siam score:  -0.9039785
first move QE:  -0.024855570458589955
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.671]] [[5.094]
 [5.094]
 [5.094]
 [5.094]
 [4.581]] [[2.261]
 [2.261]
 [2.261]
 [2.261]
 [1.913]]
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.767]
 [1.13 ]
 [0.869]
 [0.749]] [[2.193]
 [4.124]
 [3.782]
 [4.266]
 [4.498]] [[0.554]
 [1.813]
 [2.056]
 [1.959]
 [1.917]]
maxi score, test score, baseline:  0.3581 0.92 0.92
probs:  [0.3447067502766506, 0.24437476945776462, 0.200110660272962, 0.0850239763924751, 0.05846551088159352, 0.06731833271855404]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3581 0.92 0.92
using explorer policy with actor:  1
start point for exploration sampling:  20015
maxi score, test score, baseline:  0.3581 0.92 0.92
actor:  0 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.086]
 [1.086]
 [1.086]
 [1.086]
 [1.086]] [[3.421]
 [3.421]
 [3.421]
 [3.421]
 [3.421]] [[1.761]
 [1.761]
 [1.761]
 [1.761]
 [1.761]]
siam score:  -0.9009253
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.90078944
maxi score, test score, baseline:  0.3621 0.92 0.92
probs:  [0.3436925340198603, 0.2465980139757264, 0.19952188304523724, 0.08477381390216993, 0.0582934902537698, 0.06712026480323652]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
Printing some Q and Qe and total Qs values:  [[1.029]
 [1.31 ]
 [1.075]
 [1.039]
 [1.124]] [[3.912]
 [2.047]
 [1.702]
 [2.519]
 [2.581]] [[1.657]
 [1.604]
 [1.104]
 [1.276]
 [1.439]]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.004]
 [1.039]
 [0.309]
 [0.348]] [[ 0.562]
 [-0.089]
 [ 0.675]
 [ 0.106]
 [ 0.488]] [[0.522]
 [0.005]
 [1.51 ]
 [0.442]
 [0.575]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [1.176]
 [1.282]
 [0.959]
 [0.912]] [[4.966]
 [3.328]
 [4.103]
 [4.534]
 [4.238]] [[1.393]
 [0.904]
 [1.423]
 [1.448]
 [1.245]]
siam score:  -0.89863527
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3641 0.92 0.92
maxi score, test score, baseline:  0.3641 0.92 0.92
probs:  [0.3475319926873857, 0.24515539305301642, 0.19835466179159053, 0.08427787934186473, 0.05795246800731264, 0.06672760511883]
maxi score, test score, baseline:  0.3641 0.92 0.92
probs:  [0.3475319926873857, 0.24515539305301642, 0.19835466179159053, 0.08427787934186473, 0.05795246800731264, 0.06672760511883]
maxi score, test score, baseline:  0.3641 0.92 0.92
probs:  [0.3475319926873857, 0.24515539305301642, 0.19835466179159053, 0.08427787934186473, 0.05795246800731264, 0.06672760511883]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1468 6977
1468 6978
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3681 0.92 0.92
maxi score, test score, baseline:  0.3681 0.92 0.92
probs:  [0.3494349252643091, 0.24444039372949322, 0.19777615749179728, 0.08403208166241341, 0.057783448778709445, 0.06653299307327744]
Printing some Q and Qe and total Qs values:  [[0.847]
 [1.412]
 [1.352]
 [1.071]
 [1.165]] [[3.836]
 [2.37 ]
 [2.732]
 [3.285]
 [3.036]] [[1.32 ]
 [1.041]
 [1.164]
 [1.223]
 [1.174]]
maxi score, test score, baseline:  0.3681 0.92 0.92
Printing some Q and Qe and total Qs values:  [[1.418]
 [1.418]
 [1.404]
 [1.418]
 [1.418]] [[1.44 ]
 [1.44 ]
 [0.397]
 [1.44 ]
 [1.44 ]] [[2.6  ]
 [2.6  ]
 [2.284]
 [2.6  ]
 [2.6  ]]
maxi score, test score, baseline:  0.3681 0.92 0.92
Printing some Q and Qe and total Qs values:  [[0.047]
 [1.114]
 [1.052]
 [0.887]
 [0.918]] [[0.457]
 [3.412]
 [2.584]
 [3.724]
 [3.803]] [[0.008]
 [1.772]
 [1.396]
 [1.788]
 [1.836]]
maxi score, test score, baseline:  0.3681 0.92 0.92
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3681 0.92 0.92
probs:  [0.34841875681570333, 0.24663758633224278, 0.19720101781170485, 0.08378771355870593, 0.05761541257724465, 0.0663395129043984]
1482 6985
maxi score, test score, baseline:  0.3681 0.92 0.92
probs:  [0.34841875681570333, 0.24663758633224278, 0.19720101781170485, 0.08378771355870593, 0.05761541257724465, 0.0663395129043984]
maxi score, test score, baseline:  0.3681 0.92 0.92
probs:  [0.34841875681570333, 0.24663758633224278, 0.19720101781170485, 0.08378771355870593, 0.05761541257724465, 0.0663395129043984]
siam score:  -0.8915816
Printing some Q and Qe and total Qs values:  [[1.067]
 [1.067]
 [1.067]
 [1.067]
 [1.067]] [[1.897]
 [1.897]
 [1.897]
 [1.897]
 [1.897]] [[1.729]
 [1.729]
 [1.729]
 [1.729]
 [1.729]]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3701 0.92 0.92
1488 6988
Printing some Q and Qe and total Qs values:  [[1.087]
 [0.951]
 [0.973]
 [0.951]
 [0.87 ]] [[1.238]
 [2.038]
 [1.155]
 [2.038]
 [1.636]] [[1.722]
 [1.719]
 [1.529]
 [1.719]
 [1.495]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
maxi score, test score, baseline:  0.3701 0.92 0.92
probs:  [0.3484187568157033, 0.24663758633224278, 0.19720101781170485, 0.08378771355870591, 0.05761541257724465, 0.0663395129043984]
maxi score, test score, baseline:  0.3701 0.92 0.92
probs:  [0.3484187568157033, 0.24663758633224278, 0.19720101781170485, 0.08378771355870591, 0.05761541257724465, 0.0663395129043984]
maxi score, test score, baseline:  0.3701 0.92 0.92
siam score:  -0.899956
maxi score, test score, baseline:  0.3701 0.92 0.92
from probs:  [0.3484187568157033, 0.24663758633224278, 0.19720101781170485, 0.08378771355870591, 0.05761541257724465, 0.0663395129043984]
maxi score, test score, baseline:  0.3701 0.92 0.92
probs:  [0.3484187568157033, 0.24663758633224278, 0.19720101781170485, 0.08378771355870591, 0.05761541257724465, 0.0663395129043984]
Printing some Q and Qe and total Qs values:  [[1.253]
 [1.253]
 [1.253]
 [1.253]
 [1.253]] [[1.381]
 [1.381]
 [1.381]
 [1.381]
 [1.381]] [[2.967]
 [2.967]
 [2.967]
 [2.967]
 [2.967]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3701 0.92 0.92
UNIT TEST: sample policy line 217 mcts : [0.042 0.125 0.375 0.042 0.417]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3701 0.92 0.92
using explorer policy with actor:  0
first move QE:  -0.022008456185192524
actor:  0 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.829]
 [0.829]
 [1.068]
 [0.829]
 [0.829]] [[-0.069]
 [-0.069]
 [ 0.412]
 [-0.069]
 [-0.069]] [[0.881]
 [0.881]
 [1.359]
 [0.881]
 [0.881]]
Printing some Q and Qe and total Qs values:  [[0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.802]] [[2.244]
 [2.244]
 [2.244]
 [2.244]
 [2.121]] [[2.112]
 [2.112]
 [2.112]
 [2.112]
 [1.994]]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.762]
 [0.762]
 [0.762]
 [0.742]] [[3.263]
 [3.263]
 [3.263]
 [3.263]
 [4.187]] [[1.038]
 [1.038]
 [1.038]
 [1.038]
 [1.548]]
maxi score, test score, baseline:  0.3721 0.92 0.92
Printing some Q and Qe and total Qs values:  [[0.994]
 [0.994]
 [1.142]
 [0.994]
 [0.994]] [[0.906]
 [0.906]
 [0.99 ]
 [0.906]
 [0.906]] [[1.564]
 [1.564]
 [1.887]
 [1.564]
 [1.564]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3721 0.92 0.92
probs:  [0.3550777014817492, 0.24521142031080595, 0.1960607155764366, 0.08041199855439105, 0.057282255149981955, 0.06595590892663537]
maxi score, test score, baseline:  0.3721 0.92 0.92
Printing some Q and Qe and total Qs values:  [[0.869]
 [1.023]
 [0.869]
 [0.831]
 [0.818]] [[2.115]
 [2.21 ]
 [2.115]
 [2.42 ]
 [2.567]] [[1.51 ]
 [1.728]
 [1.51 ]
 [1.677]
 [1.762]]
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.247]
 [0.247]
 [0.247]
 [0.286]] [[4.845]
 [4.115]
 [4.115]
 [4.115]
 [4.658]] [[1.845]
 [1.358]
 [1.358]
 [1.358]
 [1.75 ]]
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.386]] [[4.376]
 [4.376]
 [4.376]
 [4.376]
 [4.911]] [[1.538]
 [1.538]
 [1.538]
 [1.538]
 [1.943]]
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]] [[5.577]
 [5.577]
 [5.577]
 [5.577]
 [5.577]] [[2.085]
 [2.085]
 [2.085]
 [2.085]
 [2.085]]
using another actor
from probs:  [0.3550777014817492, 0.245211420310806, 0.1960607155764366, 0.08041199855439105, 0.057282255149981955, 0.06595590892663537]
actor:  0 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3741 0.92 0.92
Printing some Q and Qe and total Qs values:  [[0.428]
 [1.258]
 [1.014]
 [1.014]
 [0.879]] [[3.471]
 [2.438]
 [2.118]
 [2.118]
 [2.98 ]] [[1.444]
 [2.415]
 [1.715]
 [1.715]
 [2.019]]
start point for exploration sampling:  20015
UNIT TEST: sample policy line 217 mcts : [0.042 0.333 0.208 0.167 0.25 ]
maxi score, test score, baseline:  0.3741 0.92 0.92
maxi score, test score, baseline:  0.3741 0.92 0.92
maxi score, test score, baseline:  0.3741 0.92 0.92
probs:  [0.35693693693693695, 0.24450450450450456, 0.19549549549549552, 0.08018018018018021, 0.05711711711711715, 0.0657657657657658]
from probs:  [0.35693693693693695, 0.24450450450450456, 0.19549549549549552, 0.08018018018018021, 0.05711711711711715, 0.0657657657657658]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3761 0.92 0.92
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3761 0.92 0.92
probs:  [0.36245087531261166, 0.2424080028581637, 0.1938192211504109, 0.07949267595569848, 0.05662736691675602, 0.06520185780635944]
Printing some Q and Qe and total Qs values:  [[0.85 ]
 [0.85 ]
 [1.029]
 [0.85 ]
 [0.806]] [[1.546]
 [1.546]
 [1.172]
 [1.546]
 [1.43 ]] [[1.837]
 [1.837]
 [1.947]
 [1.837]
 [1.672]]
maxi score, test score, baseline:  0.3761 0.92 0.92
probs:  [0.3624508753126116, 0.2424080028581637, 0.19381922115041086, 0.07949267595569846, 0.056627366916756, 0.06520185780635944]
maxi score, test score, baseline:  0.3761 0.92 0.92
probs:  [0.3624508753126116, 0.2424080028581637, 0.19381922115041086, 0.07949267595569846, 0.056627366916756, 0.06520185780635944]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
1515 7002
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3781 0.92 0.92
probs:  [0.3632326820603908, 0.24387211367673178, 0.19271758436944939, 0.07904085257548846, 0.05630550621669629, 0.06483126110124336]
Printing some Q and Qe and total Qs values:  [[0.811]
 [0.847]
 [0.984]
 [0.163]
 [0.853]] [[1.028]
 [0.956]
 [1.061]
 [0.567]
 [1.569]] [[1.402]
 [1.403]
 [1.683]
 [0.097]
 [1.865]]
maxi score, test score, baseline:  0.3781 0.92 0.92
maxi score, test score, baseline:  0.3781 0.92 0.92
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3781 0.92 0.92
maxi score, test score, baseline:  0.3781 0.92 0.92
probs:  [0.3632326820603908, 0.24387211367673178, 0.19271758436944939, 0.07904085257548846, 0.05630550621669629, 0.06483126110124336]
maxi score, test score, baseline:  0.3781 0.92 0.92
probs:  [0.3632326820603908, 0.24387211367673178, 0.19271758436944939, 0.07904085257548846, 0.05630550621669629, 0.06483126110124336]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1519 7003
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.234]
 [0.084]
 [0.432]
 [0.202]] [[2.595]
 [3.944]
 [2.397]
 [0.559]
 [4.709]] [[0.866]
 [1.557]
 [0.687]
 [0.149]
 [1.887]]
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.257]] [[5.806]
 [5.806]
 [5.806]
 [5.806]
 [7.118]] [[1.515]
 [1.515]
 [1.515]
 [1.515]
 [2.076]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
maxi score, test score, baseline:  0.3801 0.92 0.92
maxi score, test score, baseline:  0.3801 0.92 0.92
probs:  [0.3632326820603908, 0.24387211367673178, 0.1927175843694494, 0.07904085257548846, 0.05630550621669629, 0.06483126110124336]
Printing some Q and Qe and total Qs values:  [[0.844]
 [0.955]
 [1.038]
 [0.879]
 [0.81 ]] [[2.078]
 [2.351]
 [2.27 ]
 [3.429]
 [3.185]] [[1.148]
 [1.541]
 [1.609]
 [2.28 ]
 [1.975]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
maxi score, test score, baseline:  0.3801 0.92 0.92
maxi score, test score, baseline:  0.3801 0.92 0.92
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3801 0.92 0.92
maxi score, test score, baseline:  0.3801 0.92 0.92
maxi score, test score, baseline:  0.3801 0.92 0.92
probs:  [0.3650371944739639, 0.24318101310662418, 0.19217144881331918, 0.07881686149486364, 0.05614594403117254, 0.0646475380800567]
maxi score, test score, baseline:  0.3801 0.92 0.92
probs:  [0.3650371944739639, 0.24318101310662418, 0.19217144881331918, 0.07881686149486364, 0.05614594403117254, 0.0646475380800567]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
maxi score, test score, baseline:  0.3801 0.92 0.92
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.8223],
        [0.2032],
        [0.1933],
        [0.6071],
        [0.7521],
        [0.6098],
        [0.4257],
        [0.0000],
        [0.9416],
        [0.6167]], dtype=torch.float64)
0.0 0.8223380539584539
0.0 0.20319191801619518
0.0 0.19330037935049585
0.0 0.6070788184904495
0.0 0.7520609066904302
0.0 0.6097604065978578
0.0 0.4256917517382293
0.0 0.0
0.0 0.9415645437847939
0.0 0.6166914995749555
maxi score, test score, baseline:  0.3801 0.92 0.92
probs:  [0.3650371944739639, 0.24318101310662418, 0.19217144881331918, 0.07881686149486364, 0.05614594403117254, 0.0646475380800567]
maxi score, test score, baseline:  0.3801 0.92 0.92
probs:  [0.3650371944739639, 0.24318101310662418, 0.19217144881331918, 0.07881686149486364, 0.05614594403117254, 0.0646475380800567]
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.164]
 [0.164]
 [0.181]
 [0.21 ]] [[2.687]
 [4.547]
 [4.547]
 [2.806]
 [3.629]] [[1.131]
 [2.971]
 [2.971]
 [1.241]
 [2.103]]
using explorer policy with actor:  1
using another actor
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3821 0.92 0.92
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
maxi score, test score, baseline:  0.3821 0.92 0.92
probs:  [0.3650371944739638, 0.24318101310662413, 0.19217144881331918, 0.07881686149486362, 0.05614594403117253, 0.06464753808005669]
maxi score, test score, baseline:  0.3821 0.92 0.92
maxi score, test score, baseline:  0.3821 0.92 0.92
probs:  [0.3650371944739638, 0.24318101310662413, 0.19217144881331918, 0.07881686149486362, 0.05614594403117253, 0.06464753808005669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
start point for exploration sampling:  20015
first move QE:  -0.023253535149544726
maxi score, test score, baseline:  0.3821 0.92 0.92
probs:  [0.3678710591569252, 0.24318101310662407, 0.19217144881331916, 0.07881686149486361, 0.053312079348211125, 0.06464753808005667]
siam score:  -0.9066568
maxi score, test score, baseline:  0.3821 0.92 0.92
actor:  0 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.3678710591569252, 0.24318101310662407, 0.19217144881331916, 0.07881686149486361, 0.053312079348211125, 0.06464753808005667]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3861 0.92 0.92
probs:  [0.3678710591569252, 0.24318101310662407, 0.19217144881331916, 0.07881686149486361, 0.053312079348211125, 0.06464753808005667]
Printing some Q and Qe and total Qs values:  [[0.87 ]
 [0.811]
 [0.949]
 [0.811]
 [0.762]] [[0.843]
 [0.901]
 [0.94 ]
 [0.901]
 [1.436]] [[1.299]
 [1.199]
 [1.488]
 [1.199]
 [1.28 ]]
maxi score, test score, baseline:  0.3861 0.92 0.92
1543 7016
Printing some Q and Qe and total Qs values:  [[1.123]
 [1.123]
 [1.123]
 [1.06 ]
 [1.083]] [[2.491]
 [2.491]
 [2.491]
 [2.797]
 [3.634]] [[1.631]
 [1.631]
 [1.631]
 [1.607]
 [1.933]]
in main func line 156:  1544
siam score:  -0.9070367
first move QE:  -0.023504365824213523
actor:  1 policy actor:  1  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 0.8987345205237808
from probs:  [0.3668315083009537, 0.2424938184387142, 0.1944542564464853, 0.07859413634758036, 0.053161427057576836, 0.06446485340868952]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.706]
 [0.862]
 [0.706]
 [0.706]
 [0.815]] [[ 0.   ]
 [-0.158]
 [ 0.   ]
 [ 0.   ]
 [-0.01 ]] [[0.807]
 [1.013]
 [0.807]
 [0.807]
 [1.017]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3861 0.92 0.92
maxi score, test score, baseline:  0.3861 0.92 0.92
maxi score, test score, baseline:  0.3861 0.92 0.92
probs:  [0.3686157097569566, 0.24181049665375132, 0.19390630503698486, 0.07837266643184221, 0.05301162381120114, 0.06428319830926384]
siam score:  -0.9000652
maxi score, test score, baseline:  0.3861 0.92 0.92
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
maxi score, test score, baseline:  0.3861 0.92 0.92
probs:  [0.3686157097569566, 0.24181049665375132, 0.19390630503698486, 0.07837266643184221, 0.05301162381120114, 0.06428319830926384]
maxi score, test score, baseline:  0.3861 0.92 0.92
probs:  [0.3686157097569566, 0.24181049665375132, 0.19390630503698486, 0.07837266643184221, 0.05301162381120114, 0.06428319830926384]
from probs:  [0.3686157097569566, 0.24181049665375132, 0.19390630503698486, 0.07837266643184221, 0.05301162381120114, 0.06428319830926384]
actor:  0 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.89897656
Printing some Q and Qe and total Qs values:  [[0.904]
 [1.218]
 [1.315]
 [0.974]
 [1.153]] [[1.47 ]
 [2.623]
 [2.097]
 [4.095]
 [3.191]] [[0.586]
 [1.808]
 [1.671]
 [2.237]
 [2.025]]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Starting evaluation
maxi score, test score, baseline:  0.3901 0.92 0.92
probs:  [0.367579908675799, 0.24394099051633297, 0.19336143308746048, 0.0781524411661398, 0.052862662451703554, 0.06410256410256411]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.367579908675799, 0.24394099051633297, 0.19336143308746048, 0.0781524411661398, 0.052862662451703554, 0.06410256410256411]
maxi score, test score, baseline:  0.4401 1.0 1.0
maxi score, test score, baseline:  0.4401 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4401 1.0 1.0
maxi score, test score, baseline:  0.4401 1.0 1.0
maxi score, test score, baseline:  0.4401 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4401 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
maxi score, test score, baseline:  0.4441 1.0 1.0
siam score:  -0.89593995
maxi score, test score, baseline:  0.4441 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.027]
 [0.997]
 [1.027]
 [1.027]
 [0.937]] [[4.988]
 [4.041]
 [4.988]
 [4.988]
 [5.035]] [[2.31 ]
 [1.752]
 [2.31 ]
 [2.31 ]
 [2.236]]
maxi score, test score, baseline:  0.4441 1.0 1.0
maxi score, test score, baseline:  0.4441 1.0 1.0
using explorer policy with actor:  1
siam score:  -0.8985784
siam score:  -0.897177
maxi score, test score, baseline:  0.4441 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20015
start point for exploration sampling:  20015
maxi score, test score, baseline:  0.4441 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4441 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4441 1.0 1.0
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4441 1.0 1.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
Printing some Q and Qe and total Qs values:  [[1.038]
 [1.038]
 [1.038]
 [1.038]
 [0.832]] [[5.237]
 [5.237]
 [5.237]
 [5.237]
 [7.89 ]] [[1.097]
 [1.097]
 [1.097]
 [1.097]
 [1.711]]
maxi score, test score, baseline:  0.4461 1.0 1.0
maxi score, test score, baseline:  0.4461 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.257]
 [1.412]
 [1.257]
 [0.989]
 [1.257]] [[1.534]
 [1.363]
 [1.534]
 [0.505]
 [1.534]] [[2.293]
 [2.546]
 [2.293]
 [1.414]
 [2.293]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 1.108840240223692
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.179]
 [1.236]
 [1.04 ]
 [1.042]
 [1.167]] [[3.652]
 [3.61 ]
 [3.979]
 [4.976]
 [3.806]] [[1.432]
 [1.453]
 [1.491]
 [1.99 ]
 [1.499]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4481 1.0 1.0
1590 7045
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1591 7045
Printing some Q and Qe and total Qs values:  [[0.941]
 [0.598]
 [1.17 ]
 [0.941]
 [0.941]] [[2.859]
 [1.428]
 [3.221]
 [2.859]
 [2.859]] [[1.484]
 [0.154]
 [1.928]
 [1.484]
 [1.484]]
1592 7045
line 256 mcts: sample exp_bonus 3.2397834681838145
maxi score, test score, baseline:  0.4481 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.871]
 [0.868]
 [1.017]
 [0.871]
 [0.871]] [[2.004]
 [2.293]
 [2.715]
 [2.004]
 [2.004]] [[1.016]
 [1.099]
 [1.286]
 [1.016]
 [1.016]]
in main func line 156:  1596
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4481 1.0 1.0
siam score:  -0.8887686
maxi score, test score, baseline:  0.4481 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.88770956
maxi score, test score, baseline:  0.4481 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
1605 7049
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.042 0.292 0.583 0.042 0.042]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4521 1.0 1.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]] [[0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]] [[2.468]
 [2.468]
 [2.468]
 [2.468]
 [2.468]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.667]
 [0.025]
 [1.08 ]
 [0.609]
 [0.622]] [[-0.338]
 [-0.35 ]
 [-1.054]
 [-0.448]
 [-0.515]] [[1.272]
 [0.048]
 [1.831]
 [1.128]
 [1.131]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4581 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.497]
 [1.001]
 [0.611]
 [0.558]
 [0.611]] [[0.15 ]
 [0.169]
 [0.   ]
 [0.121]
 [0.   ]] [[0.16 ]
 [0.644]
 [0.149]
 [0.194]
 [0.149]]
1615 7056
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.681]
 [0.773]
 [0.681]
 [0.62 ]] [[1.202]
 [1.202]
 [1.851]
 [1.202]
 [2.032]] [[0.681]
 [0.681]
 [0.773]
 [0.681]
 [0.62 ]]
maxi score, test score, baseline:  0.4581 1.0 1.0
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Printing some Q and Qe and total Qs values:  [[0.968]
 [0.884]
 [1.018]
 [0.968]
 [0.968]] [[0.831]
 [1.217]
 [0.853]
 [0.831]
 [0.831]] [[1.556]
 [2.013]
 [1.666]
 [1.556]
 [1.556]]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4621 1.0 1.0
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.913]
 [0.8  ]
 [0.735]
 [0.8  ]
 [0.84 ]] [[2.133]
 [1.82 ]
 [0.685]
 [1.82 ]
 [2.069]] [[1.988]
 [1.563]
 [0.561]
 [1.563]
 [1.822]]
maxi score, test score, baseline:  0.4641 1.0 1.0
line 256 mcts: sample exp_bonus 3.558049899544912
maxi score, test score, baseline:  0.4641 1.0 1.0
maxi score, test score, baseline:  0.4641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.958]
 [1.084]
 [1.099]
 [1.087]
 [0.958]] [[1.465]
 [2.009]
 [1.293]
 [1.481]
 [1.465]] [[1.63 ]
 [2.196]
 [1.785]
 [1.878]
 [1.63 ]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4641 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
maxi score, test score, baseline:  0.4641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 4.023374431280257
maxi score, test score, baseline:  0.4641 1.0 1.0
first move QE:  -0.02161576308971629
maxi score, test score, baseline:  0.4641 1.0 1.0
maxi score, test score, baseline:  0.4641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.888]
 [1.1  ]
 [0.978]
 [0.888]
 [0.862]] [[3.277]
 [3.022]
 [2.701]
 [3.277]
 [3.175]] [[2.06 ]
 [2.107]
 [1.733]
 [2.06 ]
 [1.955]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.053]
 [1.342]
 [1.342]
 [1.342]
 [1.301]] [[5.233]
 [3.682]
 [3.682]
 [3.682]
 [3.507]] [[2.15 ]
 [1.603]
 [1.603]
 [1.603]
 [1.488]]
Printing some Q and Qe and total Qs values:  [[0.832]
 [1.143]
 [1.143]
 [1.143]
 [0.917]] [[4.256]
 [5.076]
 [5.076]
 [5.076]
 [4.437]] [[1.644]
 [2.31 ]
 [2.31 ]
 [2.31 ]
 [1.804]]
Printing some Q and Qe and total Qs values:  [[1.342]
 [1.342]
 [1.342]
 [1.342]
 [1.319]] [[3.666]
 [3.666]
 [3.666]
 [3.666]
 [3.536]] [[2.486]
 [2.486]
 [2.486]
 [2.486]
 [2.362]]
maxi score, test score, baseline:  0.4641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
maxi score, test score, baseline:  0.4641 1.0 1.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.663]
 [0.727]
 [0.663]
 [0.663]] [[0.674]
 [0.674]
 [1.041]
 [0.674]
 [0.674]] [[0.663]
 [0.663]
 [0.727]
 [0.663]
 [0.663]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4681 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]] [[4.952]
 [4.952]
 [4.952]
 [4.952]
 [4.952]] [[1.72]
 [1.72]
 [1.72]
 [1.72]
 [1.72]]
UNIT TEST: sample policy line 217 mcts : [0.042 0.125 0.417 0.25  0.167]
start point for exploration sampling:  20015
Printing some Q and Qe and total Qs values:  [[1.215]
 [1.213]
 [1.215]
 [1.215]
 [1.122]] [[3.512]
 [4.454]
 [3.512]
 [3.512]
 [3.813]] [[2.179]
 [2.651]
 [2.179]
 [2.179]
 [2.19 ]]
start point for exploration sampling:  20015
Printing some Q and Qe and total Qs values:  [[0.977]
 [0.977]
 [1.072]
 [0.977]
 [0.977]] [[1.387]
 [1.387]
 [1.797]
 [1.387]
 [1.387]] [[1.268]
 [1.268]
 [1.581]
 [1.268]
 [1.268]]
1658 7067
maxi score, test score, baseline:  0.4681 1.0 1.0
using explorer policy with actor:  1
first move QE:  -0.021261253173472605
maxi score, test score, baseline:  0.4681 1.0 1.0
siam score:  -0.88419837
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1662 7067
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.444]
 [0.444]
 [0.444]
 [0.458]] [[4.724]
 [6.094]
 [6.094]
 [6.094]
 [5.842]] [[1.183]
 [1.805]
 [1.805]
 [1.805]
 [1.711]]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.47209999999999996 1.0 1.0
maxi score, test score, baseline:  0.47209999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.47209999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.47209999999999996 1.0 1.0
maxi score, test score, baseline:  0.47209999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.47209999999999996 1.0 1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.68 ]] [[3.597]
 [3.597]
 [3.597]
 [3.597]
 [3.596]] [[1.992]
 [1.992]
 [1.992]
 [1.992]
 [1.925]]
Printing some Q and Qe and total Qs values:  [[1.112]
 [1.112]
 [1.112]
 [1.112]
 [1.079]] [[2.675]
 [2.675]
 [2.675]
 [2.675]
 [2.542]] [[2.446]
 [2.446]
 [2.446]
 [2.446]
 [2.273]]
maxi score, test score, baseline:  0.47209999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.838]
 [0.953]
 [0.897]
 [0.838]
 [0.838]] [[1.794]
 [1.504]
 [1.951]
 [1.794]
 [1.794]] [[0.838]
 [0.953]
 [0.897]
 [0.838]
 [0.838]]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8793536
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.082]
 [1.196]
 [1.082]
 [1.082]
 [1.015]] [[2.88 ]
 [2.534]
 [2.88 ]
 [2.88 ]
 [2.636]] [[2.318]
 [2.186]
 [2.318]
 [2.318]
 [2.06 ]]
first move QE:  -0.021803956843522387
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
1680 7073
Printing some Q and Qe and total Qs values:  [[1.009]
 [1.009]
 [1.009]
 [1.009]
 [1.021]] [[2.214]
 [2.214]
 [2.214]
 [2.214]
 [2.76 ]] [[1.591]
 [1.591]
 [1.591]
 [1.591]
 [2.038]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.142]
 [1.142]
 [1.142]
 [1.142]
 [1.142]] [[1.832]
 [1.832]
 [1.832]
 [1.832]
 [1.832]] [[1.388]
 [1.388]
 [1.388]
 [1.388]
 [1.388]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]] [[0.71]
 [0.71]
 [0.71]
 [0.71]
 [0.71]] [[0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]]
maxi score, test score, baseline:  0.4821 1.0 1.0
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
Printing some Q and Qe and total Qs values:  [[0.992]
 [0.992]
 [0.992]
 [0.992]
 [0.962]] [[2.873]
 [2.873]
 [2.873]
 [2.873]
 [2.934]] [[2.152]
 [2.152]
 [2.152]
 [2.152]
 [2.17 ]]
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4821 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4821 1.0 1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4821 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.987]
 [0.987]
 [0.987]
 [0.987]
 [0.987]] [[3.444]
 [3.444]
 [3.444]
 [3.444]
 [3.444]] [[2.461]
 [2.461]
 [2.461]
 [2.461]
 [2.461]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
first move QE:  -0.020459580660235566
siam score:  -0.8745099
maxi score, test score, baseline:  0.4841 1.0 1.0
siam score:  -0.8755088
start point for exploration sampling:  20015
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4861 1.0 1.0
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.87 ]] [[1.317]
 [1.317]
 [1.317]
 [1.317]
 [1.49 ]] [[1.892]
 [1.892]
 [1.892]
 [1.892]
 [2.061]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.875]
 [0.984]
 [0.798]
 [0.853]
 [0.79 ]] [[1.857]
 [0.848]
 [1.475]
 [1.506]
 [1.842]] [[0.875]
 [0.984]
 [0.798]
 [0.853]
 [0.79 ]]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.003]
 [0.716]
 [0.648]
 [0.472]] [[-0.523]
 [-0.366]
 [-0.514]
 [ 0.886]
 [ 1.416]] [[0.819]
 [0.005]
 [1.089]
 [1.739]
 [1.74 ]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.4881 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4881 1.0 1.0
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8643186
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
first move QE:  -0.02263507538357577
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20015
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.86536485
maxi score, test score, baseline:  0.4901 1.0 1.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.589]
 [0.691]
 [0.589]
 [0.562]] [[-0.695]
 [-0.695]
 [-1.137]
 [-0.695]
 [-0.365]] [[0.589]
 [0.589]
 [0.691]
 [0.589]
 [0.562]]
maxi score, test score, baseline:  0.4941 1.0 1.0
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.4941 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.906]
 [0.94 ]
 [0.906]
 [0.906]
 [0.906]] [[-0.033]
 [ 0.031]
 [-0.033]
 [-0.033]
 [-0.033]] [[0.906]
 [0.94 ]
 [0.906]
 [0.906]
 [0.906]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5441 1.0 1.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
Printing some Q and Qe and total Qs values:  [[0.995]
 [1.013]
 [1.115]
 [0.995]
 [0.995]] [[-0.108]
 [ 0.069]
 [ 0.428]
 [-0.108]
 [-0.108]] [[1.195]
 [1.309]
 [1.637]
 [1.195]
 [1.195]]
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.477]
 [0.477]
 [0.477]
 [0.268]] [[3.705]
 [3.705]
 [3.705]
 [3.705]
 [7.055]] [[0.943]
 [0.943]
 [0.943]
 [0.943]
 [1.847]]
siam score:  -0.8659807
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.99 ]
 [1.   ]
 [1.011]
 [0.083]
 [0.984]] [[0.216]
 [0.425]
 [0.569]
 [0.737]
 [0.346]] [[0.713]
 [1.029]
 [1.251]
 [0.159]
 [0.892]]
Printing some Q and Qe and total Qs values:  [[1.186]
 [1.186]
 [1.186]
 [1.186]
 [1.186]] [[1.017]
 [1.017]
 [1.017]
 [1.017]
 [1.017]] [[1.874]
 [1.874]
 [1.874]
 [1.874]
 [1.874]]
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus 0.029312061222565137
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.093]
 [1.093]
 [1.093]
 [1.093]
 [1.062]] [[0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.621]] [[1.669]
 [1.669]
 [1.669]
 [1.669]
 [1.6  ]]
start point for exploration sampling:  20015
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5461 1.0 1.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5481 1.0 1.0
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 2.9732446585748957
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5481 1.0 1.0
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.604]
 [0.653]
 [0.693]
 [0.685]] [[3.031]
 [1.762]
 [3.056]
 [1.784]
 [1.895]] [[2.139]
 [1.104]
 [2.142]
 [1.221]
 [1.297]]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
in main func line 156:  1751
siam score:  -0.87361985
maxi score, test score, baseline:  0.5501 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.211]
 [1.026]
 [1.047]
 [1.038]
 [0.95 ]] [[1.876]
 [2.34 ]
 [3.319]
 [2.326]
 [1.756]] [[1.506]
 [1.632]
 [2.317]
 [1.634]
 [1.159]]
maxi score, test score, baseline:  0.5501 1.0 1.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.922]
 [0.922]
 [0.993]
 [0.922]
 [0.922]] [[-0.087]
 [-0.087]
 [ 0.526]
 [-0.087]
 [-0.087]] [[1.171]
 [1.171]
 [2.076]
 [1.171]
 [1.171]]
Printing some Q and Qe and total Qs values:  [[1.171]
 [1.067]
 [1.003]
 [0.643]
 [0.952]] [[1.907]
 [1.779]
 [2.289]
 [1.465]
 [1.89 ]] [[2.252]
 [1.975]
 [2.451]
 [1.067]
 [1.944]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.86518157
Printing some Q and Qe and total Qs values:  [[0.092]
 [0.075]
 [0.076]
 [0.046]
 [0.067]] [[ 0.084]
 [ 0.057]
 [ 0.082]
 [ 0.059]
 [-0.031]] [[0.178]
 [0.13 ]
 [0.157]
 [0.096]
 [0.029]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[1.151]
 [1.151]
 [1.223]
 [1.151]
 [1.151]] [[0.575]
 [0.575]
 [0.866]
 [0.575]
 [0.575]] [[1.198]
 [1.198]
 [1.567]
 [1.198]
 [1.198]]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Printing some Q and Qe and total Qs values:  [[0.964]
 [1.046]
 [1.069]
 [1.069]
 [1.084]] [[3.154]
 [3.543]
 [2.835]
 [2.835]
 [2.966]] [[1.813]
 [2.072]
 [1.711]
 [1.711]
 [1.79 ]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.037]
 [0.971]
 [1.096]
 [0.971]
 [0.971]] [[ 0.69 ]
 [ 0.111]
 [-0.107]
 [ 0.111]
 [ 0.111]] [[1.935]
 [1.417]
 [1.521]
 [1.417]
 [1.417]]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5561 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.893]
 [1.07 ]
 [0.881]
 [0.949]] [[ 1.172]
 [ 0.766]
 [ 0.781]
 [-0.04 ]
 [ 0.395]] [[0.458]
 [0.782]
 [0.915]
 [0.216]
 [0.564]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.86125696
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20015
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.893]
 [0.893]
 [0.893]
 [0.893]
 [0.882]] [[3.826]
 [3.826]
 [3.826]
 [3.826]
 [3.567]] [[2.288]
 [2.288]
 [2.288]
 [2.288]
 [2.104]]
siam score:  -0.8671017
maxi score, test score, baseline:  0.5601 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.11 ]
 [1.159]
 [1.11 ]
 [1.11 ]
 [1.11 ]] [[-0.003]
 [ 0.026]
 [-0.003]
 [-0.003]
 [-0.003]] [[2.366]
 [2.473]
 [2.366]
 [2.366]
 [2.366]]
maxi score, test score, baseline:  0.5601 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]] [[1.182]
 [1.182]
 [1.182]
 [1.182]
 [1.182]] [[0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]]
maxi score, test score, baseline:  0.5601 1.0 1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20015
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.184]
 [1.308]
 [1.184]
 [1.184]
 [1.184]] [[1.103]
 [1.184]
 [1.103]
 [1.103]
 [1.103]] [[1.759]
 [1.991]
 [1.759]
 [1.759]
 [1.759]]
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
start point for exploration sampling:  20015
start point for exploration sampling:  20015
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
1785 7146
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.239]
 [1.239]
 [1.239]
 [1.239]
 [1.239]] [[2.408]
 [2.408]
 [2.408]
 [2.408]
 [2.408]] [[2.443]
 [2.443]
 [2.443]
 [2.443]
 [2.443]]
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.026]
 [1.026]
 [1.17 ]
 [1.026]
 [1.026]] [[0.373]
 [0.373]
 [0.843]
 [0.373]
 [0.373]] [[1.702]
 [1.702]
 [2.155]
 [1.702]
 [1.702]]
start point for exploration sampling:  20015
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5601 1.0 1.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.789]
 [1.02 ]
 [1.223]
 [1.02 ]
 [0.872]] [[2.787]
 [1.935]
 [2.065]
 [1.935]
 [2.866]] [[1.78 ]
 [1.372]
 [1.723]
 [1.372]
 [1.944]]
maxi score, test score, baseline:  0.5601 1.0 1.0
maxi score, test score, baseline:  0.5601 1.0 1.0
siam score:  -0.8645718
maxi score, test score, baseline:  0.5601 1.0 1.0
maxi score, test score, baseline:  0.5601 1.0 1.0
maxi score, test score, baseline:  0.5601 1.0 1.0
maxi score, test score, baseline:  0.5601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.973232291678439
line 256 mcts: sample exp_bonus 1.3504677027655665
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5621 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.959]
 [0.959]
 [0.959]
 [0.959]
 [0.892]] [[2.894]
 [2.894]
 [2.894]
 [2.894]
 [3.945]] [[1.371]
 [1.371]
 [1.371]
 [1.371]
 [1.701]]
maxi score, test score, baseline:  0.5621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5621 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]] [[5.638]
 [5.638]
 [5.638]
 [5.638]
 [5.663]] [[2.159]
 [2.159]
 [2.159]
 [2.159]
 [2.171]]
using explorer policy with actor:  1
1801 7157
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1803 7160
maxi score, test score, baseline:  0.5581 1.0 1.0
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20015
in main func line 156:  1805
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.058]
 [1.168]
 [0.997]
 [0.997]
 [0.997]] [[2.025]
 [1.795]
 [1.871]
 [1.871]
 [1.871]] [[2.1  ]
 [2.166]
 [1.875]
 [1.875]
 [1.875]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.136]
 [1.136]
 [1.136]
 [1.136]
 [1.136]] [[0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]] [[2.127]
 [2.127]
 [2.127]
 [2.127]
 [2.127]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5561 1.0 1.0
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.86427516
maxi score, test score, baseline:  0.5561 1.0 1.0
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 2.5741782231836514
Printing some Q and Qe and total Qs values:  [[1.17]
 [1.17]
 [1.17]
 [1.17]
 [1.17]] [[1.027]
 [1.027]
 [1.027]
 [1.027]
 [1.027]] [[2.654]
 [2.654]
 [2.654]
 [2.654]
 [2.654]]
1808 7166
maxi score, test score, baseline:  0.5561 1.0 1.0
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.5561 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 1.7027831246971727
maxi score, test score, baseline:  0.5541 1.0 1.0
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.303]
 [1.385]
 [1.356]
 [1.356]
 [1.268]] [[1.383]
 [1.284]
 [1.195]
 [1.195]
 [1.572]] [[2.18 ]
 [2.272]
 [2.162]
 [2.162]
 [2.234]]
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
