dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 25}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
res_block_channels:[32, 64, 64]
res_block_ds:[False, False, False]
reward_support:[-1, 1, 41]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64, 64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 41]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:2
max_workers:32
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
resampling:False
resampling_use_max:False
resampling_assess_best_child:False
rs_start:1000
ep_to_batch_ratio:[11, 12]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
env:<class 'game_play.frozen_lakeGym_Image.gridWorld'>
same_env_each_time:True
channels:3
env_size:[8, 8]
observable_size:[8, 8]
game_modes:1
env_map:[['S' 'H' 'F' 'F' 'F' 'H' 'F' 'F']
 ['F' 'F' 'F' 'H' 'F' 'H' 'H' 'F']
 ['H' 'F' 'F' 'H' 'F' 'F' 'F' 'H']
 ['H' 'F' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'H' 'H' 'F' 'H' 'F']
 ['H' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'G']]
max_steps:100
actions_size:5
optimal_score:1
total_frames:205000
exp_gamma:0.95
atari_env:False
reward_clipping:False
memory_size:100
image_size:[48, 48]
timesteps_in_obs:2
store_prev_actions:True
running_reward_in_obs:False
deque_length:3
PRESET_CONFIG:1
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:rdn
rdn_beta:[0.16666666666666666, 1, 6]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
state_size:[6, 6]
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 64)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:25
detach_expV_calc:True
use_new_episode_expV:True
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['S' 'H' 'F' 'F' 'F' 'H' 'F' 'F']
 ['F' 'F' 'F' 'H' 'F' 'H' 'H' 'F']
 ['H' 'F' 'F' 'H' 'F' 'F' 'F' 'H']
 ['H' 'F' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'H' 'H' 'F' 'H' 'F']
 ['H' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'G']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
0 27
0 38
1 45
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
3 60
Printing some Q and Qe and total Qs values:  [[ 1.5  ]
 [ 1.5  ]
 [-0.003]
 [-0.003]
 [ 1.5  ]] [[0.   ]
 [0.   ]
 [0.002]
 [0.001]
 [0.   ]] [[3.02 ]
 [3.02 ]
 [0.015]
 [0.014]
 [3.02 ]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
6 153
siam score:  0.000452848294199529
maxi score, test score, baseline:  0.0001 0.0 0.0001
7 176
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
deleting a thread, now have 2 threads
Frames:  1183 train batches done:  33 episodes:  196
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
Printing some Q and Qe and total Qs values:  [[-0.004]
 [-0.005]
 [-0.002]
 [-0.006]
 [-0.006]] [[-1.5]
 [-1.5]
 [ 0. ]
 [-1.5]
 [-1.5]] [[-0.008]
 [-0.01 ]
 [ 2.494]
 [-0.012]
 [-0.011]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
9 242
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.2715575319094052
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.29974672
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
9 372
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[ 0.014]
 [-0.   ]
 [ 0.005]
 [-0.   ]
 [-0.001]] [[-0.149]
 [ 0.   ]
 [-0.162]
 [-0.157]
 [-0.158]] [[ 0.023]
 [ 0.143]
 [-0.008]
 [-0.013]
 [-0.016]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.2878481
siam score:  -0.29450652
siam score:  -0.29643792
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.23520312
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.14165805
11 533
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
11 571
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
11 604
UNIT TEST: sample policy line 217 mcts : [0.2 0.2 0.  0.4 0.2]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.20264706459791004
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.15999341
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.18002385
maxi score, test score, baseline:  0.0001 0.0 0.0001
11 636
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
12 669
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.19591266
Printing some Q and Qe and total Qs values:  [[ 1.003]
 [ 0.001]
 [ 0.001]
 [ 0.   ]
 [-0.   ]] [[ 0.   ]
 [-0.037]
 [-0.111]
 [-0.311]
 [-0.119]] [[ 1.003]
 [ 0.001]
 [ 0.001]
 [ 0.   ]
 [-0.   ]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.17613859672445062
siam score:  -0.17401436
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.16774756
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.12814504
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.08492901
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.713]
 [-0.474]
 [ 0.   ]
 [-0.498]
 [-0.796]] [[0.138]
 [0.537]
 [1.327]
 [0.497]
 [0.   ]]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.08438087
first move QE:  -0.1790731794009897
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
19 961
using explorer policy with actor:  1
19 981
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
19 983
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.13104596916828856
maxi score, test score, baseline:  0.0001 0.0 0.0001
20 1033
in main func line 156:  21
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.24020919
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
24 1098
25 1112
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.2558186
siam score:  -0.27043778
first move QE:  -0.10753114578303229
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.10351201951954747
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.319]
 [0.781]
 [0.831]
 [0.605]
 [0.601]] [[0.439]
 [0.747]
 [0.78 ]
 [0.629]
 [0.627]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.021]
 [-0.035]
 [-0.035]
 [-0.035]
 [-0.035]] [[0.043]
 [0.005]
 [0.005]
 [0.005]
 [0.005]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
27 1168
maxi score, test score, baseline:  0.0001 0.0 0.0001
27 1171
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
27 1194
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.255]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]] [[0.725]
 [0.267]
 [0.267]
 [0.267]
 [0.267]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.043]
 [0.043]
 [0.043]
 [0.043]
 [0.043]] [[0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [-0.012]
 [-0.11 ]
 [-0.057]
 [ 0.168]] [[0.178]
 [0.172]
 [0.106]
 [0.142]
 [0.291]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.0955679730884658
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[ 0.]
 [-0.]
 [ 0.]
 [ 0.]
 [ 0.]] [[-0.008]
 [ 0.012]
 [ 0.834]
 [ 1.463]
 [ 1.497]] [[-0.   ]
 [ 0.013]
 [ 0.561]
 [ 0.98 ]
 [ 1.002]]
siam score:  -0.18588004
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
29 1306
29 1311
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.172]
 [-0.097]
 [-0.163]
 [-0.254]
 [-0.149]] [[0.075]
 [0.1  ]
 [0.078]
 [0.048]
 [0.083]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.25248426
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.193]
 [-0.084]
 [-0.068]
 [-0.13 ]
 [ 0.031]] [[0.235]
 [0.38 ]
 [0.401]
 [0.318]
 [0.533]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.27263868
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
29 1444
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0]
 [0]
 [0]
 [0]
 [0]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
29 1463
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.2857478
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[ 0.]
 [ 0.]
 [ 0.]
 [-0.]
 [-0.]] [[0.004]
 [0.007]
 [0.069]
 [1.3  ]
 [1.3  ]] [[0.013]
 [0.018]
 [0.113]
 [2.   ]
 [2.   ]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.764]
 [-0.508]
 [-0.106]
 [-0.039]
 [-0.187]] [[0.   ]
 [0.17 ]
 [0.438]
 [0.482]
 [0.384]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.22 ]
 [-0.223]
 [ 0.341]
 [ 0.341]
 [ 0.004]] [[0.367]
 [0.365]
 [0.741]
 [0.741]
 [0.516]]
33 1511
first move QE:  -0.07084971529171107
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
33 1555
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
33 1572
33 1576
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
35 1603
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]] [[1.372]
 [1.372]
 [1.372]
 [1.372]
 [1.372]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]] [[1.037]
 [1.037]
 [1.037]
 [1.037]
 [1.037]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.252]] [[1.289]
 [1.289]
 [1.289]
 [1.289]
 [1.289]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.055]
 [-0.056]
 [-0.007]
 [-0.007]
 [-0.007]] [[0.002]
 [0.   ]
 [0.081]
 [0.081]
 [0.081]]
Printing some Q and Qe and total Qs values:  [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]] [[0]
 [0]
 [0]
 [0]
 [0]] [[3.]
 [3.]
 [3.]
 [3.]
 [3.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  -0.05558775948147366
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
38 1712
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.30388543
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
40 1758
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.01188086483238585
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.022]
 [-0.011]
 [-0.009]
 [-0.009]
 [-0.007]] [[0.003]
 [0.014]
 [0.016]
 [0.016]
 [0.018]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.004673472114631307
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.007]
 [ 0.   ]
 [ 0.   ]
 [-0.001]
 [ 0.003]] [[0.001]
 [0.003]
 [0.003]
 [0.002]
 [0.004]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.04972280934361684
42 1918
42 1925
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
43 1945
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
46 1991
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
47 2032
maxi score, test score, baseline:  0.0001 0.0 0.0001
47 2046
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
47 2078
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.17524108
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
48 2111
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.001]
 [-0.002]
 [-0.   ]
 [-0.   ]
 [-0.   ]] [[0.   ]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus -0.00034072199229180004
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.019]
 [-0.03 ]
 [ 0.008]
 [ 0.008]
 [ 0.005]] [[0.062]
 [0.013]
 [0.052]
 [0.051]
 [0.048]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
50 2320
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.009]
 [-0.03 ]
 [ 0.01 ]
 [ 0.01 ]
 [ 0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
50 2349
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
51 2380
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.003]
 [ 0.004]
 [-0.001]
 [ 0.008]
 [ 0.008]] [[0.014]
 [0.015]
 [0.01 ]
 [0.019]
 [0.019]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.002]
 [-0.018]
 [-0.004]
 [-0.003]
 [-0.002]] [[0.063]
 [0.029]
 [0.054]
 [0.055]
 [0.057]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.003]
 [-0.002]
 [-0.004]
 [-0.004]
 [-0.004]] [[0.057]
 [0.057]
 [0.054]
 [0.054]
 [0.054]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
58 2506
58 2516
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus -0.0033398837938867464
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
61 2609
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
61 2636
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.083 0.167 0.167 0.208 0.375]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.4102752
first move QE:  -0.03743795958598467
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
64 2756
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.4363228
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
65 2868
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.4609623
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.03527878056120186
65 2894
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
67 2921
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.002]
 [0.002]
 [0.003]
 [0.003]] [[0.003]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]]
siam score:  -0.4282342
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.007]
 [-0.026]
 [-0.007]
 [-0.007]
 [-0.007]] [[0.044]
 [0.013]
 [0.045]
 [0.045]
 [0.044]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
71 3021
maxi score, test score, baseline:  0.0001 0.0 0.0001
71 3033
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
71 3052
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus -0.028290566878310906
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.002]
 [-0.019]
 [ 0.004]
 [ 0.007]
 [ 0.006]] [[0.023]
 [0.009]
 [0.025]
 [0.027]
 [0.026]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
74 3126
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.011]
 [ 0.011]
 [-0.033]
 [ 0.011]
 [-0.033]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.03238352612683584
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: -0.02648393367635341
STARTED EXPV TRAINING ON FRAME NO.  20005
Starting evaluation
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.147]
 [-0.195]
 [-0.088]
 [-0.06 ]
 [ 0.051]] [[0.098]
 [0.033]
 [0.178]
 [0.215]
 [0.365]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.131]
 [-0.062]
 [ 0.003]
 [ 0.086]
 [ 0.182]] [[0.109]
 [0.204]
 [0.293]
 [0.407]
 [0.538]]
81 3203
Printing some Q and Qe and total Qs values:  [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]] [[0]
 [0]
 [0]
 [0]
 [0]] [[3.]
 [3.]
 [3.]
 [3.]
 [3.]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.083 0.25  0.208 0.208 0.25 ]
84 3214
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.503777
86 3222
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
89 3232
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[0.725]
 [0.725]
 [0.725]
 [0.725]
 [0.725]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.15243280495884637
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.262]] [[1.092]
 [1.092]
 [1.092]
 [1.092]
 [1.092]]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.018259693333827882
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 6.20176261420559e-08
0.0 6.690903217006729e-08
0.0 0.0
0.0 0.0
0.0 6.690903217006729e-08
0.0 5.13449407203722e-08
0.0 0.0
0.0 0.0
0.0 5.2826116925617173e-08
0.0 6.221729301912648e-08
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.324]
 [-0.217]
 [-0.05 ]
 [ 0.047]
 [ 0.022]] [[0.   ]
 [0.178]
 [0.455]
 [0.617]
 [0.577]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.38401347
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.3918032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20005
103 3330
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.12 ]
 [ 0.12 ]
 [ 0.12 ]
 [-0.025]
 [-0.056]] [[0.636]
 [0.636]
 [0.636]
 [0.395]
 [0.343]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.526]
 [0.526]
 [0.526]
 [0.526]
 [0.526]] [[1.791]
 [1.791]
 [1.791]
 [1.791]
 [1.791]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.549]
 [0.549]
 [0.549]
 [0.426]
 [0.611]] [[1.627]
 [1.627]
 [1.627]
 [1.38 ]
 [1.75 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.42962334
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.798]
 [1.798]
 [1.798]
 [1.798]
 [1.798]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.45116958
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.731]
 [-0.015]
 [ 0.042]
 [ 0.598]
 [ 1.856]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.4517661
line 256 mcts: sample exp_bonus 1.0103436966996593
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.058]
 [0.181]
 [1.23 ]
 [0.848]
 [1.512]] [[0.014]
 [0.11 ]
 [0.93 ]
 [0.631]
 [1.151]]
using explorer policy with actor:  1
siam score:  -0.42381096
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.281]
 [2.281]
 [2.281]
 [1.666]
 [1.951]] [[2.   ]
 [2.   ]
 [2.   ]
 [1.394]
 [1.674]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.372]
 [0.408]
 [0.979]
 [1.458]
 [1.396]] [[0.054]
 [0.084]
 [0.543]
 [0.929]
 [0.879]]
first move QE:  -0.0024167655258114113
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.607]
 [0.547]
 [1.047]
 [1.534]
 [1.458]] [[0.236]
 [0.181]
 [0.643]
 [1.093]
 [1.023]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.5036815
siam score:  -0.5170977
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.882]
 [-0.046]
 [ 0.025]
 [ 0.882]
 [ 0.882]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 7.087112413247238
line 256 mcts: sample exp_bonus 3.7243756933430734
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.5583916
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.28926018701906986
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.54524076
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.53184104
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.168]
 [1.897]
 [4.069]
 [3.307]
 [3.574]] [[0.793]
 [0.633]
 [1.913]
 [1.464]
 [1.622]]
using explorer policy with actor:  1
first move QE:  0.0703275807739135
first move QE:  0.07155248019797047
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.5197977
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.57 ]
 [1.106]
 [2.241]
 [2.634]
 [2.762]] [[0.696]
 [0.387]
 [1.142]
 [1.405]
 [1.49 ]]
first move QE:  0.07355458315254391
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.07482639668959029
siam score:  -0.52543855
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.50173867
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.404]
 [1.469]
 [1.763]
 [1.763]
 [1.763]] [[1.426]
 [0.25 ]
 [0.368]
 [0.368]
 [0.368]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
185 3563
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.325]
 [1.994]
 [1.994]
 [1.994]
 [2.759]] [[0.51 ]
 [1.001]
 [1.001]
 [1.001]
 [1.562]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.996]
 [1.996]
 [1.996]
 [1.996]
 [2.648]] [[0.822]
 [0.822]
 [0.822]
 [0.822]
 [1.256]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
190 3571
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.754]
 [1.207]
 [2.754]
 [2.754]
 [3.968]] [[0.968]
 [0.032]
 [0.968]
 [0.968]
 [1.702]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.098]
 [1.226]
 [2.01 ]
 [2.431]
 [2.524]] [[1.156]
 [0.491]
 [1.089]
 [1.41 ]
 [1.481]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.3321866
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.394]
 [1.701]
 [2.829]
 [2.303]
 [2.84 ]] [[0.512]
 [0.802]
 [1.864]
 [1.369]
 [1.875]]
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.309]
 [3.309]
 [3.309]
 [3.309]
 [3.72 ]] [[1.546]
 [1.546]
 [1.546]
 [1.546]
 [1.815]]
Printing some Q and Qe and total Qs values:  [[0. ]
 [1.5]
 [0. ]
 [1.5]
 [0. ]] [[1.055]
 [0.   ]
 [1.848]
 [0.   ]
 [2.914]] [[0.343]
 [0.83 ]
 [0.862]
 [0.83 ]
 [1.558]]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
first move QE:  0.11877070798933825
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.301]
 [4.301]
 [4.301]
 [4.301]
 [5.388]] [[1.133]
 [1.133]
 [1.133]
 [1.133]
 [1.679]]
219 3631
siam score:  -0.57434684
line 256 mcts: sample exp_bonus 1.0485774124790006
siam score:  -0.5648681
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
223 3636
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20005
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
228 3640
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 1.5878564568210045
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.73 ]
 [1.574]
 [1.581]
 [2.584]
 [2.524]] [[0.715]
 [0.601]
 [0.606]
 [1.335]
 [1.292]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.125 0.292 0.208 0.083 0.292]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.703]
 [3.045]
 [3.045]
 [3.045]
 [3.246]] [[0.595]
 [1.489]
 [1.489]
 [1.489]
 [1.624]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.5090825
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
255 3674
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.147]
 [2.147]
 [2.147]
 [2.147]
 [2.147]] [[1.247]
 [1.247]
 [1.247]
 [1.247]
 [1.247]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
261 3677
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.17433119868113717
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.487]
 [2.487]
 [2.487]
 [2.487]
 [2.487]] [[0.869]
 [0.869]
 [0.869]
 [0.869]
 [0.869]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
271 3683
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.725]
 [1.746]
 [1.673]
 [2.306]
 [2.534]] [[0.631]
 [0.644]
 [0.598]
 [0.996]
 [1.139]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.5162389
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.017]
 [2.017]
 [2.017]
 [2.017]
 [2.017]] [[0.893]
 [0.893]
 [0.893]
 [0.893]
 [0.893]]
278 3692
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.011]
 [1.165]
 [2.051]
 [2.324]
 [2.563]] [[0.868]
 [0.134]
 [0.903]
 [1.139]
 [1.347]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.257]
 [3.257]
 [3.257]
 [3.257]
 [3.829]] [[1.522]
 [1.522]
 [1.522]
 [1.522]
 [1.967]]
line 256 mcts: sample exp_bonus 3.9163494350553227
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.114]
 [3.391]
 [3.252]
 [3.24 ]
 [3.346]] [[1.584]
 [1.876]
 [1.73 ]
 [1.717]
 [1.828]]
using explorer policy with actor:  1
siam score:  -0.5351008
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.727]
 [3.727]
 [3.727]
 [3.727]
 [3.727]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
siam score:  -0.49319085
maxi score, test score, baseline:  0.0001 0.0 0.0001
296 3710
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.5019570595311897
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.625]
 [2.273]
 [0.863]
 [2.273]
 [1.453]] [[0.358]
 [3.104]
 [0.754]
 [3.104]
 [1.738]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.43269685
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20005
307 3720
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.424]
 [4.424]
 [4.424]
 [3.27 ]
 [4.605]] [[1.637]
 [1.637]
 [1.637]
 [1.19 ]
 [1.707]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.753]
 [3.175]
 [3.233]
 [2.448]
 [3.667]] [[0.889]
 [1.125]
 [1.157]
 [0.718]
 [1.4  ]]
line 256 mcts: sample exp_bonus 0.44655588407755054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.02 ]
 [4.02 ]
 [4.02 ]
 [4.02 ]
 [4.096]] [[1.668]
 [1.668]
 [1.668]
 [1.668]
 [1.704]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.07 ]
 [4.07 ]
 [4.07 ]
 [4.07 ]
 [4.315]] [[1.583]
 [1.583]
 [1.583]
 [1.583]
 [1.739]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.664]
 [3.664]
 [3.664]
 [3.664]
 [4.785]] [[1.146]
 [1.146]
 [1.146]
 [1.146]
 [1.721]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.5599119
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.598]
 [0.671]
 [2.116]
 [2.35 ]
 [3.478]] [[0.447]
 [0.081]
 [0.651]
 [0.744]
 [1.189]]
start point for exploration sampling:  20005
siam score:  -0.54336
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.567]
 [1.999]
 [2.567]
 [2.987]
 [3.192]] [[0.711]
 [0.168]
 [0.711]
 [1.113]
 [1.309]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.169]
 [4.169]
 [4.169]
 [4.169]
 [3.75 ]] [[1.572]
 [1.572]
 [1.572]
 [1.572]
 [1.398]]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.833]
 [1.875]
 [2.45 ]
 [3.142]
 [5.672]] [[0.338]
 [0.353]
 [0.559]
 [0.807]
 [1.713]]
line 256 mcts: sample exp_bonus 5.479257587246546
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
350 3764
siam score:  -0.60172385
line 256 mcts: sample exp_bonus 2.2488985562934776
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.199]
 [3.667]
 [3.667]
 [2.894]
 [3.718]] [[0.779]
 [1.647]
 [1.647]
 [1.19 ]
 [1.677]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.798]
 [1.738]
 [1.289]
 [2.002]
 [2.097]] [[0.92 ]
 [0.874]
 [0.528]
 [1.078]
 [1.151]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.27291041205901967
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 2.830740015611599
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.78]
 [3.78]
 [3.78]
 [3.78]
 [3.78]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.934]
 [2.934]
 [2.934]
 [2.934]
 [3.968]] [[0.788]
 [0.788]
 [0.788]
 [0.788]
 [1.588]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
first move QE:  0.28354055904804754
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20005
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.265]
 [0.298]
 [0.609]
 [1.585]
 [1.776]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.569]
 [3.455]
 [2.261]
 [3.372]
 [3.295]] [[0.931]
 [1.502]
 [0.733]
 [1.449]
 [1.399]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.153]
 [4.153]
 [4.153]
 [4.153]
 [4.153]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.451]
 [5.451]
 [5.451]
 [5.451]
 [5.722]] [[1.755]
 [1.755]
 [1.755]
 [1.755]
 [1.846]]
siam score:  -0.6307802
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.134]
 [4.134]
 [4.134]
 [4.134]
 [4.713]] [[1.512]
 [1.512]
 [1.512]
 [1.512]
 [1.834]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.919]
 [3.919]
 [3.919]
 [3.919]
 [6.87 ]] [[0.873]
 [0.873]
 [0.873]
 [0.873]
 [1.64 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.689]
 [2.689]
 [2.06 ]
 [2.689]
 [3.464]] [[0.745]
 [0.745]
 [0.357]
 [0.745]
 [1.224]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.451]
 [1.526]
 [2.961]
 [2.869]
 [3.335]] [[0.897]
 [0.404]
 [1.169]
 [1.12 ]
 [1.369]]
using explorer policy with actor:  1
first move QE:  0.3000692999757127
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.505]
 [3.505]
 [3.505]
 [3.397]
 [3.562]] [[1.629]
 [1.629]
 [1.629]
 [1.535]
 [1.678]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
395 3817
395 3819
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.295]
 [3.295]
 [3.295]
 [2.835]
 [3.381]] [[1.563]
 [1.563]
 [1.563]
 [1.217]
 [1.628]]
siam score:  -0.6328086
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.716]
 [2.716]
 [2.716]
 [2.374]
 [3.461]] [[0.731]
 [0.731]
 [0.731]
 [0.617]
 [0.98 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 5.710967290190659
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.964]
 [2.391]
 [3.783]
 [3.356]
 [3.556]] [[0.263]
 [0.547]
 [1.474]
 [1.19 ]
 [1.323]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6573182
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.    0.    0.    0.042 0.958]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.722]
 [3.722]
 [3.722]
 [3.722]
 [4.884]] [[1.321]
 [1.321]
 [1.321]
 [1.321]
 [1.865]]
start point for exploration sampling:  20005
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.978]
 [3.978]
 [3.978]
 [3.978]
 [3.322]] [[1.355]
 [1.355]
 [1.355]
 [1.355]
 [1.033]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.672]
 [1.617]
 [1.895]
 [2.574]
 [3.519]] [[0.264]
 [0.227]
 [0.409]
 [0.854]
 [1.473]]
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
431 3847
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.869]
 [4.084]
 [4.459]
 [4.427]
 [4.724]] [[0.189]
 [1.035]
 [1.296]
 [1.273]
 [1.481]]
first move QE:  0.33503236135487424
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.657164
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.523]
 [2.737]
 [2.737]
 [2.737]
 [3.478]] [[0.953]
 [1.107]
 [1.107]
 [1.107]
 [1.641]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
in main func line 156:  445
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.651741
first move QE:  0.3451518005070823
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6549916
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.862]
 [1.862]
 [1.862]
 [1.862]
 [1.862]] [[0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
464 3883
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.819]
 [2.819]
 [2.819]
 [2.819]
 [2.643]] [[1.228]
 [1.228]
 [1.228]
 [1.228]
 [1.111]]
first move QE:  0.3635339157939266
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.708]
 [4.708]
 [4.708]
 [4.708]
 [4.669]] [[1.889]
 [1.889]
 [1.889]
 [1.889]
 [1.867]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.798]
 [3.798]
 [3.798]
 [3.798]
 [4.172]] [[1.415]
 [1.415]
 [1.415]
 [1.415]
 [1.604]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.863]
 [3.852]
 [4.863]
 [4.863]
 [5.863]] [[1.379]
 [0.92 ]
 [1.379]
 [1.379]
 [1.834]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.393]
 [1.683]
 [4.029]
 [3.824]
 [4.705]] [[0.   ]
 [0.097]
 [0.88 ]
 [0.812]
 [1.106]]
482 3895
482 3896
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.5762007
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 3.13146341199376
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.61255515
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 3.3774667906282265
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
496 3903
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.794]
 [3.794]
 [3.794]
 [3.794]
 [3.525]] [[2.   ]
 [2.   ]
 [2.   ]
 [2.   ]
 [1.773]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.157]
 [3.157]
 [3.157]
 [3.157]
 [4.337]] [[1.089]
 [1.089]
 [1.089]
 [1.089]
 [1.708]]
siam score:  -0.6223147
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.14]
 [3.14]
 [3.14]
 [3.14]
 [3.14]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.427]
 [3.427]
 [2.669]
 [3.427]
 [3.946]] [[1.365]
 [1.365]
 [0.737]
 [1.365]
 [1.796]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20005
first move QE:  0.40204269277045424
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6584198
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.285]
 [3.121]
 [2.995]
 [3.383]
 [4.245]] [[0.21 ]
 [0.584]
 [0.528]
 [0.702]
 [1.088]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
siam score:  -0.6814158
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6920938
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.471]
 [2.471]
 [2.911]
 [2.471]
 [3.073]] [[1.112]
 [1.112]
 [1.405]
 [1.112]
 [1.512]]
siam score:  -0.69046396
using explorer policy with actor:  1
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.905996246780363
start point for exploration sampling:  20005
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.805]
 [2.426]
 [2.474]
 [2.474]
 [2.785]] [[0.173]
 [0.587]
 [0.618]
 [0.618]
 [0.826]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.721]
 [2.593]
 [2.489]
 [2.345]
 [2.78 ]] [[0.099]
 [0.68 ]
 [0.611]
 [0.515]
 [0.804]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.42717593250411084
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.504]
 [0.866]
 [1.139]
 [2.504]
 [2.523]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.427]
 [3.427]
 [3.427]
 [3.427]
 [3.362]] [[1.776]
 [1.776]
 [1.776]
 [1.776]
 [1.721]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [0. ]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [2.752]] [[1.693]
 [1.693]
 [1.693]
 [1.693]
 [1.445]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.234]
 [3.234]
 [3.234]
 [3.234]
 [8.379]] [[0.586]
 [0.586]
 [0.586]
 [0.586]
 [1.976]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.612]
 [3.612]
 [3.612]
 [3.612]
 [5.626]] [[0.982]
 [0.982]
 [0.982]
 [0.982]
 [1.925]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.42 ]
 [2.42 ]
 [2.42 ]
 [2.42 ]
 [2.798]] [[1.222]
 [1.222]
 [1.222]
 [1.222]
 [1.662]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.643]
 [2.643]
 [2.643]
 [2.643]
 [2.917]] [[1.383]
 [1.383]
 [1.383]
 [1.383]
 [1.657]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.645]
 [2.645]
 [2.645]
 [2.645]
 [2.918]] [[1.384]
 [1.384]
 [1.384]
 [1.384]
 [1.657]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.753]
 [2.344]
 [2.344]
 [2.707]
 [2.759]] [[0.494]
 [1.083]
 [1.083]
 [1.445]
 [1.498]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.817]
 [3.278]
 [3.278]
 [3.008]
 [3.064]] [[0.391]
 [1.975]
 [1.975]
 [1.682]
 [1.743]]
using explorer policy with actor:  1
siam score:  -0.6876321
line 256 mcts: sample exp_bonus 3.0520788688027536
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.742]
 [2.742]
 [2.742]
 [2.742]
 [2.837]] [[1.412]
 [1.412]
 [1.412]
 [1.412]
 [1.477]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.948]
 [2.066]
 [2.269]
 [2.436]
 [2.169]] [[0.408]
 [1.323]
 [1.49 ]
 [1.627]
 [1.408]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.813]
 [1.813]
 [1.813]
 [1.813]
 [2.353]] [[0.762]
 [0.762]
 [0.762]
 [0.762]
 [1.416]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
567 4020
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.159]
 [2.473]
 [2.473]
 [1.789]
 [2.082]] [[0.466]
 [1.342]
 [1.342]
 [0.886]
 [1.081]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.8446898312637794
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.818]
 [4.818]
 [4.818]
 [4.818]
 [4.636]] [[1.415]
 [1.415]
 [1.415]
 [1.415]
 [1.315]]
line 256 mcts: sample exp_bonus 6.519880004639131
siam score:  -0.6813156
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.804]
 [2.39 ]
 [2.456]
 [3.017]
 [4.744]] [[0.37 ]
 [0.667]
 [0.7  ]
 [0.984]
 [1.858]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.863]
 [2.863]
 [2.863]
 [2.863]
 [2.863]] [[1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.33 ]
 [3.33 ]
 [3.33 ]
 [3.33 ]
 [3.366]] [[1.912]
 [1.912]
 [1.912]
 [1.912]
 [1.945]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
583 4049
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
587 4050
in main func line 156:  588
siam score:  -0.69142205
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
591 4056
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.753]
 [1.471]
 [1.332]
 [1.871]
 [2.725]] [[0.277]
 [0.092]
 [0.   ]
 [0.355]
 [0.917]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.614]
 [3.614]
 [3.614]
 [3.614]
 [4.292]] [[0.943]
 [0.943]
 [0.943]
 [0.943]
 [1.248]]
Starting evaluation
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
line 256 mcts: sample exp_bonus 4.293056582387371
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 5.278784067558317
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.023]
 [2.023]
 [2.023]
 [2.023]
 [2.353]] [[0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.616]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.474]
 [2.474]
 [2.474]
 [2.474]
 [2.911]] [[1.114]
 [1.114]
 [1.114]
 [1.114]
 [1.63 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.815]
 [1.436]
 [2.404]
 [2.339]
 [2.081]] [[0.879]
 [0.344]
 [1.71 ]
 [1.619]
 [1.254]]
siam score:  -0.7146729
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 2.716614024051473
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7080867
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.7034852
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.275]
 [3.275]
 [3.275]
 [3.275]
 [3.974]] [[1.247]
 [1.247]
 [1.247]
 [1.247]
 [1.754]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.5024575276943961
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 3.0327172978677894
first move QE:  0.5064941600066698
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.945]
 [2.945]
 [2.945]
 [2.945]
 [3.075]] [[1.845]
 [1.845]
 [1.845]
 [1.845]
 [2.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.    0.042 0.042 0.375 0.542]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[7.314]
 [7.314]
 [7.314]
 [7.314]
 [9.295]] [[1.16]
 [1.16]
 [1.16]
 [1.16]
 [1.78]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7152135
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 2.8527472254192268
using explorer policy with actor:  1
first move QE:  0.5167878978099832
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.935]
 [3.935]
 [3.935]
 [3.935]
 [3.426]] [[1.916]
 [1.916]
 [1.916]
 [1.916]
 [1.469]]
650 4137
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
651 4138
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.703]
 [3.703]
 [3.703]
 [3.703]
 [2.712]] [[1.938]
 [1.938]
 [1.938]
 [1.938]
 [1.307]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
656 4141
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.491313809351784
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.72891617
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.72237897
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
660 4144
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.4400128194460002
siam score:  -0.72790235
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.74160826
siam score:  -0.73519
670 4152
671 4154
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.156]
 [2.977]
 [2.977]
 [2.378]
 [2.758]] [[0.783]
 [1.77 ]
 [1.77 ]
 [1.05 ]
 [1.507]]
line 256 mcts: sample exp_bonus 2.4838914666730494
siam score:  -0.722657
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.5  ]
 [1.462]
 [1.462]
 [2.053]
 [1.868]] [[0.144]
 [0.132]
 [0.132]
 [0.329]
 [0.267]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.756]
 [1.909]
 [2.236]
 [1.909]
 [2.195]] [[0.976]
 [1.127]
 [1.449]
 [1.127]
 [1.409]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.72331923
siam score:  -0.71844953
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.885]
 [0.885]
 [0.885]
 [0.885]
 [2.704]] [[0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.654]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.404]
 [3.298]
 [3.298]
 [2.827]
 [2.982]] [[0.967]
 [2.   ]
 [2.   ]
 [1.455]
 [1.635]]
first move QE:  0.5438170433955855
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 1.200678190143724
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.8332447454996417
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.5478412071934641
siam score:  -0.67221254
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
695 4196
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.294]
 [2.294]
 [2.294]
 [2.294]
 [2.294]] [[1.436]
 [1.436]
 [1.436]
 [1.436]
 [1.436]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.724]
 [1.724]
 [1.724]
 [1.724]
 [2.228]] [[0.675]
 [0.675]
 [0.675]
 [0.675]
 [1.347]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.044]
 [2.044]
 [2.044]
 [2.044]
 [2.044]] [[0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.435]
 [2.435]
 [2.435]
 [2.435]
 [2.435]] [[1.604]
 [1.604]
 [1.604]
 [1.604]
 [1.604]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.271]
 [4.175]
 [3.17 ]
 [2.595]
 [3.318]] [[0.39 ]
 [1.658]
 [0.988]
 [0.606]
 [1.087]]
line 256 mcts: sample exp_bonus 10.0
line 256 mcts: sample exp_bonus 4.724789244510477
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7437716
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
line 256 mcts: sample exp_bonus 4.523278187268481
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.318]
 [2.318]
 [2.318]
 [2.318]
 [2.87 ]] [[0.577]
 [0.577]
 [0.577]
 [0.577]
 [1.13 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.5581533646800427
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.793]
 [2.793]
 [2.793]
 [2.793]
 [3.297]] [[1.143]
 [1.143]
 [1.143]
 [1.143]
 [1.675]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.71912277
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.867]
 [2.867]
 [2.867]
 [2.867]
 [3.079]] [[1.524]
 [1.524]
 [1.524]
 [1.524]
 [1.723]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.662]
 [2.662]
 [2.662]
 [2.662]
 [2.695]] [[1.091]
 [1.091]
 [1.091]
 [1.091]
 [1.113]]
line 256 mcts: sample exp_bonus 4.41964627606785
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.    0.    0.917]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.938]
 [1.241]
 [2.622]
 [2.622]
 [3.415]] [[0.   ]
 [0.24 ]
 [1.332]
 [1.332]
 [1.959]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 2.422178880988299
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.035]
 [2.035]
 [2.035]
 [2.035]
 [2.035]] [[1.39]
 [1.39]
 [1.39]
 [1.39]
 [1.39]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.864]
 [1.864]
 [1.864]
 [1.864]
 [2.47 ]] [[1.324]
 [1.324]
 [1.324]
 [1.324]
 [1.877]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.042 0.    0.875]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.512]
 [2.512]
 [2.512]
 [2.512]
 [2.512]] [[1.836]
 [1.836]
 [1.836]
 [1.836]
 [1.836]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.38 ]
 [2.38 ]
 [2.38 ]
 [2.38 ]
 [2.338]] [[1.529]
 [1.529]
 [1.529]
 [1.529]
 [1.485]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.703]
 [2.703]
 [2.703]
 [2.703]
 [2.565]] [[1.87 ]
 [1.87 ]
 [1.87 ]
 [1.87 ]
 [1.686]]
siam score:  -0.65145665
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.362]
 [4.362]
 [4.362]
 [4.362]
 [5.572]] [[1.051]
 [1.051]
 [1.051]
 [1.051]
 [1.39 ]]
using explorer policy with actor:  1
siam score:  -0.65516037
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.242]
 [4.242]
 [4.242]
 [4.242]
 [4.876]] [[1.719]
 [1.719]
 [1.719]
 [1.719]
 [1.997]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.779]
 [5.779]
 [5.779]
 [5.779]
 [5.779]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.746]
 [5.746]
 [5.746]
 [5.746]
 [5.998]] [[1.812]
 [1.812]
 [1.812]
 [1.812]
 [1.901]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20005
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
764 4277
UNIT TEST: sample policy line 217 mcts : [0.042 0.083 0.    0.208 0.667]
first move QE:  0.5838168052162105
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.72 ]
 [4.72 ]
 [4.72 ]
 [4.72 ]
 [4.742]] [[1.896]
 [1.896]
 [1.896]
 [1.896]
 [1.909]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.903]
 [2.903]
 [2.903]
 [2.903]
 [2.903]] [[1.496]
 [1.496]
 [1.496]
 [1.496]
 [1.496]]
in main func line 156:  780
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.913]
 [2.913]
 [2.913]
 [2.913]
 [2.913]] [[1.369]
 [1.369]
 [1.369]
 [1.369]
 [1.369]]
siam score:  -0.79480803
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.5892828421005603
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 4.630892281939097
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.097]
 [2.097]
 [2.097]
 [1.665]
 [1.832]] [[1.469]
 [1.469]
 [1.469]
 [0.892]
 [1.116]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.603]
 [6.603]
 [6.603]
 [5.793]
 [5.998]] [[1.821]
 [1.821]
 [1.821]
 [1.427]
 [1.527]]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 6.563836470738315
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.464]
 [2.464]
 [2.464]
 [2.464]
 [2.216]] [[1.715]
 [1.715]
 [1.715]
 [1.715]
 [1.442]]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
796 4313
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
798 4318
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.18]
 [4.18]
 [4.18]
 [4.18]
 [5.56]] [[1.222]
 [1.222]
 [1.222]
 [1.222]
 [1.682]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20005
810 4333
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.651]
 [2.651]
 [2.651]
 [2.651]
 [2.651]] [[1.372]
 [1.372]
 [1.372]
 [1.372]
 [1.372]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.101]
 [2.101]
 [2.101]
 [2.101]
 [2.838]] [[0.773]
 [0.773]
 [0.773]
 [0.773]
 [1.575]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.434]
 [2.434]
 [2.434]
 [2.434]
 [2.486]] [[0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.58 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
813 4338
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.507]
 [2.507]
 [2.604]
 [2.041]
 [2.615]] [[1.143]
 [1.143]
 [1.24 ]
 [0.677]
 [1.251]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.688]
 [1.342]
 [2.547]
 [3.088]
 [3.487]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7866327
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.657]] [[0.033]
 [0.033]
 [0.033]
 [0.033]
 [0.   ]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.635]
 [5.635]
 [5.635]
 [5.635]
 [5.698]] [[1.235]
 [1.235]
 [1.235]
 [1.235]
 [1.251]]
siam score:  -0.7789127
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.35 ]
 [3.35 ]
 [1.599]
 [2.664]
 [3.26 ]] [[1.663]
 [1.663]
 [0.492]
 [1.204]
 [1.603]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.582]
 [3.582]
 [3.582]
 [3.582]
 [3.834]] [[1.577]
 [1.577]
 [1.577]
 [1.577]
 [1.774]]
in main func line 156:  819
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.78443015
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 4.555390254908408
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.224]
 [3.224]
 [3.224]
 [3.224]
 [3.224]] [[1.361]
 [1.361]
 [1.361]
 [1.361]
 [1.361]]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7832146
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.352]
 [3.352]
 [3.352]
 [3.352]
 [3.297]] [[1.156]
 [1.156]
 [1.156]
 [1.156]
 [1.119]]
siam score:  -0.7809906
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.608]
 [4.608]
 [4.608]
 [4.608]
 [5.695]] [[0.835]
 [0.835]
 [0.835]
 [0.835]
 [1.459]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.833]
 [5.199]
 [4.366]
 [4.218]
 [5.363]] [[1.629]
 [1.292]
 [0.849]
 [0.77 ]
 [1.379]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.314]
 [5.314]
 [5.314]
 [3.342]
 [6.53 ]] [[1.508]
 [1.508]
 [1.508]
 [0.71 ]
 [2.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.6218678988699998
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.368]
 [0.453]
 [1.831]
 [1.749]
 [1.739]] [[1.007]
 [0.208]
 [1.412]
 [1.34 ]
 [1.331]]
line 256 mcts: sample exp_bonus 3.486133589019182
maxi score, test score, baseline:  0.0001 0.0 0.0001
in main func line 156:  841
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.386]
 [1.315]
 [1.616]
 [1.315]
 [1.463]] [[0.197]
 [0.15 ]
 [0.351]
 [0.15 ]
 [0.248]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.6242084669395646
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
848 4389
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
siam score:  -0.81656265
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 1.833293315504653
siam score:  -0.8092535
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.456262184326446
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.665]
 [2.665]
 [2.665]
 [2.665]
 [2.587]] [[1.089]
 [1.089]
 [1.089]
 [1.089]
 [1.038]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7762579
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7752767
siam score:  -0.77780867
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.805]
 [4.805]
 [4.805]
 [4.805]
 [4.811]] [[1.996]
 [1.996]
 [1.996]
 [1.996]
 [2.   ]]
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.351]
 [0.85 ]
 [1.463]
 [1.7  ]
 [1.611]] [[0.472]
 [0.   ]
 [0.578]
 [0.8  ]
 [0.717]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 3.044735325634667
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.6041715747080623
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[7.149]
 [8.155]
 [7.955]
 [8.291]
 [7.424]] [[0.914]
 [1.297]
 [1.221]
 [1.349]
 [1.019]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8179451
in main func line 156:  894
siam score:  -0.8069342
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.528]
 [0.91 ]
 [0.729]
 [0.924]
 [0.972]] [[0.37 ]
 [0.879]
 [0.639]
 [0.899]
 [0.962]]
UNIT TEST: sample policy line 217 mcts : [0.25  0.042 0.083 0.167 0.458]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.83464324
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
907 4454
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.6439334616317167
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.645254318142192
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.281]
 [1.281]
 [1.472]
 [1.281]
 [1.694]] [[0.353]
 [0.353]
 [0.48 ]
 [0.353]
 [0.627]]
924 4476
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.6475621817860432
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.82314855
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.937]
 [3.937]
 [3.937]
 [3.937]
 [3.937]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.171]
 [2.171]
 [2.171]
 [2.171]
 [1.819]] [[0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.296]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.053]
 [0.639]
 [1.337]
 [1.259]
 [1.259]] [[0.138]
 [0.   ]
 [0.233]
 [0.207]
 [0.207]]
line 256 mcts: sample exp_bonus 2.738780906076033
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.963]
 [2.563]
 [1.077]
 [2.624]
 [2.497]] [[0.815]
 [1.367]
 [0.   ]
 [1.424]
 [1.306]]
siam score:  -0.83014697
siam score:  -0.83339095
using explorer policy with actor:  1
siam score:  -0.83268267
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8322572
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
937 4504
siam score:  -0.8300587
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
939 4504
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.921]
 [0.48 ]
 [1.214]
 [0.981]
 [1.402]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.891]
 [1.399]
 [0.581]
 [1.039]
 [1.374]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8382263
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
943 4507
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.819]
 [2.819]
 [2.819]
 [2.939]
 [3.294]] [[1.094]
 [1.094]
 [1.094]
 [1.195]
 [1.493]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8337571
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 2.6114622067866735
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.942]
 [2.942]
 [2.942]
 [2.942]
 [2.942]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20005
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.624]
 [3.624]
 [3.624]
 [3.624]
 [3.624]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.435]
 [3.435]
 [3.435]
 [3.435]
 [3.527]] [[1.922]
 [1.922]
 [1.922]
 [1.922]
 [2.   ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.074]
 [4.074]
 [4.074]
 [4.074]
 [5.972]] [[1.271]
 [1.271]
 [1.271]
 [1.271]
 [1.954]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.3899567650078444
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.415]
 [4.226]
 [3.477]
 [3.049]
 [3.562]] [[1.019]
 [1.537]
 [1.059]
 [0.785]
 [1.113]]
973 4539
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.091]
 [4.091]
 [4.874]
 [4.091]
 [4.758]] [[1.172]
 [1.172]
 [1.743]
 [1.172]
 [1.658]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.18 ]
 [4.18 ]
 [4.18 ]
 [4.18 ]
 [5.092]] [[1.063]
 [1.063]
 [1.063]
 [1.063]
 [1.57 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.355]
 [3.355]
 [3.355]
 [3.355]
 [3.629]] [[1.489]
 [1.489]
 [1.489]
 [1.489]
 [1.671]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.544]
 [5.544]
 [4.508]
 [3.947]
 [5.051]] [[2.   ]
 [2.   ]
 [1.288]
 [0.902]
 [1.661]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8425247
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.544]
 [4.544]
 [4.544]
 [3.355]
 [5.49 ]] [[0.931]
 [0.931]
 [0.931]
 [0.508]
 [1.268]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.127]
 [4.127]
 [3.944]
 [4.127]
 [3.643]] [[1.312]
 [1.312]
 [1.191]
 [1.312]
 [0.99 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.85200876
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.042 0.    0.375 0.    0.583]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20005
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.044]
 [0.873]
 [2.196]
 [2.112]
 [1.983]] [[1.363]
 [0.   ]
 [1.54 ]
 [1.442]
 [1.292]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20005
first move QE:  0.6994341378468726
1019 4584
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
1022 4588
siam score:  -0.85797197
siam score:  -0.8598428
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.86260957
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.313]
 [6.972]
 [6.146]
 [6.046]
 [6.017]] [[1.237]
 [1.555]
 [1.157]
 [1.108]
 [1.094]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.833]
 [5.833]
 [5.833]
 [5.833]
 [7.859]] [[0.968]
 [0.968]
 [0.968]
 [0.968]
 [1.549]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  93 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.597]
 [3.597]
 [3.597]
 [3.597]
 [3.597]] [[1.318]
 [1.318]
 [1.318]
 [1.318]
 [1.318]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.779]
 [3.779]
 [3.779]
 [3.779]
 [3.779]] [[1.149]
 [1.149]
 [1.149]
 [1.149]
 [1.149]]
from probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.122]
 [3.122]
 [3.122]
 [3.122]
 [3.18 ]] [[1.706]
 [1.706]
 [1.706]
 [1.706]
 [1.759]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.861]
 [2.861]
 [2.861]
 [2.861]
 [2.772]] [[1.553]
 [1.553]
 [1.553]
 [1.553]
 [1.464]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.25]
 [5.25]
 [5.25]
 [5.25]
 [5.25]] [[1.415]
 [1.415]
 [1.415]
 [1.415]
 [1.415]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.191]
 [4.191]
 [4.191]
 [4.191]
 [4.685]] [[1.543]
 [1.543]
 [1.543]
 [1.543]
 [1.749]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.698]
 [5.698]
 [5.698]
 [5.698]
 [5.711]] [[1.992]
 [1.992]
 [1.992]
 [1.992]
 [2.   ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.767]
 [3.767]
 [3.767]
 [3.767]
 [3.781]] [[1.65 ]
 [1.65 ]
 [1.65 ]
 [1.65 ]
 [1.657]]
from probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[6.226]
 [6.226]
 [6.226]
 [6.226]
 [5.979]] [[1.41]
 [1.41]
 [1.41]
 [1.41]
 [1.29]]
from probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.200686708744861
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8063743
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
using explorer policy with actor:  1
from probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.013]
 [0.981]
 [0.981]
 [0.981]
 [1.368]] [[0.691]
 [0.649]
 [0.649]
 [0.649]
 [1.156]]
1062 4643
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.915]
 [1.915]
 [1.915]
 [1.915]
 [1.761]] [[1.199]
 [1.199]
 [1.199]
 [1.199]
 [1.045]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.083]
 [2.045]
 [2.382]
 [1.889]
 [2.189]] [[0.249]
 [0.236]
 [0.349]
 [0.184]
 [0.284]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.383]
 [3.383]
 [3.383]
 [3.383]
 [3.383]] [[0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]]
1067 4650
from probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.929]
 [3.2  ]
 [3.046]
 [2.947]
 [3.022]] [[1.056]
 [1.237]
 [1.135]
 [1.069]
 [1.119]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.616]
 [2.503]
 [1.927]
 [1.72 ]
 [2.219]] [[0.571]
 [1.753]
 [0.984]
 [0.708]
 [1.374]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
siam score:  -0.79103655
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.452]
 [1.452]
 [1.452]
 [1.452]
 [1.452]] [[1.937]
 [1.937]
 [1.937]
 [1.937]
 [1.937]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 3.415995344072548
siam score:  -0.794962
siam score:  -0.80009085
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
1075 4656
1075 4658
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 4.679313190553415
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.316]
 [1.316]
 [1.316]
 [1.316]
 [3.639]] [[0.462]
 [0.462]
 [0.462]
 [0.462]
 [1.864]]
from probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8232994
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
first move QE:  0.7156043307511105
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[5.776]
 [5.776]
 [5.776]
 [5.776]
 [6.975]] [[1.071]
 [1.071]
 [1.071]
 [1.071]
 [1.486]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.236]
 [1.236]
 [1.236]
 [1.236]
 [1.236]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
first move QE:  0.7165192765677953
from probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
from probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
using explorer policy with actor:  1
from probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
siam score:  -0.8139627
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.7169650862662218
using another actor
from probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.7176781440171384
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.883]
 [1.883]
 [1.883]
 [1.883]
 [2.076]] [[0.942]
 [0.942]
 [0.942]
 [0.942]
 [1.134]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.189]
 [1.081]
 [1.192]
 [1.227]
 [1.493]] [[0.371]
 [0.256]
 [0.374]
 [0.411]
 [0.693]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.824]
 [1.824]
 [1.824]
 [1.824]
 [2.795]] [[0.335]
 [0.335]
 [0.335]
 [0.335]
 [0.981]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.263]
 [2.263]
 [2.263]
 [2.263]
 [2.319]] [[1.83 ]
 [1.83 ]
 [1.83 ]
 [1.83 ]
 [1.884]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.245954486531689
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.115]
 [2.115]
 [2.115]
 [2.115]
 [1.656]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.183]
 [2.183]
 [2.183]
 [2.183]
 [2.059]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.912]
 [1.912]
 [1.912]
 [1.912]
 [1.912]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.222]
 [0.748]
 [1.767]
 [1.442]
 [1.688]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.013]
 [2.013]
 [2.013]
 [2.013]
 [1.9  ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.252]
 [2.252]
 [2.252]
 [2.252]
 [2.158]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
in main func line 156:  1107
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.352773864783801
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.248]
 [0.682]
 [1.912]
 [1.794]
 [1.736]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.233]
 [2.233]
 [2.233]
 [2.233]
 [2.012]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.162]
 [2.162]
 [2.162]
 [2.162]
 [2.338]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.56 ]
 [0.699]
 [2.15 ]
 [1.804]
 [1.793]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.324]
 [2.324]
 [2.324]
 [2.324]
 [2.283]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.143]
 [2.143]
 [2.143]
 [2.143]
 [2.074]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
siam score:  -0.81602556
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.003]
 [4.003]
 [4.003]
 [4.003]
 [6.795]] [[0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [1.718]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.81421465
line 256 mcts: sample exp_bonus 2.032449596522936
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.042 0.042 0.833]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.7223344186761289
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.214]
 [1.214]
 [0.937]
 [1.707]
 [2.013]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8177379
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[2.216]
 [2.216]
 [0.808]
 [1.788]
 [2.107]] [[1.642]
 [1.642]
 [0.   ]
 [1.144]
 [1.515]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[3.314]
 [3.314]
 [3.314]
 [3.314]
 [3.136]] [[0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.782]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
start point for exploration sampling:  20005
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.29 ]
 [1.29 ]
 [1.29 ]
 [1.29 ]
 [1.989]] [[0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.633]]
in main func line 156:  1134
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
siam score:  -0.81560224
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
line 256 mcts: sample exp_bonus 1.8355976283572326
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[6.109]
 [6.109]
 [6.109]
 [6.109]
 [5.9  ]] [[1.329]
 [1.329]
 [1.329]
 [1.329]
 [1.246]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.477]
 [2.073]
 [1.477]
 [1.477]
 [1.973]] [[0.375]
 [1.254]
 [0.375]
 [0.375]
 [1.107]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
in main func line 156:  1153
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
first move QE:  0.7323463682577657
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
1159 4763
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[6.131]
 [6.131]
 [6.131]
 [6.131]
 [6.018]] [[1.864]
 [1.864]
 [1.864]
 [1.864]
 [1.81 ]]
siam score:  -0.8449216
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.001]] [[1.614]
 [1.614]
 [1.614]
 [1.614]
 [2.425]] [[0.259]
 [0.259]
 [0.259]
 [0.259]
 [1.299]]
from probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.322]
 [2.322]
 [2.322]
 [2.322]
 [2.268]] [[1.521]
 [1.521]
 [1.521]
 [1.521]
 [1.473]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.   ]
 [0.   ]] [[2.491]
 [2.491]
 [2.491]
 [2.157]
 [2.221]] [[1.845]
 [1.845]
 [1.845]
 [1.51 ]
 [1.575]]
UNIT TEST: sample policy line 217 mcts : [0.042 0.667 0.042 0.042 0.208]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[3.573]
 [3.573]
 [3.573]
 [3.573]
 [3.46 ]] [[0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.319]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.03846745623582509, 0.8076627188208749, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509, 0.03846745623582509]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]] [[5.302]
 [4.34 ]
 [4.765]
 [4.938]
 [4.609]] [[0.901]
 [0.58 ]
 [0.722]
 [0.779]
 [0.67 ]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[4.275]
 [4.275]
 [4.275]
 [4.275]
 [4.275]] [[0.27]
 [0.27]
 [0.27]
 [0.27]
 [0.27]]
actor:  1 policy actor:  1  step number:  90 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
in main func line 156:  1170
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[7.684]
 [7.684]
 [7.684]
 [7.684]
 [7.684]] [[1.321]
 [1.321]
 [1.321]
 [1.321]
 [1.321]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670386, 0.45651417675670386, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670386, 0.45651417675670386, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008]
maxi score, test score, baseline:  0.0001 0.0 0.0001
1175 4782
using another actor
from probs:  [0.45651417675670386, 0.45651417675670386, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.001]] [[2.901]
 [2.901]
 [3.497]
 [2.901]
 [3.658]] [[0.624]
 [0.624]
 [1.021]
 [0.624]
 [1.128]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670386, 0.45651417675670386, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008]
using explorer policy with actor:  1
first move QE:  0.7394534631806275
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670386, 0.45651417675670386, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0006],
        [0.0000],
        [0.0006],
        [0.0005],
        [0.0007],
        [0.0005],
        [0.0005],
        [0.0006],
        [0.0000],
        [0.0008]], dtype=torch.float64)
0.0 0.0005969051504696749
0.0 0.0
0.0 0.0006269973092118221
0.0 0.0005037068987089405
0.0 0.0007221064382876009
0.0 0.0004707402136507255
0.0 0.000542811451683317
0.0 0.0005745711486100691
0.0 0.0
0.0 0.0008441652771991386
siam score:  -0.8200579
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.193]
 [2.193]
 [2.193]
 [2.193]
 [2.193]] [[1.295]
 [1.295]
 [1.295]
 [1.295]
 [1.295]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.012]
 [2.012]
 [2.012]
 [2.012]
 [2.384]] [[1.122]
 [1.122]
 [1.122]
 [1.122]
 [1.606]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.531]
 [2.531]
 [2.531]
 [2.531]
 [2.531]] [[2.001]
 [2.001]
 [2.001]
 [2.001]
 [2.001]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.742]
 [1.39 ]
 [1.742]
 [2.004]
 [2.061]] [[0.77 ]
 [0.312]
 [0.77 ]
 [1.111]
 [1.185]]
siam score:  -0.8229553
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.45651417675670386, 0.45651417675670386, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20005
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670386, 0.45651417675670386, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[7.132]
 [7.132]
 [7.132]
 [7.132]
 [8.103]] [[1.242]
 [1.242]
 [1.242]
 [1.242]
 [1.522]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.204]
 [1.204]
 [1.204]
 [1.204]
 [1.013]] [[0.22 ]
 [0.22 ]
 [0.22 ]
 [0.22 ]
 [0.157]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670386, 0.45651417675670386, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670386, 0.45651417675670386, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.001]
 [0.002]
 [0.002]] [[1.62 ]
 [1.62 ]
 [1.626]
 [1.898]
 [1.878]] [[0.722]
 [0.722]
 [0.732]
 [1.139]
 [1.109]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.002]] [[1.954]
 [1.954]
 [1.954]
 [1.954]
 [1.951]] [[1.621]
 [1.621]
 [1.621]
 [1.621]
 [1.619]]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.001]
 [0.002]] [[2.168]
 [2.168]
 [2.168]
 [2.003]
 [2.097]] [[1.574]
 [1.574]
 [1.574]
 [1.357]
 [1.481]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670386, 0.45651417675670386, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008]
1200 4816
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670386, 0.45651417675670386, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008]
from probs:  [0.45651417675670386, 0.45651417675670386, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[1.873]
 [1.873]
 [1.873]
 [1.873]
 [2.065]] [[0.847]
 [0.847]
 [0.847]
 [0.847]
 [1.094]]
using another actor
using explorer policy with actor:  0
siam score:  -0.851227
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[1.638]
 [1.638]
 [1.772]
 [1.638]
 [2.111]] [[0.598]
 [0.598]
 [0.772]
 [0.598]
 [1.213]]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.691]
 [2.661]
 [3.109]
 [2.661]
 [2.837]] [[1.277]
 [1.255]
 [1.591]
 [1.255]
 [1.387]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.393]
 [1.946]
 [2.393]
 [3.079]
 [2.926]] [[0.707]
 [0.449]
 [0.707]
 [1.104]
 [1.016]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.5338901620244543
line 256 mcts: sample exp_bonus 1.6604697251758733
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.704]
 [1.497]
 [1.514]
 [1.588]
 [1.668]] [[1.085]
 [0.868]
 [0.886]
 [0.963]
 [1.047]]
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670386, 0.45651417675670386, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670386, 0.45651417675670386, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.7487489562371527
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.277]
 [2.277]
 [2.277]
 [2.129]
 [2.12 ]] [[1.691]
 [1.691]
 [1.691]
 [1.482]
 [1.469]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670386, 0.45651417675670386, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.337]
 [2.337]
 [2.337]
 [2.337]
 [2.249]] [[1.733]
 [1.733]
 [1.733]
 [1.733]
 [1.62 ]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.755]
 [1.755]
 [1.755]
 [1.755]
 [2.344]] [[0.909]
 [0.909]
 [0.909]
 [0.909]
 [1.786]]
siam score:  -0.85946244
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.002]] [[4.033]
 [4.033]
 [4.033]
 [4.033]
 [4.109]] [[1.415]
 [1.415]
 [1.415]
 [1.415]
 [1.463]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[4.826]
 [4.766]
 [3.827]
 [4.48 ]
 [4.927]] [[1.695]
 [1.654]
 [1.031]
 [1.465]
 [1.761]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670386, 0.45651417675670386, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.671]
 [2.671]
 [2.671]
 [2.276]
 [2.332]] [[1.404]
 [1.404]
 [1.404]
 [1.008]
 [1.065]]
siam score:  -0.8721544
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[2.099]
 [2.099]
 [2.099]
 [2.099]
 [1.95 ]] [[1.937]
 [1.937]
 [1.937]
 [1.937]
 [1.705]]
from probs:  [0.45651417675670386, 0.45651417675670386, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.45651417675670386, 0.45651417675670386, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.45651417675670386, 0.45651417675670386, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.006]] [[4.004]
 [4.004]
 [4.004]
 [4.004]
 [4.312]] [[0.44 ]
 [0.44 ]
 [0.44 ]
 [0.44 ]
 [0.539]]
siam score:  -0.8687232
using another actor
from probs:  [0.45651417675670386, 0.45651417675670386, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008, 0.021742911621648008]
UNIT TEST: sample policy line 217 mcts : [0.    0.083 0.    0.208 0.708]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
using another actor
from probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
first move QE:  0.757119639814776
line 256 mcts: sample exp_bonus 3.926696593313352
siam score:  -0.8381737
from probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
siam score:  -0.83714586
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
line 256 mcts: sample exp_bonus 1.6916473317893854
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
using another actor
using explorer policy with actor:  1
siam score:  -0.8522309
line 256 mcts: sample exp_bonus 1.4802750522151096
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[1.412]
 [1.449]
 [1.044]
 [1.617]
 [1.508]] [[0.575]
 [0.624]
 [0.111]
 [0.837]
 [0.698]]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.003]
 [0.003]
 [0.003]] [[1.485]
 [1.229]
 [1.568]
 [1.569]
 [1.597]] [[1.075]
 [0.707]
 [1.196]
 [1.198]
 [1.238]]
line 256 mcts: sample exp_bonus 1.6647080147266482
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.85639876
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
siam score:  -0.8587889
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
1277 4901
first move QE:  0.763267253425309
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]] [[3.769]
 [3.769]
 [3.769]
 [3.769]
 [4.821]] [[0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.967]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
using explorer policy with actor:  1
first move QE:  0.7645158210413063
Starting evaluation
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[1.511]
 [1.326]
 [1.601]
 [1.489]
 [1.406]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.6582802317221677
line 256 mcts: sample exp_bonus 1.8767898324996484
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[1.924]
 [1.924]
 [1.924]
 [1.924]
 [1.924]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[1.567]
 [1.345]
 [1.345]
 [1.532]
 [1.78 ]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 1.8791687170506421
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[1.131]
 [1.131]
 [1.131]
 [1.131]
 [1.131]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.004]] [[1.364]
 [1.364]
 [1.364]
 [1.345]
 [2.021]] [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.004]]
using explorer policy with actor:  0
rdn probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
in main func line 156:  1290
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.005]
 [0.005]
 [0.005]
 [0.003]] [[2.348]
 [2.83 ]
 [2.528]
 [2.83 ]
 [5.059]] [[0.503]
 [0.689]
 [0.573]
 [0.689]
 [1.547]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
1292 4927
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.006]] [[0.956]
 [1.928]
 [0.631]
 [1.983]
 [2.087]] [[0.219]
 [0.867]
 [0.004]
 [0.903]
 [0.975]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
siam score:  -0.8954979
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
from probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
first move QE:  0.7699930976468243
first move QE:  0.7700579028893934
from probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[1.999]
 [1.999]
 [1.999]
 [1.999]
 [1.999]] [[0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[2.507]
 [2.507]
 [2.507]
 [2.507]
 [2.473]] [[1.109]
 [1.109]
 [1.109]
 [1.109]
 [1.072]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
line 256 mcts: sample exp_bonus 3.020185357097151
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.01 ]
 [0.01 ]
 [0.009]
 [0.011]] [[1.73 ]
 [1.944]
 [2.323]
 [1.985]
 [2.165]] [[1.029]
 [1.378]
 [1.991]
 [1.442]
 [1.738]]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[1.926]
 [1.926]
 [1.926]
 [1.926]
 [1.926]] [[1.537]
 [1.537]
 [1.537]
 [1.537]
 [1.537]]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.012]
 [0.01 ]
 [0.012]
 [0.01 ]] [[4.112]
 [4.421]
 [3.591]
 [4.421]
 [4.456]] [[1.478]
 [1.671]
 [1.157]
 [1.671]
 [1.692]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
1311 4950
siam score:  -0.9188879
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
from probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.007]] [[2.126]
 [2.126]
 [2.126]
 [2.126]
 [1.988]] [[1.319]
 [1.319]
 [1.319]
 [1.319]
 [1.185]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.91859645
1323 4986
1324 4988
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.008]
 [0.01 ]] [[1.394]
 [1.306]
 [1.306]
 [1.397]
 [1.634]] [[0.209]
 [0.179]
 [0.179]
 [0.208]
 [0.292]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
start point for exploration sampling:  20005
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[2.113]
 [2.113]
 [2.113]
 [2.113]
 [2.736]] [[1.164]
 [1.164]
 [1.164]
 [1.164]
 [1.578]]
1327 4996
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
siam score:  -0.9240264
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.022]
 [0.017]
 [0.017]
 [0.02 ]] [[1.46 ]
 [1.706]
 [1.68 ]
 [1.42 ]
 [1.518]] [[0.463]
 [0.635]
 [0.608]
 [0.435]
 [0.506]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.013]
 [0.017]
 [0.019]
 [0.02 ]] [[1.408]
 [1.14 ]
 [1.451]
 [1.276]
 [1.383]] [[0.414]
 [0.218]
 [0.433]
 [0.32 ]
 [0.393]]
from probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[2.734]
 [2.799]
 [2.553]
 [1.85 ]
 [2.655]] [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.021]] [[1.524]
 [1.524]
 [1.524]
 [1.524]
 [1.946]] [[1.106]
 [1.106]
 [1.106]
 [1.106]
 [1.882]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.01 ]
 [0.01 ]
 [0.01 ]
 [0.011]] [[1.996]
 [1.996]
 [1.996]
 [1.996]
 [1.928]] [[0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.189]]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.015]
 [0.01 ]
 [0.015]
 [0.015]] [[2.145]
 [2.145]
 [2.085]
 [2.145]
 [1.868]] [[0.257]
 [0.257]
 [0.227]
 [0.257]
 [0.164]]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.009]
 [0.008]
 [0.01 ]
 [0.013]] [[1.101]
 [1.183]
 [0.576]
 [1.341]
 [1.557]] [[0.564]
 [0.647]
 [0.01 ]
 [0.814]
 [1.045]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
siam score:  -0.9323103
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.004]
 [0.007]
 [0.009]
 [0.01 ]] [[1.807]
 [1.298]
 [2.099]
 [2.124]
 [2.218]] [[0.449]
 [0.004]
 [0.697]
 [0.721]
 [0.802]]
using another actor
from probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
using explorer policy with actor:  1
siam score:  -0.9355803
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
Printing some Q and Qe and total Qs values:  [[0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]] [[3.188]
 [3.188]
 [3.188]
 [3.188]
 [3.188]] [[1.163]
 [1.163]
 [1.163]
 [1.163]
 [1.163]]
siam score:  -0.9295802
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
from probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
from probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
using explorer policy with actor:  1
first move QE:  0.801327855824366
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.011]] [[2.238]
 [2.238]
 [2.238]
 [2.238]
 [3.055]] [[0.486]
 [0.486]
 [0.486]
 [0.486]
 [1.035]]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]] [[4.634]
 [4.634]
 [4.634]
 [4.634]
 [4.634]] [[1.486]
 [1.486]
 [1.486]
 [1.486]
 [1.486]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 7.346354028911729
1368 5088
from probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
using another actor
from probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
first move QE:  0.8062596543045194
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.01 ]] [[0.878]
 [0.878]
 [0.878]
 [0.878]
 [1.329]] [[0.086]
 [0.086]
 [0.086]
 [0.086]
 [0.24 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.013]] [[3.098]
 [3.098]
 [3.098]
 [3.098]
 [2.857]] [[0.634]
 [0.634]
 [0.634]
 [0.634]
 [0.558]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
siam score:  -0.9102521
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.31817906299770504, 0.31817906299770504, 0.31817906299770504, 0.015154270335628262, 0.015154270335628262, 0.015154270335628262]
siam score:  -0.92122346
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.069]] [[6.864]
 [6.864]
 [6.864]
 [6.864]
 [7.715]] [[1.596]
 [1.596]
 [1.596]
 [1.596]
 [1.768]]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.91723496
using another actor
from probs:  [0.2441849646986885, 0.47673985879475395, 0.2441849646986885, 0.01163007060262309, 0.01163007060262309, 0.01163007060262309]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
in main func line 156:  1386
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[2.646]
 [2.646]
 [2.646]
 [2.646]
 [2.624]] [[1.584]
 [1.584]
 [1.584]
 [1.584]
 [1.569]]
from probs:  [0.2441849646986885, 0.47673985879475395, 0.2441849646986885, 0.01163007060262309, 0.01163007060262309, 0.01163007060262309]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.004]
 [0.007]
 [0.009]
 [0.009]] [[1.839]
 [0.979]
 [1.968]
 [1.81 ]
 [2.086]] [[1.   ]
 [0.007]
 [1.14 ]
 [0.962]
 [1.277]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2441849646986885, 0.47673985879475395, 0.2441849646986885, 0.01163007060262309, 0.01163007060262309, 0.01163007060262309]
using explorer policy with actor:  1
siam score:  -0.91462797
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2441849646986885, 0.47673985879475395, 0.2441849646986885, 0.01163007060262309, 0.01163007060262309, 0.01163007060262309]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2441849646986885, 0.47673985879475395, 0.2441849646986885, 0.01163007060262309, 0.01163007060262309, 0.01163007060262309]
from probs:  [0.2441849646986885, 0.47673985879475395, 0.2441849646986885, 0.01163007060262309, 0.01163007060262309, 0.01163007060262309]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.042 0.    0.833 0.    0.125]
actor:  1 policy actor:  1  step number:  99 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
in main func line 156:  1401
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 3.156010371054957
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.1438358041174326, 0.4178061547082413, 0.28082097941283696, 0.1438358041174326, 0.006850628822028332, 0.006850628822028332]
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.2222074093134483
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1438358041174326, 0.4178061547082413, 0.28082097941283696, 0.1438358041174326, 0.006850628822028332, 0.006850628822028332]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.    0.    0.    0.042 0.958]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8999293
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1438358041174326, 0.4178061547082413, 0.28082097941283696, 0.1438358041174326, 0.006850628822028332, 0.006850628822028332]
Printing some Q and Qe and total Qs values:  [[0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]] [[2.071]
 [2.071]
 [2.071]
 [2.071]
 [2.113]] [[1.831]
 [1.831]
 [1.831]
 [1.831]
 [1.9  ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
1416 5139
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12650631445573782, 0.3674684277213109, 0.24698737108852434, 0.12650631445573782, 0.12650631445573782, 0.006025257822951336]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12650631445573782, 0.3674684277213109, 0.24698737108852434, 0.12650631445573782, 0.12650631445573782, 0.006025257822951336]
in main func line 156:  1417
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12650631445573782, 0.3674684277213109, 0.24698737108852434, 0.12650631445573782, 0.12650631445573782, 0.006025257822951336]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.013]
 [0.011]
 [0.017]
 [0.023]] [[1.661]
 [1.526]
 [1.68 ]
 [1.593]
 [1.535]] [[1.308]
 [1.105]
 [1.331]
 [1.211]
 [1.136]]
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.025]] [[1.457]
 [1.457]
 [1.457]
 [1.457]
 [1.457]] [[0.29]
 [0.29]
 [0.29]
 [0.29]
 [0.29]]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.009]
 [0.012]
 [0.01 ]
 [0.014]] [[2.347]
 [2.253]
 [2.037]
 [2.317]
 [2.044]] [[2.007]
 [1.867]
 [1.556]
 [1.962]
 [1.57 ]]
from probs:  [0.12650631445573782, 0.3674684277213109, 0.24698737108852434, 0.12650631445573782, 0.12650631445573782, 0.006025257822951336]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12650631445573782, 0.3674684277213109, 0.24698737108852434, 0.12650631445573782, 0.12650631445573782, 0.006025257822951336]
1426 5154
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12650631445573782, 0.3674684277213109, 0.24698737108852434, 0.12650631445573782, 0.12650631445573782, 0.006025257822951336]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.31 ]
 [0.221]
 [0.312]
 [0.32 ]] [[4.482]
 [5.301]
 [4.71 ]
 [3.515]
 [4.335]] [[1.563]
 [2.072]
 [1.655]
 [1.008]
 [1.503]]
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.11290357271715797, 0.32795594851519266, 0.22042976061617534, 0.11290357271715797, 0.22042976061617534, 0.005377384818140646]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.11290357271715797, 0.32795594851519266, 0.22042976061617534, 0.11290357271715797, 0.22042976061617534, 0.005377384818140646]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.3835167051815727
line 256 mcts: sample exp_bonus 5.959810606049016
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.9077862
1443 5184
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]] [[4.664]
 [4.664]
 [4.664]
 [4.664]
 [4.664]] [[2.104]
 [2.104]
 [2.104]
 [2.104]
 [2.104]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.027]
 [0.027]
 [0.027]
 [0.027]
 [0.027]] [[1.642]
 [1.642]
 [1.642]
 [1.642]
 [1.529]] [[0.956]
 [0.956]
 [0.956]
 [0.956]
 [0.843]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.11290357271715797, 0.32795594851519266, 0.22042976061617534, 0.11290357271715797, 0.22042976061617534, 0.005377384818140646]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.11290357271715797, 0.32795594851519266, 0.22042976061617534, 0.11290357271715797, 0.22042976061617534, 0.005377384818140646]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.011]
 [0.018]
 [0.018]
 [0.022]] [[1.044]
 [1.144]
 [1.044]
 [1.044]
 [1.282]] [[0.386]
 [0.471]
 [0.386]
 [0.386]
 [0.632]]
from probs:  [0.11290357271715797, 0.32795594851519266, 0.22042976061617534, 0.11290357271715797, 0.22042976061617534, 0.005377384818140646]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.10194212466554549, 0.29611575066890905, 0.29611575066890905, 0.10194212466554549, 0.19902893766722726, 0.004855311663863744]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]] [[1.55 ]
 [1.55 ]
 [1.55 ]
 [1.55 ]
 [0.813]] [[0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.01 ]
 [0.01 ]
 [0.012]
 [0.012]] [[1.146]
 [1.146]
 [1.146]
 [1.538]
 [1.96 ]] [[0.01 ]
 [0.01 ]
 [0.01 ]
 [0.012]
 [0.012]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
line 256 mcts: sample exp_bonus 1.0984920436975218
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.007]
 [0.012]
 [0.012]
 [0.012]] [[1.12 ]
 [1.029]
 [1.699]
 [1.305]
 [1.169]] [[0.011]
 [0.007]
 [0.012]
 [0.012]
 [0.012]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.007]
 [0.012]
 [0.011]
 [0.012]] [[1.17 ]
 [1.029]
 [1.701]
 [1.311]
 [1.286]] [[0.011]
 [0.007]
 [0.012]
 [0.011]
 [0.012]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.227750887935091
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.043]] [[3.774]
 [3.774]
 [3.774]
 [3.774]
 [6.711]] [[0.932]
 [0.932]
 [0.932]
 [0.932]
 [2.003]]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
siam score:  -0.9105894
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]] [[2.015]
 [2.015]
 [2.015]
 [2.015]
 [2.015]] [[0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1465 5213
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.012]
 [0.012]
 [0.014]
 [0.014]] [[1.526]
 [1.464]
 [1.464]
 [1.551]
 [1.878]] [[0.014]
 [0.012]
 [0.012]
 [0.014]
 [0.014]]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]] [[1.93 ]
 [1.93 ]
 [1.93 ]
 [1.93 ]
 [1.748]] [[0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.022]
 [0.022]
 [0.02 ]
 [0.02 ]] [[1.816]
 [1.816]
 [1.816]
 [1.707]
 [1.916]] [[1.315]
 [1.315]
 [1.315]
 [1.168]
 [1.445]]
using explorer policy with actor:  0
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.24796708302790388, 0.32926749938914096, 0.0853662503054295, 0.16666666666666669, 0.004065833944192357]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.002]
 [0.009]
 [0.009]
 [0.011]] [[0.825]
 [0.733]
 [1.463]
 [1.195]
 [0.992]] [[0.008]
 [0.002]
 [0.009]
 [0.009]
 [0.011]]
rdn probs:  [0.16666666666666669, 0.24796708302790388, 0.32926749938914096, 0.0853662503054295, 0.16666666666666669, 0.004065833944192357]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]] [[2.426]
 [2.426]
 [2.426]
 [2.426]
 [3.056]] [[0.871]
 [0.871]
 [0.871]
 [0.871]
 [1.293]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.24796708302790388, 0.32926749938914096, 0.0853662503054295, 0.16666666666666669, 0.004065833944192357]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.014]] [[1.393]
 [1.393]
 [1.393]
 [1.393]
 [2.453]] [[0.684]
 [0.684]
 [0.684]
 [0.684]
 [1.774]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.90983397
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.90589964
maxi score, test score, baseline:  0.0001 0.0 0.0001
1479 5234
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.289905182677165
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.24796708302790388, 0.32926749938914096, 0.0853662503054295, 0.16666666666666669, 0.004065833944192357]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.24796708302790388, 0.32926749938914096, 0.0853662503054295, 0.16666666666666669, 0.004065833944192357]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
1483 5239
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.389]] [[4.535]
 [4.535]
 [4.535]
 [4.535]
 [4.514]] [[0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.752]]
from probs:  [0.16666666666666669, 0.24796708302790388, 0.32926749938914096, 0.0853662503054295, 0.16666666666666669, 0.004065833944192357]
siam score:  -0.9091007
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.24796708302790388, 0.32926749938914096, 0.0853662503054295, 0.16666666666666669, 0.004065833944192357]
1486 5249
from probs:  [0.16666666666666669, 0.24796708302790388, 0.32926749938914096, 0.0853662503054295, 0.16666666666666669, 0.004065833944192357]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.035]] [[2.038]
 [2.038]
 [2.038]
 [2.038]
 [2.038]] [[2.022]
 [2.022]
 [2.022]
 [2.022]
 [2.022]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
1489 5264
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.016]
 [0.016]
 [0.019]
 [0.017]] [[1.523]
 [1.6  ]
 [1.6  ]
 [1.871]
 [1.813]] [[0.787]
 [0.898]
 [0.898]
 [1.264]
 [1.184]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.018]
 [0.037]
 [0.015]
 [0.016]] [[0.653]
 [1.116]
 [0.659]
 [0.638]
 [0.836]] [[0.296]
 [0.933]
 [0.361]
 [0.288]
 [0.553]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.24796708302790388, 0.32926749938914096, 0.0853662503054295, 0.16666666666666669, 0.004065833944192357]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.24796708302790388, 0.32926749938914096, 0.0853662503054295, 0.16666666666666669, 0.004065833944192357]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.019]
 [0.025]
 [0.02 ]
 [0.02 ]] [[2.841]
 [3.028]
 [2.778]
 [2.934]
 [3.343]] [[1.33 ]
 [1.484]
 [1.287]
 [1.409]
 [1.74 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.24796708302790388, 0.32926749938914096, 0.0853662503054295, 0.16666666666666669, 0.004065833944192357]
siam score:  -0.90228117
from probs:  [0.16666666666666669, 0.24796708302790388, 0.32926749938914096, 0.0853662503054295, 0.16666666666666669, 0.004065833944192357]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.24796708302790388, 0.32926749938914096, 0.0853662503054295, 0.16666666666666669, 0.004065833944192357]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.24796708302790388, 0.32926749938914096, 0.0853662503054295, 0.16666666666666669, 0.004065833944192357]
siam score:  -0.89260083
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.24796708302790388, 0.32926749938914096, 0.0853662503054295, 0.16666666666666669, 0.004065833944192357]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.24796708302790388, 0.32926749938914096, 0.0853662503054295, 0.16666666666666669, 0.004065833944192357]
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.25  0.125 0.542]
1513 5289
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.013]
 [0.024]
 [0.023]
 [0.027]] [[1.167]
 [0.888]
 [1.484]
 [1.207]
 [1.303]] [[0.51 ]
 [0.22 ]
 [0.839]
 [0.559]
 [0.663]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
1516 5296
start point for exploration sampling:  20005
from probs:  [0.15413539488639877, 0.3045106562496138, 0.3045106562496138, 0.07894776420479122, 0.15413539488639877, 0.0037601335231836784]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.15413539488639877, 0.3045106562496138, 0.3045106562496138, 0.07894776420479122, 0.15413539488639877, 0.0037601335231836784]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.14335674117519906, 0.28321629412400456, 0.35314607059840736, 0.0734269647007963, 0.14335674117519906, 0.0034971882263935346]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
first move QE:  0.8396026752571005
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.187116489130465, 0.30981542391325484, 0.30981542391325484, 0.06441755434767507, 0.12576702173907003, 0.0030680869562801335]
from probs:  [0.187116489130465, 0.30981542391325484, 0.30981542391325484, 0.06441755434767507, 0.12576702173907003, 0.0030680869562801335]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
siam score:  -0.91600555
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.187116489130465, 0.30981542391325484, 0.30981542391325484, 0.06441755434767507, 0.12576702173907003, 0.0030680869562801335]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.187116489130465, 0.30981542391325484, 0.30981542391325484, 0.06441755434767507, 0.12576702173907003, 0.0030680869562801335]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.187116489130465, 0.30981542391325484, 0.30981542391325484, 0.06441755434767507, 0.12576702173907003, 0.0030680869562801335]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.043]
 [0.046]
 [0.051]
 [0.047]] [[1.472]
 [1.472]
 [1.675]
 [1.735]
 [1.753]] [[0.75 ]
 [0.75 ]
 [1.034]
 [1.125]
 [1.144]]
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.073]] [[2.442]
 [2.442]
 [2.442]
 [2.442]
 [2.905]] [[1.379]
 [1.379]
 [1.379]
 [1.379]
 [1.973]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.234]
 [0.234]
 [0.234]
 [0.228]
 [0.241]] [[4.05 ]
 [4.05 ]
 [4.05 ]
 [4.558]
 [4.658]] [[1.055]
 [1.055]
 [1.055]
 [1.336]
 [1.4  ]]
siam score:  -0.9094511
Printing some Q and Qe and total Qs values:  [[0.097]
 [0.097]
 [0.097]
 [0.097]
 [0.097]] [[4.896]
 [4.896]
 [4.896]
 [4.896]
 [5.071]] [[1.4  ]
 [1.4  ]
 [1.4  ]
 [1.4  ]
 [1.491]]
Printing some Q and Qe and total Qs values:  [[0.052]
 [0.12 ]
 [0.182]
 [0.075]
 [0.106]] [[2.201]
 [3.127]
 [2.413]
 [3.53 ]
 [4.012]] [[0.023]
 [0.649]
 [0.301]
 [0.835]
 [1.155]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.187116489130465, 0.30981542391325484, 0.30981542391325484, 0.06441755434767507, 0.12576702173907003, 0.0030680869562801335]
using another actor
line 256 mcts: sample exp_bonus 1.6000021066498975
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.187116489130465, 0.30981542391325484, 0.30981542391325484, 0.06441755434767507, 0.12576702173907003, 0.0030680869562801335]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.187116489130465, 0.30981542391325484, 0.30981542391325484, 0.06441755434767507, 0.12576702173907003, 0.0030680869562801335]
Printing some Q and Qe and total Qs values:  [[0.167]
 [0.167]
 [0.167]
 [0.167]
 [0.153]] [[3.4  ]
 [3.4  ]
 [3.4  ]
 [3.4  ]
 [3.414]] [[1.849]
 [1.849]
 [1.849]
 [1.849]
 [1.837]]
actor:  1 policy actor:  1  step number:  94 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
in main func line 156:  1546
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.419]
 [0.384]
 [0.384]
 [0.436]] [[3.984]
 [4.859]
 [3.984]
 [3.984]
 [4.586]] [[1.012]
 [1.489]
 [1.012]
 [1.012]
 [1.358]]
actor:  1 policy actor:  1  step number:  94 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.144]
 [0.144]
 [0.144]
 [0.144]
 [0.116]] [[2.633]
 [2.633]
 [2.633]
 [2.633]
 [3.108]] [[1.034]
 [1.034]
 [1.034]
 [1.034]
 [1.295]]
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.215]
 [0.216]
 [0.206]
 [0.218]] [[4.032]
 [3.905]
 [4.093]
 [4.032]
 [3.891]] [[1.793]
 [1.726]
 [1.854]
 [1.793]
 [1.723]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.19950729208211182, 0.29802916832844734, 0.24876823020527958, 0.10098541583577636, 0.15024635395894412, 0.002463539589440879]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.19950729208211182, 0.29802916832844734, 0.24876823020527958, 0.10098541583577636, 0.15024635395894412, 0.002463539589440879]
using explorer policy with actor:  1
siam score:  -0.9258633
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.19950729208211182, 0.29802916832844734, 0.24876823020527958, 0.10098541583577636, 0.15024635395894412, 0.002463539589440879]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.19014077893624262, 0.3309854525536986, 0.23708900347539463, 0.09624432985793868, 0.14319255439709067, 0.002347880779634718]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]] [[2.659]
 [2.659]
 [2.659]
 [2.659]
 [2.659]] [[0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]]
siam score:  -0.9241516
Printing some Q and Qe and total Qs values:  [[0.155]
 [0.131]
 [0.216]
 [0.205]
 [0.178]] [[2.626]
 [2.453]
 [2.771]
 [2.561]
 [2.606]] [[1.394]
 [1.184]
 [1.623]
 [1.389]
 [1.403]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18161430955187802, 0.31614309551878034, 0.27130016686314623, 0.09192845224060983, 0.13677138089624394, 0.002242594929341624]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.096]] [[6.066]
 [6.066]
 [6.066]
 [6.066]
 [5.783]] [[1.568]
 [1.568]
 [1.568]
 [1.568]
 [1.425]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18161430955187802, 0.31614309551878034, 0.27130016686314623, 0.09192845224060983, 0.13677138089624394, 0.002242594929341624]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.18161430955187802, 0.31614309551878034, 0.27130016686314623, 0.09192845224060983, 0.13677138089624394, 0.002242594929341624]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.1455998530387075
Printing some Q and Qe and total Qs values:  [[0.16]
 [0.16]
 [0.16]
 [0.16]
 [0.16]] [[2.709]
 [2.709]
 [2.709]
 [2.709]
 [2.709]] [[1.869]
 [1.869]
 [1.869]
 [1.869]
 [1.869]]
using another actor
start point for exploration sampling:  20005
Printing some Q and Qe and total Qs values:  [[0.11 ]
 [0.11 ]
 [0.11 ]
 [0.11 ]
 [0.109]] [[3.137]
 [3.137]
 [3.137]
 [3.137]
 [3.928]] [[0.942]
 [0.942]
 [0.942]
 [0.942]
 [1.364]]
using explorer policy with actor:  0
using explorer policy with actor:  0
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.1786],
        [0.2849],
        [0.2786],
        [0.0730],
        [0.0794],
        [0.0923],
        [0.3295],
        [0.0719],
        [0.0994],
        [0.0000]], dtype=torch.float64)
0.0 0.17861139312932334
0.0 0.2849118503482753
0.0 0.27864454113975606
0.0 0.07301907109066107
0.0 0.07943572140567436
0.0 0.09231051843785228
0.0 0.329484376793687
0.0 0.07186493393272562
0.0 0.0993735931594118
0.0 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]] [[6.535]
 [6.535]
 [6.535]
 [6.535]
 [6.535]] [[1.267]
 [1.267]
 [1.267]
 [1.267]
 [1.267]]
Printing some Q and Qe and total Qs values:  [[0.15]
 [0.15]
 [0.15]
 [0.15]
 [0.15]] [[3.297]
 [3.297]
 [3.297]
 [3.297]
 [3.297]] [[0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.27865586085534827, 0.27865586085534827, 0.2391302629064018, 0.08102787111061602, 0.12055346905956248, 0.001976675212723115]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
1583 5378
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.27865586085534827, 0.27865586085534827, 0.2391302629064018, 0.08102787111061602, 0.12055346905956248, 0.001976675212723115]
in main func line 156:  1585
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.7438153770522655
from probs:  [0.27865586085534827, 0.27865586085534827, 0.2391302629064018, 0.08102787111061602, 0.12055346905956248, 0.001976675212723115]
Printing some Q and Qe and total Qs values:  [[0.111]
 [0.102]
 [0.111]
 [0.111]
 [0.113]] [[2.136]
 [2.387]
 [2.136]
 [2.136]
 [1.997]] [[1.805]
 [2.019]
 [1.805]
 [1.805]
 [1.682]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 2.5695693843440535
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.27865586085534827, 0.27865586085534827, 0.2391302629064018, 0.08102787111061602, 0.12055346905956248, 0.001976675212723115]
siam score:  -0.9222832
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [0.1  ]
 [0.082]
 [0.095]
 [0.096]] [[2.1  ]
 [2.1  ]
 [2.169]
 [1.788]
 [1.857]] [[1.745]
 [1.745]
 [1.812]
 [1.297]
 [1.398]]
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.089]
 [0.089]
 [0.098]
 [0.09 ]] [[1.525]
 [1.525]
 [1.525]
 [1.929]
 [1.585]] [[0.929]
 [0.929]
 [0.929]
 [1.299]
 [0.984]]
using explorer policy with actor:  1
from probs:  [0.27865586085534827, 0.27865586085534827, 0.2391302629064018, 0.08102787111061602, 0.12055346905956248, 0.001976675212723115]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.061]
 [0.061]
 [0.061]
 [0.061]
 [0.072]] [[3.548]
 [3.548]
 [3.548]
 [3.548]
 [5.326]] [[1.194]
 [1.194]
 [1.194]
 [1.194]
 [2.008]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2948715130606307, 0.2948715130606307, 0.2216116008355084, 0.07509177638526376, 0.11172173249782494, 0.0018318641601414445]
Printing some Q and Qe and total Qs values:  [[0.08 ]
 [0.08 ]
 [0.08 ]
 [0.08 ]
 [0.099]] [[1.831]
 [1.831]
 [1.831]
 [1.831]
 [2.202]] [[0.787]
 [0.787]
 [0.787]
 [0.787]
 [1.196]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
Printing some Q and Qe and total Qs values:  [[0.093]
 [0.093]
 [0.093]
 [0.093]
 [0.104]] [[4.524]
 [4.524]
 [4.524]
 [4.524]
 [4.768]] [[1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.634]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.9136528
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.565071364923621
Printing some Q and Qe and total Qs values:  [[0.085]
 [0.085]
 [0.089]
 [0.085]
 [0.1  ]] [[1.892]
 [1.892]
 [2.047]
 [1.892]
 [2.002]] [[0.967]
 [0.967]
 [1.127]
 [0.967]
 [1.099]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.2948715130606307, 0.2948715130606307, 0.2216116008355084, 0.07509177638526376, 0.11172173249782494, 0.0018318641601414445]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2948715130606307, 0.2948715130606307, 0.2216116008355084, 0.07509177638526376, 0.11172173249782494, 0.0018318641601414445]
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.167]] [[6.241]
 [6.241]
 [6.241]
 [6.241]
 [6.227]] [[1.547]
 [1.547]
 [1.547]
 [1.547]
 [1.538]]
actor:  1 policy actor:  1  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using another actor
1626 5419
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2844520470606702, 0.2844520470606702, 0.24911643294246916, 0.07243836235146385, 0.10777397646966491, 0.001767134115061724]
Printing some Q and Qe and total Qs values:  [[0.078]
 [0.059]
 [0.041]
 [0.078]
 [0.081]] [[2.086]
 [2.498]
 [0.819]
 [2.086]
 [2.369]] [[1.107]
 [1.345]
 [0.189]
 [1.107]
 [1.302]]
Printing some Q and Qe and total Qs values:  [[0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.07 ]] [[3.297]
 [3.297]
 [3.297]
 [3.297]
 [3.935]] [[1.114]
 [1.114]
 [1.114]
 [1.114]
 [1.5  ]]
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.084]
 [0.084]
 [0.084]
 [0.082]] [[5.494]
 [6.444]
 [6.444]
 [6.444]
 [5.52 ]] [[1.454]
 [2.017]
 [2.017]
 [2.017]
 [1.473]]
from probs:  [0.2844520470606702, 0.2844520470606702, 0.24911643294246916, 0.07243836235146385, 0.10777397646966491, 0.001767134115061724]
Printing some Q and Qe and total Qs values:  [[0.091]
 [0.119]
 [0.117]
 [0.107]
 [0.104]] [[6.259]
 [6.669]
 [6.711]
 [6.699]
 [6.72 ]] [[1.324]
 [1.536]
 [1.555]
 [1.542]
 [1.549]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2844520470606702, 0.2844520470606702, 0.24911643294246916, 0.07243836235146385, 0.10777397646966491, 0.001767134115061724]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  94 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.2747438059521781, 0.2747438059521781, 0.24061418301991136, 0.10409569129084426, 0.10409569129084426, 0.0017068224940439225]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  0.8640837906734082
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.044]
 [0.035]
 [0.049]
 [0.046]] [[1.957]
 [3.738]
 [1.499]
 [2.766]
 [2.544]] [[0.048]
 [0.044]
 [0.035]
 [0.049]
 [0.046]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.059]
 [0.059]
 [0.059]
 [0.068]] [[2.114]
 [1.826]
 [1.826]
 [1.826]
 [2.73 ]] [[1.085]
 [0.795]
 [0.795]
 [0.795]
 [1.694]]
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.069]
 [0.07 ]
 [0.065]
 [0.069]] [[4.792]
 [6.19 ]
 [5.818]
 [4.792]
 [5.261]] [[0.813]
 [1.331]
 [1.194]
 [0.813]
 [0.988]]
siam score:  -0.9074704
start point for exploration sampling:  20005
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
first move QE:  0.8657388710570332
first move QE:  0.8657388710570332
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.91004646
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.377133335988846
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.24922585280822598, 0.28018554761131076, 0.21826615800514124, 0.09442737879280226, 0.125387073595887, 0.032507989186632745]
1647 5439
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]] [[2.676]
 [2.676]
 [2.676]
 [2.676]
 [2.676]] [[0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]]
siam score:  -0.90224874
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.890207
from probs:  [0.24922585280822598, 0.28018554761131076, 0.21826615800514124, 0.09442737879280226, 0.125387073595887, 0.032507989186632745]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.24922585280822598, 0.28018554761131076, 0.21826615800514124, 0.09442737879280226, 0.125387073595887, 0.032507989186632745]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.24922585280822598, 0.28018554761131076, 0.21826615800514124, 0.09442737879280226, 0.125387073595887, 0.032507989186632745]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]] [[2.861]
 [2.861]
 [2.861]
 [2.861]
 [2.601]] [[0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]]
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.038]
 [0.038]
 [0.038]
 [0.038]] [[2.814]
 [2.814]
 [2.814]
 [2.814]
 [2.028]] [[0.038]
 [0.038]
 [0.038]
 [0.038]
 [0.038]]
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.033]
 [0.033]
 [0.034]
 [0.038]] [[2.483]
 [2.483]
 [2.483]
 [2.765]
 [2.811]] [[0.033]
 [0.033]
 [0.033]
 [0.034]
 [0.038]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.042]
 [0.042]
 [0.034]
 [0.032]
 [0.039]] [[3.258]
 [3.258]
 [3.775]
 [3.164]
 [2.762]] [[0.042]
 [0.042]
 [0.034]
 [0.032]
 [0.039]]
Printing some Q and Qe and total Qs values:  [[0.042]
 [0.042]
 [0.042]
 [0.042]
 [0.042]] [[3.258]
 [3.258]
 [3.258]
 [3.258]
 [3.258]] [[0.042]
 [0.042]
 [0.042]
 [0.042]
 [0.042]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.0446892997718287
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
rdn probs:  [0.24922585280822598, 0.28018554761131076, 0.21826615800514124, 0.09442737879280226, 0.125387073595887, 0.032507989186632745]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.24922585280822598, 0.28018554761131076, 0.21826615800514124, 0.09442737879280226, 0.125387073595887, 0.032507989186632745]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.24922585280822598, 0.28018554761131076, 0.21826615800514124, 0.09442737879280226, 0.125387073595887, 0.032507989186632745]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.24922585280822598, 0.28018554761131076, 0.21826615800514124, 0.09442737879280226, 0.125387073595887, 0.032507989186632745]
UNIT TEST: sample policy line 217 mcts : [0.042 0.083 0.042 0.042 0.792]
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]] [[8.844]
 [8.844]
 [8.844]
 [8.844]
 [8.844]] [[1.638]
 [1.638]
 [1.638]
 [1.638]
 [1.638]]
actor:  1 policy actor:  1  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23469375853534, 0.2638482264790572, 0.23469375853534, 0.11807588676047141, 0.11807588676047141, 0.03061248292931997]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23469375853534, 0.2638482264790572, 0.23469375853534, 0.11807588676047141, 0.11807588676047141, 0.03061248292931997]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8697834
using explorer policy with actor:  1
1667 5452
using another actor
1667 5455
1672 5460
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.025]
 [0.038]
 [0.039]
 [0.039]] [[3.542]
 [3.055]
 [3.542]
 [3.265]
 [3.155]] [[1.599]
 [1.298]
 [1.599]
 [1.435]
 [1.369]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.025]
 [0.029]
 [0.029]
 [0.029]] [[3.756]
 [4.726]
 [4.782]
 [4.782]
 [4.2  ]] [[1.156]
 [1.67 ]
 [1.707]
 [1.707]
 [1.391]]
siam score:  -0.87307036
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23469375853534, 0.2638482264790572, 0.23469375853534, 0.11807588676047141, 0.11807588676047141, 0.03061248292931997]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.23469375853534, 0.2638482264790572, 0.23469375853534, 0.11807588676047141, 0.11807588676047141, 0.03061248292931997]
using explorer policy with actor:  1
from probs:  [0.23469375853534, 0.2638482264790572, 0.23469375853534, 0.11807588676047141, 0.11807588676047141, 0.03061248292931997]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0001 0.0 0.0001
in main func line 156:  1684
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.880213
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.077]
 [0.077]
 [0.077]
 [0.077]
 [0.076]] [[4.433]
 [4.433]
 [4.433]
 [4.433]
 [4.564]] [[1.952]
 [1.952]
 [1.952]
 [1.952]
 [2.05 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.061]
 [0.082]
 [0.078]
 [0.069]
 [0.067]] [[1.041]
 [2.786]
 [3.668]
 [3.553]
 [4.394]] [[0.048]
 [1.111]
 [1.632]
 [1.553]
 [2.051]]
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0431],
        [0.3246],
        [0.0422],
        [0.0466],
        [0.1675],
        [0.0850],
        [0.1297],
        [0.0434],
        [0.0524],
        [0.0399]], dtype=torch.float64)
0.0 0.04312105120318929
0.0 0.32460990466740874
0.0 0.04222424455466984
0.0 0.046630596929451276
0.0 0.16748680953675135
0.0 0.08501107858764075
0.0 0.12974809986397126
0.0 0.04339470321591223
0.0 0.05236991657976991
0.0 0.03991571533232968
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.21018269944246312, 0.26240193877341883, 0.26240193877341883, 0.13185384044602952, 0.10574422078055165, 0.02741536178411806]
actor:  1 policy actor:  1  step number:  94 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.056]] [[5.885]
 [5.885]
 [5.885]
 [5.885]
 [6.985]] [[1.347]
 [1.347]
 [1.347]
 [1.347]
 [1.782]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.20483454731764894, 0.28117030861961356, 0.255725054852292, 0.12849878601568437, 0.10305353224836283, 0.026717770946398246]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.20483454731764894, 0.28117030861961356, 0.255725054852292, 0.12849878601568437, 0.10305353224836283, 0.026717770946398246]
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]] [[2.679]
 [2.679]
 [2.679]
 [2.679]
 [2.679]] [[1.646]
 [1.646]
 [1.646]
 [1.646]
 [1.646]]
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.039]
 [0.042]
 [0.039]
 [0.038]] [[2.037]
 [2.9  ]
 [2.226]
 [2.9  ]
 [3.055]] [[0.954]
 [1.536]
 [1.093]
 [1.536]
 [1.638]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.20483454731764894, 0.28117030861961356, 0.255725054852292, 0.12849878601568437, 0.10305353224836283, 0.026717770946398246]
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.055]] [[6.715]
 [6.715]
 [6.715]
 [6.715]
 [8.333]] [[1.401]
 [1.401]
 [1.401]
 [1.401]
 [1.955]]
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.8049029124811158
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
1697 5474
UNIT TEST: sample policy line 217 mcts : [0.    0.    0.292 0.    0.708]
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.047]
 [0.047]
 [0.047]
 [0.044]] [[4.894]
 [4.894]
 [4.894]
 [4.894]
 [4.558]] [[1.243]
 [1.243]
 [1.243]
 [1.243]
 [1.061]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
from probs:  [0.19491521319208086, 0.2917673727077865, 0.2433412929499337, 0.146489133434228, 0.09806305367637518, 0.02542393403959593]
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.048]
 [0.046]
 [0.053]
 [0.038]] [[0.521]
 [0.695]
 [0.507]
 [1.351]
 [0.759]] [[0.255]
 [0.398]
 [0.269]
 [0.846]
 [0.422]]
using explorer policy with actor:  1
from probs:  [0.19491521319208086, 0.2917673727077865, 0.2433412929499337, 0.146489133434228, 0.09806305367637518, 0.02542393403959593]
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.034]
 [0.037]
 [0.037]
 [0.047]] [[3.115]
 [2.93 ]
 [3.115]
 [3.115]
 [3.002]] [[1.613]
 [1.483]
 [1.613]
 [1.613]
 [1.557]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.19491521319208086, 0.2917673727077865, 0.2433412929499337, 0.146489133434228, 0.09806305367637518, 0.02542393403959593]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
using explorer policy with actor:  1
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.19491521319208086, 0.2917673727077865, 0.2433412929499337, 0.146489133434228, 0.09806305367637518, 0.02542393403959593]
from probs:  [0.19491521319208086, 0.2917673727077865, 0.2433412929499337, 0.146489133434228, 0.09806305367637518, 0.02542393403959593]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.86627823
start point for exploration sampling:  20005
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.398]
 [0.386]
 [0.386]
 [0.392]] [[5.099]
 [3.919]
 [5.099]
 [5.099]
 [4.293]] [[2.145]
 [1.553]
 [2.145]
 [2.145]
 [1.739]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8638551
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1859122135125625, 0.3013854945879375, 0.25519618215778755, 0.1397229010824125, 0.0935335886522625, 0.024249620007037502]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
in main func line 156:  1719
Printing some Q and Qe and total Qs values:  [[0.257]
 [0.306]
 [0.96 ]
 [0.305]
 [0.326]] [[5.803]
 [5.522]
 [5.784]
 [5.753]
 [6.17 ]] [[1.419]
 [1.312]
 [1.947]
 [1.43 ]
 [1.659]]
siam score:  -0.84315336
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
from probs:  [0.1817155552354679, 0.2945822195014771, 0.24943555379507346, 0.1365688895290642, 0.11399555667586236, 0.02370222526305498]
Printing some Q and Qe and total Qs values:  [[0.091]
 [0.091]
 [0.091]
 [0.091]
 [0.11 ]] [[3.331]
 [3.331]
 [3.331]
 [3.331]
 [3.701]] [[1.264]
 [1.264]
 [1.264]
 [1.264]
 [1.534]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1817155552354679, 0.2945822195014771, 0.24943555379507346, 0.1365688895290642, 0.11399555667586236, 0.02370222526305498]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.118]
 [0.142]
 [0.102]
 [0.118]
 [0.133]] [[4.158]
 [5.078]
 [4.779]
 [4.158]
 [4.58 ]] [[1.259]
 [1.878]
 [1.616]
 [1.259]
 [1.551]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1817155552354679, 0.2945822195014771, 0.24943555379507346, 0.1365688895290642, 0.11399555667586236, 0.02370222526305498]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8456428
siam score:  -0.84659225
from probs:  [0.17770417963907, 0.3101543353079099, 0.24392925747349, 0.13355412774945669, 0.11147910180465002, 0.023178998025423392]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17770417963907, 0.3101543353079099, 0.24392925747349, 0.13355412774945669, 0.11147910180465002, 0.023178998025423392]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17770417963907, 0.3101543353079099, 0.24392925747349, 0.13355412774945669, 0.11147910180465002, 0.023178998025423392]
siam score:  -0.85365564
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17770417963907, 0.3101543353079099, 0.24392925747349, 0.13355412774945669, 0.11147910180465002, 0.023178998025423392]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.17386608138164825, 0.30345554625131665, 0.23866081381648246, 0.1522678372367035, 0.10907134894681404, 0.022678372367035106]
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.144]
 [0.119]
 [0.119]
 [0.158]] [[4.297]
 [6.21 ]
 [4.297]
 [4.297]
 [4.378]] [[1.304]
 [1.995]
 [1.304]
 [1.304]
 [1.35 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1701902703710795, 0.29704000372994205, 0.23361513705051076, 0.1701902703710795, 0.10676540369164823, 0.02219891478573987]
Printing some Q and Qe and total Qs values:  [[0.159]
 [0.104]
 [0.146]
 [0.121]
 [0.166]] [[4.227]
 [3.921]
 [4.074]
 [4.229]
 [3.906]] [[2.067]
 [1.758]
 [1.93 ]
 [2.023]
 [1.82 ]]
siam score:  -0.87474334
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 4.57207514604346
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.1701902703710795, 0.29704000372994205, 0.23361513705051076, 0.1701902703710795, 0.10676540369164823, 0.02219891478573987]
Printing some Q and Qe and total Qs values:  [[0.077]
 [0.063]
 [0.068]
 [0.104]
 [0.084]] [[0.444]
 [0.876]
 [0.574]
 [0.77 ]
 [0.433]] [[0.112]
 [0.228]
 [0.136]
 [0.274]
 [0.121]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.146]
 [0.141]
 [0.146]
 [0.137]
 [0.167]] [[5.77 ]
 [4.381]
 [5.77 ]
 [5.169]
 [5.666]] [[1.456]
 [0.69 ]
 [1.456]
 [1.12 ]
 [1.413]]
line 256 mcts: sample exp_bonus 5.355073332166027
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.3115940228375592, 0.22877839073990633, 0.16666666666666666, 0.10455494259342701, 0.021739310495774128]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.077]
 [0.083]
 [0.077]
 [0.083]
 [0.084]] [[2.152]
 [2.764]
 [2.152]
 [2.239]
 [2.225]] [[1.262]
 [1.683]
 [1.262]
 [1.334]
 [1.326]]
Printing some Q and Qe and total Qs values:  [[0.093]
 [0.093]
 [0.093]
 [0.093]
 [0.093]] [[4.91]
 [4.91]
 [4.91]
 [4.91]
 [4.91]] [[2.045]
 [2.045]
 [2.045]
 [2.045]
 [2.045]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20005
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.348]
 [0.301]
 [0.301]
 [0.31 ]] [[8.376]
 [7.833]
 [8.376]
 [8.376]
 [7.578]] [[2.341]
 [2.167]
 [2.341]
 [2.341]
 [2.007]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16328600817180405, 0.305273664956034, 0.22413786107933117, 0.1835699591409798, 0.10243415526427696, 0.02129835138757416]
from probs:  [0.16328600817180405, 0.305273664956034, 0.22413786107933117, 0.1835699591409798, 0.10243415526427696, 0.02129835138757416]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  86 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16003976933744513, 0.2992046132510975, 0.23956253728810362, 0.17992046132510978, 0.10039769337445126, 0.020874925423792746]
siam score:  -0.8816934
actor:  0 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.147]] [[5.554]
 [5.554]
 [5.554]
 [5.554]
 [5.774]] [[1.393]
 [1.393]
 [1.393]
 [1.393]
 [1.488]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
Printing some Q and Qe and total Qs values:  [[0.064]
 [0.058]
 [0.064]
 [0.064]
 [0.06 ]] [[2.912]
 [4.148]
 [2.912]
 [2.912]
 [2.514]] [[1.334]
 [2.05 ]
 [1.334]
 [1.334]
 [1.092]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.16003992795360125, 0.2992014409279752, 0.23956079251038637, 0.17992014409279755, 0.10039927953601244, 0.020878414979227333]
using explorer policy with actor:  0
using another actor
from probs:  [0.16003992795360125, 0.2992014409279752, 0.23956079251038637, 0.17992014409279755, 0.10039927953601244, 0.020878414979227333]
Printing some Q and Qe and total Qs values:  [[0.057]
 [0.057]
 [0.06 ]
 [0.057]
 [0.051]] [[2.903]
 [2.903]
 [4.122]
 [2.903]
 [3.503]] [[1.282]
 [1.282]
 [2.02 ]
 [1.282]
 [1.64 ]]
start point for exploration sampling:  20005
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.052]
 [0.052]
 [0.052]
 [0.052]
 [0.06 ]] [[5.686]
 [5.686]
 [5.686]
 [5.686]
 [6.008]] [[1.067]
 [1.067]
 [1.067]
 [1.067]
 [1.19 ]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
Printing some Q and Qe and total Qs values:  [[0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.193]] [[3.755]
 [3.755]
 [3.755]
 [3.755]
 [3.689]] [[0.914]
 [0.914]
 [0.914]
 [0.914]
 [0.825]]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.061]
 [0.061]
 [0.064]
 [0.062]
 [0.065]] [[5.679]
 [6.376]
 [5.553]
 [6.073]
 [5.486]] [[1.251]
 [1.561]
 [1.196]
 [1.427]
 [1.167]]
siam score:  -0.86010635
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.17641301522044772, 0.29336919786582005, 0.23489110654313386, 0.17641301522044772, 0.0984422267901995, 0.020471438359951268]
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.242]
 [0.209]
 [0.19 ]
 [0.201]] [[4.93 ]
 [4.339]
 [4.93 ]
 [4.38 ]
 [4.604]] [[1.983]
 [1.58 ]
 [1.983]
 [1.565]
 [1.738]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
1761 5556
maxi score, test score, baseline:  0.0021 0.0 0.0021
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.341]
 [0.448]
 [0.341]
 [0.299]] [[4.441]
 [4.441]
 [4.711]
 [4.441]
 [6.018]] [[1.265]
 [1.265]
 [1.5  ]
 [1.265]
 [1.989]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  86 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  20005
1765 5559
using another actor
from probs:  [0.18173564110516355, 0.27214948773614467, 0.27214948773614467, 0.1636528717789673, 0.0913217944741824, 0.018990717169397493]
UNIT TEST: sample policy line 217 mcts : [0.    0.042 0.167 0.375 0.417]
Printing some Q and Qe and total Qs values:  [[0.062]
 [0.056]
 [0.053]
 [0.053]
 [0.057]] [[1.593]
 [2.228]
 [1.874]
 [1.874]
 [1.909]] [[0.537]
 [1.412]
 [0.913]
 [0.913]
 [0.969]]
using explorer policy with actor:  1
siam score:  -0.8583414
UNIT TEST: sample policy line 217 mcts : [0.    0.167 0.083 0.042 0.708]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.18173564110516355, 0.27214948773614467, 0.27214948773614467, 0.1636528717789673, 0.0913217944741824, 0.018990717169397493]
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]] [[3.67]
 [3.67]
 [3.67]
 [3.67]
 [3.67]] [[1.893]
 [1.893]
 [1.893]
 [1.893]
 [1.893]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
1773 5562
siam score:  -0.8660501
siam score:  -0.8668625
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.19284409969013053, 0.2626505877527008, 0.2801022097683433, 0.15794085565884539, 0.08813436759627515, 0.018327879533704895]
maxi score, test score, baseline:  0.0021 0.0 0.0021
siam score:  -0.86902815
from probs:  [0.19284409969013053, 0.2626505877527008, 0.2801022097683433, 0.15794085565884539, 0.08813436759627515, 0.018327879533704895]
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.036]] [[4.061]
 [4.061]
 [4.061]
 [4.061]
 [3.677]] [[0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.036]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.19284409969013053, 0.2626505877527008, 0.2801022097683433, 0.15794085565884539, 0.08813436759627515, 0.018327879533704895]
using another actor
UNIT TEST: sample policy line 217 mcts : [0.083 0.042 0.375 0.125 0.375]
Printing some Q and Qe and total Qs values:  [[0.133]
 [0.385]
 [0.441]
 [0.311]
 [0.444]] [[3.828]
 [4.598]
 [3.861]
 [4.233]
 [4.48 ]] [[0.926]
 [1.665]
 [1.203]
 [1.353]
 [1.633]]
siam score:  -0.8763829
Printing some Q and Qe and total Qs values:  [[0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.695]] [[4.18 ]
 [4.18 ]
 [4.18 ]
 [4.18 ]
 [4.019]] [[1.807]
 [1.807]
 [1.807]
 [1.807]
 [1.681]]
siam score:  -0.8761454
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.18953638238650888, 0.27529781633591716, 0.27529781633591716, 0.15523180880674559, 0.08662266164721899, 0.018013514487692388]
siam score:  -0.8725371
1791 5575
maxi score, test score, baseline:  0.0021 0.0 0.0021
siam score:  -0.8618305
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.18953638238650888, 0.27529781633591716, 0.27529781633591716, 0.15523180880674559, 0.08662266164721899, 0.018013514487692388]
using explorer policy with actor:  1
using explorer policy with actor:  1
in main func line 156:  1796
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.211511841420286
siam score:  -0.8558908
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]] [[4.685]
 [4.685]
 [4.685]
 [4.685]
 [4.685]] [[2.186]
 [2.186]
 [2.186]
 [2.186]
 [2.186]]
first move QE:  0.8964975248210201
siam score:  -0.8530843
first move QE:  0.8965256762001746
maxi score, test score, baseline:  0.0021 0.0 0.0021
using another actor
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.1863402214674098, 0.27065545632773746, 0.28751850329980305, 0.15261412752327871, 0.08516193963501659, 0.017709751746754446]
Printing some Q and Qe and total Qs values:  [[0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.078]] [[3.933]
 [3.933]
 [3.933]
 [3.933]
 [3.933]] [[1.819]
 [1.819]
 [1.819]
 [1.819]
 [1.819]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
start point for exploration sampling:  20005
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.1863402214674098, 0.27065545632773746, 0.28751850329980305, 0.15261412752327871, 0.08516193963501659, 0.017709751746754446]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.18325006698028706, 0.28275046886200944, 0.28275046886200944, 0.15008326635304628, 0.08374966509856473, 0.01741606384408318]
actor:  1 policy actor:  1  step number:  92 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.202]
 [0.202]
 [0.208]
 [0.202]
 [0.207]] [[2.222]
 [2.222]
 [2.339]
 [2.222]
 [2.382]] [[1.411]
 [1.411]
 [1.502]
 [1.411]
 [1.528]]
1804 5588
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.1802607311153749, 0.2781379951460743, 0.2944508724845241, 0.14763497643847512, 0.08238346708467559, 0.017131957730876037]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.684]
 [0.708]
 [0.765]
 [0.698]
 [0.715]] [[4.827]
 [4.585]
 [4.293]
 [4.86 ]
 [4.701]] [[2.049]
 [1.956]
 [1.891]
 [2.09 ]
 [2.03 ]]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.6118451051308336
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
Printing some Q and Qe and total Qs values:  [[0.081]
 [0.112]
 [0.076]
 [0.085]
 [0.087]] [[0.923]
 [1.752]
 [0.803]
 [1.399]
 [0.86 ]] [[0.081]
 [0.112]
 [0.076]
 [0.085]
 [0.087]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.17736735914184915, 0.28972463013126504, 0.28972463013126504, 0.14526528171630174, 0.08106112686520693, 0.016856972014112143]
using another actor
from probs:  [0.17736735914184915, 0.28972463013126504, 0.28972463013126504, 0.14526528171630174, 0.08106112686520693, 0.016856972014112143]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]] [[4.801]
 [4.801]
 [4.801]
 [4.801]
 [4.801]] [[1.966]
 [1.966]
 [1.966]
 [1.966]
 [1.966]]
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.353]] [[4.976]
 [4.976]
 [4.976]
 [4.976]
 [4.657]] [[2.097]
 [2.097]
 [2.097]
 [2.097]
 [1.848]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.19036287596306473, 0.28514771314865706, 0.28514771314865706, 0.1429704573702686, 0.07978056591320706, 0.016590674456145527]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.19036287596306473, 0.28514771314865706, 0.28514771314865706, 0.1429704573702686, 0.07978056591320706, 0.016590674456145527]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.19984621840284028, 0.2764144147170871, 0.2764144147170871, 0.13859166135144282, 0.09265074356289475, 0.016082547248647962]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.19984621840284028, 0.2764144147170871, 0.2764144147170871, 0.13859166135144282, 0.09265074356289475, 0.016082547248647962]
maxi score, test score, baseline:  0.0021 0.0 0.0021
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.545]
 [0.551]
 [0.49 ]
 [0.68 ]] [[3.429]
 [3.412]
 [3.284]
 [3.555]
 [3.815]] [[1.486]
 [1.561]
 [1.465]
 [1.61 ]
 [2.045]]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.19683200409671944, 0.2722453476718514, 0.2722453476718514, 0.1365013292366139, 0.09125332309153475, 0.030922648231429234]
1820 5606
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.2087659255181694, 0.2682001733085262, 0.2682001733085262, 0.1344731157802234, 0.0898974299374558, 0.030463182147099026]
from probs:  [0.2087659255181694, 0.2682001733085262, 0.2682001733085262, 0.1344731157802234, 0.0898974299374558, 0.030463182147099026]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.2087659255181694, 0.2682001733085262, 0.2682001733085262, 0.1344731157802234, 0.0898974299374558, 0.030463182147099026]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.2203503974352904, 0.26427344988234625, 0.26427344988234625, 0.13250429254117882, 0.088581240094123, 0.030017170164715282]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.559]] [[3.121]
 [3.121]
 [3.121]
 [3.121]
 [2.977]] [[1.831]
 [1.831]
 [1.831]
 [1.831]
 [1.712]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.46 ]
 [0.544]
 [0.544]
 [0.463]] [[3.103]
 [3.69 ]
 [3.103]
 [3.103]
 [3.438]] [[1.691]
 [1.912]
 [1.691]
 [1.691]
 [1.751]]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Starting evaluation
siam score:  -0.88507754
Printing some Q and Qe and total Qs values:  [[0.127]
 [0.129]
 [0.165]
 [0.126]
 [0.118]] [[0.482]
 [0.279]
 [1.274]
 [1.115]
 [0.507]] [[0.127]
 [0.129]
 [0.165]
 [0.126]
 [0.118]]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.8230890953975416
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
actor:  0 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  0
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.22830077214640856, 0.2709705374785375, 0.2567472823678278, 0.12873798637144088, 0.08606822103931193, 0.029175200596473293]
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]] [[3.781]
 [3.781]
 [3.781]
 [3.781]
 [3.763]] [[0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.129]
 [0.029]
 [0.165]
 [0.1  ]
 [0.116]] [[0.843]
 [0.877]
 [1.419]
 [0.879]
 [0.971]] [[0.129]
 [0.029]
 [0.165]
 [0.1  ]
 [0.116]]
actor:  0 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.543]
 [0.512]
 [0.512]
 [0.538]] [[3.19 ]
 [3.752]
 [3.19 ]
 [3.19 ]
 [3.586]] [[0.512]
 [0.543]
 [0.512]
 [0.512]
 [0.538]]
actor:  0 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  92 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  96 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  98 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn probs:  [0.22508861429130347, 0.2671524165810419, 0.25313114915112905, 0.12693974228191365, 0.08487593999217521, 0.04281213770243675]
maxi score, test score, baseline:  0.0361 0.64 0.64
probs:  [0.2234992001347142, 0.26441862423170837, 0.250778816199377, 0.12802054390839437, 0.08710111981140019, 0.046181695714406]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
first move QE:  0.8974743045329544
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0361 0.64 0.64
Printing some Q and Qe and total Qs values:  [[0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.234]] [[1.594]
 [1.594]
 [1.594]
 [1.594]
 [2.781]] [[0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.234]]
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.317]] [[2.66 ]
 [2.66 ]
 [2.66 ]
 [2.66 ]
 [2.442]] [[2.156]
 [2.156]
 [2.156]
 [2.156]
 [1.921]]
Printing some Q and Qe and total Qs values:  [[1.148]
 [0.788]
 [0.738]
 [1.148]
 [0.68 ]] [[2.936]
 [3.525]
 [2.522]
 [2.936]
 [3.474]] [[2.338]
 [2.253]
 [1.533]
 [2.338]
 [2.078]]
maxi score, test score, baseline:  0.0361 0.64 0.64
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
1831 5617
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  0.0361 0.64 0.64
probs:  [0.21647067865652142, 0.2662746906463761, 0.25382368764891244, 0.1293136576742756, 0.09196064868188455, 0.04215663669202982]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0361 0.64 0.64
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.89771765
maxi score, test score, baseline:  0.0361 0.64 0.64
first move QE:  0.8980616231263953
siam score:  -0.8982551
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0361 0.64 0.64
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0361 0.64 0.64
maxi score, test score, baseline:  0.0361 0.64 0.64
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.23617964671386688, 0.27093613673746697, 0.23617964671386688, 0.1319101766430666, 0.08556818994493315, 0.03922620324679969]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
first move QE:  0.8989671766074636
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.731]
 [0.731]
 [0.766]
 [0.731]
 [0.745]] [[3.512]
 [3.512]
 [4.508]
 [3.512]
 [3.343]] [[1.78 ]
 [1.78 ]
 [2.408]
 [1.78 ]
 [1.707]]
maxi score, test score, baseline:  0.0361 0.64 0.64
maxi score, test score, baseline:  0.0361 0.64 0.64
probs:  [0.23944294699011678, 0.2730319994470938, 0.23944294699011678, 0.1274794388001935, 0.08269403552422422, 0.037908632248254893]
maxi score, test score, baseline:  0.0361 0.64 0.64
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  0.8984255465919063
maxi score, test score, baseline:  0.0361 0.64 0.64
probs:  [0.2506118938942912, 0.2613283058807965, 0.23989548190778595, 0.12201495005622809, 0.08986571409671232, 0.036283654164186024]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0361 0.64 0.64
probs:  [0.2506118938942912, 0.2613283058807965, 0.23989548190778595, 0.12201495005622809, 0.08986571409671232, 0.036283654164186024]
using another actor
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.3  ]
 [0.346]
 [0.284]
 [0.294]] [[2.25 ]
 [3.066]
 [3.426]
 [2.583]
 [3.105]] [[0.255]
 [0.3  ]
 [0.346]
 [0.284]
 [0.294]]
actor:  0 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.268]] [[2.998]
 [2.998]
 [2.998]
 [2.998]
 [2.998]] [[0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.268]]
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]] [[4.934]
 [4.934]
 [4.934]
 [4.934]
 [4.934]] [[2.13]
 [2.13]
 [2.13]
 [2.13]
 [2.13]]
actor:  1 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.24795470907781925, 0.26916028535898945, 0.23735192093723412, 0.12072125139079781, 0.08891288696904248, 0.03589894626611689]
line 256 mcts: sample exp_bonus 3.6203332582907817
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]] [[3.321]
 [3.321]
 [3.321]
 [3.321]
 [3.321]] [[2.235]
 [2.235]
 [2.235]
 [2.235]
 [2.235]]
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0381 0.64 0.64
maxi score, test score, baseline:  0.0381 0.64 0.64
Printing some Q and Qe and total Qs values:  [[0.52]
 [0.52]
 [0.52]
 [0.52]
 [0.52]] [[4.125]
 [4.125]
 [4.125]
 [4.125]
 [4.125]] [[2.131]
 [2.131]
 [2.131]
 [2.131]
 [2.131]]
maxi score, test score, baseline:  0.0381 0.64 0.64
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
from probs:  [0.24280587066589757, 0.26357110812023327, 0.2324232519387297, 0.12859706466705118, 0.0870665897583798, 0.045536114849708396]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  20005
siam score:  -0.90295947
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
probs:  [0.24031081509673324, 0.2608626704725658, 0.24031081509673324, 0.12727561052965428, 0.08617189977798921, 0.04506818902632414]
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
siam score:  -0.90391296
1867 5643
actor:  1 policy actor:  1  step number:  95 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Printing some Q and Qe and total Qs values:  [[0.919]
 [0.939]
 [1.081]
 [0.919]
 [0.905]] [[5.003]
 [4.476]
 [4.49 ]
 [5.003]
 [4.616]] [[2.206]
 [1.878]
 [1.979]
 [2.206]
 [1.947]]
Printing some Q and Qe and total Qs values:  [[0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]] [[4.14]
 [4.14]
 [4.14]
 [4.14]
 [4.14]] [[2.089]
 [2.089]
 [2.089]
 [2.089]
 [2.089]]
Printing some Q and Qe and total Qs values:  [[0.181]
 [0.248]
 [0.248]
 [0.248]
 [0.41 ]] [[0.775]
 [1.69 ]
 [1.69 ]
 [1.69 ]
 [2.888]] [[0.338]
 [0.928]
 [0.928]
 [0.928]
 [1.744]]
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.65 ]
 [0.582]
 [0.582]
 [0.582]] [[3.953]
 [5.119]
 [3.953]
 [3.953]
 [3.953]] [[0.985]
 [1.646]
 [0.985]
 [0.985]
 [0.985]]
line 256 mcts: sample exp_bonus 5.409224041465972
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
Printing some Q and Qe and total Qs values:  [[0.906]
 [0.906]
 [0.906]
 [0.906]
 [0.889]] [[3.845]
 [3.845]
 [3.845]
 [3.845]
 [3.804]] [[2.165]
 [2.165]
 [2.165]
 [2.165]
 [2.113]]
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
probs:  [0.23312411543904993, 0.25306135007076486, 0.2430927327549074, 0.13343794228047504, 0.09356347301704511, 0.043720386437757684]
Printing some Q and Qe and total Qs values:  [[0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.84 ]] [[3.414]
 [3.414]
 [3.414]
 [3.414]
 [4.208]] [[1.422]
 [1.422]
 [1.422]
 [1.422]
 [2.086]]
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]] [[0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.806]] [[0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.836]]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.0955999692966056
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
probs:  [0.24335759512397181, 0.24335759512397181, 0.24335759512397181, 0.1379075684951772, 0.0899757382093615, 0.04204390792354578]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 3.5310279571828196
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.567521194956434
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
probs:  [0.25054217220561514, 0.2410468319559229, 0.2410468319559229, 0.1365980892093078, 0.0891213879608464, 0.04164468671238499]
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
probs:  [0.25054217220561514, 0.2410468319559229, 0.2410468319559229, 0.1365980892093078, 0.0891213879608464, 0.04164468671238499]
first move QE:  0.898116821338506
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
1890 5655
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using another actor
actor:  1 policy actor:  1  step number:  97 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
probs:  [0.2551912568306011, 0.24587287891860798, 0.23655450100661493, 0.13405234397469082, 0.08746045441472534, 0.04086856485475986]
using explorer policy with actor:  1
siam score:  -0.90805507
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
probs:  [0.2551912568306011, 0.24587287891860796, 0.23655450100661488, 0.13405234397469085, 0.08746045441472533, 0.040868564854759856]
siam score:  -0.9094383
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
probs:  [0.2551912568306011, 0.24587287891860796, 0.23655450100661488, 0.13405234397469085, 0.08746045441472533, 0.040868564854759856]
1898 5659
from probs:  [0.2551912568306011, 0.24587287891860796, 0.23655450100661488, 0.13405234397469085, 0.08746045441472533, 0.040868564854759856]
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.54 ]
 [0.548]
 [0.548]
 [0.522]] [[5.614]
 [5.704]
 [5.878]
 [5.878]
 [5.06 ]] [[1.876]
 [1.892]
 [1.989]
 [1.989]
 [1.539]]
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
siam score:  -0.9110581
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
probs:  [0.25283524249159406, 0.24360289508178035, 0.23437054767196672, 0.14204707357383028, 0.08665298911494843, 0.040491252065880216]
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.261]
 [0.376]
 [0.261]
 [0.297]] [[3.838]
 [3.838]
 [3.897]
 [3.838]
 [3.855]] [[1.705]
 [1.705]
 [1.849]
 [1.705]
 [1.749]]
Printing some Q and Qe and total Qs values:  [[0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.892]] [[3.921]
 [3.921]
 [3.921]
 [3.921]
 [5.019]] [[1.451]
 [1.451]
 [1.451]
 [1.451]
 [2.045]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.90755767
siam score:  -0.9071926
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
probs:  [0.2505223332768649, 0.25052233327686485, 0.23222655147100343, 0.14074764244169632, 0.08586029702411203, 0.04012084250945848]
from probs:  [0.2505223332768649, 0.25052233327686485, 0.23222655147100343, 0.14074764244169632, 0.08586029702411203, 0.04012084250945848]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.208]
 [0.208]
 [0.208]
 [0.215]
 [0.205]] [[2.213]
 [2.213]
 [2.213]
 [2.774]
 [2.76 ]] [[1.143]
 [1.143]
 [1.143]
 [1.527]
 [1.51 ]]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
probs:  [0.24602118338601453, 0.2460211833860145, 0.23703765319137138, 0.13821882105029668, 0.08431763988243775, 0.04838351910386514]
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.56 ]
 [0.544]
 [0.539]
 [0.541]] [[4.142]
 [4.08 ]
 [3.856]
 [3.7  ]
 [3.878]] [[1.944]
 [1.942]
 [1.758]
 [1.639]
 [1.769]]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
using another actor
from probs:  [0.24602118338601453, 0.2460211833860145, 0.23703765319137138, 0.13821882105029668, 0.08431763988243775, 0.04838351910386514]
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.809]
 [0.878]
 [0.845]
 [0.845]] [[3.302]
 [3.617]
 [3.84 ]
 [3.484]
 [3.461]] [[1.6  ]
 [1.887]
 [2.109]
 [1.832]
 [1.817]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
UNIT TEST: sample policy line 217 mcts : [0.333 0.    0.    0.    0.667]
actor:  1 policy actor:  1  step number:  91 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
probs:  [0.24615919918633905, 0.24615919918633902, 0.2288153739093196, 0.14209624752422248, 0.09006477169316417, 0.046705208500615596]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
Printing some Q and Qe and total Qs values:  [[0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.881]] [[2.991]
 [2.991]
 [2.991]
 [2.991]
 [2.991]] [[2.184]
 [2.184]
 [2.184]
 [2.184]
 [2.184]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.558]
 [0.647]
 [0.579]
 [0.577]] [[4.862]
 [3.499]
 [2.123]
 [1.97 ]
 [3.54 ]] [[1.944]
 [1.199]
 [0.517]
 [0.378]
 [1.237]]
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.554]
 [0.659]
 [0.56 ]
 [0.568]] [[4.72 ]
 [4.018]
 [2.738]
 [1.626]
 [3.427]] [[1.879]
 [1.515]
 [0.94 ]
 [0.294]
 [1.222]]
Printing some Q and Qe and total Qs values:  [[1.01]
 [1.01]
 [1.01]
 [1.01]
 [1.01]] [[3.86]
 [3.86]
 [3.86]
 [3.86]
 [3.86]] [[2.517]
 [2.517]
 [2.517]
 [2.517]
 [2.517]]
siam score:  -0.900693
first move QE:  0.902354368640156
1925 5679
siam score:  -0.9013331
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
probs:  [0.2526402377540731, 0.24404288064533247, 0.22684816642785116, 0.14087459534044475, 0.08929045268800084, 0.04630366714429762]
siam score:  -0.9010399
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
maxi score, test score, baseline:  0.040100000000000004 0.64 0.64
from probs:  [0.2526402377540731, 0.24404288064533247, 0.22684816642785116, 0.14087459534044475, 0.08929045268800084, 0.04630366714429762]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.042100000000000005 0.64 0.64
probs:  [0.2526402377540731, 0.24404288064533247, 0.22684816642785116, 0.14087459534044475, 0.08929045268800084, 0.04630366714429762]
from probs:  [0.2526402377540731, 0.24404288064533247, 0.22684816642785116, 0.14087459534044475, 0.08929045268800084, 0.04630366714429762]
siam score:  -0.9017694
siam score:  -0.9005722
maxi score, test score, baseline:  0.042100000000000005 0.64 0.64
probs:  [0.2526402377540731, 0.24404288064533247, 0.22684816642785116, 0.14087459534044475, 0.08929045268800084, 0.04630366714429762]
using explorer policy with actor:  1
using another actor
from probs:  [0.2526402377540731, 0.24404288064533247, 0.22684816642785116, 0.14087459534044475, 0.08929045268800084, 0.04630366714429762]
maxi score, test score, baseline:  0.042100000000000005 0.64 0.64
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.042100000000000005 0.64 0.64
actor:  0 policy actor:  0  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0441 0.64 0.64
probs:  [0.259010786635096, 0.2419626414101552, 0.2249144961852144, 0.1396737700605104, 0.08852933438568797, 0.04590897132333596]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.8983287
maxi score, test score, baseline:  0.0441 0.64 0.64
probs:  [0.2568216204935566, 0.23991756665101474, 0.23146553972974382, 0.13849324359576357, 0.08778108206813794, 0.04552094746178327]
maxi score, test score, baseline:  0.0441 0.64 0.64
probs:  [0.2568216204935566, 0.23991756665101474, 0.23146553972974382, 0.13849324359576357, 0.08778108206813794, 0.04552094746178327]
maxi score, test score, baseline:  0.0441 0.64 0.64
probs:  [0.2568216204935566, 0.23991756665101474, 0.23146553972974382, 0.13849324359576357, 0.08778108206813794, 0.04552094746178327]
maxi score, test score, baseline:  0.0441 0.64 0.64
probs:  [0.2568216204935566, 0.23991756665101474, 0.23146553972974382, 0.13849324359576357, 0.08778108206813794, 0.04552094746178327]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 4.866250149690242
actor:  0 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0461 0.64 0.64
maxi score, test score, baseline:  0.0461 0.64 0.64
probs:  [0.2630503388690568, 0.23790677220756373, 0.22952558332039938, 0.1373325055615914, 0.08704537223860519, 0.04513942780278338]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.0461 0.64 0.64
probs:  [0.2630503388690568, 0.23790677220756373, 0.22952558332039938, 0.1373325055615914, 0.08704537223860519, 0.04513942780278338]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0461 0.64 0.64
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.752]
 [0.631]
 [0.646]
 [0.699]] [[2.577]
 [4.211]
 [2.903]
 [4.38 ]
 [3.752]] [[1.019]
 [2.193]
 [1.187]
 [2.166]
 [1.826]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.838]
 [0.838]
 [0.838]
 [0.838]
 [0.838]] [[4.009]
 [4.009]
 [4.009]
 [4.009]
 [4.009]] [[1.915]
 [1.915]
 [1.915]
 [1.915]
 [1.915]]
maxi score, test score, baseline:  0.0461 0.64 0.64
probs:  [0.2587136823894571, 0.24222764972268862, 0.23398463338930445, 0.13506843738869387, 0.08561033938838854, 0.04439525772146746]
from probs:  [0.2587136823894571, 0.24222764972268862, 0.23398463338930445, 0.13506843738869387, 0.08561033938838854, 0.04439525772146746]
maxi score, test score, baseline:  0.0461 0.64 0.64
probs:  [0.2587136823894571, 0.24222764972268862, 0.23398463338930445, 0.13506843738869387, 0.08561033938838854, 0.04439525772146746]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  0.9048664345334083
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8983032
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0461 0.64 0.64
from probs:  [0.2584355450470419, 0.258435545047042, 0.22651593517560714, 0.1307571055613024, 0.08287769075415002, 0.042978178414856405]
Printing some Q and Qe and total Qs values:  [[0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]] [[3.48]
 [3.48]
 [3.48]
 [3.48]
 [3.48]] [[8.672]
 [8.672]
 [8.672]
 [8.672]
 [8.672]]
1967 5700
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.876]
 [0.97 ]
 [0.82 ]
 [0.816]] [[2.809]
 [2.526]
 [1.097]
 [2.453]
 [2.229]] [[1.811]
 [1.818]
 [0.953]
 [1.688]
 [1.527]]
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.632]
 [0.632]
 [0.632]
 [1.256]] [[2.315]
 [2.315]
 [2.315]
 [2.315]
 [2.299]] [[1.807]
 [1.807]
 [1.807]
 [1.807]
 [2.535]]
actor:  1 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  96 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0461 0.64 0.64
probs:  [0.2601866551209891, 0.2523933227497955, 0.22901332563621496, 0.13549333718189255, 0.08094001058353778, 0.041973348727570116]
siam score:  -0.89663106
maxi score, test score, baseline:  0.0461 0.64 0.64
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.9005762
1973 5706
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.549]
 [0.593]
 [0.482]
 [0.426]] [[5.896]
 [5.244]
 [4.228]
 [5.896]
 [5.131]] [[2.069]
 [1.769]
 [1.262]
 [2.069]
 [1.632]]
maxi score, test score, baseline:  0.0461 0.64 0.64
probs:  [0.25817461454007345, 0.25044154852260253, 0.22724235047018954, 0.1421786242780085, 0.08031409613824048, 0.04164876605088549]
maxi score, test score, baseline:  0.0461 0.64 0.64
maxi score, test score, baseline:  0.0461 0.64 0.64
probs:  [0.25817461454007345, 0.25044154852260253, 0.22724235047018954, 0.1421786242780085, 0.08031409613824048, 0.04164876605088549]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0461 0.64 0.64
probs:  [0.26386717824830647, 0.24851972905120553, 0.22549855525555398, 0.1410875846714983, 0.07969778788309412, 0.041329164890341524]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
first move QE:  0.9042668154303797
maxi score, test score, baseline:  0.0461 0.64 0.64
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.813]
 [0.954]
 [0.954]
 [0.954]
 [0.885]] [[4.57 ]
 [3.346]
 [3.346]
 [3.346]
 [3.795]] [[2.347]
 [1.716]
 [1.716]
 [1.716]
 [1.925]]
Printing some Q and Qe and total Qs values:  [[0.853]
 [0.862]
 [1.074]
 [0.853]
 [0.841]] [[4.221]
 [3.68 ]
 [3.188]
 [4.221]
 [3.488]] [[2.527]
 [2.217]
 [2.306]
 [2.527]
 [2.064]]
start point for exploration sampling:  20005
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.296]
 [0.296]
 [0.296]
 [0.315]] [[0.156]
 [0.415]
 [0.415]
 [0.415]
 [0.447]] [[0.402]
 [0.783]
 [0.783]
 [0.783]
 [0.866]]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.048100000000000004 0.64 0.64
probs:  [0.26743643573594583, 0.24476323769535804, 0.22964777233496617, 0.13895498017261487, 0.07849311873104733, 0.04070445533006764]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Printing some Q and Qe and total Qs values:  [[0.876]
 [0.876]
 [0.876]
 [0.876]
 [0.843]] [[4.4  ]
 [4.4  ]
 [4.4  ]
 [4.4  ]
 [4.133]] [[2.249]
 [2.249]
 [2.249]
 [2.249]
 [2.036]]
siam score:  -0.8993421
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.048100000000000004 0.64 0.64
probs:  [0.2708993979502734, 0.24111861758352862, 0.22622822740015622, 0.13688588629992188, 0.08476952065811845, 0.04009835010800128]
siam score:  -0.89798707
1992 5715
Printing some Q and Qe and total Qs values:  [[0.57 ]
 [0.57 ]
 [0.746]
 [0.57 ]
 [0.562]] [[3.492]
 [3.492]
 [4.445]
 [3.492]
 [3.567]] [[1.399]
 [1.399]
 [1.904]
 [1.399]
 [1.422]]
Printing some Q and Qe and total Qs values:  [[0.883]
 [0.945]
 [0.883]
 [0.883]
 [0.861]] [[4.214]
 [3.526]
 [4.214]
 [4.214]
 [3.873]] [[2.524]
 [2.121]
 [2.524]
 [2.524]
 [2.249]]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.733]
 [0.801]
 [0.792]
 [0.733]
 [0.767]] [[4.307]
 [3.811]
 [4.552]
 [4.307]
 [3.61 ]] [[1.648]
 [1.469]
 [1.827]
 [1.648]
 [1.337]]
Starting evaluation
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.048100000000000004 0.64 0.64
siam score:  -0.8903901
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.433]
 [0.366]
 [0.366]
 [0.344]] [[2.885]
 [3.228]
 [2.885]
 [2.885]
 [3.497]] [[0.366]
 [0.433]
 [0.366]
 [0.366]
 [0.344]]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.0541 0.64 0.64
probs:  [0.2688974043154966, 0.23933670909173854, 0.231946535285799, 0.13587427580858538, 0.08414305916700879, 0.039802016331371745]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0821 0.64 0.64
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.2688974043154966, 0.23933670909173854, 0.231946535285799, 0.13587427580858538, 0.08414305916700879, 0.039802016331371745]
siam score:  -0.8876856
maxi score, test score, baseline:  0.0981 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
Printing some Q and Qe and total Qs values:  [[0.737]
 [0.776]
 [0.79 ]
 [0.762]
 [0.749]] [[3.215]
 [3.93 ]
 [3.893]
 [3.848]
 [3.577]] [[1.699]
 [2.072]
 [2.064]
 [2.023]
 [1.883]]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0981 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0981 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0981 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0981 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.745]
 [0.766]
 [0.745]
 [0.745]
 [0.745]] [[4.768]
 [4.54 ]
 [4.768]
 [4.768]
 [4.768]] [[2.239]
 [2.133]
 [2.239]
 [2.239]
 [2.239]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.647]] [[3.2  ]
 [3.2  ]
 [3.2  ]
 [3.2  ]
 [3.342]] [[1.523]
 [1.523]
 [1.523]
 [1.523]
 [1.518]]
start point for exploration sampling:  20005
siam score:  -0.8999057
maxi score, test score, baseline:  0.0981 1.0 1.0
maxi score, test score, baseline:  0.0981 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0981 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.9018271
first move QE:  0.9044904518278183
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.8939763
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.631]
 [0.631]
 [0.809]
 [0.631]
 [0.61 ]] [[3.487]
 [3.487]
 [3.558]
 [3.487]
 [3.153]] [[1.89 ]
 [1.89 ]
 [2.27 ]
 [1.89 ]
 [1.737]]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.181]
 [0.109]
 [0.205]
 [0.189]
 [0.186]] [[0.828]
 [0.918]
 [1.246]
 [1.204]
 [0.315]] [[0.181]
 [0.109]
 [0.205]
 [0.189]
 [0.186]]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1021 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.333 0.    0.167 0.458]
maxi score, test score, baseline:  0.1021 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]] [[3.919]
 [3.919]
 [3.919]
 [3.919]
 [3.919]] [[1.86]
 [1.86]
 [1.86]
 [1.86]
 [1.86]]
first move QE:  0.9062475427818039
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.678]
 [1.072]
 [1.279]
 [0.994]
 [1.032]] [[3.723]
 [3.193]
 [2.97 ]
 [3.133]
 [2.97 ]] [[1.958]
 [2.392]
 [2.657]
 [2.198]
 [2.164]]
line 256 mcts: sample exp_bonus 2.175941087253372
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1021 1.0 1.0
maxi score, test score, baseline:  0.1021 1.0 1.0
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.758]] [[3.633]
 [3.633]
 [3.633]
 [3.633]
 [3.633]] [[1.974]
 [1.974]
 [1.974]
 [1.974]
 [1.974]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1021 1.0 1.0
2030 5736
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.641]
 [0.814]
 [0.527]
 [0.555]] [[3.768]
 [3.768]
 [4.359]
 [4.069]
 [4.037]] [[1.534]
 [1.534]
 [1.937]
 [1.565]
 [1.575]]
actor:  1 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1021 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1021 1.0 1.0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.2467329723001845
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.904]
 [1.032]
 [0.828]
 [0.887]] [[3.666]
 [3.262]
 [2.725]
 [3.23 ]
 [3.146]] [[2.134]
 [2.303]
 [2.001]
 [2.175]
 [2.179]]
maxi score, test score, baseline:  0.1021 1.0 1.0
actor:  1 policy actor:  1  step number:  96 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.789]
 [0.789]
 [0.789]
 [0.789]
 [0.815]] [[3.211]
 [3.211]
 [3.211]
 [3.211]
 [3.26 ]] [[1.488]
 [1.488]
 [1.488]
 [1.488]
 [1.549]]
line 256 mcts: sample exp_bonus 4.728423957961861
siam score:  -0.8987314
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1021 1.0 1.0
maxi score, test score, baseline:  0.1021 1.0 1.0
2044 5742
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.8336],
        [0.7509],
        [0.3459],
        [0.7509],
        [0.8743],
        [0.3398],
        [0.2339],
        [0.8850],
        [0.1453],
        [0.0000]], dtype=torch.float64)
0.0 0.8335786063604234
0.0 0.7508809523748748
0.0 0.34588554135828187
0.0 0.7508809523748748
0.0 0.8743479726270877
0.0 0.3397692023743305
0.0 0.23391959402505835
0.0 0.885020709702428
0.0 0.14526729265139943
0.0 0.0
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.245]
 [0.282]
 [0.282]
 [0.241]] [[5.581]
 [4.672]
 [5.581]
 [5.581]
 [5.35 ]] [[2.103]
 [1.475]
 [2.103]
 [2.103]
 [1.913]]
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.328]
 [0.412]
 [0.412]
 [0.412]] [[0.527]
 [1.224]
 [0.527]
 [0.527]
 [0.527]] [[0.859]
 [1.676]
 [0.859]
 [0.859]
 [0.859]]
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  88 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1021 1.0 1.0
maxi score, test score, baseline:  0.1021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1021 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.04 ]
 [1.09 ]
 [1.059]
 [1.04 ]
 [1.016]] [[3.984]
 [3.56 ]
 [3.134]
 [3.984]
 [4.028]] [[1.799]
 [1.576]
 [1.291]
 [1.799]
 [1.809]]
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.739]
 [0.739]
 [0.739]
 [1.047]] [[4.01 ]
 [4.01 ]
 [4.01 ]
 [4.01 ]
 [4.793]] [[1.505]
 [1.505]
 [1.505]
 [1.505]
 [2.292]]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 2.0553061178350553
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1061 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]] [[3.306]
 [3.306]
 [3.306]
 [3.306]
 [3.306]] [[0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]]
maxi score, test score, baseline:  0.1061 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1081 1.0 1.0
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8994179
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1081 1.0 1.0
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1081 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
2064 5754
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1081 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.90251875
maxi score, test score, baseline:  0.1081 1.0 1.0
maxi score, test score, baseline:  0.1081 1.0 1.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1081 1.0 1.0
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.1101 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1101 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.    0.    0.917 0.042 0.042]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1101 1.0 1.0
siam score:  -0.9026683
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.912]
 [0.912]
 [0.912]
 [0.912]
 [0.912]] [[3.79]
 [3.79]
 [3.79]
 [3.79]
 [3.79]] [[1.996]
 [1.996]
 [1.996]
 [1.996]
 [1.996]]
maxi score, test score, baseline:  0.1121 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.889]
 [0.855]
 [0.835]
 [0.823]] [[2.127]
 [2.122]
 [2.043]
 [2.229]
 [2.112]] [[1.618]
 [1.896]
 [1.775]
 [1.858]
 [1.758]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1121 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Printing some Q and Qe and total Qs values:  [[0.74]
 [0.74]
 [0.74]
 [0.74]
 [0.74]] [[3.792]
 [3.792]
 [3.792]
 [3.792]
 [3.792]] [[2.426]
 [2.426]
 [2.426]
 [2.426]
 [2.426]]
actor:  1 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8952214
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
first move QE:  0.9075977217827033
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.497]] [[3.553]
 [3.553]
 [3.553]
 [3.553]
 [4.218]] [[1.239]
 [1.239]
 [1.239]
 [1.239]
 [1.775]]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  100 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.429304811460558
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  88 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.897033
siam score:  -0.8967881
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8976271
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.913352998571974
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11610000000000001 1.0 1.0
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1181 1.0 1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.89805585
siam score:  -0.90010256
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  20005
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]] [[2.952]
 [2.952]
 [2.952]
 [2.952]
 [2.952]] [[5.995]
 [5.995]
 [5.995]
 [5.995]
 [5.995]]
maxi score, test score, baseline:  0.1201 1.0 1.0
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.    0.25  0.167 0.    0.583]
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.328]] [[5.086]
 [5.086]
 [5.086]
 [5.086]
 [5.012]] [[1.672]
 [1.672]
 [1.672]
 [1.672]
 [1.648]]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.35 ]
 [0.371]
 [0.284]
 [0.354]] [[5.388]
 [5.343]
 [5.388]
 [4.594]
 [4.876]] [[1.835]
 [1.805]
 [1.835]
 [1.44 ]
 [1.598]]
in main func line 156:  2108
2108 5772
maxi score, test score, baseline:  0.1201 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
line 256 mcts: sample exp_bonus 1.7768000699112039
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.036]
 [1.036]
 [1.036]
 [1.036]
 [1.036]] [[3.759]
 [3.759]
 [3.759]
 [3.759]
 [3.759]] [[2.354]
 [2.354]
 [2.354]
 [2.354]
 [2.354]]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1221 1.0 1.0
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.9132335
maxi score, test score, baseline:  0.1221 1.0 1.0
maxi score, test score, baseline:  0.1221 1.0 1.0
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.982]
 [0.919]
 [1.07 ]
 [0.845]] [[2.32 ]
 [2.136]
 [2.813]
 [2.344]
 [2.662]] [[1.441]
 [1.52 ]
 [2.031]
 [1.766]
 [1.843]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
line 256 mcts: sample exp_bonus 6.246412669686911
siam score:  -0.9067565
maxi score, test score, baseline:  0.1221 1.0 1.0
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 4.060290649649294
maxi score, test score, baseline:  0.1221 1.0 1.0
maxi score, test score, baseline:  0.1221 1.0 1.0
maxi score, test score, baseline:  0.1221 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.351]
 [1.351]
 [1.351]
 [1.351]
 [1.351]] [[1.943]
 [1.943]
 [1.943]
 [1.943]
 [1.943]] [[2.184]
 [2.184]
 [2.184]
 [2.184]
 [2.184]]
maxi score, test score, baseline:  0.1221 1.0 1.0
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2126 5784
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]] [[3.107]
 [3.107]
 [3.107]
 [3.107]
 [3.107]] [[0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]]
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1261 1.0 1.0
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.89367163
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1261 1.0 1.0
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.9074640562513553
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]] [[4.179]
 [4.179]
 [4.179]
 [4.179]
 [4.179]] [[1.925]
 [1.925]
 [1.925]
 [1.925]
 [1.925]]
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]] [[5.657]
 [5.657]
 [5.657]
 [5.657]
 [5.657]] [[2.207]
 [2.207]
 [2.207]
 [2.207]
 [2.207]]
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.626]
 [0.62 ]
 [0.442]
 [0.626]] [[3.417]
 [3.417]
 [4.298]
 [2.153]
 [3.417]] [[1.945]
 [1.945]
 [2.37 ]
 [1.052]
 [1.945]]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.89297086
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.025]
 [0.811]
 [0.765]
 [0.811]
 [0.807]] [[2.619]
 [2.758]
 [2.687]
 [2.758]
 [2.536]] [[2.329]
 [1.992]
 [1.853]
 [1.992]
 [1.837]]
maxi score, test score, baseline:  0.1261 1.0 1.0
maxi score, test score, baseline:  0.1261 1.0 1.0
first move QE:  0.9077431117400471
maxi score, test score, baseline:  0.1261 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 3.0151015888196615
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]] [[3.902]
 [3.902]
 [3.902]
 [3.902]
 [3.517]] [[0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]]
Printing some Q and Qe and total Qs values:  [[0.984]
 [0.984]
 [0.984]
 [0.984]
 [0.974]] [[4.727]
 [4.727]
 [4.727]
 [4.727]
 [4.615]] [[2.193]
 [2.193]
 [2.193]
 [2.193]
 [2.11 ]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1281 1.0 1.0
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.532]
 [0.758]
 [0.532]
 [0.488]] [[1.852]
 [1.852]
 [2.56 ]
 [1.852]
 [2.059]] [[1.116]
 [1.116]
 [1.658]
 [1.116]
 [1.202]]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.046]
 [1.17 ]
 [1.046]
 [0.971]
 [1.079]] [[2.127]
 [0.819]
 [2.127]
 [2.432]
 [2.399]] [[2.068]
 [1.446]
 [2.068]
 [2.121]
 [2.316]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1281 1.0 1.0
maxi score, test score, baseline:  0.1281 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.721]
 [0.658]
 [0.721]
 [0.721]] [[1.892]
 [1.892]
 [2.214]
 [1.892]
 [1.892]] [[1.673]
 [1.673]
 [1.653]
 [1.673]
 [1.673]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.8949985
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.507]] [[4.233]
 [4.233]
 [4.233]
 [4.233]
 [5.492]] [[0.946]
 [0.946]
 [0.946]
 [0.946]
 [1.716]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]] [[0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]] [[0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]]
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 0.12613153116443504
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1621 1.0 1.0
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.86]
 [0.86]
 [0.86]
 [0.86]
 [0.86]] [[2.995]
 [2.995]
 [2.995]
 [2.995]
 [2.995]] [[2.052]
 [2.052]
 [2.052]
 [2.052]
 [2.052]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]] [[4.225]
 [4.225]
 [4.225]
 [4.225]
 [4.225]] [[1.98]
 [1.98]
 [1.98]
 [1.98]
 [1.98]]
line 256 mcts: sample exp_bonus 4.6503284637660425
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 5.371153398910158
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
siam score:  -0.88515335
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.019]
 [1.068]
 [0.977]
 [0.971]
 [1.037]] [[3.7  ]
 [2.769]
 [2.991]
 [3.34 ]
 [3.569]] [[2.217]
 [1.798]
 [1.771]
 [1.949]
 [2.177]]
Printing some Q and Qe and total Qs values:  [[0.943]
 [1.122]
 [0.898]
 [0.995]
 [1.076]] [[3.531]
 [3.272]
 [4.441]
 [3.688]
 [4.104]] [[1.096]
 [1.068]
 [1.631]
 [1.233]
 [1.554]]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8941609
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.89530146
2189 5824
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
siam score:  -0.895503
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
first move QE:  0.908188755553495
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.873]
 [0.873]
 [0.873]
 [0.873]
 [0.873]] [[1.81]
 [1.81]
 [1.81]
 [1.81]
 [1.81]] [[0.873]
 [0.873]
 [0.873]
 [0.873]
 [0.873]]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.453]
 [0.453]
 [0.453]
 [0.318]] [[2.213]
 [2.679]
 [2.679]
 [2.679]
 [4.139]] [[0.783]
 [1.158]
 [1.158]
 [1.158]
 [1.877]]
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.08 ]
 [0.523]
 [0.401]
 [0.424]] [[0.823]
 [0.79 ]
 [0.984]
 [0.721]
 [0.893]] [[0.802]
 [0.096]
 [1.242]
 [0.647]
 [0.922]]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]] [[4.757]
 [4.757]
 [4.757]
 [4.757]
 [4.757]] [[1.94]
 [1.94]
 [1.94]
 [1.94]
 [1.94]]
Printing some Q and Qe and total Qs values:  [[1.224]
 [1.224]
 [1.224]
 [1.224]
 [1.214]] [[1.73]
 [1.73]
 [1.73]
 [1.73]
 [2.39]] [[1.894]
 [1.894]
 [1.894]
 [1.894]
 [2.095]]
siam score:  -0.8870166
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.9114144171586572
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  95 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1861 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  0.9129422355360651
start point for exploration sampling:  20005
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1861 1.0 1.0
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.89237154
first move QE:  0.9130061917679306
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2242 5852
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
2243 5852
Printing some Q and Qe and total Qs values:  [[0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]] [[0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.804]] [[0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]]
maxi score, test score, baseline:  0.1881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1901 1.0 1.0
siam score:  -0.88575256
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.1901 1.0 1.0
maxi score, test score, baseline:  0.1901 1.0 1.0
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.88276863
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.124]
 [1.205]
 [1.237]
 [1.124]
 [1.094]] [[2.769]
 [2.649]
 [2.952]
 [2.769]
 [3.221]] [[1.878]
 [1.906]
 [2.151]
 [1.878]
 [2.14 ]]
Printing some Q and Qe and total Qs values:  [[1.249]
 [1.249]
 [1.249]
 [1.249]
 [1.249]] [[3.067]
 [3.067]
 [3.067]
 [3.067]
 [3.067]] [[2.022]
 [2.022]
 [2.022]
 [2.022]
 [2.022]]
Printing some Q and Qe and total Qs values:  [[1.003]
 [1.003]
 [1.003]
 [1.003]
 [1.011]] [[3.996]
 [3.996]
 [3.996]
 [3.996]
 [3.856]] [[2.498]
 [2.498]
 [2.498]
 [2.498]
 [2.407]]
maxi score, test score, baseline:  0.1901 1.0 1.0
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1901 1.0 1.0
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.733]
 [1.169]
 [1.167]
 [1.053]
 [1.049]] [[4.066]
 [3.592]
 [2.963]
 [3.578]
 [3.785]] [[1.836]
 [1.867]
 [1.465]
 [1.769]
 [1.899]]
maxi score, test score, baseline:  0.1901 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1921 1.0 1.0
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1921 1.0 1.0
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.383]
 [0.349]
 [0.349]
 [0.392]] [[4.935]
 [4.463]
 [4.935]
 [4.935]
 [4.713]] [[1.83 ]
 [1.624]
 [1.83 ]
 [1.83 ]
 [1.76 ]]
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.372]
 [0.336]
 [0.336]
 [0.38 ]] [[5.078]
 [4.908]
 [5.078]
 [5.078]
 [4.982]] [[1.964]
 [1.879]
 [1.964]
 [1.964]
 [1.935]]
2270 5863
maxi score, test score, baseline:  0.1921 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 2.197282829249485
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.87638605
maxi score, test score, baseline:  0.1921 1.0 1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1921 1.0 1.0
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.6548959249960022
maxi score, test score, baseline:  0.1921 1.0 1.0
maxi score, test score, baseline:  0.1921 1.0 1.0
maxi score, test score, baseline:  0.1921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.87762284
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.1961 1.0 1.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.1961 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.82 ]
 [0.82 ]
 [0.832]
 [0.55 ]
 [0.742]] [[4.014]
 [4.014]
 [4.042]
 [4.271]
 [4.139]] [[1.731]
 [1.731]
 [1.762]
 [1.461]
 [1.677]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1961 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1981 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1981 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2001 1.0 1.0
maxi score, test score, baseline:  0.2001 1.0 1.0
maxi score, test score, baseline:  0.2001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 2.498467033688869
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2021 1.0 1.0
start point for exploration sampling:  20005
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]] [[2.189]
 [2.189]
 [2.189]
 [2.189]
 [2.189]] [[1.515]
 [1.515]
 [1.515]
 [1.515]
 [1.515]]
2313 5877
line 256 mcts: sample exp_bonus 2.852472241679256
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.756]
 [0.836]
 [0.729]
 [0.775]] [[2.279]
 [3.009]
 [3.06 ]
 [2.279]
 [2.909]] [[1.325]
 [1.857]
 [2.   ]
 [1.325]
 [1.816]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.87763613
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.71 ]
 [0.599]
 [0.71 ]
 [0.71 ]
 [0.71 ]] [[1.986]
 [2.639]
 [1.986]
 [1.986]
 [1.986]] [[1.746]
 [2.267]
 [1.746]
 [1.746]
 [1.746]]
maxi score, test score, baseline:  0.2021 1.0 1.0
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
2320 5888
siam score:  -0.880254
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20409999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.71 ]
 [0.737]
 [0.738]
 [0.71 ]
 [0.717]] [[2.553]
 [2.044]
 [2.787]
 [2.553]
 [2.411]] [[0.71 ]
 [0.737]
 [0.738]
 [0.71 ]
 [0.717]]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8824562
Printing some Q and Qe and total Qs values:  [[0.862]
 [0.862]
 [0.862]
 [0.862]
 [0.862]] [[4.372]
 [4.372]
 [4.372]
 [4.372]
 [4.372]] [[2.072]
 [2.072]
 [2.072]
 [2.072]
 [2.072]]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.20809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.9113669574361505
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8874724
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]] [[2.167]
 [2.167]
 [2.167]
 [2.167]
 [2.167]] [[1.742]
 [1.742]
 [1.742]
 [1.742]
 [1.742]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
siam score:  -0.87807417
using explorer policy with actor:  1
using explorer policy with actor:  1
Starting evaluation
maxi score, test score, baseline:  0.21009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 1.2879221853356413
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.785]
 [0.858]
 [0.785]
 [0.758]
 [0.776]] [[1.51 ]
 [1.806]
 [1.51 ]
 [1.923]
 [2.092]] [[0.785]
 [0.858]
 [0.785]
 [0.758]
 [0.776]]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.819]
 [0.897]
 [0.819]
 [0.819]
 [0.791]] [[2.24 ]
 [1.173]
 [2.24 ]
 [2.24 ]
 [1.827]] [[0.819]
 [0.897]
 [0.819]
 [0.819]
 [0.791]]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
line 256 mcts: sample exp_bonus 2.368459228180423
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2621 1.0 1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.2641 1.0 1.0
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 3.5131013399531605
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.167 0.333 0.167 0.    0.333]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  0.9118923869637062
actor:  0 policy actor:  0  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.9117505538678371
Printing some Q and Qe and total Qs values:  [[0.78 ]
 [1.243]
 [0.981]
 [0.981]
 [1.183]] [[2.232]
 [1.887]
 [2.268]
 [2.268]
 [1.947]] [[2.121]
 [2.62 ]
 [2.498]
 [2.498]
 [2.568]]
Printing some Q and Qe and total Qs values:  [[0.714]
 [0.714]
 [0.714]
 [0.714]
 [0.714]] [[3.342]
 [3.342]
 [3.342]
 [3.342]
 [3.342]] [[1.876]
 [1.876]
 [1.876]
 [1.876]
 [1.876]]
Printing some Q and Qe and total Qs values:  [[1.173]
 [1.284]
 [1.415]
 [1.336]
 [1.349]] [[1.488]
 [1.466]
 [0.853]
 [1.651]
 [1.093]] [[2.167]
 [2.368]
 [2.018]
 [2.657]
 [2.124]]
siam score:  -0.8818623
Printing some Q and Qe and total Qs values:  [[0.988]
 [1.133]
 [0.932]
 [0.823]
 [1.011]] [[2.54 ]
 [2.094]
 [2.855]
 [2.622]
 [2.738]] [[1.874]
 [1.753]
 [2.03 ]
 [1.681]
 [2.062]]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
in main func line 156:  2370
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.9114529717913294
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.882]
 [0.632]
 [1.028]
 [0.502]
 [0.613]] [[3.653]
 [3.512]
 [3.953]
 [3.294]
 [3.542]] [[1.892]
 [1.494]
 [2.22 ]
 [1.223]
 [1.482]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.862]
 [0.862]
 [0.994]
 [0.502]
 [0.613]] [[3.54 ]
 [3.54 ]
 [4.099]
 [3.293]
 [3.544]] [[1.818]
 [1.818]
 [2.244]
 [1.224]
 [1.485]]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.83 ]
 [0.83 ]
 [0.998]
 [0.83 ]
 [0.83 ]] [[1.874]
 [1.874]
 [2.808]
 [1.874]
 [1.874]] [[1.175]
 [1.175]
 [1.972]
 [1.175]
 [1.175]]
maxi score, test score, baseline:  0.2681 1.0 1.0
maxi score, test score, baseline:  0.2681 1.0 1.0
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2701 1.0 1.0
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2701 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.86 ]
 [0.86 ]
 [1.041]
 [0.86 ]
 [0.86 ]] [[2.866]
 [2.866]
 [3.084]
 [2.866]
 [2.866]] [[1.787]
 [1.787]
 [2.174]
 [1.787]
 [1.787]]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.2721 1.0 1.0
maxi score, test score, baseline:  0.2721 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.    0.125 0.542 0.042 0.292]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]] [[1.61]
 [1.61]
 [1.61]
 [1.61]
 [1.61]] [[0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]]
maxi score, test score, baseline:  0.2741 1.0 1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.93 ]
 [1.097]
 [0.93 ]
 [0.93 ]
 [0.921]] [[2.291]
 [2.587]
 [2.291]
 [2.291]
 [3.039]] [[1.47 ]
 [1.928]
 [1.47 ]
 [1.47 ]
 [2.201]]
maxi score, test score, baseline:  0.2761 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.9115594575155646
maxi score, test score, baseline:  0.2761 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.482]
 [0.398]
 [0.382]
 [0.434]] [[0.672]
 [1.766]
 [0.989]
 [1.243]
 [1.493]] [[0.385]
 [0.482]
 [0.398]
 [0.382]
 [0.434]]
maxi score, test score, baseline:  0.2761 1.0 1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8791762
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.652]
 [0.748]
 [0.652]
 [0.697]] [[4.544]
 [4.544]
 [5.169]
 [4.544]
 [5.16 ]] [[1.558]
 [1.558]
 [2.   ]
 [1.558]
 [1.963]]
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.694]
 [0.787]
 [0.694]
 [0.669]] [[4.442]
 [4.442]
 [5.181]
 [4.442]
 [5.246]] [[1.493]
 [1.493]
 [1.992]
 [1.493]
 [1.96 ]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.5924],
        [0.2282],
        [0.5422],
        [0.7597],
        [0.6599],
        [0.0000],
        [0.4015],
        [0.8224],
        [0.4057],
        [0.0000]], dtype=torch.float64)
0.0 0.5923965887317315
0.0 0.2281688536008971
0.0 0.5421559541934083
0.0 0.7596662671170437
0.0 0.6598625186354854
0.0 0.0
0.0 0.4014984303793763
0.0 0.8223534181981065
0.0 0.4057196044626856
0.0 0.0
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.966]
 [0.966]
 [1.07 ]
 [0.966]
 [0.966]] [[3.012]
 [3.012]
 [4.384]
 [3.012]
 [3.012]] [[1.494]
 [1.494]
 [2.544]
 [1.494]
 [1.494]]
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.758]] [[1.954]
 [1.954]
 [1.954]
 [1.954]
 [1.954]] [[2.168]
 [2.168]
 [2.168]
 [2.168]
 [2.168]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.985]
 [0.985]
 [0.971]
 [0.985]
 [0.985]] [[3.899]
 [3.899]
 [3.873]
 [3.899]
 [3.899]] [[2.492]
 [2.492]
 [2.458]
 [2.492]
 [2.492]]
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2801 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.    0.458 0.333 0.    0.208]
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2801 1.0 1.0
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2801 1.0 1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.52 ]
 [0.52 ]
 [0.372]
 [0.449]] [[6.773]
 [6.773]
 [6.773]
 [5.382]
 [5.987]] [[1.756]
 [1.756]
 [1.756]
 [0.988]
 [1.334]]
maxi score, test score, baseline:  0.2801 1.0 1.0
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8711354
actor:  0 policy actor:  0  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.87212205
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
first move QE:  0.9110010010564618
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.984]
 [0.984]
 [1.126]
 [0.938]
 [1.018]] [[3.423]
 [3.423]
 [3.801]
 [3.715]
 [3.896]] [[1.407]
 [1.407]
 [1.756]
 [1.572]
 [1.749]]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.401]
 [0.433]
 [0.505]
 [0.419]] [[0.685]
 [0.529]
 [0.425]
 [0.57 ]
 [0.665]] [[0.885]
 [0.836]
 [0.866]
 [1.057]
 [0.918]]
actor:  0 policy actor:  0  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20005
siam score:  -0.865778
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.906]
 [0.616]
 [1.011]
 [0.861]
 [0.923]] [[3.042]
 [1.399]
 [3.092]
 [3.07 ]
 [2.984]] [[1.469]
 [0.144]
 [1.576]
 [1.457]
 [1.441]]
actor:  0 policy actor:  0  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.77 ]
 [0.543]
 [0.918]
 [0.794]
 [0.747]] [[2.493]
 [2.444]
 [2.091]
 [2.211]
 [2.503]] [[2.25 ]
 [1.769]
 [2.149]
 [2.026]
 [2.217]]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Printing some Q and Qe and total Qs values:  [[0.77 ]
 [0.77 ]
 [0.823]
 [0.77 ]
 [0.679]] [[1.481]
 [1.481]
 [1.982]
 [1.481]
 [2.083]] [[1.245]
 [1.245]
 [1.519]
 [1.245]
 [1.265]]
Printing some Q and Qe and total Qs values:  [[1.093]
 [1.232]
 [1.23 ]
 [1.27 ]
 [1.13 ]] [[1.979]
 [2.251]
 [2.061]
 [1.897]
 [2.324]] [[1.783]
 [2.152]
 [2.085]
 [2.111]
 [1.974]]
line 256 mcts: sample exp_bonus 2.6855712492372494
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.926]
 [1.014]
 [0.828]
 [0.926]
 [0.911]] [[2.383]
 [2.719]
 [3.154]
 [2.383]
 [2.823]] [[1.651]
 [2.058]
 [2.193]
 [1.651]
 [2.014]]
actor:  0 policy actor:  0  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
first move QE:  0.9103548444163709
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.5   0.    0.    0.458]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8605034
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 2.4579974399867206
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]] [[0.962]
 [0.962]
 [0.962]
 [0.962]
 [0.962]] [[1.223]
 [1.223]
 [1.223]
 [1.223]
 [1.223]]
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
siam score:  -0.85561776
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.75 ]
 [0.728]
 [0.772]
 [0.75 ]
 [0.738]] [[2.632]
 [2.897]
 [4.117]
 [2.632]
 [3.407]] [[1.537]
 [1.581]
 [2.077]
 [1.537]
 [1.77 ]]
Printing some Q and Qe and total Qs values:  [[0.796]
 [1.042]
 [0.978]
 [1.042]
 [0.723]] [[4.393]
 [3.716]
 [2.79 ]
 [3.716]
 [3.243]] [[2.244]
 [2.147]
 [1.591]
 [2.147]
 [1.562]]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.613]
 [0.558]
 [0.558]
 [0.493]] [[3.557]
 [4.77 ]
 [3.557]
 [3.557]
 [4.702]] [[1.264]
 [2.107]
 [1.264]
 [1.264]
 [1.91 ]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 1.0381175905970477
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
actor:  0 policy actor:  0  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.85388684
siam score:  -0.85639846
maxi score, test score, baseline:  0.2941 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2941 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.25  0.083 0.583]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8530278
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8501464
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[1.171]
 [1.199]
 [1.079]
 [1.079]
 [1.085]] [[2.748]
 [2.719]
 [2.957]
 [2.957]
 [2.984]] [[2.054]
 [2.057]
 [2.143]
 [2.143]
 [2.174]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2941 1.0 1.0
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2941 1.0 1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2941 1.0 1.0
maxi score, test score, baseline:  0.2941 1.0 1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.642]
 [0.534]
 [0.534]
 [0.566]] [[2.953]
 [3.97 ]
 [2.953]
 [2.953]
 [2.873]] [[0.534]
 [0.642]
 [0.534]
 [0.534]
 [0.566]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  99 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 4.14562428901842
maxi score, test score, baseline:  0.2941 1.0 1.0
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2941 1.0 1.0
first move QE:  0.9088528512791523
Printing some Q and Qe and total Qs values:  [[0.847]
 [1.075]
 [0.847]
 [0.847]
 [0.902]] [[3.942]
 [3.494]
 [3.942]
 [3.942]
 [4.598]] [[1.541]
 [1.474]
 [1.541]
 [1.541]
 [2.007]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.403]
 [0.331]
 [0.307]
 [0.343]] [[1.552]
 [1.338]
 [1.552]
 [0.758]
 [0.818]] [[0.331]
 [0.403]
 [0.331]
 [0.307]
 [0.343]]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
in main func line 156:  2515
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.293]
 [0.296]
 [0.296]
 [0.296]] [[0.752]
 [0.36 ]
 [0.752]
 [0.752]
 [0.303]] [[0.296]
 [0.293]
 [0.296]
 [0.296]
 [0.296]]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.083 0.042 0.75  0.    0.125]
maxi score, test score, baseline:  0.3001 1.0 1.0
line 256 mcts: sample exp_bonus 3.983911465734537
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3001 1.0 1.0
maxi score, test score, baseline:  0.3001 1.0 1.0
siam score:  -0.8581146
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
siam score:  -0.8543881
start point for exploration sampling:  20005
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3021 1.0 1.0
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3021 1.0 1.0
start point for exploration sampling:  20005
Printing some Q and Qe and total Qs values:  [[0.886]
 [0.886]
 [0.836]
 [0.886]
 [0.886]] [[2.522]
 [2.522]
 [3.017]
 [2.522]
 [2.522]] [[1.509]
 [1.509]
 [1.826]
 [1.509]
 [1.509]]
using explorer policy with actor:  1
2535 5997
maxi score, test score, baseline:  0.3021 1.0 1.0
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.944]
 [0.944]
 [0.944]
 [0.944]
 [0.944]] [[2.395]
 [2.395]
 [2.395]
 [2.395]
 [2.395]] [[0.944]
 [0.944]
 [0.944]
 [0.944]
 [0.944]]
actor:  0 policy actor:  0  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3041 1.0 1.0
maxi score, test score, baseline:  0.3041 1.0 1.0
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8474042
UNIT TEST: sample policy line 217 mcts : [0.  0.  0.5 0.  0.5]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.   ]
 [0.454]
 [0.394]
 [0.378]] [[0.138]
 [0.612]
 [1.113]
 [1.077]
 [0.17 ]] [[0.406]
 [0.   ]
 [0.454]
 [0.394]
 [0.378]]
Printing some Q and Qe and total Qs values:  [[1.239]
 [1.321]
 [1.239]
 [1.239]
 [1.126]] [[3.304]
 [2.685]
 [3.304]
 [3.304]
 [3.387]] [[1.932]
 [1.511]
 [1.932]
 [1.932]
 [1.909]]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8468616
maxi score, test score, baseline:  0.3221 1.0 1.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.729]
 [0.816]
 [0.729]
 [0.729]] [[2.584]
 [2.584]
 [2.748]
 [2.584]
 [2.584]] [[2.079]
 [2.079]
 [2.291]
 [2.079]
 [2.079]]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.62 ]] [[4.974]
 [4.974]
 [4.974]
 [4.974]
 [6.686]] [[0.964]
 [0.964]
 [0.964]
 [0.964]
 [1.502]]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.414]] [[1.887]
 [1.887]
 [1.887]
 [1.887]
 [1.894]] [[0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.414]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
first move QE:  0.9096275199646743
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.397]] [[0.965]
 [0.965]
 [0.965]
 [0.965]
 [0.706]] [[0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.397]]
2558 6009
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.883]
 [0.883]
 [0.883]
 [0.883]
 [0.883]] [[1.112]
 [1.112]
 [1.112]
 [1.112]
 [1.112]] [[0.883]
 [0.883]
 [0.883]
 [0.883]
 [0.883]]
line 256 mcts: sample exp_bonus 1.7132324803954355
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.12 ]
 [1.12 ]
 [1.12 ]
 [1.12 ]
 [1.141]] [[2.173]
 [2.173]
 [2.173]
 [2.173]
 [3.07 ]] [[1.488]
 [1.488]
 [1.488]
 [1.488]
 [2.293]]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8492197
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.371]
 [0.415]
 [0.377]
 [0.398]] [[1.581]
 [3.582]
 [3.198]
 [3.154]
 [3.263]] [[0.277]
 [1.798]
 [1.63 ]
 [1.525]
 [1.639]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.3581 1.0 1.0
first move QE:  0.9096361093555155
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3601 1.0 1.0
siam score:  -0.84288895
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3601 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]] [[2.128]
 [2.128]
 [2.128]
 [2.128]
 [2.128]] [[0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]]
line 256 mcts: sample exp_bonus 2.7741544582564366
maxi score, test score, baseline:  0.3601 1.0 1.0
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3621 1.0 1.0
maxi score, test score, baseline:  0.3621 1.0 1.0
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8399576
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
2596 6033
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3621 1.0 1.0
siam score:  -0.8348723
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.9782116979682584
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8221895
maxi score, test score, baseline:  0.3661 1.0 1.0
siam score:  -0.8206349
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.627]
 [1.089]
 [0.93 ]
 [0.814]] [[6.249]
 [5.489]
 [3.464]
 [5.119]
 [5.448]] [[1.656]
 [1.389]
 [0.775]
 [1.428]
 [1.5  ]]
siam score:  -0.83068514
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20005
Printing some Q and Qe and total Qs values:  [[0.849]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.791]] [[3.364]
 [4.488]
 [4.488]
 [4.488]
 [5.317]] [[0.66 ]
 [1.232]
 [1.232]
 [1.232]
 [1.729]]
maxi score, test score, baseline:  0.3661 1.0 1.0
siam score:  -0.8308565
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  95 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3681 1.0 1.0
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3701 1.0 1.0
actor:  0 policy actor:  0  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3721 1.0 1.0
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8296797
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.798]
 [0.931]
 [0.75 ]
 [0.578]
 [0.737]] [[3.574]
 [3.612]
 [3.479]
 [3.037]
 [3.958]] [[2.019]
 [2.286]
 [1.872]
 [1.287]
 [2.142]]
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.786]
 [0.715]
 [0.715]
 [0.741]] [[1.92 ]
 [2.296]
 [1.92 ]
 [1.92 ]
 [1.529]] [[1.189]
 [1.457]
 [1.189]
 [1.189]
 [1.11 ]]
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.191]
 [1.191]
 [1.191]
 [1.191]
 [1.153]] [[4.065]
 [4.065]
 [4.065]
 [4.065]
 [3.784]] [[2.314]
 [2.314]
 [2.314]
 [2.314]
 [2.09 ]]
Printing some Q and Qe and total Qs values:  [[1.116]
 [1.214]
 [0.961]
 [0.961]
 [1.069]] [[3.397]
 [3.353]
 [3.019]
 [3.019]
 [3.64 ]] [[1.761]
 [1.813]
 [1.362]
 [1.362]
 [1.893]]
siam score:  -0.8257405
Printing some Q and Qe and total Qs values:  [[0.796]
 [0.809]
 [0.751]
 [0.809]
 [0.763]] [[1.902]
 [2.147]
 [2.044]
 [2.147]
 [2.153]] [[1.925]
 [2.115]
 [1.93 ]
 [2.115]
 [2.028]]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3741 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.196]
 [1.205]
 [1.237]
 [1.075]
 [1.185]] [[2.535]
 [2.411]
 [2.643]
 [2.553]
 [2.602]] [[1.968]
 [1.894]
 [2.103]
 [1.81 ]
 [2.   ]]
Printing some Q and Qe and total Qs values:  [[1.109]
 [1.204]
 [0.508]
 [0.742]
 [0.792]] [[3.146]
 [3.266]
 [3.39 ]
 [3.044]
 [3.501]] [[1.779]
 [1.954]
 [1.488]
 [1.398]
 [1.81 ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.905]
 [0.959]
 [0.905]
 [0.905]
 [0.94 ]] [[1.891]
 [2.192]
 [1.891]
 [1.891]
 [2.095]] [[1.723]
 [2.058]
 [1.723]
 [1.723]
 [1.948]]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.849]
 [0.849]
 [1.036]
 [0.849]
 [0.871]] [[3.671]
 [3.671]
 [2.464]
 [3.671]
 [3.491]] [[1.941]
 [1.941]
 [1.914]
 [1.941]
 [1.927]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.3761 1.0 1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.22]
 [1.22]
 [1.22]
 [1.22]
 [1.22]] [[2.627]
 [2.627]
 [2.627]
 [2.627]
 [2.627]] [[2.366]
 [2.366]
 [2.366]
 [2.366]
 [2.366]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20005
Printing some Q and Qe and total Qs values:  [[0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.793]] [[3.851]
 [3.851]
 [3.851]
 [3.851]
 [3.937]] [[2.223]
 [2.223]
 [2.223]
 [2.223]
 [2.246]]
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3761 1.0 1.0
first move QE:  0.9104674092185014
start point for exploration sampling:  20005
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
2652 6075
Printing some Q and Qe and total Qs values:  [[0.294]
 [1.031]
 [0.854]
 [0.85 ]
 [0.978]] [[1.534]
 [2.756]
 [3.071]
 [2.975]
 [2.98 ]] [[0.128]
 [1.948]
 [1.987]
 [1.907]
 [2.061]]
maxi score, test score, baseline:  0.3781 1.0 1.0
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.606]] [[4.283]
 [4.283]
 [4.283]
 [4.283]
 [4.728]] [[1.786]
 [1.786]
 [1.786]
 [1.786]
 [2.04 ]]
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.479]] [[6.493]
 [6.493]
 [6.493]
 [6.493]
 [4.994]] [[1.187]
 [1.187]
 [1.187]
 [1.187]
 [0.626]]
in main func line 156:  2655
maxi score, test score, baseline:  0.3781 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.12 ]
 [0.325]
 [0.352]
 [0.32 ]] [[2.494]
 [1.988]
 [5.363]
 [3.297]
 [3.753]] [[0.555]
 [0.052]
 [2.04 ]
 [0.984]
 [1.189]]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3781 1.0 1.0
start point for exploration sampling:  20005
Printing some Q and Qe and total Qs values:  [[1.098]
 [1.098]
 [1.098]
 [1.098]
 [1.098]] [[3.196]
 [3.196]
 [3.196]
 [3.196]
 [3.196]] [[5.392]
 [5.392]
 [5.392]
 [5.392]
 [5.392]]
maxi score, test score, baseline:  0.3781 1.0 1.0
maxi score, test score, baseline:  0.3781 1.0 1.0
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8290291
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3781 1.0 1.0
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
first move QE:  0.9109547371110653
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.85 ]
 [1.068]
 [0.85 ]
 [0.85 ]
 [0.821]] [[4.257]
 [2.476]
 [4.257]
 [4.257]
 [5.287]] [[1.921]
 [1.43 ]
 [1.921]
 [1.921]
 [2.333]]
line 256 mcts: sample exp_bonus 9.98177605685387
maxi score, test score, baseline:  0.3781 1.0 1.0
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]] [[3.966]
 [3.966]
 [3.966]
 [3.966]
 [3.966]] [[2.244]
 [2.244]
 [2.244]
 [2.244]
 [2.244]]
siam score:  -0.8262147
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3781 1.0 1.0
maxi score, test score, baseline:  0.3781 1.0 1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8248149
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.781]] [[1.942]
 [1.942]
 [1.942]
 [1.942]
 [2.873]] [[1.372]
 [1.372]
 [1.372]
 [1.372]
 [1.604]]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3801 1.0 1.0
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3801 1.0 1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3801 1.0 1.0
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.577]
 [0.362]
 [0.615]
 [0.793]] [[2.339]
 [2.518]
 [2.341]
 [2.711]
 [2.62 ]] [[1.405]
 [1.54 ]
 [1.198]
 [1.728]
 [1.825]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.583 0.167 0.167]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.643]
 [0.659]
 [0.64 ]
 [0.621]] [[2.209]
 [3.142]
 [3.498]
 [2.209]
 [2.72 ]] [[0.64 ]
 [0.643]
 [0.659]
 [0.64 ]
 [0.621]]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3861 1.0 1.0
maxi score, test score, baseline:  0.3861 1.0 1.0
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
siam score:  -0.8255489
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3861 1.0 1.0
maxi score, test score, baseline:  0.3861 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]] [[5.389]
 [5.389]
 [5.389]
 [5.389]
 [5.389]] [[2.116]
 [2.116]
 [2.116]
 [2.116]
 [2.116]]
siam score:  -0.83555907
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3861 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3861 1.0 1.0
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 1.1402241357716685
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3861 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3861 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3861 1.0 1.0
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8188447
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20005
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.879]
 [0.963]
 [0.887]
 [0.887]
 [0.857]] [[1.687]
 [2.129]
 [3.081]
 [3.081]
 [3.111]] [[1.386]
 [1.848]
 [2.331]
 [2.331]
 [2.292]]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  95 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.785]
 [0.785]
 [0.785]
 [0.785]
 [0.785]] [[3.689]
 [3.689]
 [3.689]
 [3.689]
 [3.689]] [[1.783]
 [1.783]
 [1.783]
 [1.783]
 [1.783]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
maxi score, test score, baseline:  0.3881 1.0 1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.3881 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.933]
 [1.171]
 [0.57 ]
 [0.67 ]] [[5.088]
 [2.149]
 [2.743]
 [4.211]
 [4.231]] [[1.666]
 [0.252]
 [0.737]
 [1.243]
 [1.313]]
Printing some Q and Qe and total Qs values:  [[0.921]
 [0.95 ]
 [0.921]
 [0.921]
 [0.923]] [[1.833]
 [1.941]
 [1.801]
 [1.833]
 [2.027]] [[2.126]
 [2.255]
 [2.102]
 [2.126]
 [2.258]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.433]
 [0.499]
 [0.431]
 [0.478]] [[-0.078]
 [ 0.   ]
 [-0.202]
 [-0.528]
 [-0.544]] [[0.473]
 [0.433]
 [0.499]
 [0.431]
 [0.478]]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.474]
 [0.519]
 [0.489]
 [0.482]] [[0.505]
 [0.689]
 [0.833]
 [0.733]
 [0.574]] [[0.433]
 [0.474]
 [0.519]
 [0.489]
 [0.482]]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.432]
 [0.438]
 [0.389]
 [0.373]] [[0.467]
 [0.403]
 [0.467]
 [0.857]
 [0.056]] [[0.438]
 [0.432]
 [0.438]
 [0.389]
 [0.373]]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.4401 1.0 1.0
maxi score, test score, baseline:  0.4401 1.0 1.0
maxi score, test score, baseline:  0.4401 1.0 1.0
using explorer policy with actor:  1
start point for exploration sampling:  20005
siam score:  -0.8240193
first move QE:  0.9074210560690719
maxi score, test score, baseline:  0.4401 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4401 1.0 1.0
maxi score, test score, baseline:  0.4401 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
unit test line 137 mCTS: want to be seeing expV values centred around 0, max(abs)=5, std hopefully 1: 2.265157400339317
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
Printing some Q and Qe and total Qs values:  [[0.844]
 [0.844]
 [0.73 ]
 [0.844]
 [0.847]] [[2.385]
 [2.385]
 [3.929]
 [2.385]
 [3.417]] [[1.008]
 [1.008]
 [2.142]
 [1.008]
 [1.842]]
maxi score, test score, baseline:  0.4421 1.0 1.0
siam score:  -0.82946473
maxi score, test score, baseline:  0.4421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4421 1.0 1.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
start point for exploration sampling:  20005
UNIT TEST: sample policy line 217 mcts : [0.083 0.    0.25  0.125 0.542]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.978]
 [0.978]
 [0.972]
 [0.978]
 [0.978]] [[2.013]
 [2.013]
 [3.786]
 [2.013]
 [2.013]] [[1.415]
 [1.415]
 [2.341]
 [1.415]
 [1.415]]
siam score:  -0.83331543
maxi score, test score, baseline:  0.4441 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.215]
 [1.221]
 [1.215]
 [1.215]
 [1.053]] [[2.773]
 [2.699]
 [2.773]
 [2.773]
 [3.649]] [[1.799]
 [1.763]
 [1.799]
 [1.799]
 [2.16 ]]
maxi score, test score, baseline:  0.4441 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
2770 6130
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.325]
 [0.527]
 [0.454]
 [0.383]] [[5.577]
 [5.01 ]
 [6.062]
 [4.663]
 [6.035]] [[1.826]
 [1.553]
 [2.071]
 [1.494]
 [1.978]]
maxi score, test score, baseline:  0.4441 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4441 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8325403
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
2776 6130
maxi score, test score, baseline:  0.4461 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.161]
 [1.229]
 [1.161]
 [1.161]
 [1.119]] [[2.695]
 [2.449]
 [2.695]
 [2.695]
 [2.885]] [[2.277]
 [2.143]
 [2.277]
 [2.277]
 [2.394]]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.4481 1.0 1.0
start point for exploration sampling:  20005
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
2783 6132
maxi score, test score, baseline:  0.4461 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.125 0.5   0.083 0.042 0.25 ]
Printing some Q and Qe and total Qs values:  [[1.073]
 [1.073]
 [1.232]
 [1.073]
 [1.073]] [[2.544]
 [2.544]
 [2.507]
 [2.544]
 [2.544]] [[2.242]
 [2.242]
 [2.465]
 [2.242]
 [2.242]]
first move QE:  0.9095064702773029
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.4461 1.0 1.0
2789 6134
Printing some Q and Qe and total Qs values:  [[0.948]
 [1.029]
 [1.096]
 [0.948]
 [0.982]] [[3.528]
 [2.465]
 [2.368]
 [3.528]
 [2.729]] [[3.242]
 [2.317]
 [2.319]
 [3.242]
 [2.507]]
Printing some Q and Qe and total Qs values:  [[0.743]
 [0.848]
 [0.76 ]
 [0.743]
 [0.685]] [[3.184]
 [3.91 ]
 [3.642]
 [3.184]
 [3.665]] [[1.484]
 [2.063]
 [1.801]
 [1.484]
 [1.743]]
maxi score, test score, baseline:  0.4461 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.048]
 [1.048]
 [0.91 ]
 [1.048]
 [1.048]] [[2.837]
 [2.837]
 [4.745]
 [2.837]
 [2.837]] [[0.978]
 [0.978]
 [1.754]
 [0.978]
 [0.978]]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
maxi score, test score, baseline:  0.4481 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  87 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.4481 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4481 1.0 1.0
siam score:  -0.828385
maxi score, test score, baseline:  0.4481 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.214]
 [1.214]
 [1.214]
 [1.214]
 [1.204]] [[2.539]
 [2.539]
 [2.539]
 [2.539]
 [2.597]] [[2.689]
 [2.689]
 [2.689]
 [2.689]
 [2.722]]
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]] [[0.907]
 [0.907]
 [0.907]
 [0.907]
 [0.907]] [[1.6]
 [1.6]
 [1.6]
 [1.6]
 [1.6]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.015]
 [1.015]
 [1.015]
 [1.015]
 [1.015]] [[1.628]
 [1.628]
 [1.628]
 [1.628]
 [1.628]] [[2.192]
 [2.192]
 [2.192]
 [2.192]
 [2.192]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.167
start point for exploration sampling:  20005
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4481 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4481 1.0 1.0
siam score:  -0.83119154
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.5506],
        [0.6132],
        [0.5933],
        [0.8274],
        [0.4471],
        [0.0000],
        [0.5824],
        [0.6027],
        [0.8313],
        [0.6868]], dtype=torch.float64)
0.0 0.550558935450744
0.0 0.613159893329276
0.0 0.5933253916559064
0.0 0.8273886065911663
0.0 0.44711231543537877
0.0 0.0
0.0 0.5823562237567075
0.0 0.6026557914101711
0.0 0.831348574021196
0.0 0.6867657994106859
maxi score, test score, baseline:  0.4481 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4481 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4481 1.0 1.0
maxi score, test score, baseline:  0.4481 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4481 1.0 1.0
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
2814 6147
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4521 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4521 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.249]
 [1.249]
 [1.325]
 [1.249]
 [1.281]] [[2.162]
 [2.162]
 [1.914]
 [2.162]
 [2.097]] [[2.193]
 [2.193]
 [2.04 ]
 [2.193]
 [2.168]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4521 1.0 1.0
maxi score, test score, baseline:  0.4521 1.0 1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
start point for exploration sampling:  20005
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.833
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.415]
 [0.415]
 [0.475]
 [0.556]] [[4.507]
 [4.507]
 [4.507]
 [4.095]
 [4.541]] [[1.67 ]
 [1.67 ]
 [1.67 ]
 [1.449]
 [1.78 ]]
first move QE:  0.9071725091788007
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.80621773
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8077739
maxi score, test score, baseline:  0.4621 1.0 1.0
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4641 1.0 1.0
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.4641 1.0 1.0
start point for exploration sampling:  20005
maxi score, test score, baseline:  0.4641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4641 1.0 1.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20005
