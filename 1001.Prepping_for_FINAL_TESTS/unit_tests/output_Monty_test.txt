dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 50}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
res_block_channels:[32, 64, 64]
res_block_ds:[False, False, False]
reward_support:[-1, 1, 3]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64, 64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-10, 10, 101]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:1024
proj_out:1024
pred_hid:512
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:2
max_workers:32
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
resampling:False
resampling_use_max:False
resampling_assess_best_child:False
rs_start:1000
ep_to_batch_ratio:[11, 12]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
env:MontezumaRevengeNoFrameskip-v4
same_env_each_time:True
channels:3
env_size:NA
observable_size:NA
game_modes:1
env_map:NA
max_steps:NA
actions_size:8
optimal_score:0.86
total_frames:305000
exp_gamma:0.975
atari_env:True
reward_clipping:True
memory_size:300
image_size:[96, 96]
timesteps_in_obs:2
store_prev_actions:True
running_reward_in_obs:False
deque_length:3
PRESET_CONFIG:6
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:episodic
rdn_beta:[0.3333333333333333, 2, 6]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
state_size:[6, 6]
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:False
log_metrics:False
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:25
detach_expV_calc:True
use_new_episode_expV:True
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
main train batch thing paused
add a thread
Adding thread: now have 3 threads
Printing some Q and Qe and total Qs values:  [[0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.034]
 [0.053]] [[50.84 ]
 [50.84 ]
 [50.84 ]
 [50.84 ]
 [50.84 ]
 [50.84 ]
 [25.018]
 [50.84 ]] [[1.72]
 [1.72]
 [1.72]
 [1.72]
 [1.72]
 [1.72]
 [0.07]
 [1.72]]
printing an ep nov before normalisation:  25.00188445951551
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.044]
 [0.044]
 [0.044]
 [0.044]
 [0.044]
 [0.044]
 [0.044]] [[24.121]
 [24.121]
 [24.121]
 [24.121]
 [24.121]
 [24.121]
 [24.121]
 [24.121]] [[40.254]
 [40.254]
 [40.254]
 [40.254]
 [40.254]
 [40.254]
 [40.254]
 [40.254]]
main train batch thing paused
add a thread
Adding thread: now have 4 threads
main train batch thing paused
add a thread
Adding thread: now have 5 threads
using explorer policy with actor:  1
Actor:  1 rooms visited:  {1}
printing an ep nov before normalisation:  25.018705868617985
Actor:  1 rooms visited:  {1}
in main func line 156:  2
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.0013390337222848427
printing an ep nov before normalisation:  25.004443575421078
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  25.008397926580983
printing an ep nov before normalisation:  25.025441615809996
printing an ep nov before normalisation:  25.024195693583806
printing an ep nov before normalisation:  50.84179401397705
printing an ep nov before normalisation:  50.84461212158203
printing an ep nov before normalisation:  50.843915939331055
printing an ep nov before normalisation:  25.024034552757257
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  50.845046043395996
printing an ep nov before normalisation:  50.844974517822266
printing an ep nov before normalisation:  50.8443546295166
Printing some Q and Qe and total Qs values:  [[0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.034]
 [0.034]
 [0.053]] [[50.846]
 [50.846]
 [50.846]
 [50.846]
 [50.846]
 [25.002]
 [25.003]
 [50.846]] [[1.72]
 [1.72]
 [1.72]
 [1.72]
 [1.72]
 [0.08]
 [0.08]
 [1.72]]
printing an ep nov before normalisation:  50.84516525268555
printing an ep nov before normalisation:  25.01807326974472
printing an ep nov before normalisation:  50.84376811981201
printing an ep nov before normalisation:  50.84510326385498
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  25.021495023734985
printing an ep nov before normalisation:  50.838894844055176
printing an ep nov before normalisation:  25.00909227276793
printing an ep nov before normalisation:  50.83834648132324
Actor:  1 rooms visited:  {1}
deleting a thread, now have 4 threads
Frames:  1457 train batches done:  15 episodes:  3
printing an ep nov before normalisation:  50.84491729736328
printing an ep nov before normalisation:  50.74666500091553
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  50.78113079071045
printing an ep nov before normalisation:  22.462380278454077
printing an ep nov before normalisation:  50.68047523498535
printing an ep nov before normalisation:  22.15962563611555
printing an ep nov before normalisation:  50.67723274230957
Actor:  0 rooms visited:  {1}
printing an ep nov before normalisation:  50.349998474121094
printing an ep nov before normalisation:  50.27308464050293
Actor:  0 rooms visited:  {1}
Actor:  1 rooms visited:  {1}
printing an ep nov before normalisation:  50.65140724182129
printing an ep nov before normalisation:  50.71659564971924
printing an ep nov before normalisation:  16.396760661365306
printing an ep nov before normalisation:  50.70608615875244
Actor:  0 rooms visited:  {1}
deleting a thread, now have 3 threads
Frames:  1881 train batches done:  25 episodes:  4
