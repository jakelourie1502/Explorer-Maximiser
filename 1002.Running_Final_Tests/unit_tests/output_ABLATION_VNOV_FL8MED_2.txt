append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 25}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:5
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.frozen_lakeGym_Image.gridWorld'>
same_env_each_time:True
env_size:[8, 8]
observable_size:[8, 8]
game_modes:1
env_map:[['S' 'F' 'F' 'F' 'H' 'F' 'H' 'F']
 ['F' 'H' 'H' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'H' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'H' 'F' 'H' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'H' 'H']
 ['H' 'H' 'H' 'F' 'F' 'F' 'F' 'G']]
max_steps:100
actions_size:5
optimal_score:1
total_frames:205000
exp_gamma:0.95
atari_env:False
memory_size:60
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:12
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:vNov_ablation
rdn_beta:[0.25, 1, 4]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 64)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['S' 'F' 'F' 'F' 'H' 'F' 'H' 'F']
 ['F' 'H' 'H' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'H' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'H' 'F' 'H' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'H' 'H']
 ['H' 'H' 'H' 'F' 'F' 'F' 'F' 'G']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -3.55070650261028e-05
from probs:  [0.25, 0.25, 0.25, 0.25]
main train batch thing paused
add a thread
Adding thread: now have 4 threads
Starting evaluation
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  0.000854574053929272
deleting a thread, now have 3 threads
Frames:  1225 train batches done:  10.0 episodes:  125
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
siam score:  -0.10263478825474737
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 2 threads
Frames:  1225 train batches done:  36.0 episodes:  125
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.14691848
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 1 threads
Frames:  1225 train batches done:  62.0 episodes:  125
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5189391
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.504006
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.13025225035337915
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.2540090778709084
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.459651
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5335136
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
siam score:  -0.55276734
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.56884897
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.56514096
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.57059336
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5760704
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.5594873
siam score:  -0.5450219
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.522959
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.08312667379337378
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6275598
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6212216
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.62134725
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6306524
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6245367
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.56263125
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.46931306
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.50064105
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.04 ]
 [-0.012]
 [ 0.009]
 [ 0.015]
 [ 0.029]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.42326632
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
main train batch thing paused
add a thread
Adding thread: now have 2 threads
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.018]
 [-0.009]
 [ 0.02 ]
 [-0.036]
 [ 0.026]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.36]
 [0.36]
 [0.36]
 [0.36]
 [0.36]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.058]
 [-0.071]
 [-0.096]
 [-0.063]
 [ 0.035]] [[0.231]
 [0.135]
 [0.116]
 [0.141]
 [0.214]]
first move QE:  -0.05874341439771813
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.65783256
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6706778
siam score:  -0.6753944
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6849881
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.69086754
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.757]
 [-0.757]
 [-0.757]
 [-0.01 ]
 [-0.757]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.187]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.091]
 [-0.162]
 [-0.19 ]
 [-0.046]
 [-0.043]] [[0.411]
 [0.358]
 [0.336]
 [0.444]
 [0.447]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.69163024
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.1476548199440081
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.418]
 [-0.418]
 [-0.418]
 [-0.418]
 [-0.418]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.67044747
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.66380143
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.65486693
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6528305
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.0665056469558367
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.06545954870543741
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.007]
 [-0.021]
 [ 0.007]
 [ 0.007]
 [ 0.007]] [[0.028]
 [0.   ]
 [0.028]
 [0.028]
 [0.028]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.212]
 [-0.212]
 [-0.212]
 [-0.212]
 [ 0.319]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.399]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
first move QE:  -0.06516999212937181
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.66419923
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.664577
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.029]
 [ 0.046]
 [ 0.002]
 [ 0.047]
 [ 0.063]] [[0.018]
 [0.074]
 [0.041]
 [0.075]
 [0.087]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.225]
 [-0.103]
 [-0.131]
 [-0.195]
 [ 0.068]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.422]
 [-0.502]
 [-0.338]
 [-0.169]
 [-0.502]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 1 threads
Frames:  10247 train batches done:  1200.0 episodes:  881
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.222]] [[1.   ]
 [1.   ]
 [1.   ]
 [1.   ]
 [0.491]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.282]
 [-0.222]
 [-0.272]
 [-0.19 ]
 [-0.202]] [[0.273]
 [0.318]
 [0.281]
 [0.342]
 [0.333]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.081]
 [0.081]
 [0.081]
 [0.081]
 [0.081]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.66009295
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.499]
 [-0.175]
 [-0.35 ]
 [-0.099]
 [-0.054]] [[0.   ]
 [0.081]
 [0.037]
 [0.1  ]
 [0.111]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.6950322
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.375 0.208 0.083 0.167 0.167]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6581224
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.64184374
siam score:  -0.6284069
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6232543
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.004771488536611579
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.06659274024652746
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.71270096
first move QE:  -0.06801412586487035
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]] [[0.04]
 [0.04]
 [0.04]
 [0.04]
 [0.04]]
siam score:  -0.7074259
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7089819
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.005]
 [ 0.005]
 [-0.187]
 [-0.056]
 [-0.081]] [[0.055]
 [0.055]
 [0.007]
 [0.04 ]
 [0.034]]
first move QE:  -0.06797567824638771
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.111]
 [-0.07 ]
 [-0.033]
 [-0.004]
 [ 0.021]] [[0.09 ]
 [0.12 ]
 [0.148]
 [0.17 ]
 [0.189]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6680965
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.384]
 [-0.149]
 [-0.143]
 [-0.139]
 [-0.078]] [[0.   ]
 [0.059]
 [0.06 ]
 [0.061]
 [0.077]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.66185987
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6652461
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.06986513712581856
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6919002
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.07174581751643633
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.07249590504946232
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.69867826
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]] [[0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]]
siam score:  -0.7100575
siam score:  -0.7080602
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.71065
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.25  0.125 0.25  0.25  0.125]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.044]
 [-0.118]
 [-0.007]
 [-0.032]
 [-0.135]] [[0.093]
 [0.056]
 [0.111]
 [0.099]
 [0.047]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7176942
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.027]
 [ 0.003]
 [ 0.101]
 [ 0.005]
 [ 0.534]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.71824074
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.720329
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.011937387984576695
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72960365
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.13]
 [-0.13]
 [-0.13]
 [-0.13]
 [-0.13]] [[0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]] [[0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.083]
 [-0.044]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.037]
 [0.057]
 [0.079]
 [0.079]
 [0.079]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7176006
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.71921295
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.126]
 [-0.112]
 [-0.018]
 [-0.014]
 [ 0.055]] [[0.047]
 [0.054]
 [0.101]
 [0.103]
 [0.137]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.083 0.125 0.375 0.167 0.25 ]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]] [[0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.057427212595939636
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.707946
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 1 threads
Frames:  19042 train batches done:  2227.0 episodes:  1696
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.71137977
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7178471
first move QE:  -0.07854826933997101
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7131752
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
STARTED EXPV TRAINING ON FRAME NO.  20016
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.227]
 [-0.227]
 [-0.227]
 [-0.227]
 [-0.227]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.195]
 [0.081]
 [0.016]
 [0.001]
 [0.067]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.088]
 [0.176]
 [0.176]
 [0.176]
 [0.176]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7228198
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.151]
 [-0.095]
 [-0.069]
 [-0.099]
 [-0.143]] [[0.145]
 [0.2  ]
 [0.227]
 [0.197]
 [0.153]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72145385
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.08104707788197321
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.22 ]
 [0.149]
 [0.015]
 [0.004]
 [0.165]] [[0.124]
 [0.088]
 [0.022]
 [0.016]
 [0.097]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.71997315
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7190883
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.039]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.039]] [[0.1]
 [0.1]
 [0.1]
 [0.1]
 [0.1]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [ 0.157]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.166]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.08114911237604733
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7159945
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.69576997
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [-0.024]
 [ 0.   ]
 [ 0.   ]] [[0.006]
 [0.006]
 [0.   ]
 [0.006]
 [0.006]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.08132026982902159
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.004]
 [ 0.004]
 [ 0.004]
 [-0.143]
 [-0.02 ]] [[0.258]
 [0.258]
 [0.258]
 [0.111]
 [0.234]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.70451355
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]] [[0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.403]
 [0.005]
 [0.213]
 [0.451]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7022899
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.173]
 [0.173]
 [0.173]
 [0.173]
 [0.364]] [[0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.241]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.08328769978182578
siam score:  -0.6979264
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.029]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]] [[0.157]
 [0.157]
 [0.157]
 [0.157]
 [0.157]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.216]
 [-0.216]
 [-0.216]
 [-0.216]
 [-0.216]] [[0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.062]
 [-0.062]
 [-0.062]
 [-0.062]
 [-0.062]] [[0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.2324211448431015
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.71692544
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7219446
siam score:  -0.7226895
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.017]
 [-0.003]
 [-0.021]
 [-0.091]
 [-0.017]] [[0.079]
 [0.086]
 [0.077]
 [0.042]
 [0.078]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.66765517
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.08725434551308309
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.08142871533715876
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.083 0.083 0.583 0.125 0.125]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [   -0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 0.0
0.0 0.0
0.0 7.611299647239812e-13
0.0 2.0137997619407776e-21
0.0 1.245485394990333e-12
0.0 2.508269197024953e-12
0.0 2.094351752418409e-20
0.0 0.0
0.0 0.0
0.0 0.0
siam score:  -0.7150906
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7145135
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.093]
 [-0.075]
 [-0.07 ]
 [-0.099]
 [-0.074]] [[0.021]
 [0.026]
 [0.027]
 [0.02 ]
 [0.026]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.70309466
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.033]
 [0.033]
 [0.033]
 [0.033]
 [0.662]] [[0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.496]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.08844003396321032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.71371627
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.08953404784580078
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.034]] [[0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.074]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6885011
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6658731
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.083 0.083 0.375 0.125 0.333]
siam score:  -0.6827916
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
first move QE:  -0.08873095915320083
siam score:  -0.7051572
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.70800155
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [-0.209]
 [-0.248]
 [-0.185]
 [-0.102]] [[0.124]
 [0.019]
 [0.   ]
 [0.032]
 [0.073]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.031]
 [-0.089]
 [-0.074]
 [-0.03 ]
 [-0.104]] [[0.128]
 [0.099]
 [0.107]
 [0.129]
 [0.092]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.053]
 [-0.104]
 [-0.064]
 [-0.052]
 [-0.053]] [[0.02 ]
 [0.007]
 [0.017]
 [0.02 ]
 [0.02 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.292 0.167 0.208 0.167 0.167]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.71174103
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.096]
 [-0.101]
 [-0.111]
 [-0.043]
 [-0.093]] [[0.164]
 [0.16 ]
 [0.149]
 [0.217]
 [0.167]]
siam score:  -0.7143623
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.68 ]
 [0.68 ]
 [0.68 ]
 [0.108]
 [0.68 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.08887420186110982
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.700139
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 1.850929681977477e-11
0.0 3.12582236862089e-11
0.0 3.129282050513059e-11
0.0 2.2885794092772063e-11
0.0 5.950652435983125e-11
0.0 0.0
0.0 0.0
0.0 9.444930893174285e-11
0.0 0.0
0.0 3.129282050513059e-11
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.701131
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.491]
 [0.443]
 [0.443]
 [0.443]
 [0.443]] [[ 0.391]
 [-0.133]
 [-0.133]
 [-0.133]
 [-0.133]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.009]
 [-0.062]
 [ 0.023]
 [ 0.053]
 [ 0.285]] [[0.042]
 [0.006]
 [0.049]
 [0.064]
 [0.18 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.3905196811865508
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]] [[0.163]
 [0.163]
 [0.163]
 [0.163]
 [0.163]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6864967
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.082]] [[0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.208]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6909809
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.466]
 [0.   ]
 [0.   ]
 [0.231]
 [0.122]] [[0.234]
 [0.001]
 [0.001]
 [0.117]
 [0.062]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.132]
 [-0.189]
 [-0.336]
 [-0.088]
 [-0.224]] [[0.418]
 [0.361]
 [0.215]
 [0.462]
 [0.327]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]] [[0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.318]
 [-0.197]
 [-0.174]
 [-0.198]
 [-0.248]] [[0.156]
 [0.246]
 [0.264]
 [0.245]
 [0.208]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.004297861480154097
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72403383
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
UNIT TEST: sample policy line 217 mcts : [0.083 0.292 0.125 0.333 0.167]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.219]
 [-0.151]
 [-0.165]
 [-0.172]
 [-0.143]] [[0.234]
 [0.302]
 [0.288]
 [0.28 ]
 [0.309]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.005]
 [0.259]
 [0.005]
 [0.005]] [[0.003]
 [0.003]
 [0.129]
 [0.003]
 [0.003]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
main train batch thing paused
add a thread
Adding thread: now have 2 threads
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
first move QE:  -0.08774612125316622
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.09 ]
 [-0.215]
 [ 0.055]
 [-0.024]
 [ 0.041]] [[0.293]
 [0.   ]
 [0.259]
 [0.183]
 [0.246]]
siam score:  -0.704164
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.012]
 [-0.19 ]
 [-0.007]
 [ 0.002]
 [ 0.031]] [[0.152]
 [0.   ]
 [0.137]
 [0.144]
 [0.166]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [ 0.065]
 [ 0.01 ]
 [ 0.003]
 [ 0.066]] [[0.094]
 [0.127]
 [0.1  ]
 [0.097]
 [0.128]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.262]
 [-0.262]
 [-0.262]
 [-0.262]
 [-0.262]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.131]
 [-0.131]
 [-0.131]
 [ 0.067]
 [ 0.589]] [[0.081]
 [0.081]
 [0.081]
 [0.18 ]
 [0.441]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.68502533
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.003064653377185015
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6992668
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.5   0.042 0.042 0.125 0.292]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.70303154
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.003]
 [-0.051]
 [-0.034]
 [-0.095]
 [-0.02 ]] [[0.197]
 [0.156]
 [0.169]
 [0.123]
 [0.179]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20016
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.006]
 [-0.052]
 [-0.036]
 [-0.028]
 [-0.104]] [[0.12 ]
 [0.091]
 [0.099]
 [0.103]
 [0.065]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.022]
 [-0.076]
 [-0.099]
 [-0.04 ]
 [-0.04 ]] [[0.039]
 [0.012]
 [0.   ]
 [0.03 ]
 [0.03 ]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.69928604
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.024]
 [-0.011]
 [-0.007]
 [ 0.016]
 [ 0.064]] [[0.027]
 [0.   ]
 [0.003]
 [0.021]
 [0.056]]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7117249
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72005725
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7235045
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.018]
 [-0.044]
 [-0.05 ]
 [-0.018]
 [-0.008]] [[0.088]
 [0.057]
 [0.054]
 [0.07 ]
 [0.075]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.058]
 [ 0.064]
 [-0.075]
 [ 0.06 ]
 [ 0.042]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.22134298292578664
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.7158461
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.0002252976433140856
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.957]
 [0.957]
 [0.957]
 [0.957]
 [0.957]] [[0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.05524608492851257
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.71153504
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.498]
 [1.498]
 [1.498]
 [1.498]
 [1.427]] [[0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.462]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.7133565
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.022]
 [-0.04 ]
 [ 0.112]
 [ 0.112]
 [ 0.204]] [[0.04 ]
 [0.027]
 [0.133]
 [0.133]
 [0.198]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.044]
 [-0.041]
 [-0.023]
 [-0.098]
 [-0.027]] [[0.045]
 [0.045]
 [0.05 ]
 [0.031]
 [0.049]]
first move QE:  -0.08522229065943109
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [-0.15 ]
 [-0.073]
 [ 0.   ]
 [-0.024]] [[0.075]
 [0.   ]
 [0.038]
 [0.075]
 [0.063]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
deleting a thread, now have 1 threads
Frames:  38860 train batches done:  4553.0 episodes:  3482
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.171]
 [-0.171]
 [-0.171]
 [-0.171]
 [-0.171]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6621897
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]] [[0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6864185
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6874332
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.69292694
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.69533753
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.077]
 [-0.099]
 [ 0.027]
 [-0.094]
 [-0.011]] [[0.02 ]
 [0.015]
 [0.046]
 [0.016]
 [0.037]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.103]
 [-0.291]
 [-0.215]
 [-0.125]
 [-0.259]] [[0.31 ]
 [0.122]
 [0.198]
 [0.289]
 [0.154]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6954327
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.69372267
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7121878
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.71547395
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.04]
 [-0.04]
 [-0.04]
 [-0.04]
 [-0.04]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.003]] [[0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.15 ]]
line 256 mcts: sample exp_bonus 0.0035108621933995234
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.364]
 [0.147]
 [0.147]
 [0.293]
 [0.147]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000]], dtype=torch.float64)
0.0 -5.128978042657091e-12
0.0 -8.346481974211067e-12
0.0 0.0
0.0 -2.5839497299581985e-12
0.0 0.0
0.0 0.0
0.0 -7.732388481244942e-12
0.0 0.0
0.0 -1.4150097928850645e-11
0.0 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7223457
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [-0.006]
 [ 0.   ]
 [ 0.   ]
 [ 0.664]] [[0.006]
 [0.   ]
 [0.006]
 [0.006]
 [0.67 ]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.437]] [[0.021]
 [0.02 ]
 [0.02 ]
 [0.02 ]
 [0.129]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7099358
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]] [[0.053]
 [0.053]
 [0.053]
 [0.053]
 [0.053]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.437]
 [-0.003]
 [-0.079]
 [ 0.001]
 [ 0.122]] [[0.129]
 [0.019]
 [0.   ]
 [0.02 ]
 [0.05 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.08634973345395723
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.087]
 [-0.087]
 [-0.087]
 [-0.255]
 [-0.028]] [[0.157]
 [0.157]
 [0.157]
 [0.032]
 [0.202]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7227871
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72524977
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.284]
 [-0.284]
 [-0.284]
 [-0.239]
 [-0.284]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.011]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.058]
 [-0.058]
 [-0.058]
 [-0.058]
 [-0.058]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.71306825
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [-0.002]
 [-0.179]
 [-0.096]
 [-0.042]] [[0.132]
 [0.132]
 [0.043]
 [0.085]
 [0.112]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.296]
 [-0.336]
 [-0.366]
 [-0.307]
 [-0.253]] [[0.077]
 [0.067]
 [0.059]
 [0.074]
 [0.088]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7369156
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
siam score:  -0.74164855
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.799]
 [0.533]
 [0.058]
 [1.131]
 [0.573]] [[0.399]
 [0.266]
 [0.028]
 [0.565]
 [0.286]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.264]
 [-0.264]
 [-0.264]
 [-0.264]
 [-0.016]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.248]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7149457
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7249194
siam score:  -0.7232588
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.842]
 [0.402]
 [0.402]
 [0.402]
 [0.387]] [[0.559]
 [0.263]
 [0.263]
 [0.263]
 [0.253]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7148097
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.71642613
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.034]
 [-0.034]
 [-0.034]
 [-0.034]
 [-0.034]] [[0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.71383685
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7057811
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7212759
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7207942
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.08683181821037256
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.711795
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7006134
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6749303
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.331]
 [0.331]
 [0.002]
 [0.04 ]
 [0.182]] [[0.66 ]
 [0.66 ]
 [0.414]
 [0.442]
 [0.549]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.08498909734570459
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.08463385784818664
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.068]
 [ 0.068]
 [ 0.068]
 [ 0.068]
 [-0.015]] [[0.14 ]
 [0.14 ]
 [0.14 ]
 [0.14 ]
 [0.078]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7088562
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.5]
 [1.5]
 [0. ]
 [0. ]
 [0. ]] [[ 0.   ]
 [ 0.   ]
 [-0.287]
 [-0.175]
 [-0.002]] [[1.644]
 [1.644]
 [0.   ]
 [0.056]
 [0.143]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.08610500523675385
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.046]
 [-0.   ]
 [-0.001]
 [-0.002]
 [ 0.003]] [[0.182]
 [0.159]
 [0.158]
 [0.158]
 [0.16 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 0.0
0.0 0.0
0.0 1.9183934739611077e-11
0.0 7.005855335442655e-12
0.0 1.1849409647772412e-11
0.0 1.0776908328863588e-11
0.0 0.0
0.0 1.1849409647772412e-11
0.0 7.010179937405106e-12
0.0 7.533456788152844e-12
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72026914
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0]
 [0]
 [0]
 [0]
 [0]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7185466
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.128]
 [0.772]
 [0.715]
 [1.351]
 [1.336]] [[0.458]
 [0.28 ]
 [0.252]
 [0.569]
 [0.562]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.083 0.083 0.125 0.458 0.25 ]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000]], dtype=torch.float64)
0.0 -8.839486607798182e-12
0.0 0.0
0.0 0.0
0.0 0.0
0.0 -7.896723359040188e-12
0.0 0.0
0.0 0.0
0.0 0.0
0.0 -3.719157772690833e-12
0.0 -2.1121356445371608e-11
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.08743332913058391
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0]
 [0]
 [0]
 [0]
 [0]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.173]
 [-0.288]
 [-0.252]
 [-0.099]
 [-0.193]] [[0.154]
 [0.068]
 [0.095]
 [0.21 ]
 [0.14 ]]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.08840908625978722
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.69404066
siam score:  -0.698555
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.644]
 [0.453]
 [0.001]
 [0.453]
 [0.513]] [[0.222]
 [0.174]
 [0.061]
 [0.174]
 [0.189]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7241663
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72705764
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.2  ]
 [ 0.2  ]
 [ 0.2  ]
 [-0.054]
 [-0.008]] [[0.291]
 [0.291]
 [0.291]
 [0.037]
 [0.083]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
siam score:  -0.649756
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 0.0
0.0 3.051439213207349e-10
0.0 0.0
0.0 0.0
0.0 1.0102270413699296e-10
0.0 0.0
0.0 1.2364902212007312e-10
0.0 2.6630899483279008e-11
0.0 0.0
0.0 3.016842401825322e-11
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6795494
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6931891
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.109]
 [-0.288]
 [-0.255]
 [-0.3  ]
 [-0.234]] [[0.168]
 [0.034]
 [0.058]
 [0.025]
 [0.075]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72400063
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.71387535
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.114]
 [-0.183]
 [-0.167]
 [-0.151]
 [-0.096]] [[0.04 ]
 [0.023]
 [0.027]
 [0.031]
 [0.044]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.08830552816959915
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.1  ]
 [-0.1  ]
 [-0.123]
 [-0.127]
 [-0.009]] [[0.015]
 [0.015]
 [0.009]
 [0.008]
 [0.038]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.186]
 [-0.072]
 [-0.152]
 [-0.128]
 [-0.03 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.077]
 [-0.077]
 [-0.077]
 [-0.093]
 [-0.077]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.70960635
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.003]
 [-0.003]
 [-0.003]
 [-0.034]
 [-0.001]] [[0.17 ]
 [0.17 ]
 [0.17 ]
 [0.147]
 [0.172]]
siam score:  -0.7107254
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.068]
 [-0.068]
 [-0.068]
 [-0.068]
 [ 0.021]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.045]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.667]
 [0.031]
 [0.303]
 [0.487]
 [0.199]] [[0.159]
 [0.   ]
 [0.068]
 [0.114]
 [0.042]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.032]
 [ 0.101]
 [-0.039]
 [ 0.008]
 [ 0.101]] [[0.018]
 [0.035]
 [0.   ]
 [0.012]
 [0.035]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.003]
 [ 0.   ]
 [-0.061]
 [ 0.   ]
 [ 0.   ]] [[0.064]
 [0.061]
 [0.   ]
 [0.061]
 [0.061]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.08699421166134882
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
siam score:  -0.705917
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
siam score:  -0.6746314
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.   ]
 [-0.028]
 [-0.016]
 [ 0.005]
 [ 0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0010],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0029],
        [0.0000],
        [0.0004]], dtype=torch.float64)
0.0 0.0
0.0 0.001016919845234635
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.002932435775689475
0.0 0.0
0.0 0.0003982677801228091
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7473166
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]] [[-0.019]
 [-0.043]
 [-0.005]
 [-0.001]
 [-0.031]] [[0.025]
 [0.013]
 [0.032]
 [0.034]
 [0.019]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
siam score:  -0.78310794
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[ 0.038]
 [-0.001]
 [ 0.011]
 [-0.043]
 [-0.021]] [[0.028]
 [0.018]
 [0.021]
 [0.008]
 [0.013]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]] [[0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.001]] [[-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.01 ]] [[0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.07 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
line 256 mcts: sample exp_bonus -0.20509167516476212
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
using another actor
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
siam score:  -0.7960338
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.003]
 [0.003]] [[ 0.059]
 [-0.009]
 [-0.082]
 [-0.096]
 [-0.069]] [[0.42 ]
 [0.353]
 [0.28 ]
 [0.267]
 [0.294]]
siam score:  -0.79751694
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
siam score:  -0.8044524
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
siam score:  -0.8057129
line 256 mcts: sample exp_bonus -0.004637563834569267
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
siam score:  -0.8025085
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.06 ]
 [-0.116]
 [ 0.027]
 [-0.03 ]
 [-0.003]] [[0.089]
 [0.   ]
 [0.072]
 [0.043]
 [0.057]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
line 256 mcts: sample exp_bonus -0.07197035909618414
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
using another actor
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.143]
 [-0.143]
 [-0.143]
 [-0.143]
 [-0.143]] [[0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.779799
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.056]] [[0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.095]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.104]
 [-0.16 ]
 [-0.066]
 [-0.071]
 [-0.103]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
line 256 mcts: sample exp_bonus 0.018220682532677775
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using explorer policy with actor:  0
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.006]
 [-0.006]
 [-0.24 ]
 [-0.072]
 [-0.054]] [[0.175]
 [0.175]
 [0.   ]
 [0.126]
 [0.139]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
siam score:  -0.7421523
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
first move QE:  -0.08439042386491652
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.74608374
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
rdn beta is 0 so we're just using the maxi policy
first move QE:  -0.08443739660298506
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.74705946
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.048]
 [-0.012]
 [-0.007]
 [-0.003]
 [ 0.27 ]] [[0.06 ]
 [0.   ]
 [0.005]
 [0.009]
 [0.282]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
siam score:  -0.6529836
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.192]
 [-0.158]
 [-0.214]
 [-0.164]
 [-0.127]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
siam score:  -0.7266423
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.05]
 [0.05]
 [0.05]
 [0.05]
 [0.05]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.72353137
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
siam score:  -0.7348096
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
siam score:  -0.72774154
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[ 0.14 ]
 [-0.071]
 [-0.001]
 [-0.001]
 [-0.076]] [[0.246]
 [0.035]
 [0.104]
 [0.105]
 [0.03 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
siam score:  -0.7650435
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
siam score:  -0.7663823
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
first move QE:  -0.08319797563674401
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
siam score:  -0.73095566
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
siam score:  -0.72280973
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
siam score:  -0.7139551
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.   ]] [[0.   ]
 [0.012]
 [0.   ]
 [0.   ]
 [0.023]] [[0.061]
 [0.072]
 [0.061]
 [0.061]
 [0.083]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
siam score:  -0.7590431
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.005]] [[-0.085]
 [-0.085]
 [-0.085]
 [-0.204]
 [-0.069]] [[0.213]
 [0.213]
 [0.213]
 [0.095]
 [0.23 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.12504498530497532, 0.6248650440850739]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.76690894
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.77281874
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7687706
siam score:  -0.76865906
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.76634467
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.208 0.375 0.125 0.167 0.125]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.062]
 [-0.37 ]
 [-0.253]
 [-0.32 ]
 [-0.208]] [[0.336]
 [0.028]
 [0.145]
 [0.078]
 [0.191]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.155]
 [0.155]
 [0.155]
 [0.   ]
 [0.155]] [[0.323]
 [0.323]
 [0.323]
 [0.207]
 [0.323]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.324]
 [-0.324]
 [-0.324]
 [-0.324]
 [-0.324]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
first move QE:  -0.08164191424087737
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.74764466
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.0814225145124268
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.004]
 [-0.087]
 [-0.025]
 [-0.082]
 [-0.047]] [[0.185]
 [0.117]
 [0.163]
 [0.121]
 [0.147]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0001],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0001],
        [    0.0000],
        [    0.0001],
        [    0.0000],
        [    0.0000],
        [    0.0001]], dtype=torch.float64)
0.0 0.00011874562977155091
0.0 4.58834996576037e-05
0.0 0.0
0.0 0.0
0.0 6.326629378484605e-05
0.970299 0.970299
0.0 6.137507300569818e-05
0.0 0.0
0.0 4.005088436593042e-05
0.0 6.137507300569818e-05
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.021]
 [-0.001]
 [-0.065]
 [-0.087]
 [-0.006]] [[0.104]
 [0.119]
 [0.071]
 [0.054]
 [0.115]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7284424
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
siam score:  -0.72884107
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.152]
 [-0.152]
 [-0.152]
 [-0.152]
 [-0.152]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
first move QE:  -0.08151093844454038
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.03999225204867312
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.042]
 [-0.01 ]
 [ 0.017]
 [ 0.   ]
 [-0.074]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.28 ]
 [-0.365]
 [-0.232]
 [-0.308]
 [-0.192]] [[0.046]
 [0.024]
 [0.058]
 [0.039]
 [0.068]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.044]
 [-0.24 ]
 [-0.088]
 [-0.07 ]
 [-0.104]] [[0.147]
 [0.   ]
 [0.114]
 [0.128]
 [0.102]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.00044825794098402163
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.007515587826548749
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.028]
 [-0.09 ]
 [-0.089]
 [-0.006]
 [-0.11 ]] [[0.281]
 [0.219]
 [0.22 ]
 [0.303]
 [0.199]]
first move QE:  -0.08151169000411213
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [-0.123]
 [-0.14 ]
 [-0.019]
 [-0.143]] [[0.194]
 [0.101]
 [0.088]
 [0.179]
 [0.087]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.   ]
 [0.001]
 [0.001]] [[-0.044]
 [-0.058]
 [-0.009]
 [-0.002]
 [-0.096]] [[0.15 ]
 [0.137]
 [0.185]
 [0.192]
 [0.099]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7620033
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.097]
 [-0.097]
 [-0.097]
 [-0.097]
 [-0.097]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.023908682929834647
siam score:  -0.7625676
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.76586515
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.005]
 [-0.038]
 [-0.038]
 [ 0.023]
 [-0.038]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7572011
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.021]
 [ 0.021]
 [ 0.021]
 [-0.022]
 [-0.013]] [[0.235]
 [0.235]
 [0.235]
 [0.205]
 [0.211]]
line 256 mcts: sample exp_bonus -0.12276283684048404
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7570248
siam score:  -0.7576228
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.75223887
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.285]
 [ 0.285]
 [ 0.285]
 [ 0.285]
 [-0.   ]] [[0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.091]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.7476379
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7308499
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72413635
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7366598
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.809]
 [ 0.809]
 [ 0.809]
 [-0.122]
 [-0.019]] [[0.93 ]
 [0.93 ]
 [0.93 ]
 [0.   ]
 [0.103]]
siam score:  -0.74221647
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7411041
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.004]] [[0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.013]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.003]
 [ 0.007]
 [-0.003]
 [-0.001]
 [ 0.004]] [[0.166]
 [0.169]
 [0.162]
 [0.163]
 [0.166]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.295]
 [0.164]
 [0.183]
 [0.188]
 [0.175]] [[0.268]
 [0.17 ]
 [0.184]
 [0.188]
 [0.177]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 0.14696979533519813
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.08080907913090385
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]] [[ 0.004]
 [-0.14 ]
 [ 0.11 ]
 [ 0.072]
 [ 0.084]] [[0.144]
 [0.   ]
 [0.249]
 [0.212]
 [0.223]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.066]
 [-0.206]
 [-0.042]
 [-0.047]
 [-0.039]] [[0.105]
 [0.   ]
 [0.123]
 [0.119]
 [0.125]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.072]
 [0.072]
 [0.45 ]
 [0.072]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.104]
 [0.014]
 [0.124]
 [0.105]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [-0.112]
 [-0.112]
 [-0.112]
 [ 0.019]] [[0.112]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.132]]
siam score:  -0.75179625
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]] [[-0.134]
 [-0.142]
 [-0.077]
 [-0.024]
 [-0.119]] [[0.048]
 [0.046]
 [0.061]
 [0.075]
 [0.052]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus -0.10736955994754485
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
UNIT TEST: sample policy line 217 mcts : [0.083 0.583 0.042 0.25  0.042]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.158]
 [-0.183]
 [-0.123]
 [-0.136]
 [-0.197]] [[0.104]
 [0.091]
 [0.121]
 [0.115]
 [0.084]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.096]
 [-0.173]
 [-0.101]
 [-0.144]
 [-0.124]] [[0.096]
 [0.038]
 [0.093]
 [0.06 ]
 [0.075]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.152]
 [-0.022]
 [-0.014]
 [ 0.004]
 [ 0.033]] [[0.228]
 [0.098]
 [0.103]
 [0.117]
 [0.139]]
deleting a thread, now have 1 threads
Frames:  77167 train batches done:  9038.0 episodes:  6814
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.736615
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7340448
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.017]
 [ 0.282]
 [ 0.282]
 [ 0.282]
 [ 0.282]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.02]
 [0.02]
 [0.02]
 [0.02]
 [0.02]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.257]
 [-0.257]
 [-0.257]
 [-0.257]
 [-0.051]] [[0.064]
 [0.064]
 [0.064]
 [0.064]
 [0.116]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7315891
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.73649496
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.047]
 [-0.066]
 [-0.083]
 [ 0.008]
 [-0.04 ]] [[0.038]
 [0.033]
 [0.028]
 [0.051]
 [0.039]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.034]
 [-0.071]
 [-0.027]
 [-0.005]
 [-0.052]] [[0.162]
 [0.135]
 [0.168]
 [0.184]
 [0.149]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.077]
 [0.02 ]
 [0.02 ]
 [0.001]
 [0.02 ]] [[0.104]
 [0.047]
 [0.047]
 [0.027]
 [0.047]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.71281976
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7307869
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.005]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7343283
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.049]
 [-0.298]
 [-0.213]
 [-0.084]
 [-0.224]] [[0.337]
 [0.088]
 [0.172]
 [0.302]
 [0.162]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.043]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]] [[0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7211642
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.004]
 [ 0.002]
 [ 0.027]
 [ 0.004]
 [-0.003]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.196]
 [0.196]
 [0.196]
 [0.014]
 [0.051]] [[0.31 ]
 [0.31 ]
 [0.31 ]
 [0.128]
 [0.165]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.711889
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7111771
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.70917374
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.004]
 [ 0.004]
 [ 0.004]
 [-0.037]
 [ 0.004]] [[0.015]
 [0.015]
 [0.015]
 [0.004]
 [0.015]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7086145
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.07905165394956704
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.013]
 [ 0.005]
 [ 0.018]
 [-0.034]
 [-0.028]] [[0.022]
 [0.026]
 [0.029]
 [0.016]
 [0.018]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.141]
 [0.142]
 [0.102]
 [0.07 ]
 [0.015]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.67604554
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.015]
 [-0.116]
 [-0.001]
 [-0.019]
 [ 0.001]] [[0.036]
 [0.01 ]
 [0.039]
 [0.035]
 [0.04 ]]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.001]
 [ 0.032]
 [-0.004]
 [ 0.049]
 [ 0.146]] [[0.002]
 [0.01 ]
 [0.001]
 [0.014]
 [0.039]]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.66867316
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.6991733
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7137031
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [-0.025]
 [-0.134]
 [-0.101]
 [-0.013]] [[0.231]
 [0.212]
 [0.131]
 [0.155]
 [0.221]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.131]
 [-0.292]
 [-0.216]
 [-0.105]
 [-0.103]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.124]
 [-0.347]
 [-0.149]
 [-0.154]
 [-0.14 ]] [[0.056]
 [0.   ]
 [0.049]
 [0.048]
 [0.052]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.07865518413569846
using explorer policy with actor:  1
first move QE:  -0.07865166037313735
siam score:  -0.71718246
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.06833585619754204
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.25  0.208 0.125 0.292 0.125]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7162086
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.003]
 [ 0.003]
 [-0.053]
 [-0.001]
 [-0.019]] [[0.121]
 [0.121]
 [0.093]
 [0.119]
 [0.11 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.70129853
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.009]
 [-0.099]
 [-0.054]
 [ 0.011]
 [ 0.024]] [[0.023]
 [0.   ]
 [0.011]
 [0.028]
 [0.031]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7032826
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.007]
 [-0.384]
 [ 0.007]
 [-0.04 ]
 [-0.001]] [[0.293]
 [0.   ]
 [0.293]
 [0.258]
 [0.288]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.70807654
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7054387
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.007137392883187916
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.71316564
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
first move QE:  -0.07769631218169434
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20016
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
first move QE:  -0.07782141829607443
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
start point for exploration sampling:  20016
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
first move QE:  -0.07785510188530875
UNIT TEST: sample policy line 217 mcts : [0.125 0.417 0.167 0.125 0.167]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
siam score:  -0.70383024
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
line 256 mcts: sample exp_bonus 0.09741144489312281
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.035]
 [-0.034]
 [-0.034]
 [-0.034]
 [-0.04 ]] [[0.032]
 [0.033]
 [0.033]
 [0.033]
 [0.027]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
siam score:  -0.69352585
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
siam score:  -0.6927782
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
siam score:  -0.7162281
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.002]
 [0.006]
 [0.009]
 [0.004]] [[1.497]
 [1.469]
 [1.487]
 [1.481]
 [1.497]] [[0.595]
 [0.566]
 [0.583]
 [0.58 ]
 [0.588]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.002]] [[1.293]
 [0.147]
 [0.147]
 [0.147]
 [0.463]] [[ 0.837]
 [-0.022]
 [-0.022]
 [-0.022]
 [ 0.216]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
first move QE:  -0.07682466168625122
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
siam score:  -0.71442205
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.08]
 [-0.08]
 [-0.08]
 [-0.08]
 [-0.08]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.108]
 [-0.108]
 [-0.108]
 [-0.108]
 [-0.108]] [[0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.066]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.125]] [[0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
siam score:  -0.7244598
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
using explorer policy with actor:  1
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]] [[0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.023]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.083]] [[0.134]
 [0.134]
 [0.134]
 [0.134]
 [0.089]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
siam score:  -0.73318565
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
using explorer policy with actor:  1
siam score:  -0.7364012
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0001],
        [    0.0001],
        [    0.0001],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0001]], dtype=torch.float64)
0.0 0.0
0.0 5.20087242017822e-05
0.0 0.00011567220853717997
0.0 8.042619691865394e-05
0.0 0.0
0.0 0.0
0.970299 0.970299
0.0 4.1423091857848205e-05
0.0 0.0
0.0 5.730244090158293e-05
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
siam score:  -0.72098327
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.07539301075159079
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7441031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
line 256 mcts: sample exp_bonus -0.3129437714077928
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[-0.199]
 [-0.216]
 [-0.191]
 [ 0.   ]
 [ 0.   ]] [[0.018]
 [0.   ]
 [0.026]
 [0.216]
 [0.216]]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.014]
 [0.02 ]
 [0.001]
 [0.004]] [[0.618]
 [0.183]
 [0.69 ]
 [0.394]
 [0.123]] [[0.406]
 [0.119]
 [0.467]
 [0.25 ]
 [0.07 ]]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
first move QE:  -0.0756203290051867
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
siam score:  -0.7764558
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
first move QE:  -0.07544097056067726
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
siam score:  -0.7793534
siam score:  -0.77749056
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
siam score:  -0.7747068
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
siam score:  -0.77394485
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]] [[ 0.013]
 [-0.099]
 [-0.099]
 [-0.074]
 [-0.099]] [[0.001]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
siam score:  -0.77068174
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.04 ]
 [-0.306]
 [-0.133]
 [-0.03 ]
 [ 0.009]] [[0.087]
 [0.   ]
 [0.043]
 [0.069]
 [0.079]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.009]
 [0.001]
 [0.001]
 [0.002]] [[-0.007]
 [ 0.013]
 [-0.008]
 [-0.004]
 [-0.021]] [[0.031]
 [0.054]
 [0.031]
 [0.033]
 [0.021]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
first move QE:  -0.07576823975466297
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.002]
 [0.001]
 [0.001]] [[-0.135]
 [-0.208]
 [-0.178]
 [-0.182]
 [-0.089]] [[0.056]
 [0.   ]
 [0.024]
 [0.019]
 [0.09 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.07586544344967913
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7710414
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
siam score:  -0.75758165
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
siam score:  -0.7563063
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.348]
 [0.03 ]
 [0.272]
 [0.244]
 [0.259]] [[0.261]
 [0.023]
 [0.205]
 [0.183]
 [0.195]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
siam score:  -0.7803734
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.042]
 [-0.035]
 [-0.014]
 [-0.035]
 [-0.057]] [[0.056]
 [0.061]
 [0.077]
 [0.061]
 [0.045]]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]] [[0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0041],
        [0.0015],
        [0.0199],
        [0.0000],
        [0.0258],
        [0.0273],
        [0.0000],
        [0.0000],
        [0.0015],
        [0.0000]], dtype=torch.float64)
0.0 0.004069404880259582
0.0 0.001450957341822339
0.0 0.01994967443735155
0.0 0.0
0.0 0.025843617634036975
0.0 0.02728772401552203
0.0 0.0
0.0 0.0
0.0 0.0014850225178041042
0.0 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[-0.137]
 [-0.024]
 [-0.238]
 [-0.23 ]
 [-0.202]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
first move QE:  -0.0751693397115433
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
UNIT TEST: sample policy line 217 mcts : [0.208 0.042 0.125 0.167 0.458]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
siam score:  -0.7982531
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
siam score:  -0.7929455
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.031]
 [-0.172]
 [-0.058]
 [-0.07 ]
 [-0.059]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
siam score:  -0.78617156
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
first move QE:  -0.07508199203338788
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.77017564
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
line 256 mcts: sample exp_bonus -0.050510696341933496
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
siam score:  -0.793296
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.328]
 [0.001]
 [0.001]] [[-0.024]
 [-0.   ]
 [-0.01 ]
 [-0.   ]
 [ 0.   ]] [[0.05 ]
 [0.068]
 [0.388]
 [0.067]
 [0.068]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
siam score:  -0.8114668
from probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.12504498530497532, 0.12504498530497532, 0.6248650440850739, 0.12504498530497532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0]
 [0]
 [0]
 [0]
 [0]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.78794914
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7707583
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.07442341403768647
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.023984627810038156
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.78306395
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.08]
 [0.08]
 [0.08]
 [0.08]
 [0.08]] [[0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0000],
        [0.0006],
        [0.0000],
        [0.0005],
        [0.0010],
        [0.0323],
        [0.0000],
        [0.0148],
        [0.0030]], dtype=torch.float64)
0.0 0.0
0.99 0.99
0.0 0.0005547296287427551
0.0 0.0
0.0 0.0005463998776507239
0.0 0.0009909122649897702
0.0 0.03232276401487867
0.0 0.0
0.0 0.014792217124307883
0.0 0.0030314005635489515
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7988994
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.80676764
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.118]
 [-0.114]
 [-0.252]
 [-0.246]
 [-0.158]] [[0.034]
 [0.035]
 [0.001]
 [0.002]
 [0.024]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
line 256 mcts: sample exp_bonus -0.03146225757886374
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.002]
 [0.002]
 [0.002]] [[0.   ]
 [0.   ]
 [0.27 ]
 [0.003]
 [0.081]] [[0.002]
 [0.002]
 [0.069]
 [0.002]
 [0.022]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7984139
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.8080221
siam score:  -0.80680466
siam score:  -0.80929244
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[-0.053]
 [-0.053]
 [-0.007]
 [-0.079]
 [-0.099]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.0738304536094755
siam score:  -0.78552383
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7780583
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]] [[0.177]
 [0.177]
 [0.177]
 [0.177]
 [0.177]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7611448
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7714898
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.001]
 [0.001]
 [0.002]] [[-0.108]
 [-0.108]
 [ 0.001]
 [-0.029]
 [-0.108]] [[0.141]
 [0.141]
 [0.221]
 [0.2  ]
 [0.141]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8118384
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.001]
 [0.004]
 [0.001]] [[0.498]
 [0.508]
 [1.027]
 [0.242]
 [0.503]] [[0.339]
 [0.345]
 [0.695]
 [0.166]
 [0.34 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.09443558973388988
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.   ]
 [0.004]
 [0.004]
 [0.003]] [[ 0.033]
 [-0.012]
 [ 0.045]
 [ 0.014]
 [-0.03 ]] [[0.113]
 [0.066]
 [0.127]
 [0.096]
 [0.05 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.003]
 [0.005]
 [0.005]] [[-0.258]
 [-0.258]
 [ 0.   ]
 [-0.258]
 [-0.258]] [[0.002]
 [0.002]
 [0.258]
 [0.002]
 [0.002]]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.7929349
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.07388966145634734
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7750943
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.   ]
 [0.001]
 [0.001]] [[0.092]
 [0.414]
 [0.155]
 [0.111]
 [0.408]] [[0.023]
 [0.102]
 [0.038]
 [0.027]
 [0.101]]
siam score:  -0.7741757
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.004]
 [0.   ]
 [0.001]
 [0.001]] [[ 0.096]
 [-0.005]
 [-0.001]
 [ 0.003]
 [-0.011]] [[0.036]
 [0.013]
 [0.01 ]
 [0.012]
 [0.009]]
first move QE:  -0.07331278215566774
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.   ]
 [0.001]
 [0.   ]] [[0.354]
 [0.352]
 [0.333]
 [0.305]
 [0.31 ]] [[0.132]
 [0.131]
 [0.121]
 [0.107]
 [0.11 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.74301344
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.25  0.417 0.083 0.083 0.167]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.07317248121954807
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.274]
 [ 0.235]
 [ 0.011]
 [ 0.029]
 [-0.041]] [[0.079]
 [0.069]
 [0.013]
 [0.017]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.73130304
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7302927
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7272922
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.726893
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.018]
 [-0.011]
 [-0.012]
 [-0.023]
 [-0.017]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7284402
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.049]
 [-0.193]
 [-0.193]
 [-0.086]
 [-0.035]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.72316825
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.019856219685521048
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.7361313
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.71 ]
 [-0.031]
 [ 0.235]
 [ 0.098]
 [ 0.418]] [[0.531]
 [0.   ]
 [0.19 ]
 [0.092]
 [0.322]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.193]
 [0.028]
 [0.085]
 [0.031]
 [0.001]] [[0.12 ]
 [0.037]
 [0.066]
 [0.039]
 [0.024]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.115]
 [-0.054]
 [-0.187]
 [-0.136]
 [-0.116]] [[0.054]
 [0.1  ]
 [0.   ]
 [0.039]
 [0.054]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.003]
 [0.002]
 [0.002]
 [0.003]] [[0.589]
 [0.712]
 [0.796]
 [1.314]
 [0.574]] [[0.396]
 [0.48 ]
 [0.536]
 [0.884]
 [0.387]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0]
 [0]
 [0]
 [0]
 [0]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72755796
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]] [[0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20016
first move QE:  -0.0730991959323835
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.751972
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.162]
 [-0.162]
 [-0.162]
 [-0.162]
 [-0.162]] [[0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]]
first move QE:  -0.07313586189200696
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7544853
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.103]
 [-0.167]
 [-0.166]
 [-0.162]
 [-0.235]] [[0.048]
 [0.032]
 [0.033]
 [0.033]
 [0.015]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.75191015
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.001]
 [0.   ]] [[1.296]
 [1.296]
 [0.862]
 [0.568]
 [1.296]] [[0.869]
 [0.869]
 [0.575]
 [0.376]
 [0.869]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.003]
 [0.   ]
 [0.001]
 [0.001]] [[-0.11 ]
 [ 0.049]
 [ 0.121]
 [-0.088]
 [-0.07 ]] [[0.   ]
 [0.161]
 [0.23 ]
 [0.021]
 [0.039]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.   ]
 [0.001]
 [0.   ]] [[-0.027]
 [-0.027]
 [-0.027]
 [ 0.069]
 [-0.027]] [[0.006]
 [0.006]
 [0.006]
 [0.103]
 [0.006]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.   ]
 [0.001]
 [0.001]
 [0.002]] [[1.29 ]
 [1.242]
 [1.192]
 [1.223]
 [1.134]] [[0.003]
 [0.   ]
 [0.001]
 [0.001]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.07349604083754337
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.013]
 [0.017]
 [0.007]
 [0.012]] [[1.462]
 [1.403]
 [1.446]
 [1.442]
 [1.461]] [[0.38 ]
 [0.358]
 [0.384]
 [0.372]
 [0.386]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]] [[0.477]
 [0.301]
 [0.385]
 [0.495]
 [0.47 ]] [[0.189]
 [0.102]
 [0.144]
 [0.199]
 [0.186]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [-0.001]
 [-0.004]
 [ 0.011]
 [ 0.   ]] [[0.003]
 [0.002]
 [0.   ]
 [0.011]
 [0.003]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0013],
        [0.0013],
        [0.0016],
        [0.0016],
        [0.0054],
        [0.0037],
        [0.0000],
        [0.0019],
        [0.0016],
        [0.0896]], dtype=torch.float64)
0.0 0.0012744312109899065
0.0 0.0013066426895104563
0.0 0.0016086700275822535
0.0 0.0016253220610055758
0.0 0.0053644570509599684
0.0 0.0036940727901082883
0.0 0.0
0.0 0.001867800666558672
0.0 0.0016253220610055758
0.0 0.08955249617721489
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.002]
 [0.003]
 [0.003]] [[ 0.   ]
 [ 0.   ]
 [-0.006]
 [ 0.   ]
 [ 0.   ]] [[0.007]
 [0.007]
 [0.001]
 [0.007]
 [0.007]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.   ]
 [0.002]
 [0.002]
 [0.002]] [[-0.163]
 [-0.114]
 [-0.1  ]
 [-0.14 ]
 [-0.177]] [[0.144]
 [0.179]
 [0.191]
 [0.161]
 [0.133]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.21 ]
 [-0.229]
 [-0.205]
 [-0.14 ]
 [-0.206]] [[0.062]
 [0.057]
 [0.063]
 [0.08 ]
 [0.063]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.098]
 [-0.017]
 [-0.015]
 [-0.059]
 [-0.076]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.302]
 [-0.002]
 [ 0.108]
 [ 0.072]
 [-0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.101]
 [-0.101]
 [-0.101]
 [-0.388]
 [-0.101]] [[0.072]
 [0.072]
 [0.072]
 [0.   ]
 [0.072]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.19350710486127734
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.737837
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]] [[-0.139]
 [-0.171]
 [-0.14 ]
 [-0.057]
 [-0.172]] [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.07339490948940564
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.113]
 [-0.085]
 [-0.071]
 [-0.089]
 [-0.08 ]] [[0.081]
 [0.11 ]
 [0.124]
 [0.105]
 [0.114]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.265]
 [1.248]
 [1.248]
 [1.451]
 [0.939]] [[0.141]
 [0.135]
 [0.135]
 [0.186]
 [0.058]]
siam score:  -0.73794234
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.73900694
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.73747885
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]] [[1.499]
 [1.499]
 [1.499]
 [1.499]
 [1.499]] [[0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.   ]
 [0.001]
 [0.   ]] [[-0.048]
 [-0.123]
 [-0.141]
 [-0.129]
 [-0.096]] [[0.068]
 [0.03 ]
 [0.022]
 [0.028]
 [0.044]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.001]
 [0.   ]] [[-0.058]
 [ 0.   ]
 [-0.141]
 [ 0.   ]
 [-0.095]] [[0.062]
 [0.106]
 [0.   ]
 [0.106]
 [0.035]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.72989225
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.73290706
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.07353289899448413
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.74697053
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.07368523666653919
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7647139
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.07368886176288975
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.   ]
 [0.   ]
 [0.001]
 [0.001]] [[-0.001]
 [-0.   ]
 [ 0.026]
 [-0.002]
 [-0.004]] [[0.042]
 [0.04 ]
 [0.06 ]
 [0.04 ]
 [0.039]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.001]
 [0.001]
 [0.001]] [[-0.01 ]
 [-0.062]
 [-0.053]
 [-0.018]
 [-0.059]] [[0.091]
 [0.039]
 [0.048]
 [0.083]
 [0.042]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.105]
 [-0.117]
 [-0.108]
 [-0.121]
 [-0.042]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[ 0.045]
 [-0.168]
 [-0.024]
 [-0.106]
 [-0.084]] [[0.213]
 [0.   ]
 [0.144]
 [0.062]
 [0.084]]
UNIT TEST: sample policy line 217 mcts : [0.417 0.042 0.083 0.292 0.167]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]] [[0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.097]
 [0.097]
 [0.097]
 [0.097]
 [0.097]] [[0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.001]
 [-0.014]
 [-0.001]
 [-0.   ]
 [-0.058]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.76113725
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.10467470020124736
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.750924
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.74613374
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.015655042822179277
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.79327697
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7935472
siam score:  -0.79143834
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.004]
 [0.001]
 [0.002]
 [0.001]] [[-0.007]
 [-0.008]
 [-0.007]
 [ 0.   ]
 [-0.006]] [[0.001]
 [0.004]
 [0.001]
 [0.002]
 [0.001]]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.022]
 [0.009]
 [0.003]
 [0.009]] [[0.193]
 [0.079]
 [0.193]
 [0.173]
 [0.193]] [[0.009]
 [0.022]
 [0.009]
 [0.003]
 [0.009]]
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.063]
 [0.029]
 [0.029]
 [0.029]] [[0.436]
 [0.888]
 [0.436]
 [0.436]
 [0.436]] [[0.029]
 [0.063]
 [0.029]
 [0.029]
 [0.029]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.112]
 [-0.05 ]
 [-0.05 ]
 [-0.188]
 [-0.05 ]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8000757
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.06775567365618038
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.06793235809779913
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.06794510998586636
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7855939
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.065]
 [-0.059]
 [-0.044]
 [-0.055]
 [-0.048]] [[0.104]
 [0.109]
 [0.124]
 [0.113]
 [0.121]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.013]
 [0.013]
 [0.013]
 [0.   ]] [[0.159]
 [0.303]
 [0.303]
 [0.303]
 [0.074]] [[0.062]
 [0.147]
 [0.147]
 [0.147]
 [0.019]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.057]
 [-0.106]
 [-0.065]
 [-0.021]
 [-0.04 ]] [[0.086]
 [0.049]
 [0.08 ]
 [0.113]
 [0.099]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.   ]
 [0.003]
 [0.001]
 [0.002]] [[0.92 ]
 [1.079]
 [0.718]
 [0.088]
 [0.601]] [[0.002]
 [0.   ]
 [0.003]
 [0.001]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]] [[ 0.019]
 [-0.047]
 [ 0.007]
 [ 0.056]
 [-0.002]] [[0.066]
 [0.017]
 [0.056]
 [0.094]
 [0.05 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.7527683
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.75234723
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.233]
 [-0.003]
 [-0.031]
 [-0.009]
 [ 0.004]] [[0.132]
 [0.014]
 [0.   ]
 [0.011]
 [0.018]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.07191047994177294
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.07145404520672734
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.201]
 [0.148]
 [0.182]
 [0.187]
 [0.107]] [[0.151]
 [0.111]
 [0.136]
 [0.141]
 [0.08 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
line 256 mcts: sample exp_bonus 0.14626152752903887
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.1162628833978509
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]] [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.001]
 [0.   ]] [[-0.009]
 [-0.073]
 [-0.036]
 [-0.011]
 [-0.057]] [[0.047]
 [0.015]
 [0.034]
 [0.047]
 [0.023]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]] [[ 0.   ]
 [ 0.   ]
 [-0.057]
 [-0.056]
 [-0.059]] [[0.03 ]
 [0.03 ]
 [0.001]
 [0.002]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.194]
 [ 0.004]
 [ 0.151]
 [-0.   ]
 [ 0.075]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.   ]
 [0.001]
 [0.   ]] [[0.646]
 [0.004]
 [0.004]
 [0.334]
 [0.004]] [[0.485]
 [0.003]
 [0.003]
 [0.251]
 [0.003]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.07067173504982543
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72371066
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[-0.039]
 [-0.021]
 [-0.047]
 [-0.068]
 [ 0.   ]] [[0.015]
 [0.023]
 [0.011]
 [0.   ]
 [0.034]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]] [[0.041]
 [0.041]
 [0.041]
 [0.041]
 [0.041]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7241362
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.005]
 [0.024]
 [0.015]
 [0.025]] [[0.01 ]
 [0.109]
 [0.141]
 [0.028]
 [0.004]] [[0.029]
 [0.061]
 [0.096]
 [0.031]
 [0.029]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.82769316
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.001]
 [0.008]
 [0.01 ]
 [0.01 ]] [[-0.032]
 [-0.025]
 [-0.001]
 [-0.021]
 [-0.   ]] [[0.037]
 [0.026]
 [0.04 ]
 [0.037]
 [0.042]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.333 0.125 0.083 0.333 0.125]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.024]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]] [[0.176]
 [0.176]
 [0.176]
 [0.176]
 [0.176]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8257522
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.001]] [[ 0.064]
 [ 0.061]
 [-0.026]
 [-0.024]
 [-0.007]] [[0.001]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.07057657688805491
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20016
siam score:  -0.8131889
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.07058802420971008
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [-0.048]] [[0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7465474
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.07059147202129007
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.005]
 [0.004]
 [0.003]] [[1.092]
 [1.092]
 [1.196]
 [1.175]
 [1.092]] [[0.235]
 [0.235]
 [0.289]
 [0.277]
 [0.235]]
siam score:  -0.73840696
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]] [[0.325]
 [0.031]
 [0.008]
 [0.056]
 [0.004]] [[0.242]
 [0.021]
 [0.003]
 [0.04 ]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7325154
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.001]] [[-0.093]
 [-0.063]
 [-0.027]
 [-0.049]
 [-0.068]] [[0.   ]
 [0.009]
 [0.016]
 [0.011]
 [0.007]]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.   ]] [[-0.188]
 [-0.133]
 [-0.167]
 [-0.198]
 [-0.148]] [[0.022]
 [0.035]
 [0.027]
 [0.02 ]
 [0.032]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7435389
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[ 0.004]
 [ 0.004]
 [ 0.004]
 [-0.   ]
 [ 0.004]] [[0.005]
 [0.005]
 [0.005]
 [0.003]
 [0.005]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.73736113
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7475858
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.221]
 [-0.145]
 [-0.175]
 [-0.174]
 [-0.227]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.746206
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.7418199
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
start point for exploration sampling:  20016
siam score:  -0.74289244
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.06963431364309117
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[-0.005]
 [-0.002]
 [ 0.012]
 [-0.018]
 [-0.   ]] [[0.044]
 [0.046]
 [0.057]
 [0.035]
 [0.048]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.001]
 [0.   ]
 [0.002]
 [0.002]] [[-0.045]
 [-0.062]
 [-0.047]
 [-0.059]
 [-0.169]] [[0.107]
 [0.098]
 [0.105]
 [0.1  ]
 [0.045]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7391291
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.002]
 [0.001]] [[-0.072]
 [-0.013]
 [-0.018]
 [-0.047]
 [-0.019]] [[0.02 ]
 [0.05 ]
 [0.047]
 [0.034]
 [0.047]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7454662
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.06904804179965096
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [1.5  ]
 [1.5  ]
 [1.5  ]
 [1.5  ]] [[-0.022]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.   ]
 [1.516]
 [1.516]
 [1.516]
 [1.516]]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.093]
 [-0.109]
 [-0.026]
 [-0.2  ]
 [-0.193]] [[0.097]
 [0.089]
 [0.13 ]
 [0.044]
 [0.047]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.06881180455026487
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7426512
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.7443257
siam score:  -0.7469867
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.74826247
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7469117
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.032]
 [-0.124]
 [-0.124]
 [-0.09 ]
 [-0.124]] [[0.079]
 [0.01 ]
 [0.01 ]
 [0.036]
 [0.01 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.06879727206744712
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.263]
 [0.028]
 [0.223]
 [0.185]
 [0.179]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7367093
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
siam score:  -0.74250776
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7322948
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7251154
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]] [[0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.195]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.71200323
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus 0.5599759401861676
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7141278
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72348344
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72313136
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.   ]
 [0.001]
 [0.   ]] [[-0.077]
 [-0.077]
 [-0.127]
 [-0.136]
 [-0.077]] [[0.055]
 [0.055]
 [0.043]
 [0.041]
 [0.055]]
siam score:  -0.7213585
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.001]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.038]] [[0.046]
 [0.037]
 [0.037]
 [0.037]
 [0.037]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.151]
 [-0.151]
 [-0.151]
 [-0.   ]
 [-0.151]] [[0.098]
 [0.098]
 [0.098]
 [0.25 ]
 [0.098]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[ 0.   ]
 [ 0.   ]
 [-0.121]
 [-0.177]
 [-0.172]] [[0.116]
 [0.116]
 [0.056]
 [0.028]
 [0.03 ]]
siam score:  -0.73111254
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7317412
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.114]
 [-0.114]
 [-0.183]
 [-0.186]
 [-0.114]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.096]
 [-0.14 ]
 [-0.14 ]
 [-0.14 ]
 [-0.04 ]] [[0.011]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.025]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.158]
 [-0.166]
 [-0.127]
 [-0.102]
 [-0.103]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7259152
Starting evaluation
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Sims:  25 1 epoch:  130093 pick best:  False frame count:  130093
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]] [[0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.016]
 [0.434]
 [0.434]
 [0.024]
 [0.434]] [[0.004]
 [0.109]
 [0.109]
 [0.006]
 [0.109]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[ 0.116]
 [-0.   ]
 [ 0.095]
 [-0.007]
 [-0.017]] [[0.038]
 [0.009]
 [0.033]
 [0.007]
 [0.005]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.021]
 [-0.095]
 [-0.111]
 [-0.002]
 [-0.034]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.292 0.208 0.125 0.167 0.208]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.70644003
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6926284
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]] [[0.969]
 [0.808]
 [0.696]
 [1.468]
 [0.325]] [[0.001]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7188898
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7265096
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7156184
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7162514
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.06464268892184809
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.06464471668656241
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.71886307
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.72080034
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.73111653
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.234]
 [-0.234]
 [-0.234]
 [-0.227]
 [-0.234]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
from probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
siam score:  -0.7308204
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
first move QE:  -0.0646827014883626
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
first move QE:  -0.0646504716956403
first move QE:  -0.0646504716956403
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.073]
 [-0.085]
 [-0.007]
 [-0.005]
 [ 0.   ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
from probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
line 256 mcts: sample exp_bonus -0.000531991443131119
from probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
from probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.012]
 [0.004]
 [0.002]
 [0.004]] [[1.269]
 [1.074]
 [0.819]
 [0.645]
 [0.71 ]] [[0.219]
 [0.182]
 [0.11 ]
 [0.064]
 [0.083]]
from probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.0637004313722909
siam score:  -0.7545811
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.066]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
from probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
siam score:  -0.75467384
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
using explorer policy with actor:  1
from probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
from probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.001]
 [0.   ]
 [0.   ]] [[ 0.096]
 [ 0.728]
 [ 0.039]
 [-0.002]
 [ 0.053]] [[0.025]
 [0.184]
 [0.012]
 [0.001]
 [0.014]]
from probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.006]
 [0.003]
 [0.003]] [[0.143]
 [0.143]
 [0.481]
 [0.143]
 [0.143]] [[0.11 ]
 [0.11 ]
 [0.363]
 [0.11 ]
 [0.11 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
siam score:  -0.74624914
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.223]
 [-0.223]
 [-0.146]
 [-0.163]
 [-0.223]] [[0.012]
 [0.012]
 [0.031]
 [0.027]
 [0.012]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
siam score:  -0.73305285
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[1.319]
 [1.319]
 [1.319]
 [1.319]
 [1.319]] [[0.52]
 [0.52]
 [0.52]
 [0.52]
 [0.52]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.   ]
 [0.001]
 [0.001]] [[-0.04 ]
 [-0.02 ]
 [-0.032]
 [ 0.05 ]
 [-0.033]] [[0.019]
 [0.024]
 [0.021]
 [0.042]
 [0.021]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
from probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
from probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.352]
 [0.229]
 [0.233]
 [0.012]
 [0.012]] [[0.399]
 [0.276]
 [0.28 ]
 [0.059]
 [0.059]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
first move QE:  -0.06309860033169448
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
using another actor
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.   ]
 [0.001]] [[-0.315]
 [-0.385]
 [-0.105]
 [-0.104]
 [-0.21 ]] [[0.018]
 [0.   ]
 [0.07 ]
 [0.07 ]
 [0.044]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
siam score:  -0.7234981
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
from probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.045]
 [ 0.   ]
 [ 0.   ]
 [-0.093]
 [-0.094]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.09 ]
 [-0.077]
 [-0.212]
 [-0.103]
 [-0.249]] [[0.12 ]
 [0.13 ]
 [0.028]
 [0.11 ]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.47]
 [0.47]
 [0.47]
 [0.47]
 [0.47]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
from probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[0.026]
 [0.026]
 [0.026]
 [0.026]
 [0.026]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
siam score:  -0.72775847
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
using another actor
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.189]
 [-0.189]
 [-0.189]
 [-0.189]
 [-0.189]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]] [[-0.04 ]
 [-0.02 ]
 [ 0.105]
 [ 0.06 ]
 [-0.054]] [[0.019]
 [0.024]
 [0.055]
 [0.044]
 [0.015]]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
first move QE:  -0.06123992529687369
siam score:  -0.7171088
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
from probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
from probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
siam score:  -0.7127118
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]
 [0.001]] [[-0.248]
 [-0.296]
 [-0.334]
 [-0.318]
 [-0.136]] [[0.201]
 [0.165]
 [0.137]
 [0.149]
 [0.285]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
siam score:  -0.71329206
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
from probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
using another actor
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
from probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
siam score:  -0.7209757
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
siam score:  -0.71694344
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
from probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
from probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
from probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
siam score:  -0.7229582
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[-0.056]
 [ 0.   ]
 [-0.082]
 [-0.079]
 [-0.057]] [[0.014]
 [0.028]
 [0.008]
 [0.009]
 [0.014]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
siam score:  -0.7209205
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
siam score:  -0.7199164
siam score:  -0.7217232
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
from probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.6248650440850742, 0.12504498530497535, 0.12504498530497535, 0.12504498530497535]
actor:  0 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
siam score:  -0.7215982
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [-0.026]
 [ 0.   ]
 [-0.07 ]
 [ 0.002]] [[0.021]
 [0.014]
 [0.021]
 [0.003]
 [0.021]]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
from probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
siam score:  -0.71415895
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
using another actor
siam score:  -0.72786486
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
from probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
from probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.048]
 [ 0.   ]
 [ 0.   ]
 [-0.064]
 [-0.088]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
UNIT TEST: sample policy line 217 mcts : [0.125 0.542 0.125 0.125 0.083]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
from probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
from probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
siam score:  -0.76677704
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]] [[0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
siam score:  -0.77239144
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
from probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
using another actor
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
line 256 mcts: sample exp_bonus -0.028138607919710748
siam score:  -0.7823897
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
siam score:  -0.782587
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.045]
 [0.069]
 [0.045]
 [0.03 ]] [[0.922]
 [0.922]
 [0.051]
 [0.922]
 [0.038]] [[0.045]
 [0.045]
 [0.069]
 [0.045]
 [0.03 ]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
from probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
UNIT TEST: sample policy line 217 mcts : [0.417 0.125 0.125 0.208 0.125]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.115]
 [-0.115]
 [-0.115]
 [-0.115]
 [-0.115]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
from probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
siam score:  -0.772817
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[-0.03]
 [-0.03]
 [-0.03]
 [-0.03]
 [-0.03]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
using another actor
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
from probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.119]
 [-0.119]
 [-0.119]
 [-0.119]
 [-0.119]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
siam score:  -0.76913786
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
siam score:  -0.7775728
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.   ]
 [0.003]
 [0.003]
 [0.002]] [[-0.064]
 [-0.06 ]
 [-0.072]
 [-0.078]
 [-0.047]] [[0.011]
 [0.009]
 [0.009]
 [0.008]
 [0.015]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
using another actor
from probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
using another actor
from probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
from probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
siam score:  -0.820428
from probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
using explorer policy with actor:  1
siam score:  -0.8444863
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.6248626195995207, 0.1250457934668264, 0.1250457934668264, 0.1250457934668264]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.0593836286653946
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.83057946
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.006]
 [0.   ]
 [0.004]
 [0.005]] [[-0.001]
 [-0.003]
 [ 0.023]
 [ 0.02 ]
 [ 0.027]] [[0.   ]
 [0.006]
 [0.006]
 [0.009]
 [0.012]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.82610494
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.05 0.05
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.001]
 [0.003]
 [0.003]] [[ 0.   ]
 [ 0.   ]
 [-0.015]
 [ 0.   ]
 [-0.039]] [[0.02 ]
 [0.02 ]
 [0.011]
 [0.02 ]
 [0.001]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.7907617
siam score:  -0.7878638
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.012]
 [0.003]
 [0.003]
 [0.003]] [[0.055]
 [0.106]
 [0.055]
 [0.055]
 [0.055]] [[0.012]
 [0.034]
 [0.012]
 [0.012]
 [0.012]]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.0021 0.05 0.05
siam score:  -0.7893881
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.0021 0.05 0.05
from probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
using another actor
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.   ]
 [0.004]
 [0.002]
 [0.003]] [[ 0.004]
 [-0.008]
 [-0.018]
 [ 0.004]
 [-0.03 ]] [[0.018]
 [0.01 ]
 [0.008]
 [0.018]
 [0.001]]
first move QE:  -0.05894935100648096
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
from probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
siam score:  -0.801599
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
using explorer policy with actor:  1
from probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
from probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
using another actor
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
from probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.   ]
 [0.   ]
 [0.001]
 [0.001]] [[0.285]
 [0.069]
 [0.093]
 [0.157]
 [0.157]] [[0.222]
 [0.058]
 [0.077]
 [0.125]
 [0.125]]
using another actor
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.002]] [[0.132]
 [0.237]
 [0.662]
 [0.432]
 [0.1  ]] [[0.1  ]
 [0.179]
 [0.499]
 [0.326]
 [0.076]]
maxi score, test score, baseline:  0.0021 0.05 0.05
siam score:  -0.8056234
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
siam score:  -0.8089983
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[-0.036]
 [-0.036]
 [-0.036]
 [-0.119]
 [-0.036]] [[0.064]
 [0.064]
 [0.064]
 [0.001]
 [0.064]]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
from probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.01 ]
 [0.007]
 [0.008]
 [0.008]] [[-0.084]
 [-0.083]
 [-0.115]
 [-0.12 ]
 [-0.122]] [[0.016]
 [0.02 ]
 [0.009]
 [0.008]
 [0.008]]
using another actor
from probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
from probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
from probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
from probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
start point for exploration sampling:  20016
using another actor
from probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
siam score:  -0.83515257
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
from probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
line 256 mcts: sample exp_bonus 0.021335603632704647
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
line 256 mcts: sample exp_bonus -0.1628651266506291
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
using another actor
siam score:  -0.8284466
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
siam score:  -0.8363001
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
actor:  0 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
using another actor
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
from probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
from probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0041 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[0.112]
 [0.112]
 [0.112]
 [0.112]
 [0.112]] [[0.05]
 [0.05]
 [0.05]
 [0.05]
 [0.05]]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.001]
 [0.003]
 [0.005]
 [0.003]] [[0.557]
 [0.347]
 [0.68 ]
 [0.802]
 [0.68 ]] [[0.298]
 [0.137]
 [0.39 ]
 [0.483]
 [0.39 ]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
from probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504579346682643, 0.12504579346682643, 0.6248626195995208, 0.12504579346682643]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
using another actor
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
from probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
using another actor
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
from probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[-0.024]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]] [[0.1]
 [0.1]
 [0.1]
 [0.1]
 [0.1]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.01 ]
 [0.001]
 [0.   ]] [[0.007]
 [0.006]
 [0.   ]
 [0.036]
 [0.042]] [[0.009]
 [0.009]
 [0.014]
 [0.032]
 [0.036]]
from probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
using another actor
siam score:  -0.8627895
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
siam score:  -0.8643549
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504164124808273, 0.12504164124808273, 0.6248750762557519, 0.12504164124808273]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504025711384392, 0.12504025711384392, 0.6248792286584682, 0.12504025711384392]
from probs:  [0.12504025711384392, 0.12504025711384392, 0.6248792286584682, 0.12504025711384392]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504025711384392, 0.12504025711384392, 0.6248792286584682, 0.12504025711384392]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504025711384392, 0.12504025711384392, 0.6248792286584682, 0.12504025711384392]
maxi score, test score, baseline:  0.0041 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.013]
 [0.   ]
 [0.007]
 [0.006]] [[-0.078]
 [-0.12 ]
 [-0.033]
 [-0.204]
 [-0.12 ]] [[0.097]
 [0.071]
 [0.124]
 [0.002]
 [0.064]]
siam score:  -0.8706644
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504025711384392, 0.12504025711384392, 0.6248792286584682, 0.12504025711384392]
maxi score, test score, baseline:  0.0041 0.05 0.05
siam score:  -0.8706973
Starting evaluation
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504025711384392, 0.12504025711384392, 0.6248792286584682, 0.12504025711384392]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504025711384392, 0.12504025711384392, 0.6248792286584682, 0.12504025711384392]
using another actor
using another actor
from probs:  [0.12504025711384392, 0.12504025711384392, 0.6248792286584682, 0.12504025711384392]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504025711384392, 0.12504025711384392, 0.6248792286584682, 0.12504025711384392]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.12504025711384392, 0.12504025711384392, 0.6248792286584682, 0.12504025711384392]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
from probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
from probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
from probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
using another actor
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
from probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
from probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
from probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
UNIT TEST: sample policy line 217 mcts : [0.25  0.125 0.125 0.333 0.167]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]] [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]]
siam score:  -0.8809759
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
using another actor
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
siam score:  -0.89221984
siam score:  -0.89291877
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
from probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.029]
 [0.029]
 [0.029]
 [0.029]] [[-0.121]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]] [[0.029]
 [0.029]
 [0.029]
 [0.029]
 [0.029]]
from probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
siam score:  -0.883471
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
using another actor
from probs:  [0.12504000781766425, 0.12504000781766425, 0.6248799765470072, 0.12504000781766425]
siam score:  -0.886192
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.12504001796575873, 0.12504001796575873, 0.6248799461027238, 0.12504001796575873]
actor:  0 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.12504002817529924, 0.12504002817529924, 0.6248799154741023, 0.12504002817529924]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.12504002817529924, 0.12504002817529924, 0.6248799154741023, 0.12504002817529924]
start point for exploration sampling:  20016
from probs:  [0.12504002817529924, 0.12504002817529924, 0.6248799154741023, 0.12504002817529924]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.12504002817529924, 0.12504002817529924, 0.6248799154741023, 0.12504002817529924]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.12504002817529924, 0.12504002817529924, 0.6248799154741023, 0.12504002817529924]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.12504002817529924, 0.12504002817529924, 0.6248799154741023, 0.12504002817529924]
from probs:  [0.12504002817529924, 0.12504002817529924, 0.6248799154741023, 0.12504002817529924]
maxi score, test score, baseline:  0.0081 0.0 0.0081
start point for exploration sampling:  20016
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.10719430733851257, 0.25000000000000006, 0.535611385322975, 0.10719430733851257]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.10719430733851257, 0.25000000000000006, 0.535611385322975, 0.10719430733851257]
siam score:  -0.8976691
from probs:  [0.10719430733851257, 0.25000000000000006, 0.535611385322975, 0.10719430733851257]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.10719430733851257, 0.25000000000000006, 0.535611385322975, 0.10719430733851257]
Printing some Q and Qe and total Qs values:  [[0.04 ]
 [0.102]
 [0.001]
 [0.04 ]
 [0.04 ]] [[-0.091]
 [-0.036]
 [-0.068]
 [-0.091]
 [-0.091]] [[0.039]
 [0.129]
 [0.011]
 [0.039]
 [0.039]]
maxi score, test score, baseline:  0.0081 0.0 0.0081
Printing some Q and Qe and total Qs values:  [[0.121]
 [0.465]
 [0.065]
 [0.093]
 [0.093]] [[ 0.004]
 [ 0.003]
 [ 0.001]
 [-0.002]
 [-0.002]] [[0.123]
 [0.465]
 [0.064]
 [0.091]
 [0.091]]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.10719430733851257, 0.25000000000000006, 0.535611385322975, 0.10719430733851257]
UNIT TEST: sample policy line 217 mcts : [0.083 0.083 0.417 0.208 0.208]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.10719430733851257, 0.25000000000000006, 0.535611385322975, 0.10719430733851257]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.10719430733851257, 0.25000000000000006, 0.535611385322975, 0.10719430733851257]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]] [[-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]] [[0.061]
 [0.061]
 [0.061]
 [0.061]
 [0.061]]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.10719430733851257, 0.25000000000000006, 0.535611385322975, 0.10719430733851257]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.10719430733851257, 0.25000000000000006, 0.535611385322975, 0.10719430733851257]
siam score:  -0.8865974
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.10719430733851257, 0.25000000000000006, 0.535611385322975, 0.10719430733851257]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.10719430733851257, 0.25000000000000006, 0.535611385322975, 0.10719430733851257]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.10719430733851257, 0.25000000000000006, 0.535611385322975, 0.10719430733851257]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.017]
 [0.017]
 [0.015]
 [0.017]] [[-0.026]
 [-0.026]
 [-0.026]
 [-0.114]
 [-0.026]] [[0.126]
 [0.126]
 [0.126]
 [0.059]
 [0.126]]
from probs:  [0.10719430733851257, 0.25000000000000006, 0.535611385322975, 0.10719430733851257]
siam score:  -0.87210244
from probs:  [0.10719430733851257, 0.25000000000000006, 0.535611385322975, 0.10719430733851257]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.10719430733851257, 0.25000000000000006, 0.535611385322975, 0.10719430733851257]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0000],
        [0.0110],
        [0.0078],
        [0.0071],
        [0.0039],
        [0.4795],
        [0.2573],
        [0.0000],
        [0.0074]], dtype=torch.float64)
0.0 0.0
0.9509900498999999 0.9509900498999999
0.0 0.01095248474811872
0.0 0.007805763507391352
0.0 0.0071267408089420795
0.0 0.0038621220150921623
0.0 0.4795046695397477
0.0 0.2572781638652895
0.970299 0.970299
0.0 0.007366233667475208
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.10719430733851257, 0.25000000000000006, 0.535611385322975, 0.10719430733851257]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  20016
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.   ]
 [0.012]
 [0.011]] [[-0.1  ]
 [-0.078]
 [-0.066]
 [-0.099]
 [-0.114]] [[0.057]
 [0.073]
 [0.07 ]
 [0.056]
 [0.045]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.012]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.09381095214877219, 0.3437134287107367, 0.46866466699171894, 0.09381095214877219]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.09381095214877219, 0.3437134287107367, 0.46866466699171894, 0.09381095214877219]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0081 0.0 0.0081
line 256 mcts: sample exp_bonus -0.05876350565248922
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.08340223017196996, 0.41659776982803004, 0.41659776982803004, 0.08340223017196996]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.08340223017196996, 0.41659776982803004, 0.41659776982803004, 0.08340223017196996]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.08340223017196996, 0.41659776982803004, 0.41659776982803004, 0.08340223017196996]
from probs:  [0.08340223017196996, 0.41659776982803004, 0.41659776982803004, 0.08340223017196996]
maxi score, test score, baseline:  0.0081 0.0 0.0081
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
first move QE:  -0.05750508964672623
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.09097157415891478, 0.4544651189385382, 0.3635917327436323, 0.09097157415891478]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.09097157415891478, 0.4544651189385382, 0.3635917327436323, 0.09097157415891478]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.008]
 [0.011]
 [0.009]
 [0.009]] [[-0.11 ]
 [-0.12 ]
 [-0.154]
 [-0.152]
 [-0.157]] [[0.071]
 [0.065]
 [0.051]
 [0.05 ]
 [0.048]]
maxi score, test score, baseline:  0.0081 0.0 0.0081
from probs:  [0.09097157415891478, 0.4544651189385382, 0.3635917327436323, 0.09097157415891478]
using another actor
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.09097157415891478, 0.4544651189385382, 0.3635917327436323, 0.09097157415891478]
from probs:  [0.09097157415891478, 0.4544651189385382, 0.3635917327436323, 0.09097157415891478]
maxi score, test score, baseline:  0.0081 0.0 0.0081
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.015]
 [0.   ]
 [0.015]
 [0.007]] [[-0.064]
 [ 0.   ]
 [-0.085]
 [ 0.   ]
 [-0.051]] [[0.016]
 [0.036]
 [0.   ]
 [0.036]
 [0.015]]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.09097157415891478, 0.4544651189385382, 0.3635917327436323, 0.09097157415891478]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.09097157415891478, 0.4544651189385382, 0.3635917327436323, 0.09097157415891478]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.09097157415891478, 0.4544651189385382, 0.3635917327436323, 0.09097157415891478]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.09097157415891478, 0.4544651189385382, 0.3635917327436323, 0.09097157415891478]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.09097157415891478, 0.4544651189385382, 0.3635917327436323, 0.09097157415891478]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.09097157415891478, 0.4544651189385382, 0.3635917327436323, 0.09097157415891478]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.09097157415891478, 0.4544651189385382, 0.3635917327436323, 0.09097157415891478]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.09097157415891478, 0.4544651189385382, 0.3635917327436323, 0.09097157415891478]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.09097157415891478, 0.4544651189385382, 0.3635917327436323, 0.09097157415891478]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [0.09097157415891478, 0.4544651189385382, 0.3635917327436323, 0.09097157415891478]
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.055]
 [0.59 ]
 [0.331]
 [0.277]] [[0.739]
 [0.944]
 [0.318]
 [0.161]
 [0.251]] [[0.699]
 [0.655]
 [0.721]
 [0.344]
 [0.357]]
from probs:  [0.09097157415891478, 0.4544651189385382, 0.3635917327436323, 0.09097157415891478]
siam score:  -0.8726788
actor:  0 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
first move QE:  -0.05761556892177508
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09097158128849711, 0.4544651097719322, 0.36359172765107356, 0.09097158128849711]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09097158128849711, 0.4544651097719322, 0.36359172765107356, 0.09097158128849711]
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09097158128849711, 0.4544651097719322, 0.36359172765107356, 0.09097158128849711]
from probs:  [0.09097158128849711, 0.4544651097719322, 0.36359172765107356, 0.09097158128849711]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09097158128849711, 0.4544651097719322, 0.36359172765107356, 0.09097158128849711]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09097158128849711, 0.4544651097719322, 0.36359172765107356, 0.09097158128849711]
maxi score, test score, baseline:  0.0101 0.0 0.0101
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09097158128849711, 0.4544651097719322, 0.36359172765107356, 0.09097158128849711]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09097158128849711, 0.4544651097719322, 0.36359172765107356, 0.09097158128849711]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8724239
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.089349084889241, 0.4463511184687055, 0.3035503050369197, 0.16074949160513385]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09380976679040412, 0.4686663264934342, 0.28123804664191926, 0.1562858600742425]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09380976679040412, 0.4686663264934342, 0.28123804664191926, 0.1562858600742425]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0058],
        [0.7311],
        [0.0062],
        [0.1822],
        [0.1822],
        [0.1100],
        [0.1073],
        [0.2976],
        [0.0000],
        [0.1110]], dtype=torch.float64)
0.0 0.00577857978669559
0.0 0.7310929689132507
0.0 0.006153705795105945
0.0 0.18216102991048272
0.0 0.18216102991048272
0.0 0.11001435127493064
0.0 0.10731786987129086
0.0 0.29758001316122423
0.0 0.0
0.0 0.11103030506663362
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09727925884780929, 0.48602296359884023, 0.26388370374110826, 0.15281407381224224]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09727925884780929, 0.48602296359884023, 0.26388370374110826, 0.15281407381224224]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09727925884780929, 0.48602296359884023, 0.26388370374110826, 0.15281407381224224]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09727925884780929, 0.48602296359884023, 0.26388370374110826, 0.15281407381224224]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09727925884780929, 0.48602296359884023, 0.26388370374110826, 0.15281407381224224]
siam score:  -0.8617726
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09727925884780929, 0.48602296359884023, 0.26388370374110826, 0.15281407381224224]
first move QE:  -0.05757581792517676
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09727925884780929, 0.48602296359884023, 0.26388370374110826, 0.15281407381224224]
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09727925884780929, 0.48602296359884023, 0.26388370374110826, 0.15281407381224224]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09727925884780929, 0.48602296359884023, 0.26388370374110826, 0.15281407381224224]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09727925884780929, 0.48602296359884023, 0.26388370374110826, 0.15281407381224224]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.057]
 [0.005]
 [0.024]
 [0.034]] [[-0.045]
 [-0.037]
 [-0.047]
 [-0.052]
 [-0.055]] [[0.038]
 [0.088]
 [0.032]
 [0.048]
 [0.056]]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09727925884780929, 0.48602296359884023, 0.26388370374110826, 0.15281407381224224]
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09727925884780929, 0.48602296359884023, 0.26388370374110826, 0.15281407381224224]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09727925884780929, 0.48602296359884023, 0.26388370374110826, 0.15281407381224224]
first move QE:  -0.057794016333789514
maxi score, test score, baseline:  0.0101 0.0 0.0101
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09727925884780929, 0.48602296359884023, 0.26388370374110826, 0.15281407381224224]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09727925884780929, 0.48602296359884023, 0.26388370374110826, 0.15281407381224224]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09727925884780929, 0.48602296359884023, 0.26388370374110826, 0.15281407381224224]
maxi score, test score, baseline:  0.0101 0.0 0.0101
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [0.09727925884780929, 0.48602296359884023, 0.26388370374110826, 0.15281407381224224]
start point for exploration sampling:  20016
siam score:  -0.8726431
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.09727926305748202, 0.4860229570929823, 0.2638837033584107, 0.1528140764911249]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.014]
 [0.   ]
 [0.014]
 [0.014]] [[-0.024]
 [-0.024]
 [-0.045]
 [-0.024]
 [-0.024]] [[0.035]
 [0.035]
 [0.   ]
 [0.035]
 [0.035]]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.09727926305748202, 0.4860229570929823, 0.2638837033584107, 0.1528140764911249]
from probs:  [0.09727926305748202, 0.4860229570929823, 0.2638837033584107, 0.1528140764911249]
using another actor
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.09727926305748202, 0.4860229570929823, 0.2638837033584107, 0.1528140764911249]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.09727926305748202, 0.4860229570929823, 0.2638837033584107, 0.1528140764911249]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
siam score:  -0.8798729
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.10721754213434523, 0.5355649157313096, 0.25, 0.10721754213434523]
from probs:  [0.10721754213434523, 0.5355649157313096, 0.25, 0.10721754213434523]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.10721754213434523, 0.5355649157313096, 0.25, 0.10721754213434523]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.10721754213434523, 0.5355649157313096, 0.25000000000000006, 0.10721754213434523]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.10721754213434523, 0.5355649157313096, 0.25000000000000006, 0.10721754213434523]
siam score:  -0.8843634
from probs:  [0.10721754213434523, 0.5355649157313096, 0.25000000000000006, 0.10721754213434523]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.10721754213434523, 0.5355649157313096, 0.25000000000000006, 0.10721754213434523]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.10721754213434523, 0.5355649157313096, 0.25000000000000006, 0.10721754213434523]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.8821672
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.1094446009039633, 0.5467280647582996, 0.23438273343377372, 0.1094446009039633]
using another actor
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.1094446009039633, 0.5467280647582996, 0.23438273343377367, 0.1094446009039633]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.1094446009039633, 0.5467280647582996, 0.23438273343377367, 0.1094446009039633]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.1094446009039633, 0.5467280647582996, 0.23438273343377367, 0.1094446009039633]
from probs:  [0.1094446009039633, 0.5467280647582996, 0.23438273343377367, 0.1094446009039633]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.1094446009039633, 0.5467280647582996, 0.23438273343377367, 0.1094446009039633]
siam score:  -0.8814025
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.1094446009039633, 0.5467280647582996, 0.23438273343377367, 0.1094446009039633]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.1094446009039633, 0.5467280647582996, 0.23438273343377367, 0.1094446009039633]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.1094446009039633, 0.5467280647582996, 0.23438273343377367, 0.1094446009039633]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.1094446009039633, 0.5467280647582996, 0.23438273343377367, 0.1094446009039633]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.1094446009039633, 0.5467280647582996, 0.23438273343377367, 0.1094446009039633]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.1094446009039633, 0.5467280647582996, 0.23438273343377367, 0.1094446009039633]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.25  0.25  0.292 0.125 0.083]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.1094446009039633, 0.5467280647582996, 0.23438273343377367, 0.1094446009039633]
start point for exploration sampling:  20016
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.1094446009039633, 0.5467280647582996, 0.23438273343377367, 0.1094446009039633]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.11117684354846159, 0.5554109441933844, 0.22223536870969238, 0.11117684354846159]
from probs:  [0.11117684354846159, 0.5554109441933844, 0.22223536870969238, 0.11117684354846159]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.11117684354846159, 0.5554109441933844, 0.22223536870969238, 0.11117684354846159]
from probs:  [0.11117684354846159, 0.5554109441933844, 0.22223536870969238, 0.11117684354846159]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.11117684354846159, 0.5554109441933844, 0.22223536870969238, 0.11117684354846159]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.11117684354846159, 0.5554109441933844, 0.22223536870969238, 0.11117684354846159]
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.11117684354846159, 0.5554109441933844, 0.22223536870969238, 0.11117684354846159]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
siam score:  -0.89101774
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.10533262075593426, 0.5261831785568526, 0.26315157993127875, 0.10533262075593426]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.10533262075593426, 0.5261831785568526, 0.26315157993127875, 0.10533262075593426]
maxi score, test score, baseline:  0.0121 0.0 0.0121
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.10533262075593426, 0.5261831785568526, 0.26315157993127875, 0.10533262075593426]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.10533262075593426, 0.5261831785568526, 0.26315157993127875, 0.10533262075593426]
from probs:  [0.10533262075593426, 0.5261831785568526, 0.26315157993127875, 0.10533262075593426]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.10533262075593426, 0.5261831785568526, 0.26315157993127875, 0.10533262075593426]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.10533262075593426, 0.5261831785568526, 0.26315157993127875, 0.10533262075593426]
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.10533262075593426, 0.5261831785568526, 0.26315157993127875, 0.10533262075593426]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.10007288674804457, 0.49987852208659234, 0.29997570441731847, 0.10007288674804457]
Printing some Q and Qe and total Qs values:  [[1.461]
 [1.463]
 [1.152]
 [1.461]
 [1.461]] [[0.202]
 [0.778]
 [0.458]
 [0.202]
 [0.202]] [[0.661]
 [0.969]
 [0.578]
 [0.661]
 [0.661]]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
first move QE:  -0.058241578608076154
maxi score, test score, baseline:  0.0121 0.0 0.0121
maxi score, test score, baseline:  0.0121 0.0 0.0121
probs:  [0.09531413407765475, 0.47607934250188916, 0.3332923893428013, 0.09531413407765475]
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.0953141377542056, 0.4760793371284687, 0.3332923873631201, 0.0953141377542056]
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.0953141377542056, 0.4760793371284687, 0.3332923873631201, 0.0953141377542056]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.09789832504769722, 0.48901691778219, 0.3151864321224154, 0.09789832504769722]
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.09789832504769722, 0.48901691778219, 0.3151864321224154, 0.09789832504769722]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.09789832504769722, 0.48901691778219, 0.3151864321224154, 0.09789832504769722]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.09789832504769722, 0.48901691778219, 0.3151864321224154, 0.09789832504769722]
siam score:  -0.88578737
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.09789832504769722, 0.48901691778219, 0.3151864321224154, 0.09789832504769722]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.09789832504769722, 0.48901691778219, 0.3151864321224154, 0.09789832504769722]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.09789832504769722, 0.48901691778219, 0.3151864321224154, 0.09789832504769722]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.09789832504769722, 0.48901691778219, 0.3151864321224154, 0.09789832504769722]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.09789832504769722, 0.48901691778219, 0.3151864321224154, 0.09789832504769722]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.09789832504769722, 0.48901691778219, 0.3151864321224154, 0.09789832504769722]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.0938249865018062, 0.4686450188974714, 0.34370500809891635, 0.0938249865018062]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.0938249865018062, 0.4686450188974714, 0.34370500809891635, 0.0938249865018062]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.0938249865018062, 0.4686450188974714, 0.34370500809891635, 0.0938249865018062]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.0938249865018062, 0.4686450188974714, 0.34370500809891635, 0.0938249865018062]
maxi score, test score, baseline:  0.0141 0.0 0.0141
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.09007755251624046, 0.44990305935469943, 0.3699418356128196, 0.09007755251624046]
maxi score, test score, baseline:  0.0141 0.0 0.0141
line 256 mcts: sample exp_bonus -0.037986754790224
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.09007755251624046, 0.44990305935469943, 0.3699418356128196, 0.09007755251624046]
maxi score, test score, baseline:  0.0141 0.0 0.0141
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.023]
 [0.009]
 [0.02 ]
 [0.018]] [[-0.037]
 [-0.025]
 [-0.1  ]
 [-0.056]
 [-0.07 ]] [[0.177]
 [0.194]
 [0.106]
 [0.16 ]
 [0.144]]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.09007755251624046, 0.44990305935469943, 0.3699418356128196, 0.09007755251624046]
from probs:  [0.09007755251624046, 0.44990305935469943, 0.3699418356128196, 0.09007755251624046]
line 256 mcts: sample exp_bonus -0.007858420548945238
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.09007755251624046, 0.44990305935469943, 0.3699418356128196, 0.09007755251624046]
siam score:  -0.892052
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.09007755251624046, 0.44990305935469943, 0.3699418356128196, 0.09007755251624046]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.09007755251624046, 0.44990305935469943, 0.3699418356128196, 0.09007755251624046]
siam score:  -0.8955868
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.09007755251624046, 0.44990305935469943, 0.3699418356128196, 0.09007755251624046]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.09007755251624046, 0.44990305935469943, 0.3699418356128196, 0.09007755251624046]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.09007755251624046, 0.44990305935469943, 0.3699418356128196, 0.09007755251624046]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.09007755251624046, 0.44990305935469943, 0.3699418356128196, 0.09007755251624046]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [0.09266673418640772, 0.4628626537478013, 0.3518038778793832, 0.09266673418640772]
maxi score, test score, baseline:  0.0141 0.0 0.0141
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.8938455
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [0.09489884694689484, 0.47403499885448513, 0.33616730725172506, 0.09489884694689484]
from probs:  [0.09489884694689484, 0.47403499885448513, 0.33616730725172506, 0.09489884694689484]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [0.09489884694689484, 0.47403499885448513, 0.33616730725172506, 0.09489884694689484]
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [0.09489884694689484, 0.47403499885448513, 0.33616730725172506, 0.09489884694689484]
siam score:  -0.8898594
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [0.09489884694689484, 0.47403499885448513, 0.33616730725172506, 0.09489884694689484]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [0.09684298554983502, 0.4837659694239361, 0.3225480594763939, 0.09684298554983502]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [0.09855150399397296, 0.49231759360964333, 0.31057939840241083, 0.09855150399397296]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [0.09855150399397296, 0.49231759360964333, 0.31057939840241083, 0.09855150399397296]
maxi score, test score, baseline:  0.0161 0.0 0.0161
maxi score, test score, baseline:  0.0161 0.0 0.0161
main train batch thing paused
add a thread
Adding thread: now have 2 threads
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [0.09728892285474543, 0.48600802831539336, 0.3194141259751157, 0.09728892285474543]
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.019]
 [0.019]
 [0.021]
 [0.02 ]] [[-0.117]
 [-0.027]
 [-0.06 ]
 [-0.101]
 [-0.129]] [[0.021]
 [0.063]
 [0.046]
 [0.028]
 [0.013]]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [0.09728892285474543, 0.48600802831539336, 0.3194141259751157, 0.09728892285474543]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [0.09728892285474543, 0.48600802831539336, 0.3194141259751157, 0.09728892285474543]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [0.09874918453065448, 0.49331652923329494, 0.3091851017053961, 0.09874918453065448]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [0.09874918453065448, 0.49331652923329494, 0.3091851017053961, 0.09874918453065448]
from probs:  [0.09874918453065448, 0.49331652923329494, 0.3091851017053961, 0.09874918453065448]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [0.09874918453065448, 0.49331652923329494, 0.3091851017053961, 0.09874918453065448]
Printing some Q and Qe and total Qs values:  [[0.053]
 [0.065]
 [0.   ]
 [0.045]
 [0.059]] [[-0.038]
 [-0.066]
 [-0.042]
 [-0.036]
 [-0.092]] [[0.194]
 [0.185]
 [0.138]
 [0.188]
 [0.159]]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.083 0.5   0.083 0.25  0.083]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.028]
 [0.035]
 [0.044]
 [0.044]] [[-0.15 ]
 [-0.145]
 [-0.115]
 [-0.14 ]
 [-0.187]] [[0.063]
 [0.05 ]
 [0.072]
 [0.069]
 [0.045]]
siam score:  -0.8812964
maxi score, test score, baseline:  0.0161 0.0 0.0161
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.818]
 [0.607]
 [0.251]
 [0.284]] [[0.239]
 [0.314]
 [0.239]
 [0.106]
 [0.211]] [[0.548]
 [0.778]
 [0.548]
 [0.159]
 [0.218]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [0.10233355228857137, 0.511256022874066, 0.28407687254879116, 0.10233355228857137]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [0.10233355228857137, 0.511256022874066, 0.28407687254879116, 0.10233355228857137]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [0.10233355228857137, 0.511256022874066, 0.28407687254879116, 0.10233355228857137]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [0.10233355228857137, 0.511256022874066, 0.28407687254879116, 0.10233355228857137]
from probs:  [0.10233355228857137, 0.511256022874066, 0.28407687254879116, 0.10233355228857137]
using explorer policy with actor:  0
siam score:  -0.88391465
start point for exploration sampling:  20016
actor:  0 policy actor:  0  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.002]
 [0.007]
 [0.002]
 [0.004]] [[-0.004]
 [ 0.005]
 [-0.   ]
 [-0.001]
 [-0.004]] [[0.022]
 [0.025]
 [0.027]
 [0.022]
 [0.022]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.10233355398433153, 0.5112560198738751, 0.2840768721574619, 0.10233355398433153]
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.023]
 [0.027]
 [0.035]
 [0.035]] [[-0.137]
 [-0.118]
 [-0.218]
 [-0.171]
 [-0.196]] [[0.111]
 [0.108]
 [0.062]
 [0.094]
 [0.081]]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [0.10233355398433153, 0.5112560198738751, 0.2840768721574619, 0.10233355398433153]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0201 0.0 0.0201
probs:  [0.10233355569048547, 0.5112560168552949, 0.28407687176373403, 0.10233355569048547]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  0 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [0.12227537269718398, 0.4998960099402923, 0.27776622332669915, 0.10006239403582466]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [0.1196199939999655, 0.4890300110000633, 0.2934600020000115, 0.09788999299995976]
line 256 mcts: sample exp_bonus 0.008665571932832865
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [0.1196199939999655, 0.4890300110000633, 0.2934600020000115, 0.09788999299995976]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.0241 0.0 0.0241
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.597]
 [0.006]
 [0.038]
 [0.06 ]] [[-0.009]
 [-0.098]
 [-0.039]
 [-0.04 ]
 [-0.018]] [[0.045]
 [0.566]
 [0.019]
 [0.051]
 [0.089]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [0.11707763002439656, 0.478626476358038, 0.30848584278926555, 0.0958100508283]
maxi score, test score, baseline:  0.0241 0.0 0.0241
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [0.11707763002439656, 0.478626476358038, 0.30848584278926555, 0.0958100508283]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.11740248360457205, 0.4845956059303726, 0.3009990447674723, 0.09700286569758315]
from probs:  [0.11740248360457203, 0.48459560593037254, 0.3009990447674723, 0.09700286569758312]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.11740248500583408, 0.4845956034512166, 0.30099904422852525, 0.09700286731442397]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [0.11740248500583408, 0.4845956034512166, 0.30099904422852525, 0.09700286731442397]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [0.11740248500583408, 0.4845956034512166, 0.3009990442285253, 0.09700286731442397]
from probs:  [0.11740248500583408, 0.4845956034512166, 0.3009990442285253, 0.09700286731442397]
using explorer policy with actor:  0
using explorer policy with actor:  1
from probs:  [0.11740248500583408, 0.4845956034512166, 0.3009990442285253, 0.09700286731442397]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [0.11740248500583408, 0.4845956034512166, 0.3009990442285253, 0.09700286731442397]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using another actor
from probs:  [0.11770186272521711, 0.4900966194986801, 0.2940993790915943, 0.09810213868450854]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [0.11770186272521711, 0.4900966194986801, 0.2940993790915943, 0.09810213868450854]
using another actor
from probs:  [0.11770186272521711, 0.4900966194986801, 0.2940993790915943, 0.09810213868450854]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [0.11770186272521711, 0.4900966194986801, 0.2940993790915943, 0.09810213868450854]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using another actor
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [0.11797864838263873, 0.49518251014652803, 0.28772038617638895, 0.09911845529444427]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [0.11797864838263873, 0.49518251014652803, 0.28772038617638895, 0.09911845529444427]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [0.11797864838263873, 0.49518251014652803, 0.28772038617638895, 0.09911845529444427]
start point for exploration sampling:  20016
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
line 256 mcts: sample exp_bonus 0.11096157717042739
siam score:  -0.86795455
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [0.1157962278322713, 0.4860135303639367, 0.300904879098104, 0.09728536270568802]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [0.1157962278322713, 0.4860135303639367, 0.300904879098104, 0.09728536270568802]
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.288]
 [0.587]
 [0.513]
 [0.472]] [[0.009]
 [0.011]
 [0.195]
 [0.009]
 [0.102]] [[0.341]
 [0.117]
 [0.508]
 [0.341]
 [0.346]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.8716134
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
from probs:  [0.11409118353025985, 0.4823602346095558, 0.30699401980989105, 0.09655456205029338]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [0.1121263174464094, 0.4740447341495847, 0.3189368412767954, 0.09489210712721055]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [0.1121263174464094, 0.4740447341495847, 0.3189368412767954, 0.09489210712721055]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.102]
 [1.102]
 [1.562]
 [1.102]
 [1.071]] [[-0.038]
 [-0.038]
 [-0.033]
 [-0.038]
 [-0.051]] [[0.971]
 [0.971]
 [1.386]
 [0.971]
 [0.938]]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [0.11255664159472421, 0.47907226400879294, 0.31247425382057986, 0.0958968405759029]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [0.11255664159472421, 0.47907226400879294, 0.3124742538205799, 0.0958968405759029]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [0.11255664159472421, 0.47907226400879294, 0.3124742538205799, 0.0958968405759029]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [0.11333661259033496, 0.48818476091398755, 0.30076068675216133, 0.09771793974351609]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [0.11333661259033496, 0.48818476091398755, 0.30076068675216133, 0.09771793974351609]
line 256 mcts: sample exp_bonus 6.59050485296575e-08
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [0.11237414327248835, 0.489034382737257, 0.3007042630048727, 0.09788721098538185]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [0.11237414327248835, 0.489034382737257, 0.3007042630048727, 0.09788721098538185]
Starting evaluation
line 256 mcts: sample exp_bonus -0.011784140998497604
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [0.11237414327248835, 0.489034382737257, 0.30070426300487274, 0.09788721098538185]
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]] [[1.413]
 [1.413]
 [1.413]
 [1.413]
 [1.413]] [[0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]]
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.1123741443117239, 0.489034380932269, 0.30070426262199645, 0.09788721213401065]
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [0.11237414640955491, 0.4890343772886678, 0.3007042618491113, 0.09788721445266597]
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.042100000000000005 0.0 0.042100000000000005
probs:  [0.11273083017793088, 0.4928608389159685, 0.29575638994068965, 0.09865194096541097]
actor:  0 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0441 0.45 0.45
probs:  [0.11273134408138862, 0.4928599297021586, 0.29575621863953716, 0.09865250757691565]
maxi score, test score, baseline:  0.0441 0.45 0.45
using another actor
from probs:  [0.11273134408138862, 0.4928599297021586, 0.29575621863953716, 0.09865250757691565]
maxi score, test score, baseline:  0.0441 0.45 0.45
probs:  [0.11273134408138862, 0.4928599297021586, 0.29575621863953716, 0.09865250757691565]
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.11273134408138862, 0.4928599297021586, 0.29575621863953716, 0.09865250757691565]
maxi score, test score, baseline:  0.0461 0.45 0.45
probs:  [0.11273134408138862, 0.4928599297021586, 0.29575621863953716, 0.09865250757691565]
start point for exploration sampling:  20016
from probs:  [0.11273134408138862, 0.4928599297021586, 0.29575621863953716, 0.09865250757691565]
using explorer policy with actor:  1
from probs:  [0.11273134408138862, 0.4928599297021586, 0.2957562186395372, 0.09865250757691565]
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.168]
 [0.02 ]
 [0.042]
 [0.041]] [[-0.024]
 [-0.07 ]
 [-0.089]
 [-0.176]
 [-0.149]] [[0.035]
 [0.168]
 [0.02 ]
 [0.042]
 [0.041]]
line 256 mcts: sample exp_bonus 0.07770440706215712
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0461 0.45 0.45
probs:  [0.11154223761631653, 0.4897683202254032, 0.30065527892085986, 0.09803416323742058]
maxi score, test score, baseline:  0.0461 0.45 0.45
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]] [[-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]] [[0.025]
 [0.025]
 [0.025]
 [0.025]
 [0.025]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.0461 0.45 0.45
probs:  [0.12184678484553704, 0.4806757872780334, 0.30126128606178526, 0.09621614181464445]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.0461 0.45 0.45
probs:  [0.13129866264330228, 0.46866035828865354, 0.3062269492742253, 0.09381402979381881]
Printing some Q and Qe and total Qs values:  [[0.575]
 [1.002]
 [0.097]
 [0.384]
 [0.697]] [[ 0.193]
 [-0.1  ]
 [ 0.076]
 [-0.042]
 [ 0.016]] [[0.635]
 [0.916]
 [0.099]
 [0.327]
 [0.669]]
maxi score, test score, baseline:  0.0461 0.45 0.45
probs:  [0.13129866264330228, 0.46866035828865354, 0.3062269492742253, 0.09381402979381881]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.152]
 [0.201]
 [0.001]
 [0.178]
 [0.172]] [[-0.228]
 [-0.131]
 [-0.199]
 [-0.172]
 [-0.243]] [[0.152]
 [0.201]
 [0.001]
 [0.178]
 [0.172]]
from probs:  [0.1311459674607429, 0.4724703685991222, 0.3018081680299326, 0.09457549591020226]
maxi score, test score, baseline:  0.0461 0.45 0.45
probs:  [0.1311459674607429, 0.4724703685991222, 0.30180816802993254, 0.09457549591020226]
maxi score, test score, baseline:  0.0461 0.45 0.45
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0461 0.45 0.45
probs:  [0.12185072630348358, 0.48066869265372963, 0.3012597094786066, 0.09622087156418031]
maxi score, test score, baseline:  0.0461 0.45 0.45
siam score:  -0.90211076
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0461 0.45 0.45
probs:  [0.12193038481643403, 0.48427368631140116, 0.29685473726228023, 0.0969411916098846]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0461 0.45 0.45
probs:  [0.1327669903011637, 0.4782958609924707, 0.29319110883641336, 0.09574603986995225]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.0461 0.45 0.45
probs:  [0.14333924515153706, 0.47246386011250846, 0.2896168518008576, 0.09458004293509685]
maxi score, test score, baseline:  0.0461 0.45 0.45
probs:  [0.14333924515153706, 0.47246386011250846, 0.2896168518008576, 0.09458004293509685]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
from probs:  [0.1529834034512504, 0.47049226488352186, 0.2823388655162498, 0.0941854661489779]
maxi score, test score, baseline:  0.0461 0.45 0.45
probs:  [0.15298340345125036, 0.47049226488352186, 0.28233886551624987, 0.0941854661489779]
Printing some Q and Qe and total Qs values:  [[0.995]
 [0.995]
 [0.995]
 [0.995]
 [0.949]] [[0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.478]] [[1.033]
 [1.033]
 [1.033]
 [1.033]
 [0.85 ]]
Printing some Q and Qe and total Qs values:  [[0.943]
 [0.943]
 [1.198]
 [0.943]
 [1.177]] [[ 0.102]
 [ 0.102]
 [ 0.008]
 [ 0.102]
 [-0.012]] [[0.647]
 [0.647]
 [0.878]
 [0.647]
 [0.852]]
maxi score, test score, baseline:  0.0461 0.45 0.45
probs:  [0.1529834034512504, 0.47049226488352186, 0.2823388655162498, 0.0941854661489779]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0461 0.45 0.45
maxi score, test score, baseline:  0.0461 0.45 0.45
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.114]
 [1.034]
 [1.366]
 [1.034]
 [1.034]] [[0.493]
 [0.263]
 [0.026]
 [0.263]
 [0.263]] [[1.059]
 [0.807]
 [0.961]
 [0.807]
 [0.807]]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.0461 0.45 0.45
probs:  [0.1583734420387329, 0.4582421771846979, 0.2916484354369395, 0.09173594533962957]
Printing some Q and Qe and total Qs values:  [[0.108]
 [0.156]
 [0.005]
 [0.124]
 [0.107]] [[-0.131]
 [-0.226]
 [-0.262]
 [-0.177]
 [-0.28 ]] [[0.203]
 [0.179]
 [0.002]
 [0.184]
 [0.09 ]]
Printing some Q and Qe and total Qs values:  [[0.122]
 [0.083]
 [0.093]
 [0.128]
 [0.121]] [[-0.183]
 [-0.21 ]
 [-0.229]
 [-0.219]
 [-0.296]] [[0.153]
 [0.094]
 [0.09 ]
 [0.132]
 [0.067]]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0461 0.45 0.45
probs:  [0.16582597570042007, 0.4604350607489498, 0.2815652591123425, 0.0921737044382876]
using another actor
maxi score, test score, baseline:  0.0461 0.45 0.45
probs:  [0.16582597570042007, 0.4604350607489498, 0.2815652591123425, 0.0921737044382876]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.019]
 [-0.036]
 [-0.196]
 [-0.251]
 [-0.136]] [[0.246]
 [0.228]
 [0.069]
 [0.014]
 [0.129]]
actor:  0 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.048100000000000004 0.45 0.45
using another actor
maxi score, test score, baseline:  0.048100000000000004 0.45 0.45
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.048100000000000004 0.45 0.45
probs:  [0.16498508600375264, 0.46382538974813736, 0.27833830466541576, 0.09285121958269425]
maxi score, test score, baseline:  0.048100000000000004 0.45 0.45
probs:  [0.16498508600375264, 0.46382538974813736, 0.27833830466541576, 0.09285121958269425]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.513]
 [0.002]
 [0.052]
 [0.083]] [[-0.013]
 [-0.06 ]
 [-0.06 ]
 [-0.141]
 [-0.069]] [[0.02 ]
 [0.513]
 [0.002]
 [0.052]
 [0.083]]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
siam score:  -0.9107775
maxi score, test score, baseline:  0.050100000000000006 0.45 0.45
probs:  [0.16330281300129054, 0.45909321570276984, 0.2856988417053509, 0.09190512959058865]
maxi score, test score, baseline:  0.050100000000000006 0.45 0.45
probs:  [0.16330281300129054, 0.45909321570276984, 0.2856988417053509, 0.09190512959058865]
using another actor
maxi score, test score, baseline:  0.050100000000000006 0.45 0.45
using explorer policy with actor:  0
maxi score, test score, baseline:  0.050100000000000006 0.45 0.45
probs:  [0.16330281300129054, 0.45909321570276984, 0.2856988417053509, 0.09190512959058865]
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.0521 0.45 0.45
probs:  [0.16165453094059012, 0.4544566569660631, 0.29291065640028485, 0.09097815569306215]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0521 0.45 0.45
probs:  [0.16165453094059012, 0.4544566569660631, 0.29291065640028485, 0.09097815569306215]
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
Printing some Q and Qe and total Qs values:  [[1.515]
 [1.515]
 [0.02 ]
 [1.515]
 [1.515]] [[-0.001]
 [-0.001]
 [ 0.011]
 [-0.001]
 [-0.001]] [[1.337]
 [1.337]
 [0.02 ]
 [1.337]
 [1.337]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0541 0.45 0.45
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0541 0.45 0.45
probs:  [0.16915267827800234, 0.4533432637250244, 0.28674878260090797, 0.09075527539606522]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.0541 0.45 0.45
probs:  [0.18512447244698416, 0.44462658265904753, 0.2812363651181187, 0.08901257977584959]
from probs:  [0.18512447244698416, 0.44462658265904753, 0.2812363651181187, 0.08901257977584959]
maxi score, test score, baseline:  0.0541 0.45 0.45
maxi score, test score, baseline:  0.0541 0.45 0.45
probs:  [0.18512447244698416, 0.44462658265904753, 0.2812363651181187, 0.08901257977584959]
maxi score, test score, baseline:  0.0541 0.45 0.45
probs:  [0.18512447244698416, 0.44462658265904753, 0.2812363651181187, 0.08901257977584959]
from probs:  [0.18512447244698416, 0.44462658265904753, 0.2812363651181187, 0.08901257977584959]
line 256 mcts: sample exp_bonus 0.04552059173136179
first move QE:  -0.059645034938252364
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.983]
 [0.545]
 [1.071]
 [0.983]
 [0.992]] [[-0.008]
 [ 0.113]
 [-0.02 ]
 [-0.008]
 [ 0.017]] [[0.657]
 [0.279]
 [0.739]
 [0.657]
 [0.678]]
maxi score, test score, baseline:  0.056100000000000004 0.45 0.45
probs:  [0.18512447244698416, 0.44462658265904753, 0.2812363651181187, 0.08901257977584959]
UNIT TEST: sample policy line 217 mcts : [0.083 0.083 0.458 0.208 0.167]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.058100000000000006 0.45 0.45
probs:  [0.18512447244698416, 0.44462658265904753, 0.2812363651181187, 0.08901257977584959]
maxi score, test score, baseline:  0.058100000000000006 0.45 0.45
maxi score, test score, baseline:  0.058100000000000006 0.45 0.45
probs:  [0.18512447244698416, 0.44462658265904753, 0.2812363651181187, 0.08901257977584959]
using another actor
from probs:  [0.18512447244698416, 0.44462658265904753, 0.2812363651181187, 0.08901257977584959]
maxi score, test score, baseline:  0.058100000000000006 0.45 0.45
probs:  [0.18512447244698416, 0.44462658265904753, 0.2812363651181187, 0.08901257977584959]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.058100000000000006 0.45 0.45
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
UNIT TEST: sample policy line 217 mcts : [0.042 0.25  0.375 0.042 0.292]
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.639]
 [0.727]
 [0.57 ]
 [0.643]] [[1.247]
 [1.412]
 [1.37 ]
 [1.415]
 [1.5  ]] [[0.566]
 [0.639]
 [0.727]
 [0.57 ]
 [0.643]]
Printing some Q and Qe and total Qs values:  [[0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]] [[1.479]
 [1.479]
 [1.479]
 [1.479]
 [1.479]] [[0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]]
maxi score, test score, baseline:  0.058100000000000006 0.45 0.45
maxi score, test score, baseline:  0.058100000000000006 0.45 0.45
probs:  [0.18984110463878395, 0.4397319007546045, 0.2823932513483471, 0.08803374325826453]
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0601 0.45 0.45
probs:  [0.18810053752000203, 0.4356983874399939, 0.2889737356355543, 0.08722733940444981]
first move QE:  -0.05922729398112618
maxi score, test score, baseline:  0.0601 0.45 0.45
probs:  [0.18810053752000203, 0.4356983874399939, 0.28897373563555434, 0.08722733940444981]
actor:  0 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0621 0.45 0.45
probs:  [0.18810053752000203, 0.4356983874399939, 0.2889737356355543, 0.08722733940444981]
maxi score, test score, baseline:  0.0621 0.45 0.45
maxi score, test score, baseline:  0.0621 0.45 0.45
probs:  [0.18810053752000203, 0.4356983874399939, 0.2889737356355543, 0.08722733940444981]
maxi score, test score, baseline:  0.0621 0.45 0.45
maxi score, test score, baseline:  0.0621 0.45 0.45
probs:  [0.18810053752000203, 0.4356983874399939, 0.2889737356355543, 0.08722733940444981]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
siam score:  -0.89149886
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.0621 0.45 0.45
probs:  [0.195478533261399, 0.43173822246200316, 0.28634764449240063, 0.08643559978419714]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
from probs:  [0.20091459351692637, 0.42403007753089744, 0.2901607871225148, 0.08489454182966145]
siam score:  -0.8943048
maxi score, test score, baseline:  0.0621 0.45 0.45
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  0
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  0 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
first move QE:  -0.05926043606077628
maxi score, test score, baseline:  0.0641 0.45 0.45
probs:  [0.2065409354505459, 0.4238362581978162, 0.28476725163956323, 0.08485555471207457]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0641 0.45 0.45
probs:  [0.2009156448589732, 0.4240263500454586, 0.29015992693356735, 0.08489807816200087]
actor:  0 policy actor:  0  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.061]
 [0.432]
 [0.067]
 [0.115]
 [0.15 ]] [[-0.117]
 [-0.101]
 [-0.125]
 [-0.055]
 [-0.097]] [[0.068]
 [0.442]
 [0.071]
 [0.138]
 [0.162]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
using another actor
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.2080028143251248, 0.4095893055645258, 0.30039662280985024, 0.0820112573004992]
start point for exploration sampling:  20016
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.20493901078053867, 0.40976168905081745, 0.3032538963502725, 0.08204540381837139]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.20365048636178465, 0.4132308958563235, 0.3003799061284949, 0.0827387116533969]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.2100185503352838, 0.40992579865886486, 0.2979777395976595, 0.08207791140819186]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.2100185503352838, 0.40992579865886486, 0.2979777395976595, 0.08207791140819186]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
from probs:  [0.20738389715045752, 0.4165902202300298, 0.2926161028495426, 0.0834097797699701]
start point for exploration sampling:  20016
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.2108387217053523, 0.4196988726101402, 0.28543163274277655, 0.08403077294173097]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.20957713172835457, 0.4227158917061215, 0.28307325585861903, 0.08463372070690492]
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.20957713172835457, 0.4227158917061215, 0.28307325585861903, 0.08463372070690492]
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.20957713172835457, 0.4227158917061215, 0.28307325585861903, 0.08463372070690492]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.22102767656619834, 0.4165908597443597, 0.2789723234338017, 0.08340914025564028]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.22102767656619834, 0.4165908597443597, 0.2789723234338017, 0.08340914025564028]
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.22102767656619834, 0.4165908597443597, 0.2789723234338017, 0.08340914025564028]
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.088]
 [0.11 ]
 [0.132]
 [0.116]] [[-0.278]
 [-0.274]
 [-0.325]
 [-0.265]
 [-0.352]] [[0.137]
 [0.112]
 [0.121]
 [0.158]
 [0.121]]
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.22518856631017647, 0.4165910547745295, 0.2748114336898236, 0.08340894522547049]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.22378806436636997, 0.41950385043080757, 0.27271701088247935, 0.08399107432034314]
siam score:  -0.91202486
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.2291760947901412, 0.4165912416788708, 0.2708239052098589, 0.08340875832112926]
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.2277497533744755, 0.41944418584053267, 0.2688271317600592, 0.0839789290249327]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.2218670104429548, 0.41383329212632214, 0.28144275303434463, 0.0828569443963784]
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.2218670104429548, 0.41383329212632214, 0.28144275303434463, 0.0828569443963784]
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.2218670104429548, 0.41383329212632214, 0.28144275303434463, 0.0828569443963784]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
using another actor
UNIT TEST: sample policy line 217 mcts : [0.167 0.125 0.375 0.167 0.167]
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.22040813041317897, 0.411111289972692, 0.28616784060611455, 0.08231273900801445]
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.22040813041317897, 0.411111289972692, 0.28616784060611455, 0.08231273900801445]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus -0.048729246653156554
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.22420519710277886, 0.4112175181076319, 0.2822435036215264, 0.08233378116806288]
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.22420519710277886, 0.4112175181076319, 0.2822435036215264, 0.08233378116806288]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.22771711126124683, 0.405980221171272, 0.28501596801804063, 0.08128669954944052]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.22642573933323545, 0.4087333551562148, 0.28300396493347035, 0.08183694057707942]
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.22642573933323545, 0.4087333551562148, 0.28300396493347035, 0.08183694057707942]
maxi score, test score, baseline:  0.06810000000000001 0.45 0.45
probs:  [0.22642573933323545, 0.4087333551562148, 0.28300396493347035, 0.08183694057707942]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
from probs:  [0.22642573933323545, 0.4087333551562148, 0.28300396493347035, 0.08183694057707942]
actor:  0 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.07010000000000001 0.45 0.45
probs:  [0.2250113068673472, 0.4061793320790799, 0.2874830396989792, 0.08132632135459368]
siam score:  -0.9117453
maxi score, test score, baseline:  0.07010000000000001 0.45 0.45
probs:  [0.2250113068673472, 0.4061793320790799, 0.2874830396989792, 0.08132632135459368]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
using another actor
maxi score, test score, baseline:  0.07010000000000001 0.45 0.45
maxi score, test score, baseline:  0.07010000000000001 0.45 0.45
probs:  [0.2298228128444777, 0.4036570406459007, 0.2856981003520779, 0.08082204615754371]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.07010000000000001 0.45 0.45
using explorer policy with actor:  1
maxi score, test score, baseline:  0.07010000000000001 0.45 0.45
probs:  [0.22700433253072078, 0.3987053163013391, 0.29445829044060645, 0.07983206072733369]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0721 0.45 0.45
maxi score, test score, baseline:  0.0721 0.45 0.45
probs:  [0.2245623827080989, 0.4041220341803421, 0.2904009215812547, 0.08091466153030431]
actor:  0 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.0761 0.45 0.45
probs:  [0.2245623827080989, 0.4041220341803421, 0.2904009215812547, 0.08091466153030431]
maxi score, test score, baseline:  0.0761 0.45 0.45
probs:  [0.2245623827080989, 0.4041220341803421, 0.2904009215812547, 0.08091466153030431]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.87 ]
 [0.003]
 [0.168]
 [0.191]] [[-0.197]
 [-0.087]
 [-0.102]
 [-0.161]
 [-0.175]] [[0.001]
 [0.895]
 [0.025]
 [0.175]
 [0.194]]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0761 0.45 0.45
maxi score, test score, baseline:  0.0761 0.45 0.45
maxi score, test score, baseline:  0.0761 0.45 0.45
probs:  [0.23122231700382725, 0.4045547754300371, 0.2832220545316902, 0.0810008530344454]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.0761 0.45 0.45
probs:  [0.23563865449432256, 0.4022302623601811, 0.2815949601124904, 0.08053612303300604]
maxi score, test score, baseline:  0.0761 0.45 0.45
probs:  [0.23563865449432256, 0.4022302623601811, 0.2815949601124904, 0.08053612303300604]
maxi score, test score, baseline:  0.0761 0.45 0.45
probs:  [0.23563865449432256, 0.4022302623601811, 0.2815949601124904, 0.08053612303300604]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
using another actor
maxi score, test score, baseline:  0.0761 0.45 0.45
using another actor
maxi score, test score, baseline:  0.0761 0.45 0.45
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.0781 0.45 0.45
probs:  [0.2342928046657531, 0.39993231909962956, 0.2856981712141975, 0.08007670502041993]
maxi score, test score, baseline:  0.0781 0.45 0.45
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.2342928046657531, 0.39993231909962956, 0.2856981712141975, 0.08007670502041993]
maxi score, test score, baseline:  0.0781 0.45 0.45
maxi score, test score, baseline:  0.0781 0.45 0.45
probs:  [0.23305846604574965, 0.4024738055882532, 0.2838830679085007, 0.08058466045749647]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0781 0.45 0.45
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.0781 0.45 0.45
probs:  [0.2361173426267926, 0.40270923110528106, 0.2805418462210562, 0.08063158004687006]
siam score:  -0.9019261
maxi score, test score, baseline:  0.0781 0.45 0.45
probs:  [0.2361173426267926, 0.40270923110528106, 0.2805418462210562, 0.08063158004687006]
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.0801 0.45 0.45
probs:  [0.23611734262679263, 0.40270923110528106, 0.2805418462210562, 0.08063158004687006]
siam score:  -0.90179354
maxi score, test score, baseline:  0.0801 0.45 0.45
maxi score, test score, baseline:  0.0801 0.45 0.45
probs:  [0.23611734262679263, 0.40270923110528106, 0.2805418462210562, 0.08063158004687006]
maxi score, test score, baseline:  0.0801 0.45 0.45
probs:  [0.23611734262679263, 0.40270923110528106, 0.2805418462210562, 0.08063158004687006]
from probs:  [0.23611734262679263, 0.40270923110528106, 0.2805418462210562, 0.08063158004687006]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.0821 0.45 0.45
probs:  [0.23489686453461078, 0.40515039159899835, 0.27883325861574304, 0.08111948525064781]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
from probs:  [0.23489686453461078, 0.40515039159899835, 0.27883325861574304, 0.08111948525064781]
maxi score, test score, baseline:  0.0821 0.45 0.45
probs:  [0.23907593299115912, 0.40293693812377335, 0.27731016752210236, 0.08067696136296534]
maxi score, test score, baseline:  0.0821 0.45 0.45
probs:  [0.23907593299115912, 0.40293693812377335, 0.27731016752210236, 0.08067696136296534]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
Printing some Q and Qe and total Qs values:  [[0.98 ]
 [1.034]
 [1.103]
 [1.005]
 [1.045]] [[0.181]
 [0.612]
 [0.178]
 [0.609]
 [0.131]] [[0.653]
 [0.814]
 [0.775]
 [0.785]
 [0.705]]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.08410000000000001 0.45 0.45
probs:  [0.2472985121884075, 0.39858182963758787, 0.27431339030433255, 0.07980626786967207]
siam score:  -0.8994669
maxi score, test score, baseline:  0.08410000000000001 0.45 0.45
probs:  [0.2472985121884075, 0.39858182963758787, 0.27431339030433255, 0.07980626786967207]
maxi score, test score, baseline:  0.08410000000000001 0.45 0.45
probs:  [0.2472985121884075, 0.39858182963758787, 0.27431339030433255, 0.07980626786967207]
from probs:  [0.2472985121884075, 0.39858182963758787, 0.27431339030433255, 0.07980626786967207]
maxi score, test score, baseline:  0.08410000000000001 0.45 0.45
probs:  [0.2472985121884075, 0.39858182963758787, 0.27431339030433255, 0.07980626786967207]
Printing some Q and Qe and total Qs values:  [[0.948]
 [0.948]
 [0.948]
 [0.948]
 [0.948]] [[0.45]
 [0.45]
 [0.45]
 [0.45]
 [0.45]] [[0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]]
maxi score, test score, baseline:  0.08410000000000001 0.45 0.45
probs:  [0.2472985121884075, 0.39858182963758787, 0.27431339030433255, 0.07980626786967207]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.08410000000000001 0.45 0.45
probs:  [0.2513434807567411, 0.3964394024847794, 0.2728391728645986, 0.07937794389388085]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using another actor
maxi score, test score, baseline:  0.0861 0.45 0.45
maxi score, test score, baseline:  0.0861 0.45 0.45
probs:  [0.2513434807567411, 0.3964394024847794, 0.2728391728645986, 0.07937794389388085]
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
Printing some Q and Qe and total Qs values:  [[0.13 ]
 [0.192]
 [0.179]
 [0.136]
 [0.129]] [[-0.231]
 [-0.236]
 [-0.331]
 [-0.259]
 [-0.345]] [[0.182]
 [0.242]
 [0.182]
 [0.175]
 [0.125]]
siam score:  -0.89664847
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0881 0.45 0.45
probs:  [0.25130149903690363, 0.3970693911701091, 0.27212548362736155, 0.07950362616562576]
maxi score, test score, baseline:  0.0881 0.45 0.45
probs:  [0.25130149903690363, 0.3970693911701091, 0.27212548362736155, 0.07950362616562576]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.0881 0.45 0.45
probs:  [0.2551790180225718, 0.39501250463201093, 0.2707160720902872, 0.0790924052551299]
Printing some Q and Qe and total Qs values:  [[0.99 ]
 [1.519]
 [1.05 ]
 [0.99 ]
 [0.99 ]] [[-0.228]
 [ 0.016]
 [ 0.02 ]
 [-0.228]
 [-0.228]] [[0.569]
 [1.342]
 [0.876]
 [0.569]
 [0.569]]
siam score:  -0.89843965
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
siam score:  -0.89989656
maxi score, test score, baseline:  0.0881 0.45 0.45
probs:  [0.2590165566121592, 0.39297682627852537, 0.2693211927403413, 0.0786854243689741]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0881 0.45 0.45
probs:  [0.2641686994722993, 0.3929750583113838, 0.26416869947229926, 0.07868754274401761]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.0901 0.45 0.45
probs:  [0.26648984506908935, 0.39333480713900715, 0.2614160465862926, 0.07875930120561082]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.2701926788755942, 0.3913487521291595, 0.2600963394377971, 0.07836222955744925]
siam score:  -0.89925367
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.0901 0.45 0.45
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
from probs:  [0.2709263327571352, 0.3940224077991069, 0.2561548037520986, 0.07889645569165936]
siam score:  -0.89821535
maxi score, test score, baseline:  0.0921 0.45 0.45
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
Printing some Q and Qe and total Qs values:  [[0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]] [[-0.096]
 [-0.096]
 [-0.096]
 [-0.096]
 [-0.096]] [[0.934]
 [0.934]
 [0.934]
 [0.934]
 [0.934]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
start point for exploration sampling:  20016
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  20016
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
using another actor
from probs:  [0.2765578346953213, 0.3924465679112686, 0.2524143486086656, 0.07858124878474461]
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.0961 0.45 0.45
probs:  [0.280034244320581, 0.3905602634203191, 0.2512013697728232, 0.07820412248627671]
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0981 0.45 0.45
probs:  [0.2785583193924678, 0.39279159696233895, 0.24999999999999997, 0.07865008364519332]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.0981 0.45 0.45
probs:  [0.27711023771963117, 0.39498083650063653, 0.24882129401218994, 0.07908763176754238]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.0981 0.45 0.45
probs:  [0.27580977019601133, 0.3931269074506085, 0.2523463427450919, 0.07871697960828812]
Printing some Q and Qe and total Qs values:  [[0.992]
 [0.992]
 [0.955]
 [1.116]
 [0.992]] [[ 0.   ]
 [ 0.   ]
 [-0.001]
 [-0.005]
 [ 0.   ]] [[0.679]
 [0.679]
 [0.642]
 [0.802]
 [0.679]]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.0981 0.45 0.45
probs:  [0.27452145828905933, 0.39129030728457975, 0.25583844244977605, 0.07834979197658502]
maxi score, test score, baseline:  0.0981 0.45 0.45
maxi score, test score, baseline:  0.0981 0.45 0.45
probs:  [0.27452145828905933, 0.39129030728457975, 0.25583844244977605, 0.07834979197658502]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0981 0.45 0.45
probs:  [0.27379832763762646, 0.3927899658257588, 0.2547596655275252, 0.07865204100908936]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.387]
 [0.968]
 [0.461]
 [0.451]] [[ 0.005]
 [ 0.   ]
 [-0.114]
 [ 0.072]
 [ 0.022]] [[0.004]
 [0.378]
 [0.93 ]
 [0.47 ]
 [0.448]]
siam score:  -0.89670485
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.0981 0.45 0.45
probs:  [0.27593116494918835, 0.3890853392729195, 0.2570721358952332, 0.07791135988265879]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.0981 0.45 0.45
maxi score, test score, baseline:  0.0981 0.45 0.45
probs:  [0.2793288714201413, 0.387259118246261, 0.25586577428402824, 0.07754623604956962]
from probs:  [0.2793288714201413, 0.387259118246261, 0.25586577428402824, 0.07754623604956962]
maxi score, test score, baseline:  0.0981 0.45 0.45
probs:  [0.2793288714201413, 0.387259118246261, 0.25586577428402824, 0.07754623604956962]
actor:  0 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.10010000000000001 0.45 0.45
probs:  [0.2773676860001756, 0.3868384300008779, 0.25832929573918384, 0.07746458825976266]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1021 0.45 0.45
probs:  [0.2773676860001756, 0.3868384300008779, 0.25832929573918384, 0.07746458825976266]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1021 0.45 0.45
probs:  [0.2759308715305526, 0.38908376548205514, 0.2570720558719689, 0.07791330711542337]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.1021 0.45 0.45
probs:  [0.2746359745355121, 0.38725757241213893, 0.2605582748009338, 0.07754817825141519]
maxi score, test score, baseline:  0.1021 0.45 0.45
probs:  [0.2746359745355121, 0.38725757241213893, 0.2605582748009338, 0.07754817825141519]
maxi score, test score, baseline:  0.1021 0.45 0.45
Printing some Q and Qe and total Qs values:  [[1.165]
 [1.28 ]
 [1.159]
 [1.165]
 [1.138]] [[0.018]
 [0.117]
 [0.176]
 [0.018]
 [0.033]] [[0.79 ]
 [0.929]
 [0.823]
 [0.79 ]
 [0.766]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
Printing some Q and Qe and total Qs values:  [[0.093]
 [0.103]
 [0.101]
 [0.099]
 [0.093]] [[-0.288]
 [-0.159]
 [-0.266]
 [-0.246]
 [ 0.   ]] [[0.058]
 [0.099]
 [0.071]
 [0.073]
 [0.13 ]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1041 0.45 0.45
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.28361612763338745, 0.3867055857091088, 0.25224107517555916, 0.07743721148194456]
maxi score, test score, baseline:  0.1041 0.45 0.45
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1061 0.45 0.45
probs:  [0.2822067920634857, 0.3888223795839902, 0.25111057903667194, 0.0778602493158521]
maxi score, test score, baseline:  0.1061 0.45 0.45
probs:  [0.2822067920634857, 0.3888223795839902, 0.25111057903667194, 0.0778602493158521]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.1061 0.45 0.45
maxi score, test score, baseline:  0.1061 0.45 0.45
probs:  [0.2853812601255099, 0.3871023829863509, 0.25000000000000006, 0.07751635688813921]
maxi score, test score, baseline:  0.1061 0.45 0.45
probs:  [0.2853812601255099, 0.3871023829863509, 0.25000000000000006, 0.07751635688813921]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1061 0.45 0.45
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
from probs:  [0.2839749861351985, 0.3891878464248456, 0.24890403270531614, 0.07793313473463964]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.1081 0.45 0.45
probs:  [0.285697256624794, 0.3895438213514673, 0.24675479485229143, 0.07800412717144722]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1081 0.45 0.45
probs:  [0.28431844476296, 0.39156358464720975, 0.24571019440463002, 0.07840777618520026]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.782]
 [0.782]
 [0.783]
 [0.819]
 [0.782]] [[0.449]
 [0.041]
 [0.004]
 [0.069]
 [0.   ]] [[0.893]
 [0.792]
 [0.783]
 [0.835]
 [0.782]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
from probs:  [0.2816306222568408, 0.3955008623814678, 0.24367387554863182, 0.07919463981305958]
maxi score, test score, baseline:  0.1081 0.45 0.45
probs:  [0.2846474739872527, 0.39383951321980665, 0.2426505358208858, 0.07886247697205492]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.875 0.    0.042]
maxi score, test score, baseline:  0.1081 0.45 0.45
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1081 0.45 0.45
probs:  [0.2862899183703574, 0.3941228186708479, 0.24066830670476522, 0.07891895625402956]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1081 0.45 0.45
probs:  [0.28496296441337926, 0.39602179254999564, 0.2397167751725355, 0.07929846786408956]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1081 0.45 0.45
probs:  [0.28496296441337926, 0.39602179254999564, 0.2397167751725355, 0.07929846786408956]
from probs:  [0.28496296441337926, 0.39602179254999564, 0.2397167751725355, 0.07929846786408956]
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]] [[-0.051]
 [-0.051]
 [-0.051]
 [-0.051]
 [-0.051]] [[0.619]
 [0.619]
 [0.619]
 [0.619]
 [0.619]]
maxi score, test score, baseline:  0.1081 0.45 0.45
probs:  [0.28496296441337926, 0.39602179254999564, 0.23971677517253553, 0.07929846786408956]
using another actor
from probs:  [0.28496296441337926, 0.39602179254999564, 0.23971677517253553, 0.07929846786408956]
actor:  0 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.14 ]
 [0.165]
 [0.   ]
 [0.154]
 [0.117]] [[ 0.1  ]
 [-0.022]
 [-0.007]
 [-0.183]
 [-0.045]] [[0.213]
 [0.177]
 [0.019]
 [0.084]
 [0.116]]
maxi score, test score, baseline:  0.1101 0.45 0.45
probs:  [0.28496296441337926, 0.3960217925499957, 0.23971677517253556, 0.07929846786408958]
line 256 mcts: sample exp_bonus -0.14580445013197332
maxi score, test score, baseline:  0.1101 0.45 0.45
probs:  [0.28496296441337926, 0.3960217925499957, 0.23971677517253556, 0.07929846786408958]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.12 ]
 [0.097]
 [0.122]
 [0.137]
 [0.12 ]] [[-0.271]
 [-0.334]
 [-0.212]
 [-0.288]
 [-0.257]] [[0.157]
 [0.103]
 [0.188]
 [0.165]
 [0.164]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.948]
 [0.982]
 [0.948]
 [0.948]
 [0.948]] [[-0.171]
 [-0.178]
 [-0.171]
 [-0.171]
 [-0.171]] [[0.948]
 [0.982]
 [0.948]
 [0.948]
 [0.948]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.14809999999999998 0.95 0.95
probs:  [0.2878845900232058, 0.39437100522356816, 0.23873701377688478, 0.07900739097634117]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.14809999999999998 0.95 0.95
probs:  [0.2878845900232058, 0.39437100522356816, 0.23873701377688478, 0.07900739097634117]
actor:  0 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.15009999999999998 0.95 0.95
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.15009999999999998 0.95 0.95
probs:  [0.2907891559581536, 0.3927620458535377, 0.2377632532125539, 0.07868554497575495]
maxi score, test score, baseline:  0.15009999999999998 0.95 0.95
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.15009999999999998 0.95 0.95
probs:  [0.292310661803224, 0.3930503327632811, 0.23589644606559196, 0.07874255936790289]
maxi score, test score, baseline:  0.15009999999999998 0.95 0.95
probs:  [0.292310661803224, 0.3930503327632811, 0.23589644606559196, 0.07874255936790289]
maxi score, test score, baseline:  0.15009999999999998 0.95 0.95
maxi score, test score, baseline:  0.15009999999999998 0.95 0.95
probs:  [0.292310661803224, 0.3930503327632811, 0.23589644606559196, 0.07874255936790289]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.15009999999999998 0.95 0.95
probs:  [0.2939708736932655, 0.3899073253876632, 0.23800794353820026, 0.0781138573808709]
maxi score, test score, baseline:  0.15009999999999998 0.95 0.95
maxi score, test score, baseline:  0.15009999999999998 0.95 0.95
probs:  [0.2939708736932655, 0.3899073253876632, 0.23800794353820026, 0.0781138573808709]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.15009999999999998 0.95 0.95
probs:  [0.29280034495087315, 0.3883546034458458, 0.24104178826609632, 0.0778032633371847]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.15009999999999998 0.95 0.95
probs:  [0.29147463253475575, 0.3902237576175075, 0.24012508749172487, 0.07817652235601193]
from probs:  [0.29147463253475575, 0.3902237576175075, 0.24012508749172487, 0.07817652235601193]
maxi score, test score, baseline:  0.15009999999999998 0.95 0.95
probs:  [0.29147463253475575, 0.3902237576175075, 0.24012508749172487, 0.07817652235601193]
Printing some Q and Qe and total Qs values:  [[0.825]
 [0.825]
 [0.825]
 [0.825]
 [0.825]] [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[0.825]
 [0.825]
 [0.825]
 [0.825]
 [0.825]]
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.15209999999999999 0.95 0.95
probs:  [0.2914746325347557, 0.3902237576175076, 0.24012508749172481, 0.07817652235601194]
Printing some Q and Qe and total Qs values:  [[0.886]
 [1.093]
 [0.886]
 [0.886]
 [0.886]] [[-0.366]
 [-0.099]
 [-0.366]
 [-0.366]
 [-0.366]] [[0.63 ]
 [1.058]
 [0.63 ]
 [0.63 ]
 [0.63 ]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.15209999999999999 0.95 0.95
probs:  [0.2942621673747301, 0.38868812444082124, 0.23918035908617702, 0.07786934909827145]
Printing some Q and Qe and total Qs values:  [[0.641]
 [1.405]
 [0.639]
 [0.641]
 [0.641]] [[-0.182]
 [-0.05 ]
 [-0.233]
 [-0.182]
 [-0.182]] [[0.375]
 [1.173]
 [0.36 ]
 [0.375]
 [0.375]]
line 256 mcts: sample exp_bonus -0.010348606322280712
maxi score, test score, baseline:  0.15209999999999999 0.95 0.95
probs:  [0.2942621673747301, 0.38868812444082124, 0.23918035908617702, 0.07786934909827145]
maxi score, test score, baseline:  0.15209999999999999 0.95 0.95
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.15209999999999999 0.95 0.95
probs:  [0.2997719080324442, 0.38565284738254424, 0.23731304305055342, 0.07726220153445819]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.15209999999999999 0.95 0.95
probs:  [0.30112478382898006, 0.38601121735634325, 0.23553072155783586, 0.07733327725684089]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.15209999999999999 0.95 0.95
probs:  [0.3038108870186616, 0.38452721754665403, 0.23462546085181096, 0.07703643458287336]
maxi score, test score, baseline:  0.15209999999999999 0.95 0.95
probs:  [0.3038108870186616, 0.38452721754665403, 0.23462546085181096, 0.07703643458287336]
maxi score, test score, baseline:  0.15209999999999999 0.95 0.95
probs:  [0.30381088701866166, 0.38452721754665414, 0.23462546085181094, 0.07703643458287339]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.1541 0.95 0.95
probs:  [0.30772749744295574, 0.38343569081076667, 0.23201930407514496, 0.07681750767113271]
maxi score, test score, baseline:  0.1541 0.95 0.95
probs:  [0.30772749744295574, 0.38343569081076667, 0.23201930407514496, 0.07681750767113271]
maxi score, test score, baseline:  0.1541 0.95 0.95
probs:  [0.30772749744295574, 0.38343569081076667, 0.23201930407514496, 0.07681750767113271]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.1541 0.95 0.95
probs:  [0.31152721740362593, 0.3823767404744681, 0.22949092753212463, 0.07660511458978127]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.1541 0.95 0.95
probs:  [0.31408479849138815, 0.3809558925693583, 0.2286384005028706, 0.07632090843638292]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1561 0.95 0.95
probs:  [0.31408479849138815, 0.3809558925693583, 0.2286384005028706, 0.07632090843638292]
maxi score, test score, baseline:  0.1561 0.95 0.95
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1561 0.95 0.95
probs:  [0.3202101197401679, 0.3785665829008269, 0.22538086710409702, 0.07584243025490833]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.1581 0.95 0.95
probs:  [0.3202101197401679, 0.3785665829008269, 0.22538086710409702, 0.07584243025490833]
Printing some Q and Qe and total Qs values:  [[1.21 ]
 [1.21 ]
 [1.362]
 [1.21 ]
 [1.21 ]] [[-0.141]
 [-0.141]
 [-0.04 ]
 [-0.141]
 [-0.141]] [[1.07 ]
 [1.07 ]
 [1.273]
 [1.07 ]
 [1.07 ]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1581 0.95 0.95
probs:  [0.3187963107797301, 0.38035090463527826, 0.22465399076536255, 0.07619879381962912]
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.855]
 [0.327]
 [0.327]
 [0.327]] [[-0.04 ]
 [ 0.015]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]] [[0.364]
 [0.906]
 [0.364]
 [0.364]
 [0.364]]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.1601 0.95 0.95
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
Printing some Q and Qe and total Qs values:  [[1.287]
 [1.287]
 [1.379]
 [1.287]
 [1.287]] [[-0.22 ]
 [-0.22 ]
 [-0.136]
 [-0.22 ]
 [-0.22 ]] [[0.881]
 [0.881]
 [0.993]
 [0.881]
 [0.881]]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  0 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1621 0.95 0.95
probs:  [0.323693610009102, 0.37761576367429867, 0.22303892316740162, 0.07565170314919757]
maxi score, test score, baseline:  0.1621 0.95 0.95
maxi score, test score, baseline:  0.1621 0.95 0.95
probs:  [0.323693610009102, 0.37761576367429867, 0.22303892316740162, 0.07565170314919757]
maxi score, test score, baseline:  0.1621 0.95 0.95
probs:  [0.323693610009102, 0.37761576367429867, 0.22303892316740162, 0.07565170314919757]
maxi score, test score, baseline:  0.1621 0.95 0.95
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
from probs:  [0.323693610009102, 0.37761576367429867, 0.22303892316740162, 0.07565170314919757]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.1641 0.95 0.95
probs:  [0.32253400501612933, 0.3762628976206698, 0.22582199832795682, 0.07538109903524402]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1641 0.95 0.95
probs:  [0.3259231191901157, 0.3753614293604237, 0.22351519098019218, 0.07520026046926845]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.1641 0.95 0.95
probs:  [0.32829495062667635, 0.37404031503776813, 0.22272872506261834, 0.07493600927293727]
maxi score, test score, baseline:  0.1641 0.95 0.95
probs:  [0.32829495062667635, 0.37404031503776813, 0.22272872506261834, 0.07493600927293727]
actor:  0 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.1681 0.95 0.95
probs:  [0.3268740495572055, 0.3757938992754271, 0.2220458001610162, 0.0752862510063512]
maxi score, test score, baseline:  0.1681 0.95 0.95
maxi score, test score, baseline:  0.1681 0.95 0.95
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.078]
 [1.078]
 [1.078]
 [1.078]
 [1.078]] [[-0.144]
 [-0.144]
 [-0.144]
 [-0.144]
 [-0.144]] [[0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]]
using another actor
from probs:  [0.3254728766096543, 0.37752313634045037, 0.22137235714806214, 0.07563162990183321]
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.1701 0.95 0.95
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
from probs:  [0.3254728766096543, 0.37752313634045037, 0.22137235714806214, 0.07563162990183321]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
probs:  [0.3229778607493126, 0.3779258970782068, 0.22338454490319187, 0.07571169726928864]
using another actor
using another actor
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
from probs:  [0.3229778607493126, 0.3779258970782068, 0.22338454490319187, 0.07571169726928864]
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]] [[0]
 [0]
 [0]
 [0]
 [0]] [[0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]]
actor:  0 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.17609999999999998 0.95 0.95
probs:  [0.3229778607493126, 0.3779258970782068, 0.22338454490319187, 0.07571169726928864]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.3229778607493126, 0.3779258970782068, 0.22338454490319187, 0.07571169726928864]
maxi score, test score, baseline:  0.17809999999999998 0.95 0.95
probs:  [0.3229778607493126, 0.3779258970782068, 0.22338454490319187, 0.07571169726928864]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.18009999999999998 0.95 0.95
probs:  [0.32258952144935954, 0.37998588724652754, 0.221301817101416, 0.07612277420269696]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.18209999999999998 0.95 0.95
probs:  [0.3235325575425687, 0.380353170189099, 0.219918499187131, 0.07619577308120133]
maxi score, test score, baseline:  0.18209999999999998 0.95 0.95
maxi score, test score, baseline:  0.18209999999999998 0.95 0.95
probs:  [0.3235325575425687, 0.380353170189099, 0.219918499187131, 0.07619577308120133]
siam score:  -0.91200274
maxi score, test score, baseline:  0.18209999999999998 0.95 0.95
probs:  [0.3235325575425687, 0.380353170189099, 0.219918499187131, 0.07619577308120133]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.18409999999999999 0.95 0.95
probs:  [0.3235325575425686, 0.38035317018909903, 0.21991849918713102, 0.07619577308120135]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.91236097
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.91172147
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.32445686201058677, 0.3807131577519191, 0.2185626582621967, 0.07626732197529754]
maxi score, test score, baseline:  0.1861 0.95 0.95
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.32445686201058677, 0.3807131577519191, 0.21856265826219667, 0.07626732197529754]
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.32445686201058677, 0.3807131577519191, 0.2185626582621967, 0.07626732197529754]
maxi score, test score, baseline:  0.1861 0.95 0.95
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.32445686201058677, 0.3807131577519191, 0.2185626582621967, 0.07626732197529754]
Printing some Q and Qe and total Qs values:  [[1.068]
 [1.09 ]
 [1.242]
 [0.523]
 [1.189]] [[ 0.287]
 [ 0.057]
 [-0.025]
 [ 0.236]
 [ 0.099]] [[0.797]
 [0.676]
 [0.749]
 [0.328]
 [0.78 ]]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.32782032319517473, 0.37697000100265343, 0.2196910320187214, 0.07551864378345045]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.3241476246969619, 0.378952390777325, 0.22098571207510184, 0.07591427245061125]
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.3241476246969619, 0.378952390777325, 0.22098571207510184, 0.07591427245061125]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.3231058514887565, 0.3777343998539811, 0.22348908682275867, 0.07567066183450377]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1861 0.95 0.95
maxi score, test score, baseline:  0.1861 0.95 0.95
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.32324380878222564, 0.3785835754176851, 0.22233011668227032, 0.07584249911781903]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
siam score:  -0.91206205
maxi score, test score, baseline:  0.1861 0.95 0.95
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.3254395448578135, 0.37735493056641634, 0.22160877344060784, 0.07559675113516236]
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.3254395448578135, 0.37735493056641634, 0.22160877344060784, 0.07559675113516236]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
siam score:  -0.9098251
maxi score, test score, baseline:  0.1861 0.95 0.95
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1861 0.95 0.95
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
from probs:  [0.32287112221528214, 0.3805273947372636, 0.220371082176204, 0.07623040087125035]
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.752]
 [0.729]
 [0.688]
 [0.688]] [[-0.241]
 [-0.088]
 [-0.117]
 [-0.241]
 [-0.241]] [[0.688]
 [0.752]
 [0.729]
 [0.688]
 [0.688]]
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.3250329825834079, 0.3793121614735328, 0.21966751767904788, 0.07598733826401138]
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.3250329825834079, 0.3793121614735328, 0.21966751767904788, 0.07598733826401138]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using another actor
from probs:  [0.32376357963706415, 0.3808708670980171, 0.21906688595865054, 0.0762986673062683]
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.3212725103650593, 0.38392966233434234, 0.21788820961574248, 0.07690961768485585]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.1861 0.95 0.95
maxi score, test score, baseline:  0.1861 0.95 0.95
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.3178595135336546, 0.38571902706730926, 0.21915476657561153, 0.07726669282342462]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1861 0.95 0.95
maxi score, test score, baseline:  0.1861 0.95 0.95
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.31667692622742255, 0.3871858597092947, 0.21857754051351352, 0.0775596735497692]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.245]
 [1.245]
 [1.329]
 [1.245]
 [1.245]] [[-0.327]
 [-0.327]
 [-0.117]
 [-0.327]
 [-0.327]] [[0.642]
 [0.642]
 [0.882]
 [0.642]
 [0.642]]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [1.074]
 [0.014]
 [0.253]
 [0.486]] [[-0.118]
 [-0.12 ]
 [-0.195]
 [-0.346]
 [-0.225]] [[0.01 ]
 [1.083]
 [0.005]
 [0.206]
 [0.469]]
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.3155087562355195, 0.38863480970772735, 0.2180073516059091, 0.07784908245084404]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.3166259642594101, 0.3862803814397023, 0.2197154707911773, 0.07737818350971043]
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.3166259642594101, 0.3862803814397023, 0.2197154707911773, 0.07737818350971043]
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.3166259642594101, 0.3862803814397023, 0.2197154707911773, 0.07737818350971043]
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.3166259642594101, 0.3862803814397023, 0.2197154707911773, 0.07737818350971043]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.3154721732727839, 0.38771732998757996, 0.21914529765305588, 0.07766519908658041]
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.3154721732727839, 0.38771732998757996, 0.21914529765305588, 0.07766519908658041]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
Printing some Q and Qe and total Qs values:  [[1.086]
 [1.08 ]
 [1.116]
 [1.189]
 [1.106]] [[0.028]
 [0.021]
 [0.02 ]
 [0.052]
 [0.09 ]] [[0.74 ]
 [0.727]
 [0.763]
 [0.867]
 [0.822]]
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.3154721732727839, 0.38771732998757996, 0.21914529765305588, 0.07766519908658041]
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.3154721732727839, 0.38771732998757996, 0.21914529765305588, 0.07766519908658041]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.3154721732727839, 0.38771732998757996, 0.21914529765305588, 0.07766519908658041]
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.3155082771993414, 0.3886337959334899, 0.21800758555380997, 0.07785034131335868]
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.1881 0.95 0.95
probs:  [0.31454975859909107, 0.3874530153698292, 0.22038305193688765, 0.07761417409419223]
maxi score, test score, baseline:  0.1881 0.95 0.95
probs:  [0.31454975859909107, 0.3874530153698292, 0.22038305193688765, 0.07761417409419223]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1921 0.95 0.95
probs:  [0.31547170027145915, 0.387716335053759, 0.21914552056172612, 0.07766644411305564]
maxi score, test score, baseline:  0.1921 0.95 0.95
probs:  [0.31547170027145915, 0.387716335053759, 0.21914552056172612, 0.07766644411305564]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
siam score:  -0.9067105
Printing some Q and Qe and total Qs values:  [[1.325]
 [1.346]
 [1.325]
 [1.325]
 [1.203]] [[-0.014]
 [-0.133]
 [-0.014]
 [-0.014]
 [ 0.04 ]] [[1.278]
 [1.27 ]
 [1.278]
 [1.278]
 [1.173]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.1921 0.95 0.95
probs:  [0.318410357447092, 0.386820714894184, 0.21728200296008646, 0.0774869246986375]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1921 0.95 0.95
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.103]
 [1.09 ]
 [1.117]
 [1.297]
 [1.026]] [[0.05 ]
 [0.162]
 [0.08 ]
 [0.025]
 [0.106]] [[0.764]
 [0.806]
 [0.793]
 [0.945]
 [0.714]]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
using another actor
maxi score, test score, baseline:  0.1981 0.95 0.95
probs:  [0.31411968536249296, 0.39076850464639246, 0.21683464550215883, 0.0782771644889558]
maxi score, test score, baseline:  0.1981 0.95 0.95
probs:  [0.3141196853624929, 0.39076850464639246, 0.21683464550215878, 0.0782771644889558]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.012]
 [1.012]
 [1.1  ]
 [1.012]
 [1.012]] [[0.009]
 [0.009]
 [0.887]
 [0.009]
 [0.009]] [[0.527]
 [0.527]
 [1.308]
 [0.527]
 [0.527]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.1981 0.95 0.95
probs:  [0.31501844186007094, 0.3909950480785809, 0.21566441834355804, 0.07832209171779016]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
using another actor
from probs:  [0.31501844186007094, 0.3909950480785809, 0.21566441834355804, 0.07832209171779016]
maxi score, test score, baseline:  0.1981 0.95 0.95
probs:  [0.3170141300750092, 0.3898555758087148, 0.2150361060478212, 0.07809418806845463]
maxi score, test score, baseline:  0.1981 0.95 0.95
probs:  [0.3170141300750092, 0.3898555758087148, 0.2150361060478212, 0.07809418806845463]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1981 0.95 0.95
probs:  [0.3180740321031342, 0.38759644786803726, 0.21668717577931734, 0.07764234424951132]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.1981 0.95 0.95
probs:  [0.31696177850483703, 0.3889636908756297, 0.21615910118572745, 0.07791542943380576]
siam score:  -0.90553373
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.1981 0.95 0.95
probs:  [0.318923212648788, 0.387846425297576, 0.21553839367560595, 0.07769196837803004]
maxi score, test score, baseline:  0.1981 0.95 0.95
probs:  [0.318923212648788, 0.387846425297576, 0.21553839367560595, 0.07769196837803004]
maxi score, test score, baseline:  0.1981 0.95 0.95
probs:  [0.318923212648788, 0.387846425297576, 0.21553839367560595, 0.07769196837803004]
maxi score, test score, baseline:  0.1981 0.95 0.95
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
line 256 mcts: sample exp_bonus 0.0014470211431825985
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.2021 0.95 0.95
probs:  [0.3238183387466782, 0.3834408431189952, 0.2159299975015331, 0.07681082063279351]
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
siam score:  -0.9058264
maxi score, test score, baseline:  0.20409999999999998 0.95 0.95
probs:  [0.3238183387466782, 0.3834408431189952, 0.2159299975015331, 0.07681082063279351]
maxi score, test score, baseline:  0.20409999999999998 0.95 0.95
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.731]
 [0.741]
 [0.721]
 [0.721]] [[-0.001]
 [-0.155]
 [-0.108]
 [-0.001]
 [-0.001]] [[0.721]
 [0.731]
 [0.741]
 [0.721]
 [0.721]]
Printing some Q and Qe and total Qs values:  [[1.14 ]
 [1.168]
 [1.329]
 [1.252]
 [1.308]] [[ 0.218]
 [ 0.014]
 [-0.197]
 [-0.062]
 [-0.154]] [[1.209]
 [1.135]
 [1.19 ]
 [1.18 ]
 [1.191]]
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.3234845799957337, 0.3850716565635867, 0.21430748971635794, 0.0771362737243217]
Printing some Q and Qe and total Qs values:  [[1.314]
 [1.314]
 [1.314]
 [1.314]
 [1.314]] [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]]
Printing some Q and Qe and total Qs values:  [[0.736]
 [0.736]
 [0.794]
 [0.736]
 [0.736]] [[-0.176]
 [-0.176]
 [-0.255]
 [-0.176]
 [-0.176]] [[0.736]
 [0.736]
 [0.794]
 [0.736]
 [0.736]]
maxi score, test score, baseline:  0.20609999999999998 0.95 0.95
maxi score, test score, baseline:  0.20609999999999998 0.95 0.95
probs:  [0.3234845799957337, 0.3850716565635867, 0.21430748971635794, 0.0771362737243217]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.20609999999999998 0.95 0.95
probs:  [0.3223793658676728, 0.38640726644292156, 0.21381031706616369, 0.077403050623242]
maxi score, test score, baseline:  0.20609999999999998 0.95 0.95
probs:  [0.3223793658676728, 0.38640726644292156, 0.21381031706616369, 0.077403050623242]
maxi score, test score, baseline:  0.20609999999999998 0.95 0.95
probs:  [0.3223793658676728, 0.38640726644292156, 0.21381031706616369, 0.077403050623242]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
siam score:  -0.9043751
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.20809999999999998 0.95 0.95
probs:  [0.32613097406674496, 0.38426735426316844, 0.21262661273087063, 0.07697505893921593]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.20809999999999998 0.95 0.95
probs:  [0.3279912329571901, 0.3832062651392716, 0.21203966537481894, 0.07676283652871926]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.20809999999999998 0.95 0.95
probs:  [0.32871937405639434, 0.38348067774779904, 0.21098257111987412, 0.07681737707593257]
maxi score, test score, baseline:  0.20809999999999998 0.95 0.95
probs:  [0.32871937405639434, 0.38348067774779904, 0.21098257111987412, 0.07681737707593257]
maxi score, test score, baseline:  0.20809999999999998 0.95 0.95
maxi score, test score, baseline:  0.20809999999999998 0.95 0.95
probs:  [0.32871937405639434, 0.38348067774779904, 0.21098257111987412, 0.07681737707593257]
maxi score, test score, baseline:  0.20809999999999998 0.95 0.95
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.20809999999999998 0.95 0.95
probs:  [0.3305522333448018, 0.3824333327872165, 0.21040652937289403, 0.07660790449508768]
Printing some Q and Qe and total Qs values:  [[0.687]
 [0.774]
 [0.733]
 [0.687]
 [0.687]] [[-0.381]
 [-0.258]
 [-0.154]
 [-0.381]
 [-0.381]] [[0.687]
 [0.774]
 [0.733]
 [0.687]
 [0.687]]
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.21009999999999998 0.95 0.95
probs:  [0.33305786932120524, 0.3816771098994716, 0.20880869895452425, 0.07645632182479895]
maxi score, test score, baseline:  0.21009999999999998 0.95 0.95
probs:  [0.33305786932120524, 0.3816771098994716, 0.20880869895452425, 0.07645632182479895]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.22 ]
 [0.598]
 [0.22 ]
 [0.22 ]
 [0.22 ]] [[-0.329]
 [-0.451]
 [-0.329]
 [-0.329]
 [-0.329]] [[0.22 ]
 [0.598]
 [0.22 ]
 [0.22 ]
 [0.22 ]]
maxi score, test score, baseline:  0.21009999999999998 0.95 0.95
probs:  [0.33193981909837195, 0.38298429656948874, 0.20835845258935198, 0.07671743174278742]
Printing some Q and Qe and total Qs values:  [[0.875]
 [0.959]
 [0.875]
 [0.875]
 [0.875]] [[0.005]
 [0.001]
 [0.005]
 [0.005]
 [0.005]] [[0.875]
 [0.959]
 [0.875]
 [0.875]
 [0.875]]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
siam score:  -0.9059546
maxi score, test score, baseline:  0.21209999999999998 0.95 0.95
probs:  [0.33193981909837195, 0.38298429656948874, 0.20835845258935198, 0.07671743174278742]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[0.841]
 [0.839]
 [0.98 ]
 [0.841]
 [0.841]] [[-0.031]
 [ 0.002]
 [ 0.215]
 [-0.031]
 [-0.031]] [[0.63 ]
 [0.649]
 [0.901]
 [0.63 ]
 [0.63 ]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.21209999999999998 0.95 0.95
probs:  [0.3343907202635011, 0.3822342782081631, 0.20680789907773567, 0.0765671024506002]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
from probs:  [0.3343907202635011, 0.3822342782081631, 0.20680789907773567, 0.0765671024506002]
maxi score, test score, baseline:  0.21409999999999998 0.95 0.95
probs:  [0.3343907202635011, 0.3822342782081631, 0.20680789907773567, 0.0765671024506002]
maxi score, test score, baseline:  0.21409999999999998 0.95 0.95
probs:  [0.3343907202635011, 0.3822342782081631, 0.20680789907773567, 0.0765671024506002]
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]] [[-0.124]
 [-0.124]
 [-0.124]
 [-0.124]
 [-0.124]] [[0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
siam score:  -0.9030069
maxi score, test score, baseline:  0.21409999999999998 0.95 0.95
probs:  [0.3343907202635011, 0.3822342782081631, 0.20680789907773567, 0.0765671024506002]
maxi score, test score, baseline:  0.21409999999999998 0.95 0.95
actor:  0 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
using another actor
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2201 0.95 0.95
probs:  [0.33328336331534414, 0.3835177729341231, 0.20637538112053397, 0.07682348262999872]
Printing some Q and Qe and total Qs values:  [[1.211]
 [1.211]
 [1.211]
 [1.211]
 [1.211]] [[-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]] [[0.818]
 [0.818]
 [0.818]
 [0.818]
 [0.818]]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.2201 0.95 0.95
probs:  [0.33240438322758326, 0.3825062482299539, 0.20846819085329804, 0.07662117768916486]
maxi score, test score, baseline:  0.2201 0.95 0.95
probs:  [0.33240438322758326, 0.3825062482299539, 0.20846819085329804, 0.07662117768916486]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2201 0.95 0.95
probs:  [0.33306547443074647, 0.3827739473184372, 0.20748617450394866, 0.07667440374686772]
Printing some Q and Qe and total Qs values:  [[1.065]
 [1.065]
 [1.073]
 [1.065]
 [1.065]] [[-0.002]
 [-0.002]
 [ 0.065]
 [-0.002]
 [-0.002]] [[0.709]
 [0.709]
 [0.766]
 [0.709]
 [0.709]]
maxi score, test score, baseline:  0.2201 0.95 0.95
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2221 0.95 0.95
probs:  [0.33198231680465723, 0.3840345814425349, 0.20705688167375094, 0.07692622007905699]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
Printing some Q and Qe and total Qs values:  [[1.131]
 [1.131]
 [1.188]
 [1.131]
 [1.131]] [[-0.039]
 [-0.039]
 [-0.202]
 [-0.039]
 [-0.039]] [[0.801]
 [0.801]
 [0.817]
 [0.801]
 [0.801]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.2221 0.95 0.95
probs:  [0.3354412288623578, 0.38204553551455295, 0.205984821495149, 0.0765284141279402]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.2221 0.95 0.95
maxi score, test score, baseline:  0.2221 0.95 0.95
probs:  [0.3382588705356587, 0.37979245667008626, 0.20587056473217066, 0.07607810806208441]
UNIT TEST: sample policy line 217 mcts : [0.042 0.542 0.375 0.    0.042]
maxi score, test score, baseline:  0.2221 0.95 0.95
probs:  [0.3382588705356587, 0.37979245667008626, 0.20587056473217066, 0.07607810806208441]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
from probs:  [0.3371572782137794, 0.3810587220547942, 0.20545294669073497, 0.07633105304069149]
siam score:  -0.89747244
maxi score, test score, baseline:  0.2221 0.95 0.95
probs:  [0.3352056801053321, 0.38133206332024877, 0.20707683784167474, 0.07638541873274425]
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.2241 0.95 0.95
probs:  [0.3369047789449147, 0.38035716841737205, 0.20654761052754264, 0.07619044211017074]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.2261 0.95 0.95
probs:  [0.3369047789449147, 0.38035716841737205, 0.20654761052754264, 0.07619044211017074]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.2261 0.95 0.95
probs:  [0.3385952088730842, 0.3793872474909073, 0.20602108336515962, 0.07599646027084896]
maxi score, test score, baseline:  0.2261 0.95 0.95
probs:  [0.3385952088730842, 0.3793872474909073, 0.20602108336515962, 0.07599646027084896]
maxi score, test score, baseline:  0.2261 0.95 0.95
probs:  [0.3385952088730842, 0.3793872474909073, 0.20602108336515962, 0.07599646027084896]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
Printing some Q and Qe and total Qs values:  [[1.334]
 [1.434]
 [1.359]
 [1.079]
 [1.378]] [[ 0.171]
 [-0.059]
 [-0.074]
 [ 0.034]
 [-0.052]] [[0.916]
 [0.958]
 [0.879]
 [0.626]
 [0.903]]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2261 0.95 0.95
probs:  [0.3413771183516437, 0.37716116469914057, 0.2059089428932628, 0.07555277405595295]
maxi score, test score, baseline:  0.2261 0.95 0.95
probs:  [0.3413771183516437, 0.37716116469914057, 0.2059089428932628, 0.07555277405595295]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
first move QE:  -0.06720299708252782
maxi score, test score, baseline:  0.2261 0.95 0.95
probs:  [0.3394132209812879, 0.3774614001222616, 0.20751286662591278, 0.07561251227053778]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
from probs:  [0.340570793380162, 0.3780483630547118, 0.2056515425517827, 0.07572930101334352]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.3411366563783508, 0.3783352916348205, 0.20474166043796183, 0.07578639154886686]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.3411366563783508, 0.3783352916348205, 0.20474166043796183, 0.07578639154886686]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.33901345588787796, 0.3807576972697792, 0.20395855729937346, 0.0762702895429693]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.33901345588787796, 0.3807576972697792, 0.20395855729937346, 0.0762702895429693]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
from probs:  [0.33875518062773496, 0.3800932099612006, 0.2050144974900521, 0.07613711192101227]
maxi score, test score, baseline:  0.2321 0.95 0.95
probs:  [0.3403590638752468, 0.37917101077468157, 0.20451724972722476, 0.07595267562284685]
Printing some Q and Qe and total Qs values:  [[1.008]
 [1.102]
 [1.008]
 [1.008]
 [1.008]] [[ 0.207]
 [-0.045]
 [ 0.207]
 [ 0.207]
 [ 0.207]] [[0.691]
 [0.723]
 [0.691]
 [0.691]
 [0.691]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  0 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
using explorer policy with actor:  1
siam score:  -0.9044424
maxi score, test score, baseline:  0.2341 0.95 0.95
probs:  [0.34195518007728104, 0.37825327747620785, 0.2040224099613595, 0.07576913248515167]
maxi score, test score, baseline:  0.2341 0.95 0.95
probs:  [0.34195518007728104, 0.37825327747620785, 0.2040224099613595, 0.07576913248515167]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
using another actor
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2341 0.95 0.95
probs:  [0.34009095582900495, 0.37852976364938046, 0.20555512845769083, 0.07582415206392376]
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
siam score:  -0.90274704
maxi score, test score, baseline:  0.23609999999999998 0.95 0.95
probs:  [0.34009095582900495, 0.37852976364938046, 0.20555512845769083, 0.07582415206392376]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.23609999999999998 0.95 0.95
probs:  [0.34167240667749, 0.3776223700804272, 0.2050625457463284, 0.0756426774957543]
maxi score, test score, baseline:  0.23609999999999998 0.95 0.95
probs:  [0.34167240667749, 0.3776223700804272, 0.2050625457463284, 0.0756426774957543]
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.23809999999999998 0.95 0.95
probs:  [0.3406386494650412, 0.3788022913450585, 0.2046806752674794, 0.07587838392242095]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.19]
 [0.19]
 [0.19]
 [0.19]
 [0.19]] [[-0.316]
 [-0.316]
 [-0.316]
 [-0.316]
 [-0.316]] [[0.19]
 [0.19]
 [0.19]
 [0.19]
 [0.19]]
maxi score, test score, baseline:  0.23809999999999998 0.95 0.95
probs:  [0.3406386494650412, 0.3788022913450585, 0.2046806752674794, 0.07587838392242095]
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.689]
 [0.809]
 [0.689]
 [0.726]] [[ 0.   ]
 [ 0.   ]
 [-0.374]
 [ 0.   ]
 [-0.108]] [[0.689]
 [0.689]
 [0.809]
 [0.689]
 [0.726]]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2781 1.0 1.0
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
Printing some Q and Qe and total Qs values:  [[1.196]
 [1.201]
 [1.329]
 [1.192]
 [1.155]] [[ 1.355]
 [ 0.   ]
 [ 0.144]
 [-0.041]
 [ 0.093]] [[1.186]
 [0.853]
 [1.017]
 [0.833]
 [0.83 ]]
Printing some Q and Qe and total Qs values:  [[1.483]
 [1.5  ]
 [1.5  ]
 [1.5  ]
 [1.501]] [[-0.254]
 [-0.059]
 [-0.132]
 [ 0.   ]
 [-0.001]] [[1.419]
 [1.486]
 [1.467]
 [1.5  ]
 [1.501]]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
Printing some Q and Qe and total Qs values:  [[0.854]
 [1.032]
 [0.854]
 [0.854]
 [0.854]] [[-0.287]
 [-0.161]
 [-0.287]
 [-0.287]
 [-0.287]] [[0.594]
 [0.898]
 [0.594]
 [0.594]
 [0.594]]
UNIT TEST: sample policy line 217 mcts : [0.042 0.833 0.042 0.042 0.042]
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]] [[0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.125]] [[0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89702225
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
line 256 mcts: sample exp_bonus -0.02238311685932867
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.208 0.042 0.583 0.125 0.042]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.9003411
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
siam score:  -0.9006489
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.07720669453901072
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.2  ]
 [1.2  ]
 [1.236]
 [1.2  ]
 [1.2  ]] [[-0.292]
 [-0.292]
 [-0.314]
 [-0.292]
 [-0.292]] [[0.73 ]
 [0.73 ]
 [0.755]
 [0.73 ]
 [0.73 ]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89389634
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
Printing some Q and Qe and total Qs values:  [[0.35 ]
 [1.041]
 [0.001]
 [0.282]
 [0.275]] [[-0.273]
 [-0.172]
 [-0.37 ]
 [-0.252]
 [-0.311]] [[0.477]
 [1.106]
 [0.123]
 [0.44 ]
 [0.387]]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.101]
 [1.163]
 [1.246]
 [1.163]
 [1.163]] [[ 0.737]
 [-0.001]
 [ 0.002]
 [-0.001]
 [-0.001]] [[0.993]
 [0.555]
 [0.61 ]
 [0.555]
 [0.555]]
siam score:  -0.89289415
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
line 256 mcts: sample exp_bonus 0.005844857788636524
Printing some Q and Qe and total Qs values:  [[1.048]
 [1.034]
 [0.182]
 [0.953]
 [0.953]] [[0.073]
 [0.414]
 [0.008]
 [0.206]
 [0.206]] [[0.643]
 [0.802]
 [0.041]
 [0.646]
 [0.646]]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.013]
 [1.153]
 [1.013]
 [1.013]
 [1.013]] [[ 0.   ]
 [-0.051]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.66]
 [0.75]
 [0.66]
 [0.66]
 [0.66]]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.993]
 [1.129]
 [1.03 ]
 [0.993]
 [0.993]] [[-0.037]
 [-0.041]
 [-0.003]
 [-0.037]
 [-0.037]] [[1.069]
 [1.204]
 [1.114]
 [1.069]
 [1.069]]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
using explorer policy with actor:  1
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
Printing some Q and Qe and total Qs values:  [[0.931]
 [0.969]
 [1.083]
 [0.931]
 [0.931]] [[-0.347]
 [-0.182]
 [-0.091]
 [-0.347]
 [-0.347]] [[0.969]
 [1.048]
 [1.184]
 [0.969]
 [0.969]]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.048041046050326164
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Printing some Q and Qe and total Qs values:  [[1.12 ]
 [1.186]
 [1.12 ]
 [1.12 ]
 [1.12 ]] [[-0.107]
 [-0.09 ]
 [-0.107]
 [-0.107]
 [-0.107]] [[1.197]
 [1.276]
 [1.197]
 [1.197]
 [1.197]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.014]
 [1.08 ]
 [1.153]
 [0.955]
 [1.058]] [[ 0.   ]
 [-0.068]
 [-0.119]
 [ 0.017]
 [-0.073]] [[0.649]
 [0.665]
 [0.699]
 [0.603]
 [0.639]]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
using explorer policy with actor:  1
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
Printing some Q and Qe and total Qs values:  [[1.158]
 [1.158]
 [1.292]
 [1.158]
 [1.158]] [[-0.19 ]
 [-0.19 ]
 [-0.054]
 [-0.19 ]
 [-0.19 ]] [[0.657]
 [0.657]
 [0.925]
 [0.657]
 [0.657]]
first move QE:  -0.06949980061661026
maxi score, test score, baseline:  0.2941 1.0 1.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
siam score:  -0.89650375
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2961 1.0 1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.2961 1.0 1.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
UNIT TEST: sample policy line 217 mcts : [0.125 0.375 0.333 0.    0.167]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8963309
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.088]
 [1.174]
 [1.168]
 [1.11 ]
 [1.121]] [[ 0.221]
 [-0.075]
 [-0.034]
 [-0.019]
 [ 0.147]] [[0.837]
 [0.775]
 [0.79 ]
 [0.739]
 [0.833]]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
deleting a thread, now have 1 threads
Frames:  183535 train batches done:  21506.0 episodes:  15278
maxi score, test score, baseline:  0.3021 1.0 1.0
first move QE:  -0.0695412429134607
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89532995
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.307]
 [0.364]
 [0.341]
 [0.32 ]] [[-0.404]
 [-0.452]
 [-0.557]
 [-0.43 ]
 [-0.381]] [[0.35 ]
 [0.284]
 [0.235]
 [0.339]
 [0.367]]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.01 ]
 [0.03 ]
 [0.015]
 [0.028]] [[ 0.402]
 [-0.308]
 [-0.055]
 [-0.063]
 [-0.021]] [[0.454]
 [0.002]
 [0.177]
 [0.162]
 [0.197]]
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3021 1.0 1.0
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3021 1.0 1.0
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9042626
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3021 1.0 1.0
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3041 1.0 1.0
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.3101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3101 1.0 1.0
maxi score, test score, baseline:  0.3101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.3101 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3121 1.0 1.0
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.9056256
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.3141 1.0 1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
first move QE:  -0.06941430731141444
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
siam score:  -0.8966122
siam score:  -0.8967714
siam score:  -0.8950615
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3161 1.0 1.0
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.98 ]
 [0.978]
 [1.077]
 [1.095]
 [1.046]] [[ 0.074]
 [ 0.   ]
 [-0.   ]
 [ 0.031]
 [ 0.057]] [[0.654]
 [0.615]
 [0.714]
 [0.747]
 [0.711]]
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
using explorer policy with actor:  1
first move QE:  -0.0695367073045858
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
from probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.90052325
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
Printing some Q and Qe and total Qs values:  [[0.581]
 [1.008]
 [0.581]
 [0.581]
 [0.581]] [[-0.394]
 [-0.17 ]
 [-0.394]
 [-0.394]
 [-0.394]] [[0.538]
 [1.132]
 [0.538]
 [0.538]
 [0.538]]
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3201 1.0 1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.971]
 [0.919]
 [1.018]
 [0.971]
 [0.971]] [[-0.259]
 [-0.19 ]
 [-0.052]
 [-0.259]
 [-0.259]] [[0.924]
 [0.907]
 [1.074]
 [0.924]
 [0.924]]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3221 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.221]
 [0.093]
 [1.341]
 [1.215]
 [1.174]] [[-0.011]
 [-0.033]
 [-0.223]
 [-0.041]
 [-0.094]] [[1.182]
 [0.032]
 [1.09 ]
 [1.147]
 [1.052]]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3221 1.0 1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.9009061
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.3241 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.921]
 [0.939]
 [0.921]
 [0.921]
 [0.859]] [[0.186]
 [0.234]
 [0.186]
 [0.186]
 [0.482]] [[0.687]
 [0.753]
 [0.687]
 [0.687]
 [0.921]]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
siam score:  -0.8978505
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.334]
 [1.484]
 [1.334]
 [1.395]
 [1.4  ]] [[ 0.038]
 [ 0.059]
 [ 0.038]
 [ 0.013]
 [-0.   ]] [[1.139]
 [1.278]
 [1.139]
 [1.175]
 [1.171]]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3241 1.0 1.0
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3241 1.0 1.0
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8960383
maxi score, test score, baseline:  0.3261 1.0 1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89495516
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
first move QE:  -0.06983396244880723
start point for exploration sampling:  20016
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]] [[0.901]
 [0.901]
 [0.901]
 [0.901]
 [0.901]] [[1.033]
 [1.033]
 [1.033]
 [1.033]
 [1.033]]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.3261 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.938]
 [1.153]
 [0.938]
 [0.938]
 [0.938]] [[-0.059]
 [-0.189]
 [-0.059]
 [-0.059]
 [-0.059]] [[0.594]
 [0.678]
 [0.594]
 [0.594]
 [0.594]]
siam score:  -0.90670043
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.309]
 [1.47 ]
 [1.309]
 [1.309]
 [1.309]] [[-0.231]
 [-0.119]
 [-0.231]
 [-0.231]
 [-0.231]] [[0.773]
 [0.991]
 [0.773]
 [0.773]
 [0.773]]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20016
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
line 256 mcts: sample exp_bonus -0.020690160430967806
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 1.0 1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 1.0 1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
from probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.89510256
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8964215
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89387995
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.3281 1.0 1.0
siam score:  -0.8937885
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89270735
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
Printing some Q and Qe and total Qs values:  [[1.166]
 [1.119]
 [1.293]
 [1.229]
 [1.176]] [[ 0.004]
 [-0.003]
 [ 0.001]
 [-0.053]
 [-0.006]] [[1.075]
 [1.025]
 [1.201]
 [1.11 ]
 [1.08 ]]
siam score:  -0.88981783
maxi score, test score, baseline:  0.3321 1.0 1.0
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3321 1.0 1.0
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.888891
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.511]
 [0.855]
 [0.433]
 [0.448]] [[-0.007]
 [-0.013]
 [-0.028]
 [ 0.001]
 [-0.037]] [[0.44 ]
 [0.508]
 [0.84 ]
 [0.441]
 [0.427]]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8867841
first move QE:  -0.06945578621503898
line 256 mcts: sample exp_bonus 0.09572484987763256
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.75
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
maxi score, test score, baseline:  0.3381 1.0 1.0
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.3381 1.0 1.0
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
start point for exploration sampling:  20016
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.89445037
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.5
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
siam score:  -0.89161927
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.89365804
maxi score, test score, baseline:  0.3421 1.0 1.0
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
siam score:  -0.8943544
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
siam score:  -0.8942862
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
siam score:  -0.8940501
siam score:  -0.8934536
Printing some Q and Qe and total Qs values:  [[0.737]
 [0.737]
 [0.793]
 [0.737]
 [0.737]] [[-0.107]
 [-0.107]
 [-0.215]
 [-0.107]
 [-0.107]] [[0.737]
 [0.737]
 [0.793]
 [0.737]
 [0.737]]
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
from probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.25
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.25, 0.25, 0.25, 0.25]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  