dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 25}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64, 64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:2
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
resampling:False
resampling_use_max:False
resampling_assess_best_child:False
rs_start:1000
ep_to_batch_ratio:[9, 10]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.frozen_lakeGym_Image.gridWorld'>
same_env_each_time:True
env_size:[5, 5]
observable_size:[5, 5]
game_modes:1
env_map:[['S' 'H' 'H' 'F' 'F']
 ['F' 'F' 'H' 'F' 'H']
 ['F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'H']
 ['F' 'H' 'F' 'F' 'G']]
max_steps:40
actions_size:4
optimal_score:1
total_frames:305000
exp_gamma:0.95
atari_env:False
reward_clipping:False
memory_size:100
image_size:[48, 48]
running_reward_in_obs:False
deque_length:3
PRESET_CONFIG:4
VK:False
follow_better_policy:0.0
use_two_heads:False
use_siam:True
exploration_type:none
rdn_beta:[0, 0.0, 1]
explorer_percentage:0.0
reward_exploration:False
train_dones:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 25)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:True
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['S' 'H' 'H' 'F' 'F']
 ['F' 'F' 'H' 'F' 'H']
 ['F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'H']
 ['F' 'H' 'F' 'F' 'G']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
0 11
5 69
Sims:  6 1 epoch:  713 pick best:  False frame count:  713
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
siam score:  -0.010476534720510244
rdn probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.005363157894736842 0.0 0.005363157894736842
probs:  [1.0]
maxi score, test score, baseline:  0.005308333333333333 0.0 0.005308333333333333
probs:  [1.0]
maxi score, test score, baseline:  0.005308333333333333 0.0 0.005308333333333333
probs:  [1.0]
maxi score, test score, baseline:  0.005308333333333333 0.0 0.005308333333333333
probs:  [1.0]
maxi score, test score, baseline:  0.005308333333333333 0.0 0.005308333333333333
probs:  [1.0]
maxi score, test score, baseline:  0.005308333333333333 0.0 0.005308333333333333
probs:  [1.0]
maxi score, test score, baseline:  0.005308333333333333 0.0 0.005308333333333333
probs:  [1.0]
deleting a thread, now have 2 threads
Frames:  1236 train batches done:  29 episodes:  171
maxi score, test score, baseline:  0.005308333333333333 0.0 0.005308333333333333
probs:  [1.0]
maxi score, test score, baseline:  0.005308333333333333 0.0 0.005308333333333333
probs:  [1.0]
maxi score, test score, baseline:  0.005308333333333333 0.0 0.005308333333333333
probs:  [1.0]
maxi score, test score, baseline:  0.005308333333333333 0.0 0.005308333333333333
probs:  [1.0]
maxi score, test score, baseline:  0.005308333333333333 0.0 0.005308333333333333
probs:  [1.0]
maxi score, test score, baseline:  0.005308333333333333 0.0 0.005308333333333333
probs:  [1.0]
maxi score, test score, baseline:  0.005308333333333333 0.0 0.005308333333333333
maxi score, test score, baseline:  0.005308333333333333 0.0 0.005308333333333333
probs:  [1.0]
maxi score, test score, baseline:  0.005308333333333333 0.0 0.005308333333333333
probs:  [1.0]
maxi score, test score, baseline:  0.005308333333333333 0.0 0.005308333333333333
probs:  [1.0]
maxi score, test score, baseline:  0.005308333333333333 0.0 0.005308333333333333
probs:  [1.0]
maxi score, test score, baseline:  0.005308333333333333 0.0 0.005308333333333333
probs:  [1.0]
maxi score, test score, baseline:  0.005308333333333333 0.0 0.005308333333333333
probs:  [1.0]
maxi score, test score, baseline:  0.005308333333333333 0.0 0.005308333333333333
probs:  [1.0]
maxi score, test score, baseline:  0.005308333333333333 0.0 0.005308333333333333
probs:  [1.0]
maxi score, test score, baseline:  0.005308333333333333 0.0 0.005308333333333333
probs:  [1.0]
maxi score, test score, baseline:  0.005125125628140704 0.0 0.005125125628140704
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.00883362445414847 0.0 0.00883362445414847
probs:  [1.0]
maxi score, test score, baseline:  0.00883362445414847 0.0 0.00883362445414847
probs:  [1.0]
maxi score, test score, baseline:  0.00883362445414847 0.0 0.00883362445414847
maxi score, test score, baseline:  0.00883362445414847 0.0 0.00883362445414847
maxi score, test score, baseline:  0.00883362445414847 0.0 0.00883362445414847
probs:  [1.0]
maxi score, test score, baseline:  0.007851937984496123 0.0 0.007851937984496123
maxi score, test score, baseline:  0.007792307692307693 0.0 0.007792307692307693
probs:  [1.0]
maxi score, test score, baseline:  0.007675757575757576 0.0 0.007675757575757576
probs:  [1.0]
maxi score, test score, baseline:  0.007480073800738007 0.0 0.007480073800738007
probs:  [1.0]
maxi score, test score, baseline:  0.007117543859649123 0.0 0.007117543859649123
17 250
maxi score, test score, baseline:  0.006614657980456027 0.0 0.006614657980456027
probs:  [1.0]
maxi score, test score, baseline:  0.006489776357827476 0.0 0.006489776357827476
probs:  [1.0]
maxi score, test score, baseline:  0.006409148264984227 0.0 0.006409148264984227
probs:  [1.0]
maxi score, test score, baseline:  0.006350000000000001 0.0 0.006350000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.006350000000000001 0.0 0.006350000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.006350000000000001 0.0 0.006350000000000001
probs:  [1.0]
18 288
maxi score, test score, baseline:  0.006216207951070337 0.0 0.006216207951070337
probs:  [1.0]
18 291
maxi score, test score, baseline:  0.006124096385542169 0.0 0.006124096385542169
probs:  [1.0]
maxi score, test score, baseline:  0.005947953216374269 0.0 0.005947953216374269
maxi score, test score, baseline:  0.00556448087431694 0.0 0.00556448087431694
maxi score, test score, baseline:  0.005534782608695652 0.0 0.005534782608695652
probs:  [1.0]
siam score:  -0.39621103
maxi score, test score, baseline:  0.005534782608695652 0.0 0.005534782608695652
probs:  [1.0]
maxi score, test score, baseline:  0.005534782608695652 0.0 0.005534782608695652
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0074710073710073715 0.0 0.0074710073710073715
probs:  [1.0]
maxi score, test score, baseline:  0.0074710073710073715 0.0 0.0074710073710073715
probs:  [1.0]
maxi score, test score, baseline:  0.0074710073710073715 0.0 0.0074710073710073715
probs:  [1.0]
maxi score, test score, baseline:  0.0074710073710073715 0.0 0.0074710073710073715
probs:  [1.0]
maxi score, test score, baseline:  0.0074710073710073715 0.0 0.0074710073710073715
probs:  [1.0]
20 404
maxi score, test score, baseline:  0.006766666666666667 0.0 0.006766666666666667
probs:  [1.0]
21 432
maxi score, test score, baseline:  0.006350000000000001 0.0 0.006350000000000001
probs:  [1.0]
siam score:  -0.36645442
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
23 481
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
24 507
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
24 544
25 546
siam score:  -0.40318444
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
siam score:  -0.3971363
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
26 589
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
UNIT TEST: sample policy line 217 mcts : [0.2 0.4 0.2 0.2]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
30 615
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
32 647
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
siam score:  -0.4767828
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
siam score:  -0.48008963
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
36 725
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
siam score:  -0.47537863
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
43 798
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
siam score:  -0.5273264
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
siam score:  -0.538465
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
maxi score, test score, baseline:  0.0081 0.0 0.0081
maxi score, test score, baseline:  0.0081 0.0 0.0081
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
maxi score, test score, baseline:  0.0101 0.0 0.0101
probs:  [1.0]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0141 0.0 0.0141
47 860
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
maxi score, test score, baseline:  0.0141 0.0 0.0141
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
49 886
siam score:  -0.5494696
maxi score, test score, baseline:  0.0161 0.0 0.0161
probs:  [1.0]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.018099999999999998 0.0 0.018099999999999998
probs:  [1.0]
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.5453605
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
maxi score, test score, baseline:  0.0241 0.0 0.0241
probs:  [1.0]
main train batch thing paused
add a thread
Adding thread: now have 4 threads
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
56 974
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.026099999999999998 0.0 0.026099999999999998
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
maxi score, test score, baseline:  0.0281 0.0 0.0281
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
59 1049
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
maxi score, test score, baseline:  0.0301 0.0 0.0301
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
siam score:  -0.50884557
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
61 1107
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.0301 0.0 0.0301
probs:  [1.0]
in main func line 156:  65
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
65 1132
maxi score, test score, baseline:  0.032100000000000004 0.0 0.032100000000000004
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
maxi score, test score, baseline:  0.034100000000000005 0.0 0.034100000000000005
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0361 0.0 0.0361
probs:  [1.0]
maxi score, test score, baseline:  0.0361 0.0 0.0361
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
66 1173
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.042100000000000005 0.0 0.042100000000000005
maxi score, test score, baseline:  0.042100000000000005 0.0 0.042100000000000005
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.084]
 [0.145]
 [0.008]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.084]
 [0.145]
 [0.008]]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.011]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.011]
 [0.   ]]
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.048100000000000004 0.0 0.048100000000000004
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.   ]]
main train batch thing paused
add a thread
Adding thread: now have 5 threads
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0541 0.0 0.0541
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
74 1243
maxi score, test score, baseline:  0.056100000000000004 0.0 0.056100000000000004
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0601 0.0 0.0601
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0621 0.0 0.0621
probs:  [1.0]
maxi score, test score, baseline:  0.0621 0.0 0.0621
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0661 0.0 0.0661
probs:  [1.0]
siam score:  -0.5226462
76 1280
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0661 0.0 0.0661
probs:  [1.0]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0661 0.0 0.0661
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.07010000000000001 0.0 0.07010000000000001
probs:  [1.0]
maxi score, test score, baseline:  0.07010000000000001 0.0 0.07010000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0721 0.0 0.0721
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0721 0.0 0.0721
probs:  [1.0]
77 1321
77 1322
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.07010000000000001 0.0 0.07010000000000001
probs:  [1.0]
Starting evaluation
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.06810000000000001 0.0 0.06810000000000001
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0761 0.0 0.0761
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
rdn probs:  [1.0]
maxi score, test score, baseline:  0.0801 0.3 0.3
probs:  [1.0]
77 1347
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
78 1352
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.0801 0.3 0.3
probs:  [1.0]
78 1367
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.988]
 [0.999]
 [0.997]
 [0.966]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.988]
 [0.999]
 [0.997]
 [0.966]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.957]
 [0.739]
 [0.772]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.045]
 [0.957]
 [0.739]
 [0.772]]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
79 1401
maxi score, test score, baseline:  0.0941 0.3 0.3
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0921 0.3 0.3
probs:  [1.0]
maxi score, test score, baseline:  0.0921 0.3 0.3
maxi score, test score, baseline:  0.0921 0.3 0.3
probs:  [1.0]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
80 1413
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.073]
 [0.149]
 [0.116]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.073]
 [0.149]
 [0.116]]
80 1418
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.10010000000000001 0.3 0.3
probs:  [1.0]
maxi score, test score, baseline:  0.10010000000000001 0.3 0.3
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.056]
 [0.051]
 [0.067]
 [0.049]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.056]
 [0.051]
 [0.067]
 [0.049]]
maxi score, test score, baseline:  0.1061 0.3 0.3
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.833 0.083]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
83 1450
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.11410000000000001 0.3 0.3
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.11610000000000001 0.3 0.3
probs:  [1.0]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1241 0.3 0.3
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1281 0.3 0.3
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.007]
 [0.078]
 [0.063]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.047]
 [0.007]
 [0.078]
 [0.063]]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1341 0.3 0.3
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.833 0.042 0.083]
maxi score, test score, baseline:  0.1381 0.3 0.3
probs:  [1.0]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.7288875
maxi score, test score, baseline:  0.14209999999999998 0.3 0.3
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
85 1513
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1561 0.3 0.3
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [1.0]
maxi score, test score, baseline:  0.1581 0.3 0.3
probs:  [1.0]
maxi score, test score, baseline:  0.1581 0.3 0.3
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
85 1566
maxi score, test score, baseline:  0.1601 0.3 0.3
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1621 0.3 0.3
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.1921 0.3 0.3
probs:  [1.0]
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  0.1921 0.3 0.3
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.76563513
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  0.20609999999999998 0.3 0.3
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.20609999999999998 0.3 0.3
probs:  [1.0]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.7705719
maxi score, test score, baseline:  0.21209999999999998 0.3 0.3
probs:  [1.0]
siam score:  -0.77124465
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
88 1632
maxi score, test score, baseline:  0.21409999999999998 0.3 0.3
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2181 0.3 0.3
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2221 0.3 0.3
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2261 0.3 0.3
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.77565366
maxi score, test score, baseline:  0.2321 0.3 0.3
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.582]
 [0.163]
 [0.058]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.033]
 [0.582]
 [0.163]
 [0.058]]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2661 0.3 0.3
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2741 0.3 0.3
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.2761 0.3 0.3
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.28609999999999997 0.3 0.3
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.29209999999999997 0.3 0.3
probs:  [1.0]
siam score:  -0.7938411
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.79621917
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.197]
 [0.643]
 [0.357]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.015]
 [0.197]
 [0.643]
 [0.357]]
Printing some Q and Qe and total Qs values:  [[0.969]
 [1.   ]
 [1.   ]
 [0.999]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.969]
 [1.   ]
 [1.   ]
 [0.999]]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.067]
 [0.067]
 [0.067]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.067]
 [0.067]
 [0.067]
 [0.067]]
maxi score, test score, baseline:  0.3061 0.3 0.3061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3101 0.3 0.3101
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.856]
 [0.065]
 [0.065]
 [0.065]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.856]
 [0.065]
 [0.065]
 [0.065]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3081 0.3 0.3081
probs:  [1.0]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.3081 0.3 0.3081
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.75004137
93 1765
Printing some Q and Qe and total Qs values:  [[0.933]
 [0.933]
 [0.933]
 [0.933]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.933]
 [0.933]
 [0.933]
 [0.933]]
maxi score, test score, baseline:  0.3061 0.3 0.3061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3061 0.3 0.3061
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3121 0.3 0.3121
probs:  [1.0]
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.946]
 [0.654]
 [0.654]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.654]
 [0.946]
 [0.654]
 [0.654]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3141 0.3 0.3141
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3181 0.3 0.3181
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
94 1807
maxi score, test score, baseline:  0.3321 0.3 0.3321
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Training Flag: True
Self play flag: True
resampling flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.34609999999999996 0.3 0.34609999999999996
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3421 0.3 0.3421
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
96 1835
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3641 0.3 0.3641
maxi score, test score, baseline:  0.3641 0.3 0.3641
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Printing some Q and Qe and total Qs values:  [[0.951]
 [0.986]
 [0.951]
 [0.951]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.951]
 [0.986]
 [0.951]
 [0.951]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3741 0.3 0.3741
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3781 0.3 0.3781
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.3881 0.3 0.3881
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.7828655
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
96 1886
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4041 0.3 0.4041
probs:  [1.0]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.7990113
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.41609999999999997 0.3 0.41609999999999997
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.41609999999999997 0.3 0.41609999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
STARTED EXPV TRAINING ON FRAME NO.  14887
maxi score, test score, baseline:  0.41809999999999997 0.3 0.41809999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4241 0.3 0.4241
probs:  [1.0]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
deleting a thread, now have 4 threads
Frames:  14994 train batches done:  1052 episodes:  2022
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4261 0.3 0.4261
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4301 0.3 0.4301
probs:  [1.0]
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4321 0.3 0.4321
probs:  [1.0]
maxi score, test score, baseline:  0.4321 0.3 0.4321
probs:  [1.0]
maxi score, test score, baseline:  0.4321 0.3 0.4321
probs:  [1.0]
maxi score, test score, baseline:  0.4321 0.3 0.4321
maxi score, test score, baseline:  0.4321 0.3 0.4321
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4321 0.3 0.4321
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
101 1937
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4381 0.3 0.4381
probs:  [1.0]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
101 1948
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4541 0.3 0.4541
probs:  [1.0]
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.4621 0.3 0.4621
probs:  [1.0]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8179304
Printing some Q and Qe and total Qs values:  [[1.]
 [1.]
 [1.]
 [1.]] [[0.]
 [0.]
 [0.]
 [0.]] [[1.]
 [1.]
 [1.]
 [1.]]
actor:  0 policy actor:  0  step number:  9 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8185808
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.849]
 [0.876]
 [0.582]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.582]
 [0.849]
 [0.876]
 [0.582]]
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.951]
 [0.59 ]
 [0.356]] [[0.]
 [0.]
 [0.]
 [0.]] [[0.376]
 [0.951]
 [0.59 ]
 [0.356]]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  12 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.47209999999999996 0.3 0.47209999999999996
probs:  [1.0]
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
actor:  0 policy actor:  0  step number:  11 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
maxi score, test score, baseline:  0.48009999999999997 0.3 0.48009999999999997
probs:  [1.0]
actor:  0 policy actor:  0  step number:  10 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
deleting a thread, now have 3 threads
Frames:  15616 train batches done:  1097 episodes:  2088
