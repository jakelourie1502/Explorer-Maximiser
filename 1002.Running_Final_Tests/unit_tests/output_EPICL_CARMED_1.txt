append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 50}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.Car_Driving_Env.RaceWorld'>
same_env_each_time:True
env_size:[7, 42]
observable_size:[7, 9]
game_modes:1
env_map:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
max_steps:150
actions_size:7
optimal_score:0.86
total_frames:255000
exp_gamma:0.975
atari_env:False
memory_size:30
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:7
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:episodic
rdn_beta:[0.3333333333333333, 2, 6]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
contrast_vector:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 294)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  11.264454243602984
printing an ep nov before normalisation:  10.848057090825591
printing an ep nov before normalisation:  11.294375410383708
rdn beta is 0 so we're just using the maxi policy
Starting evaluation
siam score:  -0.004455570851198651
maxi score, test score, baseline:  -0.9544454545454546 0.0 0.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.9654172413793104 0.0 0.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.9665666666666667 0.0 0.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3650606391655828, 0.05237298827778447, 0.26083142220298333, 0.07553503649169546, 0.1044875967590842, 0.1417123171028697]
actions average: 
K:  0  action  0 :  tensor([0.1657, 0.1589, 0.1234, 0.1137, 0.1214, 0.1722, 0.1446],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0954, 0.3140, 0.1055, 0.0941, 0.1181, 0.1635, 0.1094],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1717, 0.1206, 0.2050, 0.1142, 0.0987, 0.1973, 0.0925],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0901, 0.2411, 0.1293, 0.1325, 0.0865, 0.1967, 0.1237],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1829, 0.1193, 0.0988, 0.1317, 0.1310, 0.1797, 0.1567],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1143, 0.1596, 0.0922, 0.1085, 0.0919, 0.3066, 0.1271],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1523, 0.1387, 0.1227, 0.1280, 0.1317, 0.1419, 0.1846],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
deleting a thread, now have 2 threads
Frames:  1478 train batches done:  36 episodes:  48
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
siam score:  -0.1804381403901385
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
deleting a thread, now have 1 threads
Frames:  1478 train batches done:  100 episodes:  48
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
siam score:  -0.39951771
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
siam score:  -0.40484223
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
actions average: 
K:  3  action  0 :  tensor([0.2178, 0.0658, 0.1458, 0.1389, 0.1317, 0.1582, 0.1418],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.1600, 0.4723, 0.0670, 0.0569, 0.0820, 0.0651, 0.0967],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1021, 0.0708, 0.6413, 0.0377, 0.0376, 0.0647, 0.0456],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([nan, nan, nan, nan, nan, nan, nan], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1683, 0.1293, 0.1380, 0.1381, 0.1381, 0.1167, 0.1715],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1006, 0.0813, 0.1188, 0.0865, 0.0817, 0.4248, 0.1063],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2231, 0.1249, 0.1017, 0.1184, 0.1255, 0.1109, 0.1955],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
actions average: 
K:  4  action  0 :  tensor([0.3159, 0.0916, 0.0980, 0.1540, 0.1936, 0.0625, 0.0843],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0912, 0.2387, 0.1951, 0.1328, 0.2041, 0.0716, 0.0665],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2897, 0.0147, 0.0747, 0.1874, 0.2069, 0.1441, 0.0825],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2338, 0.1015, 0.1795, 0.1004, 0.1970, 0.0840, 0.1038],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1954, 0.1409, 0.1725, 0.1433, 0.1623, 0.1138, 0.0718],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1444, 0.1767, 0.1894, 0.1147, 0.1323, 0.1518, 0.0907],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2080, 0.0304, 0.1265, 0.1254, 0.2259, 0.1598, 0.1239],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.3075917264292416, 0.04420644786185802, 0.3075917264292416, 0.07347147881378953, 0.11005276750370394, 0.1570858529621653]
printing an ep nov before normalisation:  17.172725200653076
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.1268768547675143, 0.05004135592777172, 0.3464068514524929, 0.1268768547675143, 0.22292122831719238, 0.1268768547675143]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
probs:  [0.21121874120059062, 0.05846877136999418, 0.05846877136999418, 0.05846877136999418, 0.40215620348883635, 0.21121874120059062]
maxi score, test score, baseline:  -0.967641935483871 -1.0 -0.967641935483871
from probs:  [0.057192059978253565, 0.057192059978253565, 0.057192059978253565, 0.057192059978253565, 0.3856158800434929, 0.3856158800434929]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.08512903463303051, 0.08512903463303051, 0.08512903463303051, 0.08512903463303051, 0.5743548268348474, 0.08512903463303051]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.1623080751695923, 0.1623080751695923, 0.044626104748584854, 0.1623080751695923, 0.306141594573046, 0.1623080751695923]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.1623080751695923, 0.1623080751695923, 0.044626104748584854, 0.1623080751695923, 0.306141594573046, 0.1623080751695923]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.1623080751695923, 0.1623080751695923, 0.044626104748584854, 0.1623080751695923, 0.306141594573046, 0.1623080751695923]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.1623080751695923, 0.1623080751695923, 0.044626104748584854, 0.1623080751695923, 0.306141594573046, 0.1623080751695923]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.1623080751695923, 0.1623080751695923, 0.044626104748584854, 0.1623080751695923, 0.306141594573046, 0.1623080751695923]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
siam score:  -0.41954815
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
actions average: 
K:  2  action  0 :  tensor([0.4531, 0.0438, 0.0865, 0.0892, 0.1368, 0.1022, 0.0884],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0728, 0.7095, 0.0302, 0.0468, 0.0418, 0.0407, 0.0581],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0827, 0.0577, 0.5242, 0.0732, 0.0977, 0.1001, 0.0643],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1897, 0.0660, 0.1564, 0.1574, 0.1520, 0.1690, 0.1095],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0996, 0.0430, 0.0754, 0.0821, 0.5300, 0.0793, 0.0905],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1166, 0.0362, 0.1212, 0.0922, 0.1675, 0.3509, 0.1154],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1140, 0.1205, 0.1462, 0.1712, 0.1324, 0.1272, 0.1884],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.1623080751695923, 0.1623080751695923, 0.044626104748584854, 0.1623080751695923, 0.306141594573046, 0.1623080751695923]
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.1623080751695923, 0.1623080751695923, 0.044626104748584854, 0.1623080751695923, 0.306141594573046, 0.1623080751695923]
siam score:  -0.43004516
maxi score, test score, baseline:  -0.9695969696969697 -1.0 -0.9695969696969697
probs:  [0.18395827542537296, 0.18395827542537296, 0.05056586500106711, 0.18395827542537296, 0.3469934437217469, 0.05056586500106711]
siam score:  -0.44077355
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.23259405951929962, 0.03481188096140075, 0.23259405951929962, 0.03481188096140075, 0.23259405951929962, 0.23259405951929962]
actions average: 
K:  1  action  0 :  tensor([0.4158, 0.0683, 0.1290, 0.0875, 0.1261, 0.0688, 0.1046],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0399, 0.8375, 0.0237, 0.0112, 0.0182, 0.0108, 0.0588],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1252, 0.0174, 0.6290, 0.0489, 0.0657, 0.0451, 0.0686],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0949, 0.1705, 0.1328, 0.1482, 0.1164, 0.1463, 0.1910],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1580, 0.0775, 0.0983, 0.0943, 0.3767, 0.0975, 0.0978],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0902, 0.0101, 0.1260, 0.0856, 0.0695, 0.5483, 0.0703],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1426, 0.0991, 0.1639, 0.0856, 0.0840, 0.1152, 0.3095],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.23259405951929962, 0.03481188096140075, 0.23259405951929962, 0.03481188096140075, 0.23259405951929962, 0.23259405951929962]
printing an ep nov before normalisation:  18.63474130630493
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.23686594373373268, 0.034692025780582565, 0.12735507150910969, 0.12735507150910969, 0.23686594373373268, 0.23686594373373268]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.25522066841129726, 0.09861990743131893, 0.09861990743131893, 0.0370981799034705, 0.25522066841129726, 0.25522066841129726]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.25522066841129726, 0.09861990743131893, 0.09861990743131893, 0.0370981799034705, 0.25522066841129726, 0.25522066841129726]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.18619024540383167, 0.10775638995539484, 0.10775638995539484, 0.040527370999592006, 0.2788848018428933, 0.2788848018428933]
siam score:  -0.40435138
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.14596306161760192, 0.14596306161760192, 0.09011409976862617, 0.041711666166180335, 0.28812405541499486, 0.28812405541499486]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.14596306161760192, 0.14596306161760192, 0.09011409976862617, 0.041711666166180335, 0.28812405541499486, 0.28812405541499486]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.14596306161760192, 0.14596306161760192, 0.09011409976862617, 0.041711666166180335, 0.28812405541499486, 0.28812405541499486]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.14596306161760192, 0.14596306161760192, 0.09011409976862617, 0.041711666166180335, 0.28812405541499486, 0.28812405541499486]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.11643593801198394, 0.16304722927678614, 0.11643593801198394, 0.040692589706680236, 0.28169415249628293, 0.28169415249628293]
from probs:  [0.11643593801198394, 0.16304722927678614, 0.11643593801198394, 0.040692589706680236, 0.28169415249628293, 0.28169415249628293]
UNIT TEST: sample policy line 217 mcts : [0.  0.2 0.  0.  0.2 0.6 0. ]
printing an ep nov before normalisation:  44.8330569397433
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.14864468515715376, 0.1939892731082827, 0.10934604226617532, 0.04461886573985812, 0.1939892731082827, 0.3094118606202473]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.14864468515715376, 0.1939892731082827, 0.10934604226617532, 0.04461886573985812, 0.1939892731082827, 0.3094118606202473]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.14864468515715376, 0.1939892731082827, 0.10934604226617532, 0.04461886573985812, 0.1939892731082827, 0.3094118606202473]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.14864468515715376, 0.1939892731082827, 0.10934604226617532, 0.04461886573985812, 0.1939892731082827, 0.3094118606202473]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.14864468515715376, 0.1939892731082827, 0.10934604226617532, 0.04461886573985812, 0.1939892731082827, 0.3094118606202473]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.14864468515715376, 0.1939892731082827, 0.10934604226617532, 0.04461886573985812, 0.1939892731082827, 0.3094118606202473]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.1557046507322035, 0.2032048382134124, 0.11453782158182245, 0.04673363239295971, 0.1557046507322035, 0.3241144063473985]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.1787887759645649, 0.24144697605167448, 0.12448500255573672, 0.035043493411784345, 0.1787887759645649, 0.24144697605167448]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.20247386982565008, 0.2734389227184541, 0.14097082398522, 0.039671689659805796, 0.14097082398522, 0.20247386982565008]
from probs:  [0.22685243890729873, 0.22685243890729873, 0.11779097243098566, 0.03296538739385324, 0.16868632345326506, 0.22685243890729873]
maxi score, test score, baseline:  -0.9721222222222222 -1.0 -0.9721222222222222
probs:  [0.2408661392902986, 0.2408661392902986, 0.12506349415515058, 0.034994770161146514, 0.17910472855155285, 0.17910472855155285]
printing an ep nov before normalisation:  15.63751369117341
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.2408661400444158, 0.2408661400444158, 0.12506349373232184, 0.0349947688229154, 0.17910472867796565, 0.17910472867796565]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.2408661400444158, 0.2408661400444158, 0.12506349373232184, 0.0349947688229154, 0.17910472867796565, 0.17910472867796565]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.2408661400444158, 0.2408661400444158, 0.12506349373232184, 0.0349947688229154, 0.17910472867796565, 0.17910472867796565]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.2408661400444158, 0.2408661400444158, 0.12506349373232184, 0.0349947688229154, 0.17910472867796565, 0.17910472867796565]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.2408661400444158, 0.2408661400444158, 0.12506349373232184, 0.0349947688229154, 0.17910472867796565, 0.17910472867796565]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.2408661400444158, 0.2408661400444158, 0.12506349373232184, 0.0349947688229154, 0.17910472867796565, 0.17910472867796565]
maxi score, test score, baseline:  -0.972872972972973 -1.0 -0.972872972972973
probs:  [0.2408661400444158, 0.2408661400444158, 0.12506349373232184, 0.0349947688229154, 0.17910472867796565, 0.17910472867796565]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.25293000573405755, 0.25293000573405755, 0.08125112454342708, 0.03674178497548576, 0.18807353950648606, 0.18807353950648606]
printing an ep nov before normalisation:  21.293315887451172
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.21862147509303817, 0.2790844082510782, 0.04041493525881474, 0.07754129772427783, 0.16571640857975295, 0.21862147509303817]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.21862147509303817, 0.2790844082510782, 0.04041493525881474, 0.07754129772427783, 0.16571640857975295, 0.21862147509303817]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.21862147509303817, 0.2790844082510782, 0.04041493525881474, 0.07754129772427783, 0.16571640857975295, 0.21862147509303817]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.1749737585011317, 0.2946798852168778, 0.04266698686793857, 0.08186899327777358, 0.1749737585011317, 0.2308366176351467]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.1749737585011317, 0.2946798852168778, 0.04266698686793857, 0.08186899327777358, 0.1749737585011317, 0.2308366176351467]
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.19293928241057293, 0.32494566709836853, 0.04703748880827264, 0.09026764987562089, 0.09026764987562089, 0.25454226193154417]
actions average: 
K:  4  action  0 :  tensor([0.3850, 0.0424, 0.1140, 0.0998, 0.0946, 0.0723, 0.1919],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.1492, 0.5367, 0.0775, 0.0768, 0.0539, 0.0539, 0.0520],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1509, 0.0607, 0.2001, 0.1689, 0.1422, 0.0844, 0.1930],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2182, 0.1079, 0.1597, 0.1425, 0.1465, 0.1241, 0.1011],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1095, 0.1282, 0.0875, 0.1860, 0.2347, 0.1100, 0.1440],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0985, 0.1522, 0.1684, 0.1577, 0.1074, 0.1528, 0.1631],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1892, 0.0426, 0.1457, 0.1051, 0.1744, 0.0607, 0.2825],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.19293928241057293, 0.32494566709836853, 0.04703748880827264, 0.09026764987562089, 0.09026764987562089, 0.25454226193154417]
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.2588873721901236, 0.34691416285066806, 0.05040286799409647, 0.05040286799409647, 0.1121760544225489, 0.1812166745484664]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.29640929499602203, 0.29640929499602203, 0.04335603832531061, 0.04335603832531061, 0.1183347810425583, 0.2021345523147764]
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.29640929499602203, 0.29640929499602203, 0.04335603832531061, 0.04335603832531061, 0.1183347810425583, 0.2021345523147764]
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.29640929499602203, 0.29640929499602203, 0.04335603832531061, 0.04335603832531061, 0.1183347810425583, 0.2021345523147764]
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.29640929499602203, 0.29640929499602203, 0.04335603832531061, 0.04335603832531061, 0.1183347810425583, 0.2021345523147764]
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.29640929499602203, 0.29640929499602203, 0.04335603832531061, 0.04335603832531061, 0.1183347810425583, 0.2021345523147764]
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.29640929499602203, 0.29640929499602203, 0.04335603832531061, 0.04335603832531061, 0.1183347810425583, 0.2021345523147764]
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.29640929499602203, 0.29640929499602203, 0.04335603832531061, 0.04335603832531061, 0.1183347810425583, 0.2021345523147764]
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.29640929499602203, 0.29640929499602203, 0.04335603832531061, 0.04335603832531061, 0.1183347810425583, 0.2021345523147764]
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.29640929499602203, 0.29640929499602203, 0.04335603832531061, 0.04335603832531061, 0.1183347810425583, 0.2021345523147764]
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.29640929499602203, 0.29640929499602203, 0.04335603832531061, 0.04335603832531061, 0.1183347810425583, 0.2021345523147764]
printing an ep nov before normalisation:  79.03645414830673
from probs:  [0.29640929689605344, 0.29640929689605344, 0.04335603651947331, 0.04335603651947331, 0.11833478033475636, 0.20213455283419018]
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.32044509140160965, 0.32044509140160965, 0.046862545070838986, 0.046862545070838986, 0.046862545070838986, 0.21852218198426368]
printing an ep nov before normalisation:  14.849631786346436
siam score:  -0.46403295
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.28208760649506187, 0.28208760649506187, 0.09179230699350512, 0.09179230699350512, 0.04104689379309006, 0.21119327922977593]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.31831358692574296, 0.31831358692574296, 0.10356882222320496, 0.04630355163586122, 0.04630355163586122, 0.1671969006535866]
siam score:  -0.46691054
maxi score, test score, baseline:  -0.9776777777777778 -1.0 -0.9776777777777778
maxi score, test score, baseline:  -0.9776777777777778 -1.0 -0.9776777777777778
probs:  [0.2762419047070355, 0.3689858365907334, 0.05365646818616085, 0.05365646818616085, 0.05365646818616085, 0.19380285414374873]
using another actor
siam score:  -0.467482
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.15886502067077257, 0.3962108706867086, 0.057993034413999604, 0.057993034413999604, 0.057993034413999604, 0.27094500540051997]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.15886502067077257, 0.3962108706867086, 0.057993034413999604, 0.057993034413999604, 0.057993034413999604, 0.27094500540051997]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.15886502067077257, 0.3962108706867086, 0.057993034413999604, 0.057993034413999604, 0.057993034413999604, 0.27094500540051997]
siam score:  -0.4761077
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.15886502067077257, 0.3962108706867086, 0.057993034413999604, 0.057993034413999604, 0.057993034413999604, 0.27094500540051997]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.15886502067077257, 0.3962108706867086, 0.057993034413999604, 0.057993034413999604, 0.057993034413999604, 0.27094500540051997]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.15886502067077257, 0.3962108706867086, 0.057993034413999604, 0.057993034413999604, 0.057993034413999604, 0.27094500540051997]
maxi score, test score, baseline:  -0.9786234042553191 -1.0 -0.9786234042553191
probs:  [0.15886502067077257, 0.3962108706867086, 0.057993034413999604, 0.057993034413999604, 0.057993034413999604, 0.27094500540051997]
from probs:  [0.15886502067077257, 0.3962108706867086, 0.057993034413999604, 0.057993034413999604, 0.057993034413999604, 0.27094500540051997]
siam score:  -0.49432206
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.15886502028049893, 0.39621088216954575, 0.057993028977653995, 0.057993028977653995, 0.057993028977653995, 0.27094501061699316]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.15886502028049893, 0.39621088216954575, 0.057993028977653995, 0.057993028977653995, 0.057993028977653995, 0.27094501061699316]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.057193636302133505, 0.38561272739573293, 0.057193636302133505, 0.057193636302133505, 0.057193636302133505, 0.38561272739573293]
printing an ep nov before normalisation:  54.91851747451495
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.04371809495707217, 0.298094450218302, 0.12003100153544126, 0.12003100153544126, 0.12003100153544126, 0.298094450218302]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.04124518039662517, 0.2828829062379894, 0.09302469307691758, 0.14998215702523926, 0.14998215702523926, 0.2828829062379894]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.04124518039662517, 0.2828829062379894, 0.09302469307691758, 0.14998215702523926, 0.14998215702523926, 0.2828829062379894]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.04124518039662517, 0.2828829062379894, 0.09302469307691758, 0.14998215702523926, 0.14998215702523926, 0.2828829062379894]
using another actor
printing an ep nov before normalisation:  31.91402753194173
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
probs:  [0.052632429135031286, 0.14489575829476456, 0.052632429135031286, 0.14489575829476456, 0.24638542037047165, 0.35855820476993666]
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
maxi score, test score, baseline:  -0.9802921568627451 -1.0 -0.9802921568627451
probs:  [0.052632429135031286, 0.14489575829476456, 0.052632429135031286, 0.14489575829476456, 0.24638542037047165, 0.35855820476993666]
using explorer policy with actor:  1
siam score:  -0.48182747
from probs:  [0.05080983739528875, 0.18539479701206305, 0.1150435681214764, 0.1150435681214764, 0.18539479701206305, 0.3483134323376324]
actions average: 
K:  1  action  0 :  tensor([0.3770, 0.0264, 0.0902, 0.1197, 0.1633, 0.1368, 0.0867],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0811, 0.6698, 0.0364, 0.0641, 0.0631, 0.0348, 0.0507],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0762, 0.1107, 0.4550, 0.0998, 0.0968, 0.1000, 0.0615],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2073, 0.0549, 0.1372, 0.1882, 0.1626, 0.1284, 0.1214],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1429, 0.1219, 0.0519, 0.1235, 0.3816, 0.1299, 0.0484],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1434, 0.0109, 0.1032, 0.0903, 0.1226, 0.4934, 0.0363],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1162, 0.0536, 0.1132, 0.1066, 0.1226, 0.1106, 0.3771],
       grad_fn=<DivBackward0>)
UNIT TEST: sample policy line 217 mcts : [0.2 0.4 0.  0.2 0.2 0.  0. ]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
siam score:  -0.47681534
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
probs:  [0.04778361297862173, 0.22380787643147576, 0.04778361297862173, 0.13179519326293831, 0.22380787643147576, 0.3250218279168668]
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
probs:  [0.04778361297862173, 0.22380787643147576, 0.04778361297862173, 0.13179519326293831, 0.22380787643147576, 0.3250218279168668]
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
probs:  [0.04778361297862173, 0.22380787643147576, 0.04778361297862173, 0.13179519326293831, 0.22380787643147576, 0.3250218279168668]
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
probs:  [0.04778361297862173, 0.22380787643147576, 0.04778361297862173, 0.13179519326293831, 0.22380787643147576, 0.3250218279168668]
actions average: 
K:  0  action  0 :  tensor([0.3030, 0.0282, 0.1310, 0.1381, 0.1442, 0.1207, 0.1348],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0464, 0.7044, 0.0405, 0.0553, 0.0482, 0.0589, 0.0464],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1495, 0.0248, 0.3524, 0.1184, 0.1035, 0.1492, 0.1023],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2087, 0.0275, 0.1728, 0.1674, 0.1218, 0.1734, 0.1284],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2006, 0.0132, 0.0965, 0.1312, 0.2862, 0.1902, 0.0821],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1933, 0.0378, 0.1384, 0.1407, 0.1377, 0.1828, 0.1693],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1852, 0.0284, 0.1240, 0.1288, 0.1269, 0.1133, 0.2934],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
probs:  [0.04778361297862173, 0.22380787643147576, 0.04778361297862173, 0.13179519326293831, 0.22380787643147576, 0.3250218279168668]
actions average: 
K:  2  action  0 :  tensor([0.3720, 0.0692, 0.1181, 0.1155, 0.1380, 0.1016, 0.0855],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0548, 0.7592, 0.0399, 0.0404, 0.0344, 0.0401, 0.0311],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0873, 0.0789, 0.5716, 0.0424, 0.0402, 0.1083, 0.0712],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1621, 0.0607, 0.1456, 0.2134, 0.1508, 0.1634, 0.1041],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2957, 0.0138, 0.0779, 0.1001, 0.3414, 0.1077, 0.0633],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1583, 0.0675, 0.1519, 0.1283, 0.1282, 0.2496, 0.1163],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1504, 0.0283, 0.0997, 0.1433, 0.1190, 0.1126, 0.3468],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
probs:  [0.04778361297862173, 0.22380787643147576, 0.04778361297862173, 0.13179519326293831, 0.22380787643147576, 0.3250218279168668]
actions average: 
K:  4  action  0 :  tensor([0.1818, 0.2233, 0.1467, 0.0914, 0.1290, 0.1116, 0.1162],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0615, 0.6073, 0.0727, 0.0553, 0.0713, 0.0587, 0.0731],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1661, 0.0417, 0.2177, 0.1292, 0.1322, 0.1848, 0.1283],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1986, 0.1052, 0.1593, 0.1043, 0.1381, 0.1518, 0.1426],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.3692, 0.0296, 0.1011, 0.0906, 0.1795, 0.1159, 0.1142],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1746, 0.1203, 0.1668, 0.1212, 0.1119, 0.1590, 0.1462],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1902, 0.2277, 0.0716, 0.0867, 0.1664, 0.1066, 0.1509],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
Printing some Q and Qe and total Qs values:  [[1.05 ]
 [1.05 ]
 [0.888]
 [1.05 ]
 [1.05 ]
 [1.05 ]
 [1.05 ]] [[15.22 ]
 [15.22 ]
 [78.015]
 [15.22 ]
 [15.22 ]
 [15.22 ]
 [15.22 ]] [[0.376]
 [0.376]
 [2.492]
 [0.376]
 [0.376]
 [0.376]
 [0.376]]
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
siam score:  -0.47009894
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
probs:  [0.0503434792525829, 0.3373464837135003, 0.0503434792525829, 0.1873221859271113, 0.1873221859271113, 0.1873221859271113]
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
probs:  [0.058320236961465426, 0.3909185793123164, 0.058320236961465426, 0.21706035490164366, 0.21706035490164366, 0.058320236961465426]
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
probs:  [0.058320236961465426, 0.3909185793123164, 0.058320236961465426, 0.21706035490164366, 0.21706035490164366, 0.058320236961465426]
maxi score, test score, baseline:  -0.9810320754716981 -1.0 -0.9810320754716981
probs:  [0.06931017953037122, 0.4647272965147095, 0.06931017953037122, 0.25803198536380545, 0.06931017953037122, 0.06931017953037122]
from probs:  [0.1330407325257941, 0.3270789098632876, 0.1330407325257941, 0.22564986261868897, 0.04814902994064116, 0.1330407325257941]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.043841165098477866, 0.293104682986861, 0.16305415191466116, 0.293104682986861, 0.043841165098477866, 0.16305415191466116]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.043841165098477866, 0.293104682986861, 0.16305415191466116, 0.293104682986861, 0.043841165098477866, 0.16305415191466116]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.043841165098477866, 0.293104682986861, 0.16305415191466116, 0.293104682986861, 0.043841165098477866, 0.16305415191466116]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.043841165098477866, 0.293104682986861, 0.16305415191466116, 0.293104682986861, 0.043841165098477866, 0.16305415191466116]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
siam score:  -0.48493293
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.05836877569156876, 0.3904823080152023, 0.21720568245504573, 0.05836877569156876, 0.05836877569156876, 0.21720568245504573]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.16491934785647575, 0.044354349953309086, 0.2959682586207878, 0.16491934785647575, 0.16491934785647575, 0.16491934785647575]
printing an ep nov before normalisation:  79.54058204437283
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.18753103967174914, 0.050422302781207684, 0.33656227542233724, 0.050422302781207684, 0.18753103967174914, 0.18753103967174914]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.21733514448709315, 0.05842037314121048, 0.39006859160218244, 0.05842037314121048, 0.05842037314121048, 0.21733514448709315]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.21733514448709315, 0.05842037314121048, 0.39006859160218244, 0.05842037314121048, 0.05842037314121048, 0.21733514448709315]
actions average: 
K:  4  action  0 :  tensor([0.1326, 0.0639, 0.1768, 0.1398, 0.1465, 0.1671, 0.1733],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.1008, 0.1454, 0.1375, 0.1194, 0.2923, 0.1534, 0.0513],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1834, 0.1576, 0.1487, 0.1389, 0.1377, 0.1299, 0.1037],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2069, 0.0869, 0.1340, 0.1538, 0.1427, 0.1432, 0.1325],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2043, 0.1089, 0.1232, 0.1644, 0.1646, 0.1291, 0.1055],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1012, 0.0559, 0.0955, 0.0912, 0.0884, 0.4835, 0.0842],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1632, 0.1511, 0.1386, 0.1571, 0.0918, 0.1246, 0.1737],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.21733514448709315, 0.05842037314121048, 0.39006859160218244, 0.05842037314121048, 0.05842037314121048, 0.21733514448709315]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.21733514448709315, 0.05842037314121048, 0.39006859160218244, 0.05842037314121048, 0.05842037314121048, 0.21733514448709315]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.38054076757557015, 0.05972961621221493, 0.38054076757557015, 0.05972961621221493, 0.05972961621221493, 0.05972961621221493]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.088091955892111, 0.088091955892111, 0.559540220539445, 0.088091955892111, 0.088091955892111, 0.088091955892111]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.04443123893289889, 0.16505830577543307, 0.2953355379653687, 0.16505830577543307, 0.16505830577543307, 0.16505830577543307]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.04443123893289889, 0.16505830577543307, 0.2953355379653687, 0.16505830577543307, 0.16505830577543307, 0.16505830577543307]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.04443123893289889, 0.16505830577543307, 0.2953355379653687, 0.16505830577543307, 0.16505830577543307, 0.16505830577543307]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.04443123893289889, 0.16505830577543307, 0.2953355379653687, 0.16505830577543307, 0.16505830577543307, 0.16505830577543307]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.04443123893289889, 0.16505830577543307, 0.2953355379653687, 0.16505830577543307, 0.16505830577543307, 0.16505830577543307]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.04443123893289889, 0.16505830577543307, 0.2953355379653687, 0.16505830577543307, 0.16505830577543307, 0.16505830577543307]
actions average: 
K:  0  action  0 :  tensor([0.3094, 0.0165, 0.1287, 0.1209, 0.1742, 0.1352, 0.1151],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0141, 0.8352, 0.0383, 0.0194, 0.0155, 0.0448, 0.0327],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1599, 0.0154, 0.3692, 0.1080, 0.0991, 0.1385, 0.1099],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1630, 0.0418, 0.2315, 0.1695, 0.1434, 0.1051, 0.1457],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2387, 0.0333, 0.0911, 0.0933, 0.3713, 0.0934, 0.0789],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1798, 0.0127, 0.1153, 0.1591, 0.1326, 0.3161, 0.0843],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1740, 0.0819, 0.1771, 0.1540, 0.0991, 0.0835, 0.2304],
       grad_fn=<DivBackward0>)
actions average: 
K:  3  action  0 :  tensor([0.4345, 0.0175, 0.1061, 0.0899, 0.1729, 0.1123, 0.0668],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0412, 0.8011, 0.0320, 0.0248, 0.0327, 0.0349, 0.0334],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1237, 0.0596, 0.4002, 0.0894, 0.0973, 0.1284, 0.1015],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1745, 0.1291, 0.1354, 0.1496, 0.1546, 0.0988, 0.1579],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2035, 0.0781, 0.0733, 0.0610, 0.4443, 0.0878, 0.0519],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1983, 0.1352, 0.1716, 0.1024, 0.1018, 0.1897, 0.1009],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2086, 0.0111, 0.2154, 0.1205, 0.1736, 0.1407, 0.1300],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.04443123893289889, 0.16505830577543307, 0.2953355379653687, 0.16505830577543307, 0.16505830577543307, 0.16505830577543307]
siam score:  -0.5171914
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.05051330131528147, 0.1877023155098311, 0.33586645083994376, 0.1877023155098311, 0.05051330131528147, 0.1877023155098311]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.05051330131528147, 0.1877023155098311, 0.33586645083994376, 0.1877023155098311, 0.05051330131528147, 0.1877023155098311]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.05051330131528147, 0.1877023155098311, 0.33586645083994376, 0.1877023155098311, 0.05051330131528147, 0.1877023155098311]
printing an ep nov before normalisation:  45.49553816724966
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.05051330131528147, 0.1877023155098311, 0.33586645083994376, 0.1877023155098311, 0.05051330131528147, 0.1877023155098311]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.05051330131528147, 0.1877023155098311, 0.33586645083994376, 0.1877023155098311, 0.05051330131528147, 0.1877023155098311]
printing an ep nov before normalisation:  24.580981467826067
siam score:  -0.4862548
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.036670323742971855, 0.23166483812851407, 0.23166483812851407, 0.23166483812851407, 0.036670323742971855, 0.23166483812851407]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.036670323742971855, 0.23166483812851407, 0.23166483812851407, 0.23166483812851407, 0.036670323742971855, 0.23166483812851407]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.036670323742971855, 0.23166483812851407, 0.23166483812851407, 0.23166483812851407, 0.036670323742971855, 0.23166483812851407]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.036670323742971855, 0.23166483812851407, 0.23166483812851407, 0.23166483812851407, 0.036670323742971855, 0.23166483812851407]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.036670323742971855, 0.23166483812851407, 0.23166483812851407, 0.23166483812851407, 0.036670323742971855, 0.23166483812851407]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.036670323742971855, 0.23166483812851407, 0.23166483812851407, 0.23166483812851407, 0.036670323742971855, 0.23166483812851407]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.036670323742971855, 0.23166483812851407, 0.23166483812851407, 0.23166483812851407, 0.036670323742971855, 0.23166483812851407]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]] [[65.005]
 [65.005]
 [65.005]
 [65.005]
 [65.005]
 [65.005]
 [65.005]] [[2.44]
 [2.44]
 [2.44]
 [2.44]
 [2.44]
 [2.44]
 [2.44]]
printing an ep nov before normalisation:  56.62110952582607
Printing some Q and Qe and total Qs values:  [[0.3]
 [0.3]
 [0.3]
 [0.3]
 [0.3]
 [0.3]
 [0.3]] [[50.66]
 [50.66]
 [50.66]
 [50.66]
 [50.66]
 [50.66]
 [50.66]] [[1.757]
 [1.757]
 [1.757]
 [1.757]
 [1.757]
 [1.757]
 [1.757]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[1.045]
 [1.045]
 [1.045]
 [1.045]
 [1.029]
 [1.045]
 [1.045]] [[18.912]
 [18.912]
 [18.912]
 [18.912]
 [42.189]
 [18.912]
 [18.912]] [[1.39 ]
 [1.39 ]
 [1.39 ]
 [1.39 ]
 [2.115]
 [1.39 ]
 [1.39 ]]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.082 0.082 0.163 0.204 0.163 0.122 0.184]
printing an ep nov before normalisation:  34.72572326254446
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.06005967929526161, 0.3798806414094768, 0.3798806414094768, 0.06005967929526161, 0.06005967929526161, 0.06005967929526161]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.06005967929526161, 0.3798806414094768, 0.3798806414094768, 0.06005967929526161, 0.06005967929526161, 0.06005967929526161]
printing an ep nov before normalisation:  34.33074275141743
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.06005967929526161, 0.3798806414094768, 0.3798806414094768, 0.06005967929526161, 0.06005967929526161, 0.06005967929526161]
rdn beta is 0 so we're just using the maxi policy
using another actor
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.08827031959962758, 0.08827031959962758, 0.558648402001862, 0.08827031959962758, 0.08827031959962758, 0.08827031959962758]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.08827031959962758, 0.08827031959962758, 0.558648402001862, 0.08827031959962758, 0.08827031959962758, 0.08827031959962758]
using another actor
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.08827031138308125, 0.08827031138308125, 0.5586484430845938, 0.08827031138308125, 0.08827031138308125, 0.08827031138308125]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
printing an ep nov before normalisation:  32.70425788315775
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.568]
 [0.572]
 [0.579]
 [0.59 ]
 [0.582]
 [0.591]] [[54.846]
 [55.976]
 [59.121]
 [61.493]
 [61.817]
 [63.051]
 [63.258]] [[0.574]
 [0.568]
 [0.572]
 [0.579]
 [0.59 ]
 [0.582]
 [0.591]]
printing an ep nov before normalisation:  24.426827236575967
printing an ep nov before normalisation:  32.445921897888184
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.52880132837456
printing an ep nov before normalisation:  20.315012896431366
printing an ep nov before normalisation:  21.437212414628302
siam score:  -0.5172201
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.928454034951386
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.81889214878512
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.078837114933396
Printing some Q and Qe and total Qs values:  [[1.027]
 [0.971]
 [1.113]
 [1.182]
 [1.204]
 [1.198]
 [1.151]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.027]
 [0.971]
 [1.113]
 [1.182]
 [1.204]
 [1.198]
 [1.151]]
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
maxi score, test score, baseline:  -0.9829508474576272 -1.0 -0.9829508474576272
probs:  [0.2316037272266248, 0.2316037272266248, 0.03679254554675045, 0.2316037272266248, 0.2316037272266248, 0.03679254554675045]
printing an ep nov before normalisation:  74.0744723805449
main train batch thing paused
add a thread
Adding thread: now have 4 threads
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.11586233681302
using another actor
from probs:  [0.2316037272266248, 0.2316037272266248, 0.03679254554675045, 0.2316037272266248, 0.2316037272266248, 0.03679254554675045]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.207]
 [1.207]
 [1.207]
 [1.207]
 [1.207]
 [1.218]
 [1.207]] [[26.94 ]
 [26.94 ]
 [26.94 ]
 [26.94 ]
 [26.94 ]
 [48.437]
 [26.94 ]] [[1.659]
 [1.659]
 [1.659]
 [1.659]
 [1.659]
 [2.229]
 [1.659]]
actions average: 
K:  0  action  0 :  tensor([0.3537, 0.0295, 0.1395, 0.0890, 0.1733, 0.1101, 0.1047],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0388, 0.7916, 0.0427, 0.0296, 0.0335, 0.0239, 0.0401],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2058, 0.0190, 0.3610, 0.0873, 0.1136, 0.1318, 0.0814],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2569, 0.0189, 0.1720, 0.1361, 0.1386, 0.1512, 0.1264],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1839, 0.0865, 0.1336, 0.0982, 0.2493, 0.1521, 0.0963],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1513, 0.0327, 0.1563, 0.1121, 0.0949, 0.3480, 0.1047],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2197, 0.0437, 0.1621, 0.1264, 0.1318, 0.1294, 0.1868],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.23408623738215376, 0.23408623738215376, 0.03537592369440242, 0.13118268207956824, 0.23408623738215376, 0.13118268207956824]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.23408623738215376, 0.23408623738215376, 0.03537592369440242, 0.13118268207956824, 0.23408623738215376, 0.13118268207956824]
actions average: 
K:  0  action  0 :  tensor([0.4801, 0.0168, 0.1231, 0.0749, 0.1258, 0.1044, 0.0750],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0438, 0.6866, 0.0369, 0.0720, 0.0577, 0.0539, 0.0492],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1438, 0.0407, 0.4484, 0.0808, 0.0908, 0.1060, 0.0894],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2311, 0.0417, 0.2013, 0.1376, 0.1253, 0.1454, 0.1176],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2831, 0.0124, 0.1794, 0.1048, 0.1928, 0.1255, 0.1020],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1630, 0.0137, 0.1524, 0.0838, 0.1077, 0.3918, 0.0876],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1734, 0.1085, 0.1514, 0.0971, 0.1333, 0.0958, 0.2405],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  55.51948547363281
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.23408623738215376, 0.23408623738215376, 0.03537592369440242, 0.13118268207956824, 0.23408623738215376, 0.13118268207956824]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.57317107289333
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.23408623738215376, 0.23408623738215376, 0.03537592369440242, 0.13118268207956824, 0.23408623738215376, 0.13118268207956824]
printing an ep nov before normalisation:  44.6761664629993
printing an ep nov before normalisation:  29.430806690772457
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.14622842429118219, 0.2609463008503532, 0.03942212542574699, 0.14622842429118219, 0.2609463008503532, 0.14622842429118219]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.14622842429118219, 0.2609463008503532, 0.03942212542574699, 0.14622842429118219, 0.2609463008503532, 0.14622842429118219]
siam score:  -0.5357537
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.14622842429118219, 0.2609463008503532, 0.03942212542574699, 0.14622842429118219, 0.2609463008503532, 0.14622842429118219]
printing an ep nov before normalisation:  46.94812774658203
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.14622842429118219, 0.2609463008503532, 0.03942212542574699, 0.14622842429118219, 0.2609463008503532, 0.14622842429118219]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.1651770477575008, 0.1651770477575008, 0.04451791611507802, 0.1651770477575008, 0.2947738928549188, 0.1651770477575008]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.1651770477575008, 0.1651770477575008, 0.04451791611507802, 0.1651770477575008, 0.2947738928549188, 0.1651770477575008]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.1651770477575008, 0.1651770477575008, 0.04451791611507802, 0.1651770477575008, 0.2947738928549188, 0.1651770477575008]
from probs:  [0.1651770477575008, 0.1651770477575008, 0.04451791611507802, 0.1651770477575008, 0.2947738928549188, 0.1651770477575008]
printing an ep nov before normalisation:  50.33504678093437
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.1907137101568789, 0.1907137101568789, 0.11311169730001841, 0.0406831519669489, 0.2740640202623959, 0.1907137101568789]
Printing some Q and Qe and total Qs values:  [[0.177]
 [0.063]
 [0.196]
 [0.222]
 [0.22 ]
 [0.216]
 [0.198]] [[42.701]
 [41.981]
 [38.122]
 [36.193]
 [38.844]
 [40.469]
 [43.819]] [[0.177]
 [0.063]
 [0.196]
 [0.222]
 [0.22 ]
 [0.216]
 [0.198]]
printing an ep nov before normalisation:  56.92963569390427
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.1907137101568789, 0.1907137101568789, 0.11311169730001841, 0.0406831519669489, 0.2740640202623959, 0.1907137101568789]
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]] [[31.837]
 [31.837]
 [31.837]
 [31.837]
 [31.837]
 [31.837]
 [31.837]] [[1.708]
 [1.708]
 [1.708]
 [1.708]
 [1.708]
 [1.708]
 [1.708]]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.1907137101568789, 0.1907137101568789, 0.11311169730001841, 0.0406831519669489, 0.2740640202623959, 0.1907137101568789]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.1907137101568789, 0.1907137101568789, 0.11311169730001841, 0.0406831519669489, 0.2740640202623959, 0.1907137101568789]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.21219867934696152, 0.21219867934696152, 0.11906501704635822, 0.03214026556579555, 0.21219867934696152, 0.21219867934696152]
printing an ep nov before normalisation:  28.00353328715217
Printing some Q and Qe and total Qs values:  [[1.144]
 [0.996]
 [1.122]
 [1.136]
 [1.171]
 [1.132]
 [1.118]] [[16.21 ]
 [13.284]
 [13.648]
 [13.954]
 [14.01 ]
 [15.031]
 [16.814]] [[1.509]
 [1.247]
 [1.386]
 [1.412]
 [1.45 ]
 [1.45 ]
 [1.506]]
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.2339966634936407, 0.13128988867283284, 0.13128988867283284, 0.03543023217341213, 0.2339966634936407, 0.2339966634936407]
printing an ep nov before normalisation:  27.3614521628303
printing an ep nov before normalisation:  28.56841802597046
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.2339966634936407, 0.13128988867283284, 0.13128988867283284, 0.03543023217341213, 0.2339966634936407, 0.2339966634936407]
printing an ep nov before normalisation:  57.13296413421631
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.2339966634936407, 0.13128988867283284, 0.13128988867283284, 0.03543023217341213, 0.2339966634936407, 0.2339966634936407]
printing an ep nov before normalisation:  26.934711900323915
main train batch thing paused
add a thread
Adding thread: now have 5 threads
printing an ep nov before normalisation:  16.649014196071803
Printing some Q and Qe and total Qs values:  [[1.188]
 [0.959]
 [1.222]
 [1.272]
 [1.281]
 [1.259]
 [1.301]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.188]
 [0.959]
 [1.222]
 [1.272]
 [1.281]
 [1.259]
 [1.301]]
printing an ep nov before normalisation:  17.579103393712813
using explorer policy with actor:  1
printing an ep nov before normalisation:  21.676120770856837
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.1463158446056749, 0.1463158446056749, 0.1463158446056749, 0.03947402878546962, 0.2607892186987528, 0.2607892186987528]
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.1463158446056749, 0.1463158446056749, 0.1463158446056749, 0.03947402878546962, 0.2607892186987528, 0.2607892186987528]
printing an ep nov before normalisation:  42.135675985391536
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.1463158446056749, 0.1463158446056749, 0.1463158446056749, 0.03947402878546962, 0.2607892186987528, 0.2607892186987528]
using explorer policy with actor:  1
main train batch thing paused
add a thread
Adding thread: now have 6 threads
printing an ep nov before normalisation:  52.98799928438935
using explorer policy with actor:  1
from probs:  [0.1463158446056749, 0.1463158446056749, 0.1463158446056749, 0.03947402878546962, 0.2607892186987528, 0.2607892186987528]
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.407]
 [0.096]] [[91.463]
 [91.463]
 [91.463]
 [91.463]
 [91.463]
 [75.43 ]
 [91.463]] [[0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.407]
 [0.096]]
printing an ep nov before normalisation:  15.407356752798485
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.313]
 [0.333]
 [0.342]
 [0.259]
 [0.243]
 [0.283]] [[62.408]
 [55.101]
 [55.078]
 [55.716]
 [56.551]
 [56.345]
 [55.267]] [[0.961]
 [0.744]
 [0.765]
 [0.783]
 [0.712]
 [0.693]
 [0.718]]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.18790607404464496, 0.050666826371553564, 0.18790607404464496, 0.050666826371553564, 0.18790607404464496, 0.33494812512295785]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
printing an ep nov before normalisation:  76.28503799438477
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.23148181032916887, 0.03703637934166226, 0.23148181032916887, 0.03703637934166226, 0.23148181032916887, 0.23148181032916887]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.23148181032916887, 0.03703637934166226, 0.23148181032916887, 0.03703637934166226, 0.23148181032916887, 0.23148181032916887]
siam score:  -0.54427093
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.23148181032916887, 0.03703637934166226, 0.23148181032916887, 0.03703637934166226, 0.23148181032916887, 0.23148181032916887]
main train batch thing paused
using explorer policy with actor:  0
printing an ep nov before normalisation:  54.88088779691398
Printing some Q and Qe and total Qs values:  [[0.984]
 [1.005]
 [0.998]
 [1.003]
 [1.011]
 [1.006]
 [1.006]] [[55.233]
 [44.559]
 [50.489]
 [49.895]
 [50.401]
 [50.523]
 [50.976]] [[2.247]
 [1.863]
 [2.081]
 [2.063]
 [2.091]
 [2.091]
 [2.108]]
printing an ep nov before normalisation:  15.979732688801192
line 256 mcts: sample exp_bonus 62.32345022449702
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.23579011846096548, 0.035109129380742815, 0.23579011846096548, 0.16443687789910869, 0.16443687789910869, 0.16443687789910869]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.23579011846096548, 0.035109129380742815, 0.23579011846096548, 0.16443687789910869, 0.16443687789910869, 0.16443687789910869]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.23579011846096548, 0.035109129380742815, 0.23579011846096548, 0.16443687789910869, 0.16443687789910869, 0.16443687789910869]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.23579011846096548, 0.035109129380742815, 0.23579011846096548, 0.16443687789910869, 0.16443687789910869, 0.16443687789910869]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.23579011846096548, 0.035109129380742815, 0.23579011846096548, 0.16443687789910869, 0.16443687789910869, 0.16443687789910869]
Printing some Q and Qe and total Qs values:  [[1.211]
 [1.06 ]
 [1.266]
 [1.316]
 [1.325]
 [1.318]
 [1.339]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.211]
 [1.06 ]
 [1.266]
 [1.316]
 [1.325]
 [1.318]
 [1.339]]
printing an ep nov before normalisation:  23.3160682856661
printing an ep nov before normalisation:  28.906196041627954
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.2721307284856862, 0.040502757500329445, 0.2721307284856862, 0.1127308344742582, 0.1127308344742582, 0.18977411657978166]
printing an ep nov before normalisation:  41.44799928321618
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.2721307284856862, 0.040502757500329445, 0.2721307284856862, 0.1127308344742582, 0.1127308344742582, 0.18977411657978166]
actions average: 
K:  2  action  0 :  tensor([0.3365, 0.0498, 0.1140, 0.1158, 0.1520, 0.1170, 0.1149],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0448, 0.7567, 0.0433, 0.0485, 0.0325, 0.0303, 0.0440],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0913, 0.0306, 0.5471, 0.0732, 0.0936, 0.0918, 0.0723],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1448, 0.1650, 0.1309, 0.1486, 0.1258, 0.0845, 0.2005],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3538, 0.0090, 0.0674, 0.0573, 0.4237, 0.0547, 0.0341],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1035, 0.1620, 0.1132, 0.1107, 0.0926, 0.3221, 0.0959],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2409, 0.0522, 0.0988, 0.1563, 0.1149, 0.0682, 0.2686],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.2721307284856862, 0.040502757500329445, 0.2721307284856862, 0.1127308344742582, 0.1127308344742582, 0.18977411657978166]
actions average: 
K:  1  action  0 :  tensor([0.2738, 0.0432, 0.0682, 0.1240, 0.2331, 0.1462, 0.1116],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0403, 0.7751, 0.0348, 0.0384, 0.0343, 0.0359, 0.0411],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1401, 0.0403, 0.4785, 0.0806, 0.0862, 0.1034, 0.0708],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2136, 0.0634, 0.1025, 0.1641, 0.1379, 0.1403, 0.1782],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1948, 0.0955, 0.0664, 0.1116, 0.3191, 0.1278, 0.0848],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1472, 0.0617, 0.1053, 0.0970, 0.1382, 0.3777, 0.0728],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1736, 0.0406, 0.1368, 0.1330, 0.1123, 0.1128, 0.2909],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.2721307284856862, 0.040502757500329445, 0.2721307284856862, 0.1127308344742582, 0.1127308344742582, 0.18977411657978166]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.2721307284856862, 0.040502757500329445, 0.2721307284856862, 0.1127308344742582, 0.1127308344742582, 0.18977411657978166]
printing an ep nov before normalisation:  28.65765675664481
siam score:  -0.56329626
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.2721307284856862, 0.040502757500329445, 0.2721307284856862, 0.1127308344742582, 0.1127308344742582, 0.18977411657978166]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.068]
 [0.327]
 [0.197]
 [0.213]
 [0.237]
 [0.253]] [[27.448]
 [13.765]
 [23.923]
 [12.661]
 [12.551]
 [12.417]
 [12.3  ]] [[0.952]
 [0.268]
 [0.8  ]
 [0.368]
 [0.381]
 [0.401]
 [0.414]]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.2965632295608451, 0.04412899890742648, 0.20680883643962944, 0.12284504932623472, 0.12284504932623472, 0.20680883643962944]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.23382894318672123, 0.035540317270368875, 0.23382894318672123, 0.13148642658473367, 0.13148642658473367, 0.23382894318672123]
Printing some Q and Qe and total Qs values:  [[0.947]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]] [[112.844]
 [ 95.007]
 [ 95.007]
 [ 95.007]
 [ 95.007]
 [ 95.007]
 [ 95.007]] [[2.524]
 [2.058]
 [2.058]
 [2.058]
 [2.058]
 [2.058]
 [2.058]]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.23382894318672123, 0.035540317270368875, 0.23382894318672123, 0.13148642658473367, 0.13148642658473367, 0.23382894318672123]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.23382894318672123, 0.035540317270368875, 0.23382894318672123, 0.13148642658473367, 0.13148642658473367, 0.23382894318672123]
printing an ep nov before normalisation:  66.85449040461025
line 256 mcts: sample exp_bonus 43.91733202211452
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.18801460697401684, 0.050777847855338225, 0.3344004833672729, 0.050777847855338225, 0.18801460697401684, 0.18801460697401684]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.046233805858350664, 0.046233805858350664, 0.2870995274749827, 0.046233805858350664, 0.2870995274749827, 0.2870995274749827]
printing an ep nov before normalisation:  37.18731755260454
printing an ep nov before normalisation:  18.809509515369438
printing an ep nov before normalisation:  10.59520959854126
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.03963528382657412, 0.14654857436167434, 0.26035949654420143, 0.14654857436167434, 0.14654857436167434, 0.26035949654420143]
Printing some Q and Qe and total Qs values:  [[1.182]
 [1.122]
 [1.122]
 [1.122]
 [1.122]
 [1.122]
 [1.122]] [[31.288]
 [24.663]
 [24.663]
 [24.663]
 [24.663]
 [24.663]
 [24.663]] [[1.515]
 [1.341]
 [1.341]
 [1.341]
 [1.341]
 [1.341]
 [1.341]]
printing an ep nov before normalisation:  37.413234269211586
printing an ep nov before normalisation:  44.6007137025931
printing an ep nov before normalisation:  56.841697692871094
printing an ep nov before normalisation:  33.42186168405088
printing an ep nov before normalisation:  13.302241563796997
printing an ep nov before normalisation:  72.18595360060951
printing an ep nov before normalisation:  12.233517393882112
printing an ep nov before normalisation:  45.293426513671875
printing an ep nov before normalisation:  0.3469398885914643
actions average: 
K:  2  action  0 :  tensor([0.2789, 0.1170, 0.0975, 0.1174, 0.1352, 0.1302, 0.1237],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0301, 0.7954, 0.0284, 0.0376, 0.0204, 0.0167, 0.0713],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1413, 0.0233, 0.3971, 0.1009, 0.0841, 0.1541, 0.0991],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1710, 0.0870, 0.1099, 0.1569, 0.1280, 0.1649, 0.1822],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2245, 0.0442, 0.0681, 0.1032, 0.3670, 0.1012, 0.0918],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1710, 0.0283, 0.0837, 0.1080, 0.1153, 0.3883, 0.1054],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2035, 0.1956, 0.0788, 0.1372, 0.0997, 0.0988, 0.1863],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  50.21525310571298
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.044368394645313614, 0.16409196620305935, 0.291539639151627, 0.16409196620305935, 0.044368394645313614, 0.291539639151627]
line 256 mcts: sample exp_bonus 38.139330956432104
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.17 ]
 [0.307]
 [0.323]
 [0.322]
 [0.312]
 [0.303]] [[70.05 ]
 [70.4  ]
 [76.718]
 [76.259]
 [72.313]
 [68.433]
 [67.554]] [[0.336]
 [0.17 ]
 [0.307]
 [0.323]
 [0.322]
 [0.312]
 [0.303]]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.050835443714131526, 0.18806224288082926, 0.18806224288082926, 0.18806224288082926, 0.050835443714131526, 0.3341423839292493]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.1135315812433727, 0.19085009947091766, 0.19085009947091766, 0.19085009947091766, 0.04076121114685987, 0.2731569091970145]
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.1135315812433727, 0.19085009947091766, 0.19085009947091766, 0.19085009947091766, 0.04076121114685987, 0.2731569091970145]
printing an ep nov before normalisation:  18.45615410304282
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.12304233379677769, 0.2068455783499908, 0.12304233379677769, 0.2068455783499908, 0.04416869186434167, 0.2960554838421213]
using explorer policy with actor:  1
printing an ep nov before normalisation:  73.37718254126301
printing an ep nov before normalisation:  62.15650897424881
printing an ep nov before normalisation:  21.600265502929688
printing an ep nov before normalisation:  30.572851483781278
printing an ep nov before normalisation:  80.60512520013701
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  95.09421849064061
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
printing an ep nov before normalisation:  49.39973043707834
using explorer policy with actor:  1
printing an ep nov before normalisation:  76.55909200875641
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.64 ]
 [0.726]
 [0.695]
 [0.711]
 [0.694]
 [0.713]] [[28.643]
 [27.716]
 [28.014]
 [28.694]
 [28.897]
 [28.919]
 [28.882]] [[1.379]
 [1.396]
 [1.498]
 [1.505]
 [1.532]
 [1.516]
 [1.533]]
printing an ep nov before normalisation:  75.89562903036916
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.05273216939109476, 0.1469441904780915, 0.1469441904780915, 0.2470444628830261, 0.05273216939109476, 0.3536028173786014]
printing an ep nov before normalisation:  62.95746678144783
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.05273216673635672, 0.14694419001854694, 0.14694419001854694, 0.2470444647558738, 0.05273216673635672, 0.35360282173431895]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.05273216673635672, 0.14694419001854694, 0.14694419001854694, 0.2470444647558738, 0.05273216673635672, 0.35360282173431895]
printing an ep nov before normalisation:  0.8054119198314424
printing an ep nov before normalisation:  59.95359998014309
printing an ep nov before normalisation:  65.89892706142564
Printing some Q and Qe and total Qs values:  [[1.076]
 [1.066]
 [1.066]
 [1.082]
 [1.084]
 [1.079]
 [1.066]] [[53.507]
 [55.969]
 [55.969]
 [56.262]
 [56.567]
 [56.673]
 [55.969]] [[1.494]
 [1.523]
 [1.523]
 [1.544]
 [1.55 ]
 [1.547]
 [1.523]]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.046928906608631984, 0.17416296908033962, 0.10867484869049006, 0.24374409699455454, 0.10867484869049006, 0.3178143299354937]
Printing some Q and Qe and total Qs values:  [[1.116]
 [1.13 ]
 [1.13 ]
 [1.13 ]
 [1.13 ]
 [1.13 ]
 [1.13 ]] [[66.994]
 [53.24 ]
 [53.24 ]
 [53.24 ]
 [53.24 ]
 [53.24 ]
 [53.24 ]] [[1.698]
 [1.519]
 [1.519]
 [1.519]
 [1.519]
 [1.519]
 [1.519]]
printing an ep nov before normalisation:  66.72997527794223
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
printing an ep nov before normalisation:  48.54836977507436
Printing some Q and Qe and total Qs values:  [[1.108]
 [1.098]
 [1.078]
 [1.128]
 [1.13 ]
 [1.098]
 [1.115]] [[33.695]
 [36.794]
 [32.019]
 [32.454]
 [32.89 ]
 [36.794]
 [34.01 ]] [[1.737]
 [1.839]
 [1.646]
 [1.713]
 [1.73 ]
 [1.839]
 [1.756]]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.053757155088825045, 0.12451283132711873, 0.053757155088825045, 0.2792908730983858, 0.12451283132711873, 0.3641691540697267]
printing an ep nov before normalisation:  27.831842501958214
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.04770010212620051, 0.1329298797074299, 0.04770010212620051, 0.3193700181663696, 0.1329298797074299, 0.3193700181663696]
printing an ep nov before normalisation:  23.959208057622163
printing an ep nov before normalisation:  48.25866891448153
printing an ep nov before normalisation:  32.51579946118514
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.04770010212620051, 0.1329298797074299, 0.04770010212620051, 0.3193700181663696, 0.1329298797074299, 0.3193700181663696]
Printing some Q and Qe and total Qs values:  [[1.144]
 [1.205]
 [1.205]
 [1.205]
 [1.205]
 [1.187]
 [1.184]] [[ 91.878]
 [102.897]
 [102.897]
 [102.897]
 [102.897]
 [100.335]
 [100.074]] [[2.879]
 [3.205]
 [3.205]
 [3.205]
 [3.205]
 [3.125]
 [3.116]]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.04770009954530666, 0.1329298789755349, 0.04770009954530666, 0.3193700214791584, 0.1329298789755349, 0.3193700214791584]
printing an ep nov before normalisation:  26.25038697124538
printing an ep nov before normalisation:  62.99331168074389
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.04770009954530666, 0.1329298789755349, 0.04770009954530666, 0.3193700214791584, 0.1329298789755349, 0.3193700214791584]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.04770009954530666, 0.1329298789755349, 0.04770009954530666, 0.3193700214791584, 0.1329298789755349, 0.3193700214791584]
from probs:  [0.04770009954530666, 0.1329298789755349, 0.04770009954530666, 0.3193700214791584, 0.1329298789755349, 0.3193700214791584]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.05213581435147344, 0.05213581435147344, 0.05213581435147344, 0.3491395500162964, 0.1453134569129869, 0.3491395500162964]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.933991943173574
printing an ep nov before normalisation:  25.310171043475666
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.10130791769695956, 0.04371316698499337, 0.10130791769695956, 0.29569020134984636, 0.16229059492139486, 0.29569020134984636]
using explorer policy with actor:  1
printing an ep nov before normalisation:  92.65970253176023
printing an ep nov before normalisation:  17.37701175451484
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.10130791769695956, 0.04371316698499337, 0.10130791769695956, 0.29569020134984636, 0.16229059492139486, 0.29569020134984636]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.10878022193622384, 0.04693178390498292, 0.10878022193622384, 0.3175187002916619, 0.1742668033810671, 0.24372226854984042]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.10878022193622384, 0.04693178390498292, 0.10878022193622384, 0.3175187002916619, 0.1742668033810671, 0.24372226854984042]
printing an ep nov before normalisation:  61.79823875427246
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]] [[80.293]
 [80.293]
 [80.293]
 [80.293]
 [80.293]
 [80.293]
 [80.293]] [[1.554]
 [1.554]
 [1.554]
 [1.554]
 [1.554]
 [1.554]
 [1.554]]
printing an ep nov before normalisation:  23.036213439728808
printing an ep nov before normalisation:  41.767779490014455
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.10878022099949027, 0.046931781967401415, 0.10878022099949027, 0.31751870273278915, 0.17426680350405455, 0.2437222697967744]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.10878022099949027, 0.046931781967401415, 0.10878022099949027, 0.31751870273278915, 0.17426680350405455, 0.2437222697967744]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.10878022099949027, 0.046931781967401415, 0.10878022099949027, 0.31751870273278915, 0.17426680350405455, 0.2437222697967744]
printing an ep nov before normalisation:  50.5586961152745
Printing some Q and Qe and total Qs values:  [[1.026]
 [1.026]
 [1.026]
 [1.026]
 [1.026]
 [1.026]
 [1.026]] [[69.151]
 [69.151]
 [69.151]
 [69.151]
 [69.151]
 [69.151]
 [69.151]] [[2.359]
 [2.359]
 [2.359]
 [2.359]
 [2.359]
 [2.359]
 [2.359]]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.11594897464029717, 0.05001964753983486, 0.05001964753983486, 0.33846045360435734, 0.1857564974525517, 0.25979477922312405]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.11594897464029717, 0.05001964753983486, 0.05001964753983486, 0.33846045360435734, 0.1857564974525517, 0.25979477922312405]
printing an ep nov before normalisation:  85.40042634178667
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
printing an ep nov before normalisation:  39.02396222550941
printing an ep nov before normalisation:  68.74268386495538
siam score:  -0.5794444
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.12203517952903743, 0.04379060364088114, 0.04379060364088114, 0.29275061783046913, 0.20488237752826205, 0.29275061783046913]
printing an ep nov before normalisation:  53.77157494989036
printing an ep nov before normalisation:  64.44452281805027
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.13378864082317374, 0.04799993152922109, 0.04799993152922109, 0.2246237447814757, 0.2246237447814757, 0.32096400655543267]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.1471539689393113, 0.052786525619945866, 0.052786525619945866, 0.24707243833628648, 0.1471539689393113, 0.35304657254519917]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.1471539689393113, 0.052786525619945866, 0.052786525619945866, 0.24707243833628648, 0.1471539689393113, 0.35304657254519917]
printing an ep nov before normalisation:  54.013114083940906
printing an ep nov before normalisation:  17.672625502734622
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.1471539689393113, 0.052786525619945866, 0.052786525619945866, 0.24707243833628648, 0.1471539689393113, 0.35304657254519917]
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]] [[28.646]
 [23.836]
 [23.836]
 [23.836]
 [23.836]
 [23.836]
 [23.836]] [[1.587]
 [1.324]
 [1.324]
 [1.324]
 [1.324]
 [1.324]
 [1.324]]
printing an ep nov before normalisation:  33.00806782100603
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.1471539689393113, 0.052786525619945866, 0.052786525619945866, 0.24707243833628648, 0.1471539689393113, 0.35304657254519917]
printing an ep nov before normalisation:  55.213657295434416
printing an ep nov before normalisation:  68.75413826964518
Printing some Q and Qe and total Qs values:  [[1.274]
 [1.148]
 [1.148]
 [1.148]
 [1.148]
 [1.148]
 [1.148]] [[61.23 ]
 [35.927]
 [35.927]
 [35.927]
 [35.927]
 [35.927]
 [35.927]] [[2.423]
 [1.717]
 [1.717]
 [1.717]
 [1.717]
 [1.717]
 [1.717]]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.0506074150987886, 0.0506074150987886, 0.0506074150987886, 0.33075043612470106, 0.18667688245423206, 0.33075043612470106]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.0506074150987886, 0.0506074150987886, 0.0506074150987886, 0.33075043612470106, 0.18667688245423206, 0.33075043612470106]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.05911140918851914, 0.05911140918851914, 0.05911140918851914, 0.21810613763447645, 0.21810613763447645, 0.3864534971654897]
printing an ep nov before normalisation:  33.79912483089568
printing an ep nov before normalisation:  43.82371156492071
printing an ep nov before normalisation:  85.97141893135635
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.12330374023539423, 0.044240590409223894, 0.12330374023539423, 0.20688478433734617, 0.20688478433734617, 0.2953823604452953]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
siam score:  -0.5877918
printing an ep nov before normalisation:  52.90081977844238
from probs:  [0.13454718693683412, 0.04826686892667403, 0.13454718693683412, 0.22575780883328883, 0.13454718693683412, 0.32233376142953474]
UNIT TEST: sample policy line 217 mcts : [0.449 0.02  0.061 0.082 0.102 0.143 0.143]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.5864705
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.15425614419483133, 0.041519337007929986, 0.15425614419483133, 0.21545612523914917, 0.15425614419483133, 0.28025610516842686]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.15425614419483133, 0.041519337007929986, 0.15425614419483133, 0.21545612523914917, 0.15425614419483133, 0.28025610516842686]
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.15425614419483133, 0.041519337007929986, 0.15425614419483133, 0.21545612523914917, 0.15425614419483133, 0.28025610516842686]
printing an ep nov before normalisation:  79.2424123793497
printing an ep nov before normalisation:  51.158857345581055
actions average: 
K:  4  action  0 :  tensor([0.3152, 0.0525, 0.1456, 0.1169, 0.1451, 0.1072, 0.1174],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0770, 0.5774, 0.0752, 0.0495, 0.0486, 0.1182, 0.0541],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2985, 0.0598, 0.0992, 0.1169, 0.1972, 0.1159, 0.1125],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2252, 0.0829, 0.1473, 0.1264, 0.1232, 0.1160, 0.1791],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2030, 0.0349, 0.1431, 0.1614, 0.1952, 0.1428, 0.1197],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2733, 0.0470, 0.1285, 0.1210, 0.1523, 0.1416, 0.1363],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2072, 0.0701, 0.1067, 0.1670, 0.1371, 0.1387, 0.1733],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  35.921040290284694
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
actions average: 
K:  4  action  0 :  tensor([0.3865, 0.0351, 0.0987, 0.1252, 0.1229, 0.1244, 0.1071],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0277, 0.8498, 0.0097, 0.0493, 0.0212, 0.0224, 0.0198],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1973, 0.0523, 0.1391, 0.1614, 0.1676, 0.1545, 0.1279],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1544, 0.2794, 0.1030, 0.1220, 0.1299, 0.1435, 0.0678],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2287, 0.0399, 0.1324, 0.1308, 0.1605, 0.2022, 0.1054],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2031, 0.1084, 0.1112, 0.1385, 0.1242, 0.2066, 0.1080],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2309, 0.0444, 0.0783, 0.1553, 0.1846, 0.1959, 0.1106],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.17728275744573196, 0.03794656597049995, 0.17728275744573196, 0.17728275744573196, 0.17728275744573196, 0.25292240424657214]
printing an ep nov before normalisation:  38.65163362513936
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.822899995896606
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
printing an ep nov before normalisation:  37.42204643172798
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.17728275744573196, 0.03794656597049995, 0.17728275744573196, 0.17728275744573196, 0.17728275744573196, 0.25292240424657214]
using explorer policy with actor:  1
printing an ep nov before normalisation:  85.24729271288928
printing an ep nov before normalisation:  79.79213372896822
Printing some Q and Qe and total Qs values:  [[1.038]
 [1.038]
 [1.038]
 [1.038]
 [1.038]
 [1.038]
 [1.038]] [[16.841]
 [16.841]
 [16.841]
 [16.841]
 [16.841]
 [16.841]
 [16.841]] [[1.431]
 [1.431]
 [1.431]
 [1.431]
 [1.431]
 [1.431]
 [1.431]]
printing an ep nov before normalisation:  58.57524394989014
using explorer policy with actor:  1
siam score:  -0.6015351
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.17728275744573196, 0.03794656597049995, 0.17728275744573196, 0.17728275744573196, 0.17728275744573196, 0.25292240424657214]
printing an ep nov before normalisation:  38.97903559592077
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.19094656845961305, 0.04086316596860754, 0.11387671312639425, 0.19094656845961305, 0.19094656845961305, 0.27242041552615903]
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.21189899473038126, 0.03258512276351276, 0.11981889831496217, 0.21189899473038126, 0.21189899473038126, 0.21189899473038126]
printing an ep nov before normalisation:  70.83878671263426
printing an ep nov before normalisation:  69.0153980255127
using explorer policy with actor:  1
printing an ep nov before normalisation:  17.43302822113037
actions average: 
K:  2  action  0 :  tensor([0.3268, 0.1029, 0.1053, 0.1187, 0.1149, 0.0892, 0.1422],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0522, 0.6975, 0.0910, 0.0462, 0.0232, 0.0330, 0.0568],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1266, 0.0415, 0.4322, 0.0925, 0.0814, 0.1072, 0.1187],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2890, 0.0372, 0.0710, 0.1659, 0.1348, 0.1243, 0.1778],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2585, 0.0717, 0.1064, 0.1034, 0.2660, 0.0888, 0.1052],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1078, 0.1130, 0.0742, 0.0627, 0.0508, 0.5177, 0.0739],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.3170, 0.0121, 0.0902, 0.1361, 0.1193, 0.0988, 0.2264],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.29086495426445397, 0.044686205633125684, 0.044686205633125684, 0.16444884010242034, 0.29086495426445397, 0.16444884010242034]
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.33045705446025114, 0.05075346915120693, 0.05075346915120693, 0.18682548362587684, 0.33045705446025114, 0.05075346915120693]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.542]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]]
printing an ep nov before normalisation:  40.69904883702596
printing an ep nov before normalisation:  59.90826727619196
printing an ep nov before normalisation:  47.40697074909065
printing an ep nov before normalisation:  29.092389742258543
printing an ep nov before normalisation:  42.25820746055621
printing an ep nov before normalisation:  88.97704610671384
printing an ep nov before normalisation:  24.640854202953474
UNIT TEST: sample policy line 217 mcts : [0.245 0.061 0.041 0.02  0.49  0.122 0.02 ]
Printing some Q and Qe and total Qs values:  [[0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]] [[69.779]
 [69.779]
 [69.779]
 [69.779]
 [69.779]
 [69.779]
 [69.779]] [[0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]]
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.33045705446025114, 0.05075346915120693, 0.05075346915120693, 0.18682548362587684, 0.33045705446025114, 0.05075346915120693]
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]] [[44.696]
 [40.867]
 [40.867]
 [40.867]
 [40.867]
 [40.867]
 [40.867]] [[0.5  ]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]]
printing an ep nov before normalisation:  52.68620723387091
siam score:  -0.61253566
printing an ep nov before normalisation:  19.939537048339844
printing an ep nov before normalisation:  34.62036296477987
printing an ep nov before normalisation:  16.36742416376395
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.33045705446025114, 0.05075346915120693, 0.05075346915120693, 0.18682548362587684, 0.33045705446025114, 0.05075346915120693]
printing an ep nov before normalisation:  35.209801437791064
printing an ep nov before normalisation:  19.893685808087355
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
printing an ep nov before normalisation:  21.899312145470866
printing an ep nov before normalisation:  17.795802354812622
actions average: 
K:  4  action  0 :  tensor([0.2878, 0.1192, 0.1188, 0.0966, 0.1451, 0.1152, 0.1173],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0588, 0.7924, 0.0154, 0.0331, 0.0289, 0.0255, 0.0458],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1770, 0.0830, 0.2629, 0.1047, 0.1211, 0.1226, 0.1288],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2609, 0.0849, 0.1013, 0.1211, 0.1664, 0.1409, 0.1245],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1658, 0.0135, 0.1230, 0.1044, 0.3443, 0.1372, 0.1118],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2930, 0.0039, 0.1075, 0.0841, 0.1130, 0.2976, 0.1010],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2226, 0.1627, 0.1096, 0.1290, 0.1329, 0.1318, 0.1114],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.33045705446025114, 0.05075346915120693, 0.05075346915120693, 0.18682548362587684, 0.33045705446025114, 0.05075346915120693]
printing an ep nov before normalisation:  15.446553051511188
printing an ep nov before normalisation:  32.651225271062536
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.33045705446025114, 0.05075346915120693, 0.05075346915120693, 0.18682548362587684, 0.33045705446025114, 0.05075346915120693]
printing an ep nov before normalisation:  25.949801191659017
siam score:  -0.6192972
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]] [[15.447]
 [15.447]
 [15.447]
 [15.447]
 [15.447]
 [15.447]
 [15.447]] [[0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]] [[24.633]
 [16.15 ]
 [16.15 ]
 [16.15 ]
 [16.15 ]
 [16.15 ]
 [16.15 ]] [[0.571]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  32.153633464645694
printing an ep nov before normalisation:  14.598240821788382
printing an ep nov before normalisation:  18.891272511111435
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.33045705446025114, 0.05075346915120693, 0.05075346915120693, 0.18682548362587684, 0.33045705446025114, 0.05075346915120693]
Printing some Q and Qe and total Qs values:  [[0.158]
 [0.167]
 [0.179]
 [0.171]
 [0.168]
 [0.166]
 [0.173]] [[33.732]
 [33.896]
 [34.19 ]
 [33.799]
 [33.789]
 [32.977]
 [34.288]] [[1.746]
 [1.77 ]
 [1.809]
 [1.764]
 [1.76 ]
 [1.683]
 [1.812]]
printing an ep nov before normalisation:  14.567821586091188
actions average: 
K:  0  action  0 :  tensor([0.3915, 0.0551, 0.0949, 0.0947, 0.1361, 0.1157, 0.1120],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0320, 0.8382, 0.0234, 0.0282, 0.0269, 0.0224, 0.0289],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2032, 0.0502, 0.2401, 0.1022, 0.1436, 0.1516, 0.1090],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2340, 0.0745, 0.1265, 0.1235, 0.1630, 0.1405, 0.1380],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2158, 0.0261, 0.1130, 0.1297, 0.2640, 0.1296, 0.1218],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1598, 0.0122, 0.0802, 0.0759, 0.0886, 0.5026, 0.0808],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2368, 0.0869, 0.1208, 0.1298, 0.1385, 0.1228, 0.1645],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]] [[64.609]
 [79.767]
 [79.767]
 [79.767]
 [79.767]
 [79.767]
 [79.767]] [[1.477]
 [1.743]
 [1.743]
 [1.743]
 [1.743]
 [1.743]
 [1.743]]
printing an ep nov before normalisation:  78.18776002260522
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.002]
 [-0.002]
 [-0.002]
 [ 0.015]
 [ 0.012]
 [-0.002]] [[37.35 ]
 [40.105]
 [40.105]
 [40.105]
 [36.949]
 [37.099]
 [40.105]] [[1.423]
 [1.639]
 [1.639]
 [1.639]
 [1.408]
 [1.416]
 [1.639]]
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.33045705446025114, 0.05075346915120693, 0.05075346915120693, 0.18682548362587684, 0.33045705446025114, 0.05075346915120693]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.426]
 [0.536]
 [0.523]
 [0.536]
 [0.559]
 [0.541]] [[21.628]
 [17.409]
 [14.146]
 [13.118]
 [16.668]
 [13.992]
 [11.677]] [[0.569]
 [0.426]
 [0.536]
 [0.523]
 [0.536]
 [0.559]
 [0.541]]
printing an ep nov before normalisation:  18.002031484675697
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]] [[23.575]
 [21.478]
 [21.478]
 [21.478]
 [21.478]
 [21.478]
 [21.478]] [[0.491]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]]
printing an ep nov before normalisation:  16.6721248626709
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.407]
 [0.442]
 [0.448]
 [0.453]
 [0.455]
 [0.458]] [[17.018]
 [23.651]
 [21.485]
 [20.039]
 [17.8  ]
 [15.975]
 [16.993]] [[0.453]
 [0.407]
 [0.442]
 [0.448]
 [0.453]
 [0.455]
 [0.458]]
using explorer policy with actor:  1
using explorer policy with actor:  0
printing an ep nov before normalisation:  14.139685493249951
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.44 ]] [[63.681]
 [63.681]
 [63.681]
 [63.681]
 [63.681]
 [63.681]
 [62.401]] [[0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.44 ]]
printing an ep nov before normalisation:  59.76292160695277
printing an ep nov before normalisation:  48.16823445952702
printing an ep nov before normalisation:  25.7399897932248
printing an ep nov before normalisation:  47.388333169001335
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.33045705446025114, 0.05075346915120693, 0.05075346915120693, 0.18682548362587684, 0.33045705446025114, 0.05075346915120693]
printing an ep nov before normalisation:  30.239357845326545
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.33045705446025114, 0.05075346915120693, 0.05075346915120693, 0.18682548362587684, 0.33045705446025114, 0.05075346915120693]
printing an ep nov before normalisation:  18.86557510562502
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.422]
 [0.445]
 [0.459]
 [0.462]
 [0.462]
 [0.456]] [[20.689]
 [20.479]
 [25.416]
 [19.633]
 [19.477]
 [18.803]
 [20.951]] [[0.456]
 [0.422]
 [0.445]
 [0.459]
 [0.462]
 [0.462]
 [0.456]]
printing an ep nov before normalisation:  30.759464558421215
printing an ep nov before normalisation:  72.85419644929891
printing an ep nov before normalisation:  29.641406016321778
printing an ep nov before normalisation:  24.71734708509531
printing an ep nov before normalisation:  78.60688111406296
printing an ep nov before normalisation:  22.079047214416892
printing an ep nov before normalisation:  15.777685409499465
printing an ep nov before normalisation:  29.189633685266585
Printing some Q and Qe and total Qs values:  [[0.095]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]] [[24.923]
 [22.553]
 [22.553]
 [22.553]
 [22.553]
 [22.553]
 [22.553]] [[1.489]
 [1.345]
 [1.345]
 [1.345]
 [1.345]
 [1.345]
 [1.345]]
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.28626403021992686, 0.047069303113406506, 0.047069303113406506, 0.28626403021992686, 0.28626403021992686, 0.047069303113406506]
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.445]
 [0.437]
 [0.445]
 [0.445]
 [0.445]
 [0.445]] [[19.748]
 [15.698]
 [21.992]
 [15.698]
 [15.698]
 [15.698]
 [15.698]] [[0.44 ]
 [0.445]
 [0.437]
 [0.445]
 [0.445]
 [0.445]
 [0.445]]
printing an ep nov before normalisation:  22.709418747371988
printing an ep nov before normalisation:  26.552338903002425
using explorer policy with actor:  0
printing an ep nov before normalisation:  72.10202756494704
printing an ep nov before normalisation:  67.03860282897949
printing an ep nov before normalisation:  12.265272982155873
using explorer policy with actor:  0
printing an ep nov before normalisation:  27.104480198253945
line 256 mcts: sample exp_bonus 23.479883457290125
printing an ep nov before normalisation:  22.120447158813477
printing an ep nov before normalisation:  80.23355451848002
from probs:  [0.3763172499873885, 0.061841375006305745, 0.061841375006305745, 0.3763172499873885, 0.061841375006305745, 0.061841375006305745]
printing an ep nov before normalisation:  88.58282089233398
printing an ep nov before normalisation:  54.28758443118362
printing an ep nov before normalisation:  47.0636332296312
printing an ep nov before normalisation:  25.384197235107422
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.19400890436003634, 0.19400890436003634, 0.19400890436003634, 0.19400890436003634, 0.029955478199818312, 0.19400890436003634]
using explorer policy with actor:  0
printing an ep nov before normalisation:  20.823139472613466
printing an ep nov before normalisation:  28.763525123994505
printing an ep nov before normalisation:  37.54004255747867
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
probs:  [0.2118334887240745, 0.2118334887240745, 0.2118334887240745, 0.11996876589544833, 0.03269727920825345, 0.2118334887240745]
using explorer policy with actor:  0
printing an ep nov before normalisation:  53.944012118686736
printing an ep nov before normalisation:  46.08337421588167
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
probs:  [0.2118334887240745, 0.2118334887240745, 0.2118334887240745, 0.11996876589544833, 0.03269727920825345, 0.2118334887240745]
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]] [[33.373]
 [32.446]
 [32.446]
 [32.446]
 [32.446]
 [32.446]
 [32.446]] [[0.533]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]]
line 256 mcts: sample exp_bonus 30.524188463998442
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.461]
 [0.492]
 [0.499]
 [0.497]
 [0.496]
 [0.49 ]] [[48.025]
 [49.038]
 [48.6  ]
 [48.787]
 [48.76 ]
 [48.489]
 [47.904]] [[0.483]
 [0.461]
 [0.492]
 [0.499]
 [0.497]
 [0.496]
 [0.49 ]]
printing an ep nov before normalisation:  22.5142375877781
printing an ep nov before normalisation:  47.43080945341342
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]] [[20.068]
 [14.485]
 [14.485]
 [14.485]
 [14.485]
 [14.485]
 [14.485]] [[0.41 ]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]]
printing an ep nov before normalisation:  32.419062725755026
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.21183348986304928, 0.21183348986304928, 0.21183348986304928, 0.11996876471786422, 0.032697275829938484, 0.21183348986304928]
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
maxi score, test score, baseline:  -0.9863864864864865 -1.0 -0.9863864864864865
probs:  [0.21183348986304928, 0.21183348986304928, 0.21183348986304928, 0.11996876471786422, 0.032697275829938484, 0.21183348986304928]
printing an ep nov before normalisation:  57.36095905303955
printing an ep nov before normalisation:  26.400482177934087
using explorer policy with actor:  0
printing an ep nov before normalisation:  28.3077044211727
printing an ep nov before normalisation:  50.78623572078599
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
probs:  [0.21183349097103582, 0.21183349097103582, 0.21183349097103582, 0.11996876357231878, 0.032697272543537895, 0.21183349097103582]
printing an ep nov before normalisation:  18.326639007534325
maxi score, test score, baseline:  -0.9865666666666667 -1.0 -0.9865666666666667
printing an ep nov before normalisation:  34.85990908828051
printing an ep nov before normalisation:  63.77782825245361
printing an ep nov before normalisation:  62.10558891296387
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]] [[39.678]
 [39.496]
 [39.496]
 [39.496]
 [39.496]
 [39.496]
 [39.496]] [[0.378]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]]
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.303]
 [0.308]
 [0.311]
 [0.305]
 [0.31 ]
 [0.302]] [[23.959]
 [27.71 ]
 [34.557]
 [32.349]
 [31.032]
 [29.852]
 [27.224]] [[0.333]
 [0.303]
 [0.308]
 [0.311]
 [0.305]
 [0.31 ]
 [0.302]]
printing an ep nov before normalisation:  76.35679233368806
printing an ep nov before normalisation:  17.237764550120115
printing an ep nov before normalisation:  16.618452072143555
maxi score, test score, baseline:  -0.986912987012987 -1.0 -0.986912987012987
probs:  [0.2118334930989684, 0.2118334930989684, 0.2118334930989684, 0.11996876137225311, 0.03269726623187343, 0.2118334930989684]
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]] [[24.899]
 [24.899]
 [24.899]
 [24.899]
 [24.899]
 [24.899]
 [24.899]] [[0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]]
printing an ep nov before normalisation:  84.70003920137322
printing an ep nov before normalisation:  13.82916508890633
maxi score, test score, baseline:  -0.9870794871794872 -1.0 -0.9870794871794872
probs:  [0.14696826629654527, 0.2595305541258098, 0.14696826629654527, 0.14696826629654527, 0.040034092858744826, 0.2595305541258098]
printing an ep nov before normalisation:  27.294649278401035
maxi score, test score, baseline:  -0.9870794871794872 -1.0 -0.9870794871794872
probs:  [0.14696826629654527, 0.2595305541258098, 0.14696826629654527, 0.14696826629654527, 0.040034092858744826, 0.2595305541258098]
printing an ep nov before normalisation:  11.292031895259107
printing an ep nov before normalisation:  21.63354893625206
printing an ep nov before normalisation:  14.522658851916798
printing an ep nov before normalisation:  22.995126492980937
printing an ep nov before normalisation:  46.5772298546004
printing an ep nov before normalisation:  18.40672014191709
Printing some Q and Qe and total Qs values:  [[0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]] [[4.676]
 [4.676]
 [4.676]
 [4.676]
 [4.676]
 [4.676]
 [4.676]] [[0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]]
printing an ep nov before normalisation:  30.78557523469272
printing an ep nov before normalisation:  51.00903033997902
printing an ep nov before normalisation:  47.50430601438666
Printing some Q and Qe and total Qs values:  [[0.889]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]] [[12.141]
 [61.94 ]
 [61.94 ]
 [61.94 ]
 [61.94 ]
 [61.94 ]
 [61.94 ]] [[0.889]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]]
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.611]
 [0.554]
 [0.591]
 [0.54 ]
 [0.592]
 [0.586]] [[20.271]
 [17.215]
 [21.522]
 [20.471]
 [19.734]
 [19.258]
 [18.877]] [[1.084]
 [1.016]
 [1.148]
 [1.139]
 [1.056]
 [1.087]
 [1.064]]
printing an ep nov before normalisation:  20.330300331115723
actor:  1 policy actor:  1  step number:  56 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  33.94136428833008
printing an ep nov before normalisation:  54.255573110391744
maxi score, test score, baseline:  -0.9874 -1.0 -0.9874
probs:  [0.09115534719591428, 0.09740445997150625, 0.08521869005910196, 0.09115534719591428, 0.07957162595335363, 0.5554945296242096]
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.494]
 [0.601]
 [0.591]
 [0.583]
 [0.556]
 [0.523]] [[39.909]
 [37.64 ]
 [37.285]
 [40.785]
 [39.398]
 [34.74 ]
 [31.541]] [[0.951]
 [0.827]
 [0.928]
 [0.972]
 [0.943]
 [0.845]
 [0.763]]
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]] [[1.944]
 [1.944]
 [1.944]
 [1.944]
 [1.944]
 [1.944]
 [1.944]] [[0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]]
maxi score, test score, baseline:  -0.9885363636363637 -1.0 -0.9885363636363637
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]] [[11.059]
 [17.235]
 [17.235]
 [17.235]
 [17.235]
 [17.235]
 [17.235]] [[0.681]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]]
Printing some Q and Qe and total Qs values:  [[0.723]
 [0.543]
 [0.542]
 [0.554]
 [0.544]
 [0.546]
 [0.552]] [[ 6.404]
 [11.911]
 [10.758]
 [11.53 ]
 [10.13 ]
 [10.174]
 [10.993]] [[0.723]
 [0.543]
 [0.542]
 [0.554]
 [0.544]
 [0.546]
 [0.552]]
printing an ep nov before normalisation:  36.055169105529785
Printing some Q and Qe and total Qs values:  [[0.803]
 [0.728]
 [0.801]
 [0.801]
 [0.728]
 [0.802]
 [0.728]] [[0.059]
 [5.253]
 [0.328]
 [0.327]
 [5.253]
 [0.323]
 [5.253]] [[0.803]
 [0.728]
 [0.801]
 [0.801]
 [0.728]
 [0.802]
 [0.728]]
maxi score, test score, baseline:  -0.9886640449438202 -1.0 -0.9886640449438202
probs:  [0.091733888694424, 0.091733888694424, 0.0857557988577802, 0.091733888694424, 0.08006932315950932, 0.5589732118994385]
printing an ep nov before normalisation:  18.88601541519165
from probs:  [0.091733888694424, 0.091733888694424, 0.0857557988577802, 0.091733888694424, 0.08006932315950932, 0.5589732118994385]
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
probs:  [0.09173445081167904, 0.09173445081167904, 0.08575595929357309, 0.09173445081167904, 0.08006910150805771, 0.5589715867633321]
printing an ep nov before normalisation:  15.8091908889699
maxi score, test score, baseline:  -0.9889109890109891 -1.0 -0.9889109890109891
probs:  [0.09173500049094296, 0.09173500049094296, 0.08575611617940274, 0.09173500049094296, 0.08006888476110838, 0.5589699975866599]
maxi score, test score, baseline:  -0.9889109890109891 -1.0 -0.9889109890109891
printing an ep nov before normalisation:  0.06819974945244667
maxi score, test score, baseline:  -0.9892617021276596 -1.0 -0.9892617021276596
probs:  [0.09228810707662126, 0.08627209022275169, 0.08627209022275169, 0.09228810707662126, 0.08054953760565617, 0.5623300677955979]
printing an ep nov before normalisation:  45.52222728729248
maxi score, test score, baseline:  -0.9892617021276596 -1.0 -0.9892617021276596
probs:  [0.09228810707662126, 0.08627209022275169, 0.08627209022275169, 0.09228810707662126, 0.08054953760565617, 0.5623300677955979]
maxi score, test score, baseline:  -0.9892617021276596 -1.0 -0.9892617021276596
printing an ep nov before normalisation:  53.106473971576484
maxi score, test score, baseline:  -0.9893736842105263 -1.0 -0.9893736842105263
probs:  [0.09228864735413639, 0.08627226611561381, 0.08627226611561381, 0.09228864735413639, 0.08054936688872646, 0.562328806171773]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]] [[42.352]
 [42.352]
 [42.352]
 [42.352]
 [42.352]
 [42.352]
 [42.352]] [[1.565]
 [1.565]
 [1.565]
 [1.565]
 [1.565]
 [1.565]
 [1.565]]
maxi score, test score, baseline:  -0.9893736842105263 -1.0 -0.9893736842105263
probs:  [0.09228864735413639, 0.08627226611561381, 0.08627226611561381, 0.09228864735413639, 0.08054936688872646, 0.562328806171773]
STARTED EXPV TRAINING ON FRAME NO.  11087
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
probs:  [0.0867942722582381, 0.0867942722582381, 0.0867942722582381, 0.09284745982142742, 0.08103636213715561, 0.5657333612667026]
siam score:  -0.8097389
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
probs:  [0.08780468874191995, 0.08197964479774647, 0.08197964479774647, 0.09392845288835873, 0.08197964479774647, 0.5723279239764821]
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
probs:  [0.09123962600229575, 0.08560220406987909, 0.08023323080091085, 0.09716614649534916, 0.08560220406987909, 0.560156588561686]
from probs:  [0.09123962600229575, 0.08560220406987909, 0.08023323080091085, 0.09716614649534916, 0.08560220406987909, 0.560156588561686]
printing an ep nov before normalisation:  78.46143476515647
using another actor
from probs:  [0.09211486076273358, 0.08585072921943533, 0.07988488965438939, 0.09870022982107282, 0.08585072921943533, 0.5575985613229335]
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
probs:  [0.09272565603728852, 0.08641959140975905, 0.0804138155740167, 0.09272565603728852, 0.08641959140975905, 0.5612956895318882]
printing an ep nov before normalisation:  7.564744697392598
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9895907216494846 -1.0 -0.9895907216494846
probs:  [0.09305338732197614, 0.08652649845405966, 0.08031041381794878, 0.09305338732197614, 0.08652649845405966, 0.5605298146299796]
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.09305451501670639, 0.08652686618793633, 0.08031005777958393, 0.09305451501670639, 0.08652686618793633, 0.5605271798111305]
printing an ep nov before normalisation:  39.54117237958696
printing an ep nov before normalisation:  33.91848564147949
using another actor
printing an ep nov before normalisation:  86.3182448032601
using explorer policy with actor:  1
printing an ep nov before normalisation:  70.53856008593746
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.508]
 [0.501]
 [0.501]
 [0.563]
 [0.501]
 [0.501]] [[96.08 ]
 [98.417]
 [91.588]
 [91.588]
 [97.274]
 [91.588]
 [91.588]] [[2.095]
 [2.14 ]
 [1.955]
 [1.955]
 [2.165]
 [1.955]
 [1.955]]
printing an ep nov before normalisation:  17.92590293667189
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.09314313573352573, 0.08028468888317923, 0.08656083556013407, 0.09314313573352573, 0.08656083556013407, 0.5603073685295011]
printing an ep nov before normalisation:  43.16093218715101
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
printing an ep nov before normalisation:  56.51247931953549
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
from probs:  [0.0969465778417594, 0.07758765865946711, 0.0969465778417594, 0.0969465778417594, 0.09019346649909933, 0.5413791413161553]
using explorer policy with actor:  1
siam score:  -0.82538694
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.09735798470111677, 0.07741258271919442, 0.09735798470111677, 0.09735798470111677, 0.09040028633532989, 0.5401131768421253]
deleting a thread, now have 5 threads
Frames:  11837 train batches done:  1384 episodes:  321
printing an ep nov before normalisation:  54.102011392058834
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.0977774420697345, 0.07723408085043662, 0.0977774420697345, 0.0977774420697345, 0.09061115327230507, 0.5388224396680547]
printing an ep nov before normalisation:  9.924232329599363
printing an ep nov before normalisation:  29.640192985534668
siam score:  -0.82806486
printing an ep nov before normalisation:  34.199509026404805
printing an ep nov before normalisation:  16.009532813709395
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  27.063979626473603
siam score:  -0.8335541
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]] [[41.084]
 [28.186]
 [28.186]
 [28.186]
 [28.186]
 [28.186]
 [28.186]] [[1.732]
 [1.25 ]
 [1.25 ]
 [1.25 ]
 [1.25 ]
 [1.25 ]
 [1.25 ]]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  17.009513119056123
siam score:  -0.834071
printing an ep nov before normalisation:  23.13109610515545
printing an ep nov before normalisation:  15.749427484707939
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.09964239014722157, 0.07817828565057941, 0.09964239014722157, 0.0921549118344394, 0.08500777344496557, 0.5453742487755725]
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  14.971170179679802
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
printing an ep nov before normalisation:  63.20140048269652
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.385]
 [0.584]
 [0.584]
 [0.583]
 [0.536]
 [0.504]] [[77.716]
 [79.618]
 [69.075]
 [69.075]
 [78.373]
 [80.31 ]
 [79.394]] [[2.032]
 [2.079]
 [1.846]
 [1.846]
 [2.227]
 [2.259]
 [2.189]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
maxi score, test score, baseline:  -0.9903761904761905 -1.0 -0.9903761904761905
probs:  [0.10012906213191225, 0.07802314456642667, 0.10012906213191225, 0.09241769553930095, 0.08505684560999027, 0.5442441900204577]
printing an ep nov before normalisation:  47.744637584354585
deleting a thread, now have 4 threads
Frames:  12202 train batches done:  1422 episodes:  333
printing an ep nov before normalisation:  44.253003048555634
printing an ep nov before normalisation:  63.62815748647133
siam score:  -0.85949415
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.10469669189317313, 0.0792742324243906, 0.10469669189317313, 0.0792742324243906, 0.0792742324243906, 0.552783918940482]
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.10469669189317313, 0.0792742324243906, 0.10469669189317313, 0.0792742324243906, 0.0792742324243906, 0.552783918940482]
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.10532496919980414, 0.07915368997046816, 0.10532496919980414, 0.07915368997046816, 0.07915368997046816, 0.5518889916889871]
siam score:  -0.8556326
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.10532496919980414, 0.07915368997046816, 0.10532496919980414, 0.07915368997046816, 0.07915368997046816, 0.5518889916889871]
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.10532496919980414, 0.07915368997046816, 0.10532496919980414, 0.07915368997046816, 0.07915368997046816, 0.5518889916889871]
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.10532496919980414, 0.07915368997046816, 0.10532496919980414, 0.07915368997046816, 0.07915368997046816, 0.5518889916889871]
printing an ep nov before normalisation:  44.85903845893012
Printing some Q and Qe and total Qs values:  [[0.537]
 [0.596]
 [0.477]
 [0.477]
 [0.588]
 [0.477]
 [0.477]] [[84.661]
 [84.951]
 [72.481]
 [72.481]
 [88.515]
 [72.481]
 [72.481]] [[2.172]
 [2.239]
 [1.756]
 [1.756]
 [2.335]
 [1.756]
 [1.756]]
Printing some Q and Qe and total Qs values:  [[0.801]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]] [[53.594]
 [58.111]
 [58.111]
 [58.111]
 [58.111]
 [58.111]
 [60.958]] [[2.376]
 [2.217]
 [2.217]
 [2.217]
 [2.217]
 [2.217]
 [2.301]]
printing an ep nov before normalisation:  47.49123889870951
Printing some Q and Qe and total Qs values:  [[0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.733]] [[51.959]
 [51.959]
 [51.959]
 [51.959]
 [51.959]
 [51.959]
 [51.959]] [[2.705]
 [2.705]
 [2.705]
 [2.705]
 [2.705]
 [2.705]
 [2.705]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  11087
printing an ep nov before normalisation:  36.872944831848145
printing an ep nov before normalisation:  68.20883129323467
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.10596939037411907, 0.07903004998002884, 0.10596939037411907, 0.07903004998002884, 0.07903004998002884, 0.5509710693116754]
printing an ep nov before normalisation:  58.37566692260202
using another actor
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.534]
 [0.528]
 [0.533]
 [0.533]
 [0.534]
 [0.533]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.538]
 [0.534]
 [0.528]
 [0.533]
 [0.533]
 [0.534]
 [0.533]]
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.1079938182129824, 0.07864163947241823, 0.1079938182129824, 0.07864163947241823, 0.07864163947241823, 0.5480874451567805]
using explorer policy with actor:  0
printing an ep nov before normalisation:  69.47010558463091
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.10870219055062923, 0.07850572983219258, 0.10870219055062923, 0.07850572983219258, 0.07850572983219258, 0.5470784294021639]
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.10870219055062923, 0.07850572983219258, 0.10870219055062923, 0.07850572983219258, 0.07850572983219258, 0.5470784294021639]
printing an ep nov before normalisation:  70.57352511326987
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]] [[50.296]
 [16.634]
 [16.634]
 [16.634]
 [16.634]
 [16.634]
 [16.634]] [[1.33 ]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]]
printing an ep nov before normalisation:  15.596797466278076
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
printing an ep nov before normalisation:  21.776763435673633
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.31891059875488
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]] [[62.746]
 [53.999]
 [53.999]
 [53.999]
 [53.999]
 [53.999]
 [53.999]] [[2.012]
 [1.61 ]
 [1.61 ]
 [1.61 ]
 [1.61 ]
 [1.61 ]
 [1.61 ]]
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
probs:  [0.11502356758297372, 0.08535984754251964, 0.11502356758297372, 0.07633175883455534, 0.07633175883455534, 0.5319294996224223]
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.555]
 [0.536]
 [0.538]
 [0.537]
 [0.538]
 [0.537]] [[32.363]
 [32.878]
 [33.54 ]
 [35.716]
 [35.057]
 [34.092]
 [32.302]] [[1.897]
 [1.955]
 [1.996]
 [2.196]
 [2.135]
 [2.049]
 [1.885]]
Printing some Q and Qe and total Qs values:  [[0.144]
 [0.18 ]
 [0.136]
 [0.138]
 [0.129]
 [0.13 ]
 [0.135]] [[43.679]
 [42.779]
 [44.45 ]
 [48.584]
 [45.836]
 [46.024]
 [44.392]] [[1.507]
 [1.49 ]
 [1.545]
 [1.793]
 [1.621]
 [1.633]
 [1.541]]
printing an ep nov before normalisation:  65.99604853852838
maxi score, test score, baseline:  -0.990809090909091 -1.0 -0.990809090909091
printing an ep nov before normalisation:  47.9546519865711
Printing some Q and Qe and total Qs values:  [[0.798]
 [0.798]
 [0.798]
 [0.798]
 [0.798]
 [0.798]
 [0.798]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.798]
 [0.798]
 [0.798]
 [0.798]
 [0.798]
 [0.798]
 [0.798]]
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.11588805683199854, 0.08541427770943368, 0.11588805683199854, 0.07613964928082687, 0.07613964928082687, 0.5305303100649154]
printing an ep nov before normalisation:  78.24337529150799
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.12108317865576348, 0.0918554866410053, 0.11088747213898742, 0.08296010211477448, 0.07444324458965998, 0.5187705158598093]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.12206402469638006, 0.09207624500790297, 0.11160317131667874, 0.08294952945054028, 0.07421118476795906, 0.5170958447605389]
printing an ep nov before normalisation:  59.91258231721254
maxi score, test score, baseline:  -0.990890990990991 -1.0 -0.990890990990991
probs:  [0.12206402469638006, 0.09207624500790297, 0.11160317131667874, 0.08294952945054028, 0.07421118476795906, 0.5170958447605389]
using another actor
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.581]
 [0.584]
 [0.584]
 [0.477]
 [0.584]
 [0.584]] [[49.494]
 [65.039]
 [47.494]
 [47.494]
 [54.401]
 [47.494]
 [47.494]] [[1.342]
 [1.996]
 [1.357]
 [1.357]
 [1.503]
 [1.357]
 [1.357]]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.12306767574172679, 0.09230213598277674, 0.11233551070953493, 0.08293871083874838, 0.07397372931787029, 0.5153822374093427]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.12306767574172679, 0.09230213598277674, 0.11233551070953493, 0.08293871083874838, 0.07397372931787029, 0.5153822374093427]
Printing some Q and Qe and total Qs values:  [[0.06 ]
 [0.074]
 [0.074]
 [0.074]
 [0.062]
 [0.074]
 [0.074]] [[16.846]
 [26.634]
 [26.634]
 [26.634]
 [14.077]
 [26.634]
 [26.634]] [[0.06 ]
 [0.074]
 [0.074]
 [0.074]
 [0.062]
 [0.074]
 [0.074]]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.12663714940549262, 0.0846284994268326, 0.10467808237119311, 0.0846284994268326, 0.07524358826138727, 0.5241841811082618]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.12663714940549262, 0.0846284994268326, 0.10467808237119311, 0.0846284994268326, 0.07524358826138727, 0.5241841811082618]
maxi score, test score, baseline:  -0.9909714285714286 -1.0 -0.9909714285714286
probs:  [0.12663714940549262, 0.0846284994268326, 0.10467808237119311, 0.0846284994268326, 0.07524358826138727, 0.5241841811082618]
from probs:  [0.12663714940549262, 0.0846284994268326, 0.10467808237119311, 0.0846284994268326, 0.07524358826138727, 0.5241841811082618]
maxi score, test score, baseline:  -0.9910504424778761 -1.0 -0.9910504424778761
probs:  [0.1291311532299242, 0.08556076733612615, 0.09572719071134565, 0.08556076733612615, 0.0758269577215542, 0.5281931636649236]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.395]
 [0.447]
 [0.451]
 [0.42 ]
 [0.444]
 [0.45 ]] [[35.292]
 [36.264]
 [34.152]
 [34.069]
 [51.715]
 [32.883]
 [32.646]] [[1.034]
 [1.002]
 [0.989]
 [0.99 ]
 [1.498]
 [0.948]
 [0.947]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.12913357856563093, 0.0855608834876374, 0.09572784567250256, 0.0855608834876374, 0.07582655799148992, 0.5281902507951016]
printing an ep nov before normalisation:  71.28350194654756
printing an ep nov before normalisation:  41.67113780975342
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.08356838093211
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.12913357856563093, 0.0855608834876374, 0.09572784567250256, 0.0855608834876374, 0.07582655799148992, 0.5281902507951016]
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.694]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]] [[19.213]
 [46.034]
 [19.213]
 [19.213]
 [19.213]
 [19.213]
 [19.213]] [[0.616]
 [1.727]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]]
printing an ep nov before normalisation:  44.035855541089894
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  41.83967872257655
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.13165735604004797, 0.08648226609014699, 0.09702312041179058, 0.07638995876091383, 0.07638995876091383, 0.5320573399361869]
printing an ep nov before normalisation:  30.42758789435299
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.13165735604004797, 0.08648226609014699, 0.09702312041179058, 0.07638995876091383, 0.07638995876091383, 0.5320573399361869]
printing an ep nov before normalisation:  42.59310444931798
siam score:  -0.86570215
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.13305963486984101, 0.08740289890856842, 0.08740289890856842, 0.07720298981083734, 0.07720298981083734, 0.5377285876913476]
printing an ep nov before normalisation:  81.15916716482002
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.1344305512279664, 0.07799783712816333, 0.08830294144204044, 0.07799783712816333, 0.07799783712816333, 0.543272995945503]
printing an ep nov before normalisation:  52.30958938598633
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.1344305512279664, 0.07799783712816333, 0.08830294144204044, 0.07799783712816333, 0.07799783712816333, 0.543272995945503]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.85986334
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.12492176470506522, 0.07980313097353889, 0.07980313097353889, 0.07980313097353889, 0.07980313097353889, 0.5558657114007793]
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.549]
 [0.549]
 [0.545]
 [0.54 ]
 [0.54 ]
 [0.549]] [[43.   ]
 [37.901]
 [37.901]
 [48.135]
 [46.357]
 [44.737]
 [37.901]] [[0.512]
 [0.549]
 [0.549]
 [0.545]
 [0.54 ]
 [0.54 ]
 [0.549]]
printing an ep nov before normalisation:  49.62378106205367
printing an ep nov before normalisation:  49.70556375407576
maxi score, test score, baseline:  -0.9911280701754386 -1.0 -0.9911280701754386
probs:  [0.11424908937950987, 0.0807755454900836, 0.0807755454900836, 0.0807755454900836, 0.0807755454900836, 0.5626487286601557]
printing an ep nov before normalisation:  42.57475191900694
printing an ep nov before normalisation:  32.08892051711183
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.11693611032458344, 0.08055009804447687, 0.08055009804447687, 0.08055009804447687, 0.08055009804447687, 0.5608634974975092]
printing an ep nov before normalisation:  69.64744850876227
printing an ep nov before normalisation:  38.52093097562754
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.11693611032458344, 0.08055009804447687, 0.08055009804447687, 0.08055009804447687, 0.08055009804447687, 0.5608634974975092]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
printing an ep nov before normalisation:  20.997047424316406
actions average: 
K:  1  action  0 :  tensor([0.4664, 0.0065, 0.0941, 0.1197, 0.1364, 0.0958, 0.0812],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0496, 0.7417, 0.0363, 0.0501, 0.0478, 0.0393, 0.0351],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2147, 0.0046, 0.2090, 0.1539, 0.1666, 0.1464, 0.1049],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2483, 0.0317, 0.1125, 0.2296, 0.1544, 0.1220, 0.1015],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2452, 0.0253, 0.1269, 0.1407, 0.2415, 0.1131, 0.1073],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1843, 0.0151, 0.1328, 0.1316, 0.1425, 0.2974, 0.0964],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2224, 0.0347, 0.1372, 0.1539, 0.1770, 0.1514, 0.1234],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  35.36572922132823
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.455]
 [0.455]
 [0.469]
 [0.458]
 [0.455]
 [0.455]] [[21.324]
 [24.234]
 [24.234]
 [25.162]
 [27.239]
 [24.234]
 [24.234]] [[0.867]
 [0.964]
 [0.964]
 [1.016]
 [1.088]
 [0.964]
 [0.964]]
printing an ep nov before normalisation:  50.85164904122099
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.10689477334618246, 0.08147928452331621, 0.08147928452331621, 0.08147928452331621, 0.08147928452331621, 0.5671880885605528]
siam score:  -0.85813695
Printing some Q and Qe and total Qs values:  [[0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]]
printing an ep nov before normalisation:  19.83133417521355
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.11432280498763164, 0.08932014790222807, 0.08932014790222807, 0.08932014790222807, 0.07760015239344507, 0.5401165989122391]
printing an ep nov before normalisation:  43.65736102635394
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.11514895041412716, 0.08947903239023307, 0.08947903239023307, 0.08947903239023307, 0.07744625831653278, 0.5389676940986406]
printing an ep nov before normalisation:  44.16335563697546
printing an ep nov before normalisation:  51.72473938584922
deleting a thread, now have 3 threads
Frames:  13845 train batches done:  1618 episodes:  382
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.11687014654829837, 0.08981005324656613, 0.08981005324656613, 0.08981005324656613, 0.07712563451137908, 0.5365740592006241]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.11687014654829837, 0.08981005324656613, 0.08981005324656613, 0.08981005324656613, 0.07712563451137908, 0.5365740592006241]
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.381240844726562
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.11687014654829837, 0.08981005324656613, 0.08981005324656613, 0.08981005324656613, 0.07712563451137908, 0.5365740592006241]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.11932068163687119, 0.07797314257466269, 0.09116916567962291, 0.09116916567962291, 0.07797314257466269, 0.5423947018545575]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.10646945368122454, 0.07910987384163458, 0.09249860440143395, 0.09249860440143395, 0.07910987384163458, 0.5503135898326383]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.36803400541659
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  44.252247046643234
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.10646945368122454, 0.07910987384163458, 0.09249860440143395, 0.09249860440143395, 0.07910987384163458, 0.5503135898326383]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.10646945368122454, 0.07910987384163458, 0.09249860440143395, 0.09249860440143395, 0.07910987384163458, 0.5503135898326383]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.10646945368122454, 0.07910987384163458, 0.09249860440143395, 0.09249860440143395, 0.07910987384163458, 0.5503135898326383]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.10646945368122454, 0.07910987384163458, 0.09249860440143395, 0.09249860440143395, 0.07910987384163458, 0.5503135898326383]
printing an ep nov before normalisation:  34.85650004123624
printing an ep nov before normalisation:  66.33831657142436
Printing some Q and Qe and total Qs values:  [[1.196]
 [1.001]
 [1.001]
 [1.001]
 [1.001]
 [1.001]
 [1.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.196]
 [1.001]
 [1.001]
 [1.001]
 [1.001]
 [1.001]
 [1.001]]
printing an ep nov before normalisation:  21.721575260162354
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.10844123698450936, 0.07874656521179085, 0.09327800033461049, 0.09327800033461049, 0.07874656521179085, 0.547509631922688]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.10844123698450936, 0.07874656521179085, 0.09327800033461049, 0.09327800033461049, 0.07874656521179085, 0.547509631922688]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.10844123698450936, 0.07874656521179085, 0.09327800033461049, 0.09327800033461049, 0.07874656521179085, 0.547509631922688]
actions average: 
K:  0  action  0 :  tensor([0.4506, 0.0136, 0.0964, 0.1178, 0.1257, 0.1038, 0.0920],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0154, 0.9030, 0.0144, 0.0224, 0.0138, 0.0176, 0.0135],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.3211, 0.0053, 0.1299, 0.1352, 0.1521, 0.1489, 0.1075],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2817, 0.0440, 0.1208, 0.1534, 0.1424, 0.1326, 0.1251],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.3121, 0.0031, 0.1157, 0.1357, 0.1786, 0.1331, 0.1217],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2104, 0.0140, 0.2011, 0.1466, 0.1695, 0.1478, 0.1106],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2548, 0.0061, 0.1340, 0.1700, 0.1616, 0.1361, 0.1374],
       grad_fn=<DivBackward0>)
deleting a thread, now have 2 threads
Frames:  14214 train batches done:  1660 episodes:  389
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.589]
 [0.542]
 [0.55 ]
 [0.548]
 [0.56 ]
 [0.557]] [[25.473]
 [23.246]
 [26.52 ]
 [25.563]
 [26.124]
 [24.655]
 [23.531]] [[1.807]
 [1.641]
 [1.889]
 [1.811]
 [1.859]
 [1.739]
 [1.634]]
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.53 ]
 [0.501]
 [0.478]
 [0.504]
 [0.507]
 [0.496]] [[17.14 ]
 [17.138]
 [18.133]
 [21.298]
 [18.074]
 [17.449]
 [17.008]] [[1.408]
 [1.434]
 [1.52 ]
 [1.861]
 [1.516]
 [1.446]
 [1.385]]
printing an ep nov before normalisation:  39.4068263430629
Printing some Q and Qe and total Qs values:  [[-0.08 ]
 [-0.045]
 [-0.078]
 [-0.081]
 [-0.079]
 [-0.079]
 [-0.079]] [[23.385]
 [14.832]
 [13.888]
 [14.484]
 [14.157]
 [13.987]
 [13.992]] [[1.102]
 [0.529]
 [0.428]
 [0.468]
 [0.447]
 [0.435]
 [0.435]]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.11157354912619138, 0.07970773809939334, 0.07970773809939334, 0.09530164562314554, 0.07970773809939334, 0.554001590952483]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.11157354912619138, 0.07970773809939334, 0.07970773809939334, 0.09530164562314554, 0.07970773809939334, 0.554001590952483]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.11157354912619138, 0.07970773809939334, 0.07970773809939334, 0.09530164562314554, 0.07970773809939334, 0.554001590952483]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.0968771121566958, 0.08102505513626876, 0.08102505513626876, 0.0968771121566958, 0.08102505513626876, 0.5631706102778021]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.0968771121566958, 0.08102505513626876, 0.08102505513626876, 0.0968771121566958, 0.08102505513626876, 0.5631706102778021]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.08232907072026116, 0.08232907072026116, 0.08232907072026116, 0.09843667060337108, 0.08232907072026116, 0.5722470465155843]
printing an ep nov before normalisation:  61.45033431008902
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.08232907072026116, 0.08232907072026116, 0.08232907072026116, 0.09843667060337108, 0.08232907072026116, 0.5722470465155843]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.08232907072026116, 0.08232907072026116, 0.08232907072026116, 0.09843667060337108, 0.08232907072026116, 0.5722470465155843]
printing an ep nov before normalisation:  34.7530648380966
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.94857839625261
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.09183969638582566, 0.0777001254724751, 0.09183969638582566, 0.10658095116782952, 0.09183969638582566, 0.5401998342022182]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.09183969638582566, 0.0777001254724751, 0.09183969638582566, 0.10658095116782952, 0.09183969638582566, 0.5401998342022182]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.09183969638582566, 0.0777001254724751, 0.09183969638582566, 0.10658095116782952, 0.09183969638582566, 0.5401998342022182]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.09321290533180421, 0.07886160905874863, 0.09321290533180421, 0.09321290533180421, 0.09321290533180421, 0.5482867696140347]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.09348236718150886, 0.0787383197537914, 0.09348236718150886, 0.09348236718150886, 0.09348236718150886, 0.5473322115201731]
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]] [[46.657]
 [46.657]
 [46.657]
 [46.657]
 [46.657]
 [46.657]
 [46.657]] [[2.386]
 [2.386]
 [2.386]
 [2.386]
 [2.386]
 [2.386]
 [2.386]]
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]] [[23.258]
 [13.289]
 [13.289]
 [13.289]
 [13.289]
 [13.289]
 [13.289]] [[0.067]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]]
from probs:  [0.09348236718150886, 0.0787383197537914, 0.09348236718150886, 0.09348236718150886, 0.09348236718150886, 0.5473322115201731]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.09376105192363832, 0.07861081029619954, 0.09376105192363832, 0.09376105192363832, 0.09376105192363832, 0.5463449820092472]
using another actor
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
probs:  [0.09376174557037403, 0.0786104926115916, 0.09376174557037403, 0.09376174557037403, 0.09376174557037403, 0.5463425251069124]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.09434579346048445, 0.07834326756291662, 0.09434579346048445, 0.09434579346048445, 0.09434579346048445, 0.5442735585951456]
printing an ep nov before normalisation:  62.65272646170892
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.09587919656387714, 0.07961621366220951, 0.09587919656387714, 0.09587919656387714, 0.07961621366220951, 0.5531299829839496]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.09587919656387714, 0.07961621366220951, 0.09587919656387714, 0.09587919656387714, 0.07961621366220951, 0.5531299829839496]
printing an ep nov before normalisation:  11.80932228641642
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.09587919656387714, 0.07961621366220951, 0.09587919656387714, 0.09587919656387714, 0.07961621366220951, 0.5531299829839496]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.09587919656387714, 0.07961621366220951, 0.09587919656387714, 0.09587919656387714, 0.07961621366220951, 0.5531299829839496]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.09623400708550683, 0.07950986474225036, 0.09623400708550683, 0.09623400708550683, 0.07950986474225036, 0.5522782492589788]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
printing an ep nov before normalisation:  26.72803929933977
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.08086087272126753, 0.08086087272126753, 0.09787089170352421, 0.09787089170352421, 0.08086087272126753, 0.5616755984291489]
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.08078812373763712, 0.08078812373763712, 0.09829198547049275, 0.09829198547049275, 0.08078812373763712, 0.5610516578461031]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  23.526955065249886
printing an ep nov before normalisation:  26.29848300895043
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.08063454715733767, 0.08063454715733767, 0.09918093453189834, 0.09918093453189834, 0.08063454715733767, 0.5597344894641902]
line 256 mcts: sample exp_bonus 16.18034746526208
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.08063454715733767, 0.08063454715733767, 0.09918093453189834, 0.09918093453189834, 0.08063454715733767, 0.5597344894641902]
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.08063454715733767, 0.08063454715733767, 0.09918093453189834, 0.09918093453189834, 0.08063454715733767, 0.5597344894641902]
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.08063454715733767, 0.08063454715733767, 0.09918093453189834, 0.09918093453189834, 0.08063454715733767, 0.5597344894641902]
printing an ep nov before normalisation:  37.11169296812611
printing an ep nov before normalisation:  35.18275737762451
maxi score, test score, baseline:  -0.9914966386554622 -1.0 -0.9914966386554622
probs:  [0.08063454715733767, 0.08063454715733767, 0.09918093453189834, 0.09918093453189834, 0.08063454715733767, 0.5597344894641902]
printing an ep nov before normalisation:  39.2305506613734
line 256 mcts: sample exp_bonus 0.0
from probs:  [0.09183944494003565, 0.07532167365273498, 0.10904545669764006, 0.10904545669764006, 0.09183944494003565, 0.5229085230719137]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.355]] [[39.381]
 [37.426]
 [37.426]
 [37.426]
 [37.426]
 [37.426]
 [37.426]] [[2.203]
 [2.037]
 [2.037]
 [2.037]
 [2.037]
 [2.037]
 [2.037]]
printing an ep nov before normalisation:  42.47323754105562
maxi score, test score, baseline:  -0.9916355371900827 -1.0 -0.9916355371900827
probs:  [0.09184000050450718, 0.07532114569440256, 0.10904714093169939, 0.10904714093169939, 0.09184000050450718, 0.5229045714331844]
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.09184054694650132, 0.07532062640527096, 0.10904879751028294, 0.10904879751028294, 0.09184054694650132, 0.5229006846811604]
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.52 ]
 [0.52 ]
 [0.525]
 [0.52 ]
 [0.518]
 [0.519]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.526]
 [0.52 ]
 [0.52 ]
 [0.525]
 [0.52 ]
 [0.518]
 [0.519]]
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.09206852864434908, 0.07510415603567512, 0.10973975011171806, 0.10973975011171806, 0.09206852864434908, 0.5212792864521907]
printing an ep nov before normalisation:  32.28010124686437
printing an ep nov before normalisation:  9.016976131803034
printing an ep nov before normalisation:  46.916632652282715
printing an ep nov before normalisation:  37.059379006511286
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.10333054174067614, 0.07176177570867595, 0.12010144869517649, 0.10333054174067614, 0.10333054174067614, 0.498145150374119]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  28.618660476724003
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.10383185483981883, 0.07147036568610367, 0.12102389595273008, 0.10383185483981883, 0.10383185483981883, 0.49601017384170976]
printing an ep nov before normalisation:  28.10885190963745
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.10383185483981883, 0.07147036568610367, 0.12102389595273008, 0.10383185483981883, 0.10383185483981883, 0.49601017384170976]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.10383185483981883, 0.07147036568610367, 0.12102389595273008, 0.10383185483981883, 0.10383185483981883, 0.49601017384170976]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.10383185483981883, 0.07147036568610367, 0.12102389595273008, 0.10383185483981883, 0.10383185483981883, 0.49601017384170976]
siam score:  -0.8469334
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.294]
 [0.268]
 [0.275]
 [0.274]
 [0.256]
 [0.268]] [[73.103]
 [74.95 ]
 [73.249]
 [68.392]
 [68.289]
 [66.259]
 [66.576]] [[0.287]
 [0.294]
 [0.268]
 [0.275]
 [0.274]
 [0.256]
 [0.268]]
siam score:  -0.8427165
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.1043480754990099, 0.07117028999647096, 0.12197377404723382, 0.1043480754990099, 0.1043480754990099, 0.49381170945926556]
siam score:  -0.8442644
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.11207308853358296, 0.06744830858549583, 0.12818759240372551, 0.11207308853358296, 0.11207308853358296, 0.4681448334100299]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.11207308853358296, 0.06744830858549583, 0.12818759240372551, 0.11207308853358296, 0.11207308853358296, 0.4681448334100299]
printing an ep nov before normalisation:  26.937506198883057
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.11207308853358296, 0.06744830858549583, 0.12818759240372551, 0.11207308853358296, 0.11207308853358296, 0.4681448334100299]
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.11207469870042955, 0.06744741539618503, 0.12819010656029547, 0.11207469870042955, 0.11207469870042955, 0.46813838194223084]
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
using explorer policy with actor:  1
siam score:  -0.8433239
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.11528617864334237, 0.06779833062863509, 0.1324345682042086, 0.11528617864334237, 0.09882372466491039, 0.47037101921556124]
printing an ep nov before normalisation:  21.022802959496534
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.11528617864334237, 0.06779833062863509, 0.1324345682042086, 0.11528617864334237, 0.09882372466491039, 0.47037101921556124]
maxi score, test score, baseline:  -0.9918354838709678 -1.0 -0.9918354838709678
probs:  [0.11528617864334237, 0.06779833062863509, 0.1324345682042086, 0.11528617864334237, 0.09882372466491039, 0.47037101921556124]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
siam score:  -0.8480334
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.10047795144462461, 0.06893098539820577, 0.13465383132824496, 0.11721715791823466, 0.10047795144462461, 0.47824212246606557]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.10047795144462461, 0.06893098539820577, 0.13465383132824496, 0.11721715791823466, 0.10047795144462461, 0.47824212246606557]
Printing some Q and Qe and total Qs values:  [[0.323]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]] [[41.873]
 [42.851]
 [42.851]
 [42.851]
 [42.851]
 [42.851]
 [42.851]] [[2.267]
 [2.327]
 [2.327]
 [2.327]
 [2.327]
 [2.327]
 [2.327]]
printing an ep nov before normalisation:  36.471109892265225
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
printing an ep nov before normalisation:  28.89887961334548
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.10047795144462461, 0.06893098539820577, 0.13465383132824496, 0.11721715791823466, 0.10047795144462461, 0.47824212246606557]
line 256 mcts: sample exp_bonus 24.328995554616316
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]
 [0.448]] [[18.388]
 [30.238]
 [30.238]
 [30.238]
 [30.238]
 [30.238]
 [30.238]] [[1.18 ]
 [2.448]
 [2.448]
 [2.448]
 [2.448]
 [2.448]
 [2.448]]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.10047795144462461, 0.06893098539820577, 0.13465383132824496, 0.11721715791823466, 0.10047795144462461, 0.47824212246606557]
from probs:  [0.10088973157978774, 0.06858359379419761, 0.13588804751417663, 0.1180317638741824, 0.10088973157978774, 0.475717131657868]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.1013132264119292, 0.0682263192768911, 0.1371573758082202, 0.11886954448358193, 0.1013132264119292, 0.4731203076074484]
printing an ep nov before normalisation:  41.120773475014126
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.10175004749755068, 0.06785780186031262, 0.138466646937892, 0.11973368803975867, 0.10175004749755068, 0.47044176816693534]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.10175004749755068, 0.06785780186031262, 0.138466646937892, 0.11973368803975867, 0.10175004749755068, 0.47044176816693534]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.10369155333611192, 0.06915175749700944, 0.1220187919446154, 0.1220187919446154, 0.10369155333611192, 0.47942755194153586]
printing an ep nov before normalisation:  74.13497849307164
printing an ep nov before normalisation:  70.72533912473514
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.11241465116299688, 0.06506149279441585, 0.12948755860200908, 0.12948755860200908, 0.11241465116299688, 0.4511340876755723]
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.507]
 [0.507]
 [0.507]
 [0.503]
 [0.505]
 [0.508]] [[37.503]
 [17.49 ]
 [17.49 ]
 [17.49 ]
 [18.326]
 [17.733]
 [17.169]] [[1.151]
 [0.709]
 [0.709]
 [0.709]
 [0.724]
 [0.713]
 [0.703]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.1142886847370023, 0.06614517828472552, 0.13164654760755126, 0.13164654760755126, 0.09761152237118097, 0.4586615193919886]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9920259842519685 -1.0 -0.9920259842519685
probs:  [0.11630871848753621, 0.06731127045766153, 0.11630871848753621, 0.1339744650561307, 0.09933574629418085, 0.46676108121695453]
from probs:  [0.11630871848753621, 0.06731127045766153, 0.11630871848753621, 0.1339744650561307, 0.09933574629418085, 0.46676108121695453]
maxi score, test score, baseline:  -0.9920875 -1.0 -0.9920875
probs:  [0.12475108048982556, 0.06503117007857254, 0.10894286891037629, 0.1412045251949667, 0.10894286891037629, 0.4511274864158825]
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.12475322915646066, 0.06503021781745696, 0.10894419674319493, 0.14120752819883914, 0.10894419674319493, 0.45112063134085334]
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.11257483997277584, 0.06719560446230657, 0.11257483997277584, 0.12891136475654477, 0.11257483997277584, 0.4661685108628211]
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]] [[41.097]
 [39.163]
 [39.163]
 [39.163]
 [39.163]
 [39.163]
 [39.163]] [[1.309]
 [1.197]
 [1.197]
 [1.197]
 [1.197]
 [1.197]
 [1.197]]
printing an ep nov before normalisation:  30.663704872131348
siam score:  -0.8549072
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.01172929367135
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.11515778159515286, 0.06796434437669886, 0.11515778159515286, 0.11515778159515286, 0.11515778159515286, 0.47140452924268983]
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
siam score:  -0.85755867
maxi score, test score, baseline:  -0.9921480620155039 -1.0 -0.9921480620155039
probs:  [0.11664444978407217, 0.06724407667858756, 0.11664444978407217, 0.11664444978407217, 0.11664444978407217, 0.4661781241851237]
siam score:  -0.853406
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.593]
 [0.502]
 [0.498]
 [0.471]
 [0.452]
 [0.51 ]] [[48.798]
 [56.475]
 [50.966]
 [50.978]
 [50.949]
 [51.446]
 [48.01 ]] [[1.423]
 [1.802]
 [1.496]
 [1.493]
 [1.464]
 [1.465]
 [1.388]]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.10921982504494257, 0.06494583431234482, 0.1251353119096021, 0.1251353119096021, 0.1251353119096021, 0.45042840491390634]
printing an ep nov before normalisation:  66.47784825904569
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.10921982504494257, 0.06494583431234482, 0.1251353119096021, 0.1251353119096021, 0.1251353119096021, 0.45042840491390634]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.10921982504494257, 0.06494583431234482, 0.1251353119096021, 0.1251353119096021, 0.1251353119096021, 0.45042840491390634]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.10921982504494257, 0.06494583431234482, 0.1251353119096021, 0.1251353119096021, 0.1251353119096021, 0.45042840491390634]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.10921982504494257, 0.06494583431234482, 0.1251353119096021, 0.1251353119096021, 0.1251353119096021, 0.45042840491390634]
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.669]
 [0.621]
 [0.578]
 [0.579]
 [0.569]
 [0.562]] [[32.926]
 [33.023]
 [31.547]
 [31.524]
 [31.996]
 [32.366]
 [32.601]] [[1.978]
 [2.08 ]
 [1.911]
 [1.866]
 [1.905]
 [1.926]
 [1.938]]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.10921982504494257, 0.06494583431234482, 0.1251353119096021, 0.1251353119096021, 0.1251353119096021, 0.45042840491390634]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.10921982504494257, 0.06494583431234482, 0.1251353119096021, 0.1251353119096021, 0.1251353119096021, 0.45042840491390634]
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]] [[38.799]
 [38.01 ]
 [38.01 ]
 [38.01 ]
 [38.01 ]
 [38.01 ]
 [38.01 ]] [[1.714]
 [1.663]
 [1.663]
 [1.663]
 [1.663]
 [1.663]
 [1.663]]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.10921982504494257, 0.06494583431234482, 0.1251353119096021, 0.1251353119096021, 0.1251353119096021, 0.45042840491390634]
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.55 ]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]]
printing an ep nov before normalisation:  35.124254587891016
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.11098550442859853, 0.06599489337882175, 0.1271585999040087, 0.1271585999040087, 0.11098550442859853, 0.4577168979559637]
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.536]
 [0.498]
 [0.519]
 [0.514]
 [0.498]
 [0.498]] [[55.61 ]
 [59.616]
 [58.127]
 [55.778]
 [57.711]
 [58.127]
 [58.127]] [[1.934]
 [2.126]
 [2.026]
 [1.948]
 [2.025]
 [2.026]
 [2.026]]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
printing an ep nov before normalisation:  57.193963412605164
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.11159250413976306, 0.06561310149168975, 0.12812104757534518, 0.12812104757534518, 0.11159250413976306, 0.4549597950780938]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
Printing some Q and Qe and total Qs values:  [[-0.051]
 [-0.051]
 [-0.093]
 [-0.091]
 [-0.051]
 [-0.097]
 [-0.051]] [[40.946]
 [40.946]
 [42.435]
 [43.952]
 [40.946]
 [44.678]
 [40.946]] [[1.246]
 [1.246]
 [1.303]
 [1.406]
 [1.246]
 [1.449]
 [1.246]]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
actions average: 
K:  1  action  0 :  tensor([0.4760, 0.0114, 0.0797, 0.0933, 0.1327, 0.1013, 0.1054],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0307, 0.8682, 0.0177, 0.0212, 0.0137, 0.0152, 0.0333],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1928, 0.0231, 0.2666, 0.1018, 0.1119, 0.1677, 0.1361],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2976, 0.0103, 0.1113, 0.1347, 0.1625, 0.1494, 0.1342],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2324, 0.0045, 0.1184, 0.1305, 0.2298, 0.1374, 0.1470],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2008, 0.0239, 0.1189, 0.1276, 0.1470, 0.2487, 0.1330],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2467, 0.0279, 0.1165, 0.1464, 0.1667, 0.1419, 0.1539],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.10094386532671756, 0.06860717424942028, 0.13581676746890115, 0.11804500003105753, 0.10094386532671756, 0.4756433275971859]
maxi score, test score, baseline:  -0.9922664122137405 -1.0 -0.9922664122137405
printing an ep nov before normalisation:  2.7765970624403735
maxi score, test score, baseline:  -0.9923242424242424 -1.0 -0.9923242424242424
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.10270252813246772, 0.06979720791888194, 0.13818865777457034, 0.10270252813246772, 0.10270252813246772, 0.48390654990914456]
siam score:  -0.8566022
maxi score, test score, baseline:  -0.9923812030075188 -1.0 -0.9923812030075188
probs:  [0.10270252813246772, 0.06979720791888194, 0.13818865777457034, 0.10270252813246772, 0.10270252813246772, 0.48390654990914456]
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.10445320283148173, 0.07098476478000436, 0.14054661641640834, 0.08740909086082183, 0.10445320283148173, 0.4921531222798022]
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.10445320283148173, 0.07098476478000436, 0.14054661641640834, 0.08740909086082183, 0.10445320283148173, 0.4921531222798022]
maxi score, test score, baseline:  -0.9924373134328358 -1.0 -0.9924373134328358
probs:  [0.08892365555446348, 0.07221430560533079, 0.14298331715459886, 0.08892365555446348, 0.1062635470111106, 0.5006915191200327]
printing an ep nov before normalisation:  49.51247696065377
printing an ep nov before normalisation:  68.68515447640627
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.08892396458695691, 0.07221368473319516, 0.14298663470206874, 0.08892396458695691, 0.10626482103897394, 0.5006869303518483]
printing an ep nov before normalisation:  79.45412566547816
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]] [[47.289]
 [40.777]
 [40.777]
 [40.777]
 [40.777]
 [40.777]
 [40.777]] [[2.273]
 [1.887]
 [1.887]
 [1.887]
 [1.887]
 [1.887]
 [1.887]]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.09049211868153073, 0.07348671464687928, 0.14550960232305027, 0.09049211868153073, 0.09049211868153073, 0.5095273269854783]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.0922778711130556, 0.07453746140623185, 0.14967331428219185, 0.07453746140623185, 0.0922778711130556, 0.5166960206792333]
printing an ep nov before normalisation:  47.327499953541924
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.09414720246743702, 0.07604689442539833, 0.13243631563328853, 0.07604689442539833, 0.09414720246743702, 0.5271754905810407]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.09588166515811691, 0.07744742475389699, 0.13487717370550553, 0.07744742475389699, 0.07744742475389699, 0.5368988868746867]
using explorer policy with actor:  0
printing an ep nov before normalisation:  17.473767970799894
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.381]
 [0.381]
 [0.381]
 [0.439]
 [0.381]
 [0.381]] [[13.905]
 [12.211]
 [12.211]
 [12.211]
 [12.082]
 [12.211]
 [12.211]] [[0.54 ]
 [0.381]
 [0.381]
 [0.381]
 [0.439]
 [0.381]
 [0.381]]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.09621561951858267, 0.07728947426811283, 0.13625169600996173, 0.07728947426811283, 0.07728947426811283, 0.5356642616671172]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.09621561951858267, 0.07728947426811283, 0.13625169600996173, 0.07728947426811283, 0.07728947426811283, 0.5356642616671172]
printing an ep nov before normalisation:  30.62188046687942
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.09621640294691107, 0.07728910336093765, 0.13625492130185554, 0.07728910336093765, 0.07728910336093765, 0.5356613656684205]
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.09621640294691107, 0.07728910336093765, 0.13625492130185554, 0.07728910336093765, 0.07728910336093765, 0.5356613656684205]
printing an ep nov before normalisation:  50.0389807507358
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.09656230143227611, 0.07712550364918323, 0.13767860443497293, 0.07712550364918323, 0.07712550364918323, 0.5343825831852013]
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.09656230143227611, 0.07712550364918323, 0.13767860443497293, 0.07712550364918323, 0.07712550364918323, 0.5343825831852013]
Printing some Q and Qe and total Qs values:  [[1.02 ]
 [0.814]
 [0.814]
 [0.814]
 [0.814]
 [0.814]
 [0.814]] [[ 6.501]
 [40.362]
 [40.362]
 [40.362]
 [40.362]
 [40.362]
 [40.362]] [[1.02 ]
 [2.805]
 [2.805]
 [2.805]
 [2.805]
 [2.805]
 [2.805]]
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
printing an ep nov before normalisation:  76.46624019125255
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.5083],
        [-0.5511],
        [-0.0000],
        [-0.5202],
        [-0.3986],
        [-0.5906],
        [-0.5499],
        [-0.6238],
        [-0.0000],
        [-0.4593]], dtype=torch.float64)
-0.09703970119800001 -0.6053399239008735
-0.058351887066 -0.6094838259896348
-0.2807696980439995 -0.2807696980439995
-0.032346567066 -0.5525302677731948
-0.09703970119800001 -0.49563215488109424
-0.032346567066 -0.6229088440418213
-0.032346567066 -0.5821989172458781
-0.032346567066 -0.6561245820773872
-0.5214000000000006 -0.5214000000000006
-0.09703970119800001 -0.5563631715274189
actions average: 
K:  2  action  0 :  tensor([0.4105, 0.0169, 0.0859, 0.1054, 0.1823, 0.1037, 0.0953],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0297, 0.8387, 0.0185, 0.0328, 0.0239, 0.0222, 0.0343],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2663, 0.0194, 0.1513, 0.1448, 0.1487, 0.1386, 0.1309],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2358, 0.0206, 0.1268, 0.1556, 0.1585, 0.1589, 0.1437],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2929, 0.0230, 0.1136, 0.1386, 0.1679, 0.1352, 0.1288],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2126, 0.0069, 0.1295, 0.1358, 0.1314, 0.2593, 0.1245],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2291, 0.0509, 0.1329, 0.1398, 0.1524, 0.1384, 0.1565],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.0782453469507472, 0.0782453469507472, 0.14533895558865506, 0.0782453469507472, 0.0782453469507472, 0.5416796566083562]
printing an ep nov before normalisation:  41.295055629627896
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.07824501348847433, 0.07824501348847433, 0.14534298715063076, 0.07824501348847433, 0.07824501348847433, 0.541676958895472]
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
printing an ep nov before normalisation:  73.97787065830195
printing an ep nov before normalisation:  41.630403116114394
printing an ep nov before normalisation:  27.25683827632592
printing an ep nov before normalisation:  26.612482102450322
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.07824501348847433, 0.07824501348847433, 0.14534298715063076, 0.07824501348847433, 0.07824501348847433, 0.541676958895472]
siam score:  -0.85877776
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.08010258646943921, 0.08010258646943921, 0.125034267439251, 0.08010258646943921, 0.08010258646943921, 0.554555386682992]
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.09303813092119693, 0.09303813092119693, 0.13288683581888233, 0.0741811544963994, 0.09303813092119693, 0.5138176169211276]
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.09303813092119693, 0.09303813092119693, 0.13288683581888233, 0.0741811544963994, 0.09303813092119693, 0.5138176169211276]
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.075605471376685, 0.09482516301379312, 0.13544036043560614, 0.075605471376685, 0.09482516301379312, 0.5236983707834375]
printing an ep nov before normalisation:  66.18870239095453
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.075605471376685, 0.09482516301379312, 0.13544036043560614, 0.075605471376685, 0.09482516301379312, 0.5236983707834375]
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.075605471376685, 0.09482516301379312, 0.13544036043560614, 0.075605471376685, 0.09482516301379312, 0.5236983707834375]
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.07540042222046352, 0.09512789496260586, 0.1368161392478879, 0.07540042222046352, 0.09512789496260586, 0.5221272263859733]
line 256 mcts: sample exp_bonus 42.607341288037915
printing an ep nov before normalisation:  37.6369228750122
from probs:  [0.07540042222046352, 0.09512789496260586, 0.1368161392478879, 0.07540042222046352, 0.09512789496260586, 0.5221272263859733]
printing an ep nov before normalisation:  54.179067611694336
siam score:  -0.85385376
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.07518800604642195, 0.09544150348220155, 0.13824134712007555, 0.07518800604642195, 0.09544150348220155, 0.5204996338226775]
printing an ep nov before normalisation:  37.16466407274371
printing an ep nov before normalisation:  54.36099011282053
siam score:  -0.85544723
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.07518800604642195, 0.09544150348220155, 0.13824134712007555, 0.07518800604642195, 0.09544150348220155, 0.5204996338226775]
printing an ep nov before normalisation:  45.688403678219046
maxi score, test score, baseline:  -0.9927057553956835 -1.0 -0.9927057553956835
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.324]
 [0.32 ]
 [0.337]
 [0.343]
 [0.354]
 [0.331]] [[31.155]
 [43.136]
 [39.883]
 [37.701]
 [38.208]
 [38.211]
 [42.652]] [[0.354]
 [0.324]
 [0.32 ]
 [0.337]
 [0.343]
 [0.354]
 [0.331]]
printing an ep nov before normalisation:  37.118556245247525
Printing some Q and Qe and total Qs values:  [[-0.137]
 [-0.102]
 [-0.123]
 [-0.098]
 [-0.077]
 [-0.087]
 [-0.099]] [[35.36 ]
 [18.411]
 [34.617]
 [ 9.402]
 [18.363]
 [35.631]
 [18.603]] [[ 0.189]
 [ 0.064]
 [ 0.196]
 [-0.017]
 [ 0.088]
 [ 0.241]
 [ 0.068]]
siam score:  -0.8552794
maxi score, test score, baseline:  -0.9927057553956835 -1.0 -0.9927057553956835
printing an ep nov before normalisation:  39.02150123049399
line 256 mcts: sample exp_bonus 24.56018775701523
printing an ep nov before normalisation:  59.75558358734564
printing an ep nov before normalisation:  45.23703792070343
printing an ep nov before normalisation:  25.925061469358965
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
probs:  [0.06593224107505449, 0.11465342182035125, 0.15019843418798937, 0.09783301418209422, 0.11465342182035125, 0.45672946691415944]
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]] [[65.248]
 [65.248]
 [65.248]
 [65.248]
 [65.248]
 [65.248]
 [65.248]] [[1.915]
 [1.915]
 [1.915]
 [1.915]
 [1.915]
 [1.915]
 [1.915]]
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
probs:  [0.06705804211651216, 0.11661591029891691, 0.15277133614268382, 0.09950664628356296, 0.09950664628356296, 0.4645414188747612]
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
probs:  [0.06705804211651216, 0.11661591029891691, 0.15277133614268382, 0.09950664628356296, 0.09950664628356296, 0.4645414188747612]
from probs:  [0.06822395637240296, 0.10123785593811897, 0.15543048352712446, 0.10123785593811897, 0.10123785593811897, 0.47263199228611563]
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
probs:  [0.06788086122985565, 0.10164277210444077, 0.1570632673136654, 0.10164277210444077, 0.10164277210444077, 0.4701275551431566]
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
probs:  [0.06788086122985565, 0.10164277210444077, 0.1570632673136654, 0.10164277210444077, 0.10164277210444077, 0.4701275551431566]
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
probs:  [0.06788086122985565, 0.10164277210444077, 0.1570632673136654, 0.10164277210444077, 0.10164277210444077, 0.4701275551431566]
maxi score, test score, baseline:  -0.9928577464788733 -1.0 -0.9928577464788733
maxi score, test score, baseline:  -0.9928577464788733 -1.0 -0.9928577464788733
probs:  [0.06873334377199501, 0.1038845818143087, 0.16158567067621976, 0.0860006185997982, 0.1038845818143087, 0.47591120332336967]
maxi score, test score, baseline:  -0.9928577464788733 -1.0 -0.9928577464788733
probs:  [0.06604111957585251, 0.11391082245616445, 0.1671997369833041, 0.08144746073273457, 0.11391082245616445, 0.4574900377957799]
maxi score, test score, baseline:  -0.9928577464788733 -1.0 -0.9928577464788733
probs:  [0.06604111957585251, 0.11391082245616445, 0.1671997369833041, 0.08144746073273457, 0.11391082245616445, 0.4574900377957799]
maxi score, test score, baseline:  -0.9928577464788733 -1.0 -0.9928577464788733
probs:  [0.06604111957585251, 0.11391082245616445, 0.1671997369833041, 0.08144746073273457, 0.11391082245616445, 0.4574900377957799]
siam score:  -0.8571153
printing an ep nov before normalisation:  22.426703217116845
printing an ep nov before normalisation:  38.58328367827905
printing an ep nov before normalisation:  24.764660118484105
maxi score, test score, baseline:  -0.9928577464788733 -1.0 -0.9928577464788733
probs:  [0.06842910237681168, 0.10091835304368087, 0.15416462496993855, 0.08439364796311818, 0.11803322616283517, 0.47406104548361555]
printing an ep nov before normalisation:  40.169525146484375
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]] [[33.089]
 [33.089]
 [33.089]
 [33.089]
 [33.089]
 [33.089]
 [33.089]] [[33.504]
 [33.504]
 [33.504]
 [33.504]
 [33.504]
 [33.504]
 [33.504]]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.597]
 [0.541]
 [0.548]
 [0.554]
 [0.544]
 [0.521]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.572]
 [0.597]
 [0.541]
 [0.548]
 [0.554]
 [0.544]
 [0.521]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.36666297912598
printing an ep nov before normalisation:  47.26776695817421
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
probs:  [0.0718269102827488, 0.08904603425934779, 0.14446030596585716, 0.0718269102827488, 0.12532918835289575, 0.4975106508564018]
printing an ep nov before normalisation:  101.27408650261896
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
probs:  [0.0718269102827488, 0.08904603425934779, 0.14446030596585716, 0.0718269102827488, 0.12532918835289575, 0.4975106508564018]
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
probs:  [0.07426296564864222, 0.07426296564864222, 0.15144038588968162, 0.07426296564864222, 0.11149768594037184, 0.5142730312240199]
actions average: 
K:  0  action  0 :  tensor([0.4203, 0.0041, 0.1140, 0.1126, 0.1391, 0.1177, 0.0923],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0317, 0.8172, 0.0265, 0.0342, 0.0208, 0.0206, 0.0491],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1890, 0.0192, 0.2766, 0.1255, 0.1167, 0.1573, 0.1157],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2314, 0.0257, 0.1269, 0.1844, 0.1409, 0.1587, 0.1319],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2871, 0.0052, 0.1216, 0.1320, 0.1910, 0.1442, 0.1190],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2681, 0.0018, 0.1268, 0.1469, 0.1516, 0.1667, 0.1382],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2318, 0.0260, 0.1529, 0.1501, 0.1324, 0.1635, 0.1433],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
probs:  [0.07426296564864222, 0.07426296564864222, 0.15144038588968162, 0.07426296564864222, 0.11149768594037184, 0.5142730312240199]
printing an ep nov before normalisation:  70.80681773824367
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.44 ]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]] [[43.341]
 [44.423]
 [44.949]
 [44.949]
 [44.949]
 [44.949]
 [44.949]] [[1.492]
 [1.495]
 [1.543]
 [1.543]
 [1.543]
 [1.543]
 [1.543]]
printing an ep nov before normalisation:  31.186619066730575
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
probs:  [0.07403697679507754, 0.07403697679507754, 0.15313024007854345, 0.07403697679507754, 0.11219600732657427, 0.5125628222096497]
printing an ep nov before normalisation:  42.65941411684139
maxi score, test score, baseline:  -0.9929555555555556 -1.0 -0.9929555555555556
probs:  [0.07403697679507754, 0.07403697679507754, 0.15313024007854345, 0.07403697679507754, 0.11219600732657427, 0.5125628222096497]
from probs:  [0.0874888927250291, 0.0874888927250291, 0.14104273647308166, 0.07082769689230163, 0.1225650944781395, 0.49058668670641903]
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.387]
 [0.38 ]
 [0.391]
 [0.385]
 [0.371]
 [0.339]] [[35.628]
 [45.419]
 [38.677]
 [41.238]
 [37.701]
 [33.333]
 [33.047]] [[0.732]
 [0.914]
 [0.79 ]
 [0.846]
 [0.779]
 [0.689]
 [0.653]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  11.55163181504335
printing an ep nov before normalisation:  30.62631240462117
printing an ep nov before normalisation:  52.66026456294945
maxi score, test score, baseline:  -0.9930034482758621 -1.0 -0.9930034482758621
probs:  [0.0906134782810911, 0.07335550071954591, 0.14608554901462964, 0.07335550071954591, 0.10846655851717235, 0.5081234127480152]
Printing some Q and Qe and total Qs values:  [[0.234]
 [0.223]
 [0.223]
 [0.223]
 [0.214]
 [0.223]
 [0.211]] [[20.755]
 [24.734]
 [24.734]
 [24.734]
 [24.915]
 [24.734]
 [26.059]] [[1.288]
 [1.689]
 [1.689]
 [1.689]
 [1.698]
 [1.689]
 [1.813]]
Printing some Q and Qe and total Qs values:  [[0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]] [[54.463]
 [52.209]
 [52.209]
 [52.209]
 [52.209]
 [52.209]
 [52.209]] [[2.56 ]
 [2.428]
 [2.428]
 [2.428]
 [2.428]
 [2.428]
 [2.428]]
maxi score, test score, baseline:  -0.9930506849315068 -1.0 -0.9930506849315068
probs:  [0.09096446752657705, 0.07287062476403001, 0.1491232478347644, 0.07287062476403001, 0.10968223590162587, 0.5044887992089727]
maxi score, test score, baseline:  -0.9930972789115646 -1.0 -0.9930972789115646
probs:  [0.09096486830603917, 0.07287007052539235, 0.1491267183152618, 0.07287007052539235, 0.10968362463084645, 0.5044846476970679]
from probs:  [0.09096486830603917, 0.07287007052539235, 0.1491267183152618, 0.07287007052539235, 0.10968362463084645, 0.5044846476970679]
maxi score, test score, baseline:  -0.9930972789115646 -1.0 -0.9930972789115646
probs:  [0.09133850065061525, 0.07235391470940154, 0.15236038403308796, 0.07235391470940154, 0.11097772748635369, 0.50061555841114]
printing an ep nov before normalisation:  35.9975417165578
maxi score, test score, baseline:  -0.9931432432432432 -1.0 -0.9931432432432432
maxi score, test score, baseline:  -0.9931432432432432 -1.0 -0.9931432432432432
probs:  [0.09153510201380227, 0.07208231839701598, 0.15406190649632984, 0.07208231839701598, 0.11165867127254685, 0.49857968342328896]
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]] [[41.603]
 [41.603]
 [41.603]
 [41.603]
 [41.603]
 [41.603]
 [41.603]] [[1.418]
 [1.418]
 [1.418]
 [1.418]
 [1.418]
 [1.418]
 [1.418]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9931432432432432 -1.0 -0.9931432432432432
probs:  [0.09194773857193271, 0.07151228003155674, 0.15763314102314155, 0.07151228003155674, 0.11308786809645993, 0.4943066922453524]
using explorer policy with actor:  1
from probs:  [0.0988163211354267, 0.07634935805527031, 0.0988163211354267, 0.07634935805527031, 0.12205800708041635, 0.5276106345381896]
maxi score, test score, baseline:  -0.9931885906040269 -1.0 -0.9931885906040269
maxi score, test score, baseline:  -0.9931885906040269 -1.0 -0.9931885906040269
probs:  [0.09924161223364752, 0.07615618885231906, 0.09924161223364752, 0.07615618885231906, 0.12312308469709107, 0.5260813131309758]
printing an ep nov before normalisation:  25.716371254228605
using explorer policy with actor:  1
using another actor
printing an ep nov before normalisation:  4.8216373874419105
maxi score, test score, baseline:  -0.9932333333333333 -1.0 -0.9932333333333333
probs:  [0.1006226339995495, 0.07552892323527287, 0.1006226339995495, 0.07552892323527287, 0.12658164513500803, 0.5211152403953472]
maxi score, test score, baseline:  -0.9932333333333333 -1.0 -0.9932333333333333
probs:  [0.1006226339995495, 0.07552892323527287, 0.1006226339995495, 0.07552892323527287, 0.12658164513500803, 0.5211152403953472]
printing an ep nov before normalisation:  55.541734014133
printing an ep nov before normalisation:  18.128098643667272
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9932774834437086 -1.0 -0.9932774834437086
printing an ep nov before normalisation:  41.312246322631836
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.07711203376636255, 0.07711203376636255, 0.10442019936299309, 0.07711203376636255, 0.13267002584226634, 0.531573673495653]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.07927497881002918, 0.07927497881002918, 0.07927497881002918, 0.07927497881002918, 0.13639403539520936, 0.546506049364674]
printing an ep nov before normalisation:  60.74963092803955
printing an ep nov before normalisation:  34.35100624709722
printing an ep nov before normalisation:  0.004010229640698526
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]] [[29.97 ]
 [22.918]
 [22.918]
 [22.918]
 [22.918]
 [22.918]
 [22.918]] [[2.06 ]
 [1.536]
 [1.536]
 [1.536]
 [1.536]
 [1.536]
 [1.536]]
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.08152924476135459, 0.08152924476135459, 0.08152924476135459, 0.08152924476135459, 0.112375840255646, 0.5615071806989357]
printing an ep nov before normalisation:  57.84677982330322
Printing some Q and Qe and total Qs values:  [[0.918]
 [0.856]
 [0.844]
 [0.838]
 [0.847]
 [0.864]
 [0.875]] [[12.514]
 [12.807]
 [12.447]
 [12.239]
 [12.101]
 [11.738]
 [11.577]] [[1.574]
 [1.533]
 [1.495]
 [1.474]
 [1.473]
 [1.465]
 [1.464]]
actions average: 
K:  4  action  0 :  tensor([0.4547, 0.0485, 0.0911, 0.0850, 0.1106, 0.1041, 0.1060],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0542, 0.6802, 0.0495, 0.0584, 0.0515, 0.0477, 0.0586],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.3184, 0.0288, 0.1174, 0.1155, 0.1529, 0.1419, 0.1250],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2432, 0.0250, 0.1210, 0.1295, 0.1853, 0.1838, 0.1122],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2081, 0.0684, 0.0916, 0.1087, 0.3053, 0.1188, 0.0991],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2667, 0.0395, 0.1316, 0.1124, 0.1491, 0.1733, 0.1274],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2589, 0.0101, 0.1237, 0.1180, 0.1500, 0.1383, 0.2011],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.10003700140375815, 0.07247427476076394, 0.10003700140375815, 0.10003700140375815, 0.1285340577634642, 0.49888066326449737]
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.10287099994882087, 0.0745263936673801, 0.10287099994882087, 0.0745263936673801, 0.1321764403414967, 0.5130287724261012]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
siam score:  -0.8549289
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.10587047838011665, 0.07669833776014384, 0.07669833776014384, 0.07669833776014384, 0.13603150512280054, 0.5280030032166513]
printing an ep nov before normalisation:  57.204309186614026
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
printing an ep nov before normalisation:  42.08587936980733
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.10587047838011665, 0.07669833776014384, 0.07669833776014384, 0.07669833776014384, 0.13603150512280054, 0.5280030032166513]
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
Printing some Q and Qe and total Qs values:  [[-0.094]
 [-0.08 ]
 [-0.103]
 [-0.089]
 [-0.1  ]
 [-0.098]
 [-0.1  ]] [[29.876]
 [28.61 ]
 [28.554]
 [34.824]
 [28.567]
 [28.983]
 [28.567]] [[0.781]
 [0.723]
 [0.697]
 [1.065]
 [0.701]
 [0.726]
 [0.701]]
printing an ep nov before normalisation:  30.42010088799814
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.12273498251606463, 0.09493873777301196, 0.06803914608618669, 0.09493873777301196, 0.1514734728436278, 0.467874923008097]
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.12273498251606463, 0.09493873777301196, 0.06803914608618669, 0.09493873777301196, 0.1514734728436278, 0.467874923008097]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.1261268714558123, 0.06991782502759072, 0.06991782502759072, 0.09756161835294548, 0.15566043822318285, 0.4808154219128779]
printing an ep nov before normalisation:  54.53606636047286
actor:  1 policy actor:  1  step number:  83 total reward:  0.026666666666665617  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.10073516995053067, 0.07790395440455063, 0.07790395440455063, 0.08913242106650798, 0.11273123235604555, 0.5415932678178145]
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.512]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]] [[26.395]
 [37.581]
 [26.395]
 [26.395]
 [26.395]
 [26.395]
 [26.395]] [[1.305]
 [1.728]
 [1.305]
 [1.305]
 [1.305]
 [1.305]
 [1.305]]
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.10073516995053067, 0.07790395440455063, 0.07790395440455063, 0.08913242106650798, 0.11273123235604555, 0.5415932678178145]
from probs:  [0.10073516995053067, 0.07790395440455063, 0.07790395440455063, 0.08913242106650798, 0.11273123235604555, 0.5415932678178145]
maxi score, test score, baseline:  -0.9934064935064936 -1.0 -0.9934064935064936
probs:  [0.10436891726274031, 0.08071247982672612, 0.08071247982672612, 0.08071247982672612, 0.09234679331984799, 0.5611468499372334]
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]] [[64.772]
 [61.473]
 [61.473]
 [61.473]
 [61.473]
 [61.473]
 [61.473]] [[0.621]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]]
actions average: 
K:  4  action  0 :  tensor([0.4362, 0.0303, 0.0967, 0.1175, 0.1211, 0.1089, 0.0893],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0404, 0.8341, 0.0173, 0.0370, 0.0256, 0.0162, 0.0295],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.3010, 0.0094, 0.2440, 0.1104, 0.1144, 0.1187, 0.1022],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.3035, 0.0193, 0.1415, 0.1369, 0.1293, 0.1506, 0.1189],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.3049, 0.0177, 0.1248, 0.1493, 0.1419, 0.1447, 0.1167],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2733, 0.0095, 0.1304, 0.1709, 0.1521, 0.1486, 0.1153],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2904, 0.0279, 0.1183, 0.1495, 0.1434, 0.1422, 0.1283],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
probs:  [0.1043695285852989, 0.08071240240111553, 0.08071240240111553, 0.08071240240111553, 0.09234705462284516, 0.5611462095885095]
printing an ep nov before normalisation:  60.54034233093262
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.723]
 [0.717]
 [0.51 ]
 [0.507]
 [0.516]
 [0.673]] [[53.818]
 [64.39 ]
 [62.265]
 [53.84 ]
 [53.823]
 [53.744]
 [58.282]] [[1.492]
 [2.107]
 [2.024]
 [1.509]
 [1.506]
 [1.512]
 [1.834]]
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
probs:  [0.1043695285852989, 0.08071240240111553, 0.08071240240111553, 0.08071240240111553, 0.09234705462284516, 0.5611462095885095]
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
probs:  [0.1043695285852989, 0.08071240240111553, 0.08071240240111553, 0.08071240240111553, 0.09234705462284516, 0.5611462095885095]
printing an ep nov before normalisation:  58.878230551389606
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.10437013204787213, 0.08071232597099788, 0.08071232597099788, 0.08071232597099788, 0.09234731256618188, 0.5611455774729524]
siam score:  -0.853741
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.08266644283586923, 0.08266644283586923, 0.08266644283586923, 0.08266644283586923, 0.09458358849895736, 0.5747506401575656]
printing an ep nov before normalisation:  31.059787641925986
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.08266644283586923, 0.08266644283586923, 0.08266644283586923, 0.08266644283586923, 0.09458358849895736, 0.5747506401575656]
Printing some Q and Qe and total Qs values:  [[0.771]
 [0.916]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]] [[73.476]
 [74.233]
 [73.476]
 [73.476]
 [73.476]
 [73.476]
 [73.476]] [[2.67]
 [2.85]
 [2.67]
 [2.67]
 [2.67]
 [2.67]
 [2.67]]
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.09091605612572547, 0.09091605612572547, 0.08000254436898267, 0.09091605612572547, 0.09091605612572547, 0.5563332311281154]
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.09191855777517559, 0.09191855777517559, 0.0808845296871739, 0.09191855777517559, 0.0808845296871739, 0.5624752673001254]
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.09191855777517559, 0.09191855777517559, 0.0808845296871739, 0.09191855777517559, 0.0808845296871739, 0.5624752673001254]
printing an ep nov before normalisation:  56.654767990112305
printing an ep nov before normalisation:  47.90718151454238
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
from probs:  [0.09341422842251357, 0.09341422842251357, 0.08170851965622662, 0.08170851965622662, 0.08170851965622662, 0.5680459841862928]
siam score:  -0.83892095
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.09341451858649288, 0.09341451858649288, 0.081708471593615, 0.081708471593615, 0.081708471593615, 0.5680455480461691]
using another actor
from probs:  [0.09341451858649288, 0.09341451858649288, 0.081708471593615, 0.081708471593615, 0.081708471593615, 0.5680455480461691]
maxi score, test score, baseline:  -0.9935708860759493 -1.0 -0.9935708860759493
probs:  [0.10010066620698621, 0.10010066620698621, 0.08897376090814207, 0.07819457139988685, 0.08897376090814207, 0.5436565743698566]
maxi score, test score, baseline:  -0.9935708860759493 -1.0 -0.9935708860759493
probs:  [0.08997421277327111, 0.10122642048350947, 0.08997421277327111, 0.07907363655397742, 0.08997421277327111, 0.5497773046426996]
maxi score, test score, baseline:  -0.9936106918238994 -1.0 -0.9936106918238994
probs:  [0.0899743903414262, 0.1012269006806044, 0.0899743903414262, 0.07907352095034743, 0.0899743903414262, 0.5497764073447697]
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
probs:  [0.08997456568349986, 0.10122737485770582, 0.08997456568349986, 0.07907340679598794, 0.08997456568349986, 0.5497755212958066]
printing an ep nov before normalisation:  25.79199820344986
printing an ep nov before normalisation:  53.35827717872961
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
probs:  [0.08997456568349986, 0.10122737485770582, 0.08997456568349986, 0.07907340679598794, 0.08997456568349986, 0.5497755212958066]
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
probs:  [0.08997456568349986, 0.10122737485770582, 0.08997456568349986, 0.07907340679598794, 0.08997456568349986, 0.5497755212958066]
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.08997473884109217, 0.1012278431273133, 0.08997473884109217, 0.07907329406381554, 0.08997473884109217, 0.5497746462855947]
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.08997473884109217, 0.1012278431273133, 0.08997473884109217, 0.07907329406381554, 0.08997473884109217, 0.5497746462855947]
printing an ep nov before normalisation:  19.911761356743195
actor:  1 policy actor:  1  step number:  55 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 0.0
siam score:  -0.84736943
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.08144650910185973, 0.09401269881168187, 0.08762987229240715, 0.08144650910185973, 0.08762987229240715, 0.5678345383997845]
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
printing an ep nov before normalisation:  46.57770288537224
actor:  1 policy actor:  1  step number:  60 total reward:  0.2199999999999993  reward:  1.0 rdn_beta:  2.0
siam score:  -0.84876424
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.542]] [[39.636]
 [39.636]
 [39.636]
 [39.636]
 [39.636]
 [39.636]
 [42.415]] [[2.263]
 [2.263]
 [2.263]
 [2.263]
 [2.263]
 [2.263]
 [2.532]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.08205667870371908, 0.0906296751808858, 0.08627513728772176, 0.08205667870371908, 0.08627513728772176, 0.5727066928362325]
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.08205667870371908, 0.0906296751808858, 0.08627513728772176, 0.08205667870371908, 0.08627513728772176, 0.5727066928362325]
printing an ep nov before normalisation:  43.4469108691394
printing an ep nov before normalisation:  34.664354827386624
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.08075672044124738, 0.09311523853943665, 0.0888649545374139, 0.08474749191045432, 0.0888649545374139, 0.5636506400340338]
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
printing an ep nov before normalisation:  47.539309355245784
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.08073188311592804, 0.09320801032490605, 0.08891727874509882, 0.08476063252716054, 0.08891727874509882, 0.5634649165418077]
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.08073188311592804, 0.09320801032490605, 0.08891727874509882, 0.08476063252716054, 0.08891727874509882, 0.5634649165418077]
printing an ep nov before normalisation:  44.138334355931775
printing an ep nov before normalisation:  39.17424016679287
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.08065659053612564, 0.0934892421188045, 0.08907589633640171, 0.08480046760969903, 0.08907589633640171, 0.5629019070625674]
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.08065659053612564, 0.0934892421188045, 0.08907589633640171, 0.08480046760969903, 0.08907589633640171, 0.5629019070625674]
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.08065659053612564, 0.0934892421188045, 0.08907589633640171, 0.08480046760969903, 0.08907589633640171, 0.5629019070625674]
printing an ep nov before normalisation:  29.05105159726932
printing an ep nov before normalisation:  34.22699401516077
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.08134084931322874, 0.0942825481843751, 0.08983169936625604, 0.08551993957370306, 0.08134084931322874, 0.5676841142492083]
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.08134084931322874, 0.0942825481843751, 0.08983169936625604, 0.08551993957370306, 0.08134084931322874, 0.5676841142492083]
printing an ep nov before normalisation:  28.323883140513832
printing an ep nov before normalisation:  0.1183596150417543
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.08041825432650945, 0.09271733207167752, 0.09271733207167752, 0.08848952409677602, 0.08439180252110222, 0.5612657549122572]
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.08041825432650945, 0.09271733207167752, 0.09271733207167752, 0.08848952409677602, 0.08439180252110222, 0.5612657549122572]
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.08041825432650945, 0.09271733207167752, 0.09271733207167752, 0.08848952409677602, 0.08439180252110222, 0.5612657549122572]
UNIT TEST: sample policy line 217 mcts : [0.327 0.102 0.02  0.265 0.143 0.041 0.102]
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.08041825432650945, 0.09271733207167752, 0.09271733207167752, 0.08848952409677602, 0.08439180252110222, 0.5612657549122572]
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.08041825432650945, 0.09271733207167752, 0.09271733207167752, 0.08848952409677602, 0.08439180252110222, 0.5612657549122572]
printing an ep nov before normalisation:  66.93664577058223
actor:  1 policy actor:  1  step number:  52 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  2.0
from probs:  [0.08041825432650945, 0.09271733207167752, 0.09271733207167752, 0.08848952409677602, 0.08439180252110222, 0.5612657549122572]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.08113976930570357, 0.0904290884410336, 0.0904290884410336, 0.08723588498826389, 0.08414093394942558, 0.5666252348745398]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.0811210096837833, 0.09048858350696241, 0.09048858350696241, 0.08726848000524458, 0.08414745661127193, 0.5664858866857754]
using explorer policy with actor:  1
printing an ep nov before normalisation:  11.203770126615252
printing an ep nov before normalisation:  37.49876298556539
line 256 mcts: sample exp_bonus 40.80204514707133
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.08110212659343795, 0.09054846997946142, 0.09054846997946142, 0.08730128944051585, 0.08415402214892245, 0.5663456218582009]
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.08110212659343795, 0.09054846997946142, 0.09054846997946142, 0.08730128944051585, 0.08415402214892245, 0.5663456218582009]
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.08110212659343795, 0.09054846997946142, 0.09054846997946142, 0.08730128944051585, 0.08415402214892245, 0.5663456218582009]
using another actor
from probs:  [0.08110212659343795, 0.09054846997946142, 0.09054846997946142, 0.08730128944051585, 0.08415402214892245, 0.5663456218582009]
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
actions average: 
K:  3  action  0 :  tensor([0.3927, 0.0188, 0.0881, 0.1397, 0.1306, 0.1217, 0.1084],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0695, 0.6957, 0.0450, 0.0451, 0.0393, 0.0368, 0.0686],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.3359, 0.0315, 0.1311, 0.1317, 0.1243, 0.1157, 0.1299],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2282, 0.0374, 0.1175, 0.1597, 0.1640, 0.1477, 0.1455],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.4102, 0.1057, 0.0680, 0.0910, 0.1584, 0.0953, 0.0714],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2296, 0.0645, 0.1161, 0.1306, 0.1375, 0.1965, 0.1252],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.3045, 0.0386, 0.1025, 0.1397, 0.1558, 0.1225, 0.1364],
       grad_fn=<DivBackward0>)
actions average: 
K:  1  action  0 :  tensor([0.4614, 0.0052, 0.0718, 0.1189, 0.1378, 0.1111, 0.0938],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0115, 0.9433, 0.0085, 0.0061, 0.0067, 0.0061, 0.0178],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2112, 0.0259, 0.2279, 0.1214, 0.1115, 0.1911, 0.1109],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2634, 0.0133, 0.1030, 0.2110, 0.1545, 0.1378, 0.1169],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2245, 0.0172, 0.1045, 0.1519, 0.2325, 0.1500, 0.1194],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2631, 0.0589, 0.1135, 0.1597, 0.1401, 0.1486, 0.1160],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2206, 0.0515, 0.1028, 0.1588, 0.1540, 0.1370, 0.1753],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9938393939393939 -1.0 -0.9938393939393939
probs:  [0.08037093058414692, 0.09280148237747422, 0.09280148237747422, 0.08639496722245167, 0.08639496722245167, 0.5612361702160014]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.0806171734798637, 0.09308609038314275, 0.09308609038314275, 0.08359271046814619, 0.08665980244068354, 0.562958132845021]
line 256 mcts: sample exp_bonus 47.89960861206055
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.0806171734798637, 0.09308609038314275, 0.09308609038314275, 0.08359271046814619, 0.08665980244068354, 0.562958132845021]
printing an ep nov before normalisation:  40.81376552581787
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  19.469115396860772
printing an ep nov before normalisation:  29.06154155731201
printing an ep nov before normalisation:  41.67979659354884
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.08059463220436165, 0.09316577400309838, 0.09316577400309838, 0.08359456376996928, 0.08668680092221098, 0.5627924550972614]
printing an ep nov before normalisation:  25.959360310907655
printing an ep nov before normalisation:  25.157947153608223
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
probs:  [0.08059458602785421, 0.09316593706072322, 0.09316593706072322, 0.08359456752433433, 0.08668685614378305, 0.562792116182582]
printing an ep nov before normalisation:  35.433955529414305
actor:  1 policy actor:  1  step number:  43 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.622]] [[58.838]
 [59.074]
 [59.074]
 [59.074]
 [59.074]
 [59.074]
 [59.074]] [[2.59 ]
 [2.591]
 [2.591]
 [2.591]
 [2.591]
 [2.591]
 [2.591]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.091]
 [0.121]
 [0.147]
 [0.145]
 [0.124]
 [0.088]
 [0.147]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.091]
 [0.121]
 [0.147]
 [0.145]
 [0.124]
 [0.088]
 [0.147]]
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.526]
 [0.495]
 [0.514]
 [0.495]
 [0.489]
 [0.452]] [[47.801]
 [50.174]
 [47.801]
 [46.443]
 [47.801]
 [44.059]
 [44.722]] [[2.29 ]
 [2.464]
 [2.29 ]
 [2.227]
 [2.29 ]
 [2.059]
 [2.061]]
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.799]
 [0.63 ]
 [0.61 ]
 [0.603]
 [0.609]
 [0.605]] [[55.952]
 [77.831]
 [65.552]
 [61.059]
 [58.465]
 [60.415]
 [63.799]] [[1.477]
 [2.248]
 [1.727]
 [1.578]
 [1.497]
 [1.559]
 [1.652]]
printing an ep nov before normalisation:  69.74957098892641
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.08127745397646693, 0.09075199238055101, 0.09075199238055101, 0.08353842336835066, 0.08586896104921539, 0.5678111768448649]
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.08127745397646693, 0.09075199238055101, 0.09075199238055101, 0.08353842336835066, 0.08586896104921539, 0.5678111768448649]
printing an ep nov before normalisation:  67.83470800720855
printing an ep nov before normalisation:  51.62346839904785
printing an ep nov before normalisation:  18.783325084461477
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.08127745397646693, 0.09075199238055101, 0.09075199238055101, 0.08353842336835066, 0.08586896104921539, 0.5678111768448649]
Printing some Q and Qe and total Qs values:  [[0.521]
 [0.644]
 [0.551]
 [0.461]
 [0.475]
 [0.551]
 [0.551]] [[28.139]
 [30.567]
 [30.451]
 [25.288]
 [25.354]
 [30.451]
 [30.451]] [[1.635]
 [1.929]
 [1.828]
 [1.374]
 [1.393]
 [1.828]
 [1.828]]
using explorer policy with actor:  1
using explorer policy with actor:  0
printing an ep nov before normalisation:  22.335540542164463
printing an ep nov before normalisation:  50.877351111181746
printing an ep nov before normalisation:  53.88686496760158
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.406]
 [0.464]
 [0.555]
 [0.465]
 [0.562]
 [0.539]] [[13.573]
 [23.559]
 [12.963]
 [12.764]
 [11.168]
 [12.571]
 [13.27 ]] [[0.589]
 [0.406]
 [0.464]
 [0.555]
 [0.465]
 [0.562]
 [0.539]]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]] [[50.578]
 [49.829]
 [49.829]
 [49.829]
 [49.829]
 [49.829]
 [49.829]] [[0.558]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]]
printing an ep nov before normalisation:  37.666876984670644
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
probs:  [0.0812773800766477, 0.09075225335853618, 0.09075225335853618, 0.0835384293825529, 0.08586904943633213, 0.5678106343873949]
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
probs:  [0.0812773800766477, 0.09075225335853618, 0.09075225335853618, 0.0835384293825529, 0.08586904943633213, 0.5678106343873949]
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.629]
 [0.587]
 [0.591]
 [0.587]
 [0.587]
 [0.587]] [[35.693]
 [49.129]
 [35.693]
 [37.471]
 [35.693]
 [35.693]
 [35.693]] [[0.587]
 [0.629]
 [0.587]
 [0.591]
 [0.587]
 [0.587]
 [0.587]]
printing an ep nov before normalisation:  53.81392651180818
printing an ep nov before normalisation:  32.12475715470019
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.08127734377736379, 0.0907523815498422, 0.0907523815498422, 0.08353843233670524, 0.08586909285171872, 0.5678103679345279]
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.446]
 [0.442]
 [0.437]
 [0.456]
 [0.599]
 [0.607]] [[23.064]
 [44.023]
 [15.375]
 [12.466]
 [11.851]
 [19.468]
 [20.579]] [[0.613]
 [0.446]
 [0.442]
 [0.437]
 [0.456]
 [0.599]
 [0.607]]
using explorer policy with actor:  0
Training Flag: False
Self play flag: True
add more workers flag:  True
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
main train batch thing paused
add a thread
Adding thread: now have 4 threads
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.599]
 [0.606]
 [0.61 ]
 [0.623]
 [0.618]
 [0.611]] [[18.496]
 [29.996]
 [17.772]
 [16.883]
 [14.406]
 [16.586]
 [21.695]] [[0.682]
 [0.599]
 [0.606]
 [0.61 ]
 [0.623]
 [0.618]
 [0.611]]
printing an ep nov before normalisation:  78.31795003344055
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]]
printing an ep nov before normalisation:  21.83703501524441
printing an ep nov before normalisation:  15.493638684043631
using explorer policy with actor:  0
printing an ep nov before normalisation:  12.240837164437357
printing an ep nov before normalisation:  32.072589759026314
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]] [[26.936]
 [17.934]
 [17.934]
 [17.934]
 [17.934]
 [17.934]
 [17.934]] [[0.739]
 [0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.647]]
main train batch thing paused
add a thread
Adding thread: now have 5 threads
printing an ep nov before normalisation:  12.256040573120117
actor:  1 policy actor:  1  step number:  67 total reward:  0.1466666666666664  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  10.728914930098416
Printing some Q and Qe and total Qs values:  [[1.015]
 [1.27 ]
 [1.015]
 [1.015]
 [1.015]
 [1.015]
 [1.015]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.015]
 [1.27 ]
 [1.015]
 [1.015]
 [1.015]
 [1.015]
 [1.015]]
main train batch thing paused
add a thread
Adding thread: now have 6 threads
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.824]
 [0.758]
 [0.764]
 [0.659]
 [0.742]
 [0.698]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.656]
 [0.824]
 [0.758]
 [0.764]
 [0.659]
 [0.742]
 [0.698]]
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]] [[33.329]
 [29.76 ]
 [29.76 ]
 [29.76 ]
 [29.76 ]
 [29.76 ]
 [29.76 ]] [[0.626]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]]
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.424]
 [0.494]
 [0.437]
 [0.545]
 [0.58 ]
 [0.493]] [[24.3  ]
 [40.683]
 [21.015]
 [10.389]
 [19.968]
 [18.303]
 [11.851]] [[0.607]
 [0.424]
 [0.494]
 [0.437]
 [0.545]
 [0.58 ]
 [0.493]]
maxi score, test score, baseline:  -0.9944652173913043 -1.0 -0.9944652173913043
printing an ep nov before normalisation:  42.6055952124392
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.4955],
        [-0.6082],
        [-0.6215],
        [-0.5978],
        [-0.0000],
        [-0.0000],
        [-0.3078],
        [-0.5346],
        [-0.2988],
        [-0.6575]], dtype=torch.float64)
-0.084359833866 -0.5799024883402146
-0.032346567066 -0.6405210675138205
-0.032346567066 -0.6538217867368683
-0.032346567066 -0.6301933089823611
-0.8755577446439999 -0.8755577446439999
-0.9605640000000001 -0.9605640000000001
-0.09703970119800001 -0.40486752087081235
-0.032346567066 -0.5669185787905269
-0.032346567066 -0.3311602840181532
-0.032346567066 -0.6898211824973103
printing an ep nov before normalisation:  52.32646720618041
printing an ep nov before normalisation:  69.52629422948746
maxi score, test score, baseline:  -0.9944945945945947 -1.0 -0.9944945945945947
probs:  [0.08125822379261259, 0.08926252249581672, 0.09142241262207815, 0.0851390958911358, 0.0851390958911358, 0.567778649307221]
maxi score, test score, baseline:  -0.9945236559139785 -1.0 -0.9945236559139785
probs:  [0.08143393852513046, 0.08945568647460214, 0.08945568647460214, 0.08532327086426825, 0.08532327086426825, 0.5690081467971289]
printing an ep nov before normalisation:  13.815555220615547
maxi score, test score, baseline:  -0.9945236559139785 -1.0 -0.9945236559139785
probs:  [0.08140584106361348, 0.08954420774686359, 0.08954420774686359, 0.08535171581912868, 0.08535171581912868, 0.568802311804402]
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.331]
 [0.693]
 [0.546]
 [0.602]
 [0.553]
 [0.483]] [[26.847]
 [29.006]
 [32.1  ]
 [26.245]
 [26.847]
 [26.391]
 [27.144]] [[1.703]
 [1.606]
 [2.219]
 [1.598]
 [1.703]
 [1.617]
 [1.608]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  67.24291215093673
printing an ep nov before normalisation:  80.14475340185282
using explorer policy with actor:  0
siam score:  -0.8401201
maxi score, test score, baseline:  -0.9945524064171123 -1.0 -0.9945524064171123
probs:  [0.08092426653913817, 0.09107161734341095, 0.08891725671111919, 0.08682817973435143, 0.08682817973435143, 0.5654304999376287]
using another actor
from probs:  [0.08092426653913817, 0.09107161734341095, 0.08891725671111919, 0.08682817973435143, 0.08682817973435143, 0.5654304999376287]
printing an ep nov before normalisation:  20.0750732421875
maxi score, test score, baseline:  -0.9945524064171123 -1.0 -0.9945524064171123
probs:  [0.08088986361754966, 0.09118014776641728, 0.08899544128558076, 0.08687693803143627, 0.08687693803143627, 0.5651806712675798]
siam score:  -0.8358078
printing an ep nov before normalisation:  18.75745923402308
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
printing an ep nov before normalisation:  17.784862636527244
Printing some Q and Qe and total Qs values:  [[0.134]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]] [[21.388]
 [18.888]
 [18.888]
 [18.888]
 [18.888]
 [18.888]
 [18.888]] [[1.898]
 [1.573]
 [1.573]
 [1.573]
 [1.573]
 [1.573]
 [1.573]]
printing an ep nov before normalisation:  13.569540221082564
printing an ep nov before normalisation:  47.79965400695801
printing an ep nov before normalisation:  17.925120274630977
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.08118838024190136, 0.09166555143931285, 0.08944116740047778, 0.08728418893857716, 0.08316055364376707, 0.5672601583359638]
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.23373967229307
printing an ep nov before normalisation:  32.53359882465219
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
probs:  [0.08117325977529187, 0.09172315817917151, 0.0894833335949632, 0.08731138248300367, 0.08315912300425747, 0.5671497429633122]
printing an ep nov before normalisation:  82.69747775911011
printing an ep nov before normalisation:  16.036174297332764
Printing some Q and Qe and total Qs values:  [[0.807]
 [0.812]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]] [[24.288]
 [27.58 ]
 [24.288]
 [24.288]
 [24.288]
 [24.288]
 [24.288]] [[0.807]
 [0.812]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]]
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]] [[12.27]
 [12.27]
 [12.27]
 [12.27]
 [12.27]
 [12.27]
 [12.27]] [[0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]]
maxi score, test score, baseline:  -0.9945808510638298 -1.0 -0.9945808510638298
Printing some Q and Qe and total Qs values:  [[0.26]
 [0.26]
 [0.26]
 [0.26]
 [0.26]
 [0.26]
 [0.26]] [[12.134]
 [12.134]
 [12.134]
 [12.134]
 [12.134]
 [12.134]
 [12.134]] [[0.26]
 [0.26]
 [0.26]
 [0.26]
 [0.26]
 [0.26]
 [0.26]]
printing an ep nov before normalisation:  20.14355995127536
printing an ep nov before normalisation:  29.813382520444115
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
probs:  [0.08115806900633382, 0.09178103265734579, 0.08952569575913093, 0.08733870240328623, 0.08315768569358313, 0.5670388144803201]
maxi score, test score, baseline:  -0.9946089947089947 -1.0 -0.9946089947089947
probs:  [0.08115806900633382, 0.09178103265734579, 0.08952569575913093, 0.08733870240328623, 0.08315768569358313, 0.5670388144803201]
printing an ep nov before normalisation:  16.81229372280891
maxi score, test score, baseline:  -0.9946368421052632 -1.0 -0.9946368421052632
probs:  [0.08115803981639286, 0.0917811437645658, 0.08952577708018446, 0.0873387548407844, 0.08315768291251954, 0.5670386015855529]
siam score:  -0.8410031
actor:  1 policy actor:  1  step number:  63 total reward:  0.2133333333333325  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  30.98032511596272
UNIT TEST: sample policy line 217 mcts : [0.061 0.347 0.429 0.041 0.061 0.061 0.   ]
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.08159389107250131, 0.09088717800403873, 0.08700089437812308, 0.08700089437812308, 0.08334321567137895, 0.5701739264958348]
printing an ep nov before normalisation:  36.21357360776983
printing an ep nov before normalisation:  40.84755897521973
maxi score, test score, baseline:  -0.9946643979057592 -1.0 -0.9946643979057592
probs:  [0.08159389107250131, 0.09088717800403873, 0.08700089437812308, 0.08700089437812308, 0.08334321567137895, 0.5701739264958348]
printing an ep nov before normalisation:  38.04671305542475
printing an ep nov before normalisation:  34.75409269332886
printing an ep nov before normalisation:  37.01700687408447
printing an ep nov before normalisation:  43.28978202445981
Printing some Q and Qe and total Qs values:  [[0.179]
 [0.304]
 [0.274]
 [0.269]
 [0.314]
 [0.314]
 [0.314]] [[32.189]
 [39.375]
 [33.688]
 [32.48 ]
 [34.394]
 [34.394]
 [34.394]] [[0.179]
 [0.304]
 [0.274]
 [0.269]
 [0.314]
 [0.314]
 [0.314]]
printing an ep nov before normalisation:  70.07727478949761
printing an ep nov before normalisation:  65.57922656737806
printing an ep nov before normalisation:  51.467520254146756
printing an ep nov before normalisation:  41.216251597198294
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.696]
 [0.676]
 [0.682]
 [0.873]
 [0.873]
 [0.69 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.67 ]
 [0.696]
 [0.676]
 [0.682]
 [0.873]
 [0.873]
 [0.69 ]]
printing an ep nov before normalisation:  53.03441393126798
printing an ep nov before normalisation:  51.27922825598274
printing an ep nov before normalisation:  37.44217687870285
maxi score, test score, baseline:  -0.9946916666666666 -1.0 -0.9946916666666666
probs:  [0.08160984430230644, 0.08888688358384234, 0.08698616436851579, 0.08888688358384234, 0.08335000586963023, 0.5702802182918629]
printing an ep nov before normalisation:  66.77494791952887
maxi score, test score, baseline:  -0.9946916666666666 -1.0 -0.9946916666666666
probs:  [0.08160984430230644, 0.08888688358384234, 0.08698616436851579, 0.08888688358384234, 0.08335000586963023, 0.5702802182918629]
Printing some Q and Qe and total Qs values:  [[0.734]
 [0.614]
 [0.714]
 [0.741]
 [0.741]
 [0.74 ]
 [0.719]] [[21.888]
 [39.247]
 [25.675]
 [21.991]
 [22.741]
 [21.609]
 [26.076]] [[0.734]
 [0.614]
 [0.714]
 [0.741]
 [0.741]
 [0.74 ]
 [0.719]]
maxi score, test score, baseline:  -0.9946916666666666 -1.0 -0.9946916666666666
probs:  [0.08159832682255716, 0.08892307370965812, 0.08700989355257951, 0.08892307370965812, 0.08334989673034217, 0.5701957354752047]
printing an ep nov before normalisation:  2.5823021227964205
printing an ep nov before normalisation:  16.96556915978707
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
probs:  [0.08158676008536592, 0.08895941853444331, 0.08703372416341565, 0.08895941853444331, 0.08334978710579746, 0.5701108915765344]
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
probs:  [0.08170939895146513, 0.08919140940011162, 0.08723715293964426, 0.08919140940011162, 0.08170939895146513, 0.5709612303572021]
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
probs:  [0.08170939895146513, 0.08919140940011162, 0.08723715293964426, 0.08919140940011162, 0.08170939895146513, 0.5709612303572021]
printing an ep nov before normalisation:  33.834074835352546
printing an ep nov before normalisation:  60.75648307800293
printing an ep nov before normalisation:  37.22200976848961
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
printing an ep nov before normalisation:  12.091536576489023
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.197169203026355
printing an ep nov before normalisation:  23.14959477611698
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
probs:  [0.08170937840876388, 0.08919148151693504, 0.08723720085435303, 0.08919148151693504, 0.08170937840876388, 0.5709610792942491]
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.659]
 [0.636]
 [0.597]
 [0.62 ]
 [0.605]
 [0.633]] [[53.865]
 [60.78 ]
 [55.932]
 [53.296]
 [53.308]
 [59.071]
 [55.672]] [[1.326]
 [1.546]
 [1.411]
 [1.312]
 [1.335]
 [1.452]
 [1.402]]
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
probs:  [0.08200758042178317, 0.08762885078692503, 0.08569998350476851, 0.08961616859278326, 0.08200758042178317, 0.5730398362719569]
printing an ep nov before normalisation:  45.198234609018954
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
probs:  [0.08215819792299563, 0.08588158830281158, 0.08588158830281158, 0.08983063870564667, 0.08215819792299563, 0.5740897888427389]
printing an ep nov before normalisation:  31.975684664631068
printing an ep nov before normalisation:  32.41165423110118
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
probs:  [0.08215043955389871, 0.08589773436285947, 0.08589773436285947, 0.08987213794812089, 0.08215043955389871, 0.5740315142183626]
printing an ep nov before normalisation:  54.24349237682293
maxi score, test score, baseline:  -0.9947453608247423 -1.0 -0.9947453608247423
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.71 ]
 [0.814]
 [0.71 ]
 [0.71 ]
 [0.71 ]
 [0.71 ]
 [0.71 ]] [[76.835]
 [66.9  ]
 [76.835]
 [76.835]
 [76.835]
 [76.835]
 [76.835]] [[2.71 ]
 [2.373]
 [2.71 ]
 [2.71 ]
 [2.71 ]
 [2.71 ]
 [2.71 ]]
Printing some Q and Qe and total Qs values:  [[1.461]
 [1.456]
 [1.456]
 [1.456]
 [1.456]
 [1.456]
 [1.456]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.461]
 [1.456]
 [1.456]
 [1.456]
 [1.456]
 [1.456]
 [1.456]]
printing an ep nov before normalisation:  30.577500655825013
printing an ep nov before normalisation:  53.10669527180248
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.836]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]] [[69.009]
 [84.901]
 [69.009]
 [69.009]
 [69.009]
 [69.009]
 [69.009]] [[2.054]
 [2.686]
 [2.054]
 [2.054]
 [2.054]
 [2.054]
 [2.054]]
siam score:  -0.8309501
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.08156571753285036, 0.08708169454559117, 0.08519050242693715, 0.0910334392711368, 0.08519050242693715, 0.5699381437965475]
printing an ep nov before normalisation:  50.29673666301805
printing an ep nov before normalisation:  28.067793613980196
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.08130193215983313, 0.08850683799041738, 0.08302810751507729, 0.092431898629467, 0.08662841611315793, 0.5681028075920473]
siam score:  -0.82847005
maxi score, test score, baseline:  -0.9947979591836735 -1.0 -0.9947979591836735
probs:  [0.08130193215983313, 0.08850683799041738, 0.08302810751507729, 0.092431898629467, 0.08662841611315793, 0.5681028075920473]
line 256 mcts: sample exp_bonus 55.782026036238605
printing an ep nov before normalisation:  49.48934972115963
printing an ep nov before normalisation:  28.619847297668457
printing an ep nov before normalisation:  21.518890857696533
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.501]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]] [[67.283]
 [70.201]
 [67.283]
 [67.283]
 [67.283]
 [67.283]
 [67.283]] [[2.277]
 [2.336]
 [2.277]
 [2.277]
 [2.277]
 [2.277]
 [2.277]]
UNIT TEST: sample policy line 217 mcts : [0.143 0.367 0.102 0.082 0.122 0.082 0.102]
printing an ep nov before normalisation:  20.35603548209366
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.08142884897471227, 0.08878160648776356, 0.08319044712888082, 0.0907549568497222, 0.08686463756471806, 0.568979502994203]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]] [[23.524]
 [23.524]
 [23.524]
 [23.524]
 [23.524]
 [23.524]
 [23.524]] [[1.144]
 [1.144]
 [1.144]
 [1.144]
 [1.144]
 [1.144]
 [1.144]]
printing an ep nov before normalisation:  74.54975216917177
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.569]
 [0.593]
 [0.569]
 [0.569]
 [0.569]
 [0.569]] [[ 94.37 ]
 [ 94.37 ]
 [103.386]
 [ 94.37 ]
 [ 94.37 ]
 [ 94.37 ]
 [ 94.37 ]] [[2.097]
 [2.097]
 [2.363]
 [2.097]
 [2.097]
 [2.097]
 [2.097]]
printing an ep nov before normalisation:  69.85770914962376
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.08142882655032468, 0.08878166907895937, 0.08319044507281009, 0.09075504225760031, 0.08686467799113676, 0.5689793390491689]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.08142882655032468, 0.08878166907895937, 0.08319044507281009, 0.09075504225760031, 0.08686467799113676, 0.5689793390491689]
UNIT TEST: sample policy line 217 mcts : [0.122 0.245 0.143 0.122 0.143 0.143 0.082]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.08141707878100103, 0.08881449675797773, 0.08318937683798502, 0.09079983320032808, 0.08688588421398023, 0.5688933302087278]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.08141707878100103, 0.08881449675797773, 0.08318937683798502, 0.09079983320032808, 0.08688588421398023, 0.5688933302087278]
printing an ep nov before normalisation:  23.81964605889199
printing an ep nov before normalisation:  15.657064307193703
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]] [[51.76]
 [51.76]
 [51.76]
 [51.76]
 [51.76]
 [51.76]
 [51.76]] [[0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]
 [0.322]]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.08170772539613706, 0.08726406570228791, 0.08350839123609335, 0.0912406621959057, 0.08535978005745687, 0.570919375412119]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  76 total reward:  0.13999999999999868  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  47.94477939605713
printing an ep nov before normalisation:  40.288611148318765
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.08200456266365641, 0.08703427154798585, 0.0836345609132076, 0.09063396516128043, 0.0836345609132076, 0.5730580788006622]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.08199656763985784, 0.08705579919568775, 0.08363613342183973, 0.09067662177976207, 0.08363613342183973, 0.572998744541013]
printing an ep nov before normalisation:  30.079983841373213
Printing some Q and Qe and total Qs values:  [[0.84 ]
 [0.484]
 [0.484]
 [0.484]
 [0.84 ]
 [0.484]
 [0.484]] [[ 0.019]
 [32.297]
 [32.297]
 [32.297]
 [ 0.019]
 [32.297]
 [32.297]] [[0.84 ]
 [0.484]
 [0.484]
 [0.484]
 [0.84 ]
 [0.484]
 [0.484]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.0821311026257971, 0.08719870470656432, 0.0837733810778976, 0.09082551796044676, 0.0821311026257971, 0.573940191003497]
printing an ep nov before normalisation:  52.20972117867067
line 256 mcts: sample exp_bonus 46.85381889343262
using another actor
Printing some Q and Qe and total Qs values:  [[0.61 ]
 [0.548]
 [0.44 ]
 [0.584]
 [0.44 ]
 [0.44 ]
 [0.44 ]] [[50.351]
 [38.735]
 [39.167]
 [48.512]
 [39.167]
 [39.167]
 [39.167]] [[2.076]
 [1.47 ]
 [1.382]
 [1.965]
 [1.382]
 [1.382]
 [1.382]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  40.81722358160024
printing an ep nov before normalisation:  52.35506799809109
using explorer policy with actor:  0
printing an ep nov before normalisation:  40.07916415019708
siam score:  -0.85337037
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.08174508987833123, 0.08843617588387413, 0.0849977011310257, 0.09023016271144722, 0.08334911734541343, 0.5712417530499082]
printing an ep nov before normalisation:  55.51772703009214
printing an ep nov before normalisation:  46.074276353138586
printing an ep nov before normalisation:  68.17050767153444
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  76.49875283081768
printing an ep nov before normalisation:  43.9452679875856
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.08187066094920704, 0.0868841764584406, 0.0851665831821291, 0.09046871894813416, 0.08349604725585355, 0.5721138132062357]
printing an ep nov before normalisation:  51.216751259842155
actor:  1 policy actor:  1  step number:  55 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.08205847526772538, 0.08644256912656584, 0.08494061104529642, 0.08957709033964985, 0.08347980250050013, 0.5735014517202623]
printing an ep nov before normalisation:  45.78455401857781
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.08217517701262736, 0.08656551597187837, 0.08506141836546831, 0.08970450228090808, 0.08217517701262736, 0.5743182093564904]
printing an ep nov before normalisation:  62.59749298548352
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.08217517701262736, 0.08656551597187837, 0.08506141836546831, 0.08970450228090808, 0.08217517701262736, 0.5743182093564904]
printing an ep nov before normalisation:  20.716436168501275
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.59 ]
 [0.527]
 [0.546]
 [0.518]
 [0.512]
 [0.538]] [[66.977]
 [83.111]
 [66.209]
 [63.634]
 [62.551]
 [62.972]
 [63.886]] [[1.465]
 [1.948]
 [1.444]
 [1.396]
 [1.339]
 [1.344]
 [1.394]]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.0821618734281355, 0.08660123329029157, 0.08508034148566403, 0.08977526836081863, 0.0821618734281355, 0.5742194100069548]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.0830976703431993, 0.08750003245780387, 0.0859918158074301, 0.09064761503249699, 0.08170848052036851, 0.5710543858387014]
maxi score, test score, baseline:  -0.9949738916256158 -1.0 -0.9949738916256158
probs:  [0.0830976703431993, 0.08750003245780387, 0.0859918158074301, 0.09064761503249699, 0.08170848052036851, 0.5710543858387014]
from probs:  [0.0830976703431993, 0.08750003245780387, 0.0859918158074301, 0.09064761503249699, 0.08170848052036851, 0.5710543858387014]
from probs:  [0.0848532329852825, 0.08918618702384289, 0.08770174906618794, 0.09228414450068799, 0.0808579117289476, 0.5651167746950512]
printing an ep nov before normalisation:  35.26139019095993
printing an ep nov before normalisation:  12.612122376820025
printing an ep nov before normalisation:  61.394712165365554
printing an ep nov before normalisation:  62.30998260022247
printing an ep nov before normalisation:  30.042119026184082
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.059]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]] [[20.456]
 [13.456]
 [13.456]
 [13.456]
 [13.456]
 [13.456]
 [13.456]] [[1.629]
 [0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.746]]
printing an ep nov before normalisation:  81.93943452666777
printing an ep nov before normalisation:  44.759536979532605
actor:  1 policy actor:  1  step number:  65 total reward:  0.02666666666666584  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  38.248000144958496
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
probs:  [0.08489901842556818, 0.08900403297696051, 0.0875976853991687, 0.09045056191411781, 0.08111387513792072, 0.5669348261462641]
deleting a thread, now have 5 threads
Frames:  23303 train batches done:  2730 episodes:  722
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
printing an ep nov before normalisation:  51.73216988421668
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.08382673183926824, 0.08924159282991499, 0.08645998615663755, 0.09069200202383824, 0.08133026995397005, 0.5684494171963711]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.08266281441155474, 0.08938635077998625, 0.08658641234710518, 0.09084631867713136, 0.081422889522831, 0.5690952142613913]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.08266281441155474, 0.08938635077998625, 0.08658641234710518, 0.09084631867713136, 0.081422889522831, 0.5690952142613913]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.08266281441155474, 0.08938635077998625, 0.08658641234710518, 0.09084631867713136, 0.081422889522831, 0.5690952142613913]
printing an ep nov before normalisation:  41.39322772228855
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.08266281441155474, 0.08938635077998625, 0.08658641234710518, 0.09084631867713136, 0.081422889522831, 0.5690952142613913]
actor:  1 policy actor:  1  step number:  57 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.07262866487352451, 0.0786400200498493, 0.07613666118190032, 0.07994534288813694, 0.1851129183339282, 0.5075363926726607]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.07262866487352451, 0.0786400200498493, 0.07613666118190032, 0.07994534288813694, 0.1851129183339282, 0.5075363926726607]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.07262866487352451, 0.0786400200498493, 0.07613666118190032, 0.07994534288813694, 0.1851129183339282, 0.5075363926726607]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.07262866487352451, 0.0786400200498493, 0.07613666118190032, 0.07994534288813694, 0.1851129183339282, 0.5075363926726607]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.07262866487352451, 0.0786400200498493, 0.07613666118190032, 0.07994534288813694, 0.1851129183339282, 0.5075363926726607]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.07262866487352451, 0.0786400200498493, 0.07613666118190032, 0.07994534288813694, 0.1851129183339282, 0.5075363926726607]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.07262866487352451, 0.0786400200498493, 0.07613666118190032, 0.07994534288813694, 0.1851129183339282, 0.5075363926726607]
from probs:  [0.07262866487352451, 0.0786400200498493, 0.07613666118190032, 0.07994534288813694, 0.1851129183339282, 0.5075363926726607]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.07257784744205124, 0.07861765130136036, 0.07610244531063438, 0.0799291515679532, 0.18559443159554553, 0.5071784727824553]
printing an ep nov before normalisation:  79.72635541643416
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.07257784744205124, 0.07861765130136036, 0.07610244531063438, 0.0799291515679532, 0.18559443159554553, 0.5071784727824553]
printing an ep nov before normalisation:  26.952290510345076
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.07257784744205124, 0.07861765130136036, 0.07610244531063438, 0.0799291515679532, 0.18559443159554553, 0.5071784727824553]
line 256 mcts: sample exp_bonus 64.42866140499558
printing an ep nov before normalisation:  29.51440143413805
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.518]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]] [[42.647]
 [46.311]
 [42.647]
 [42.647]
 [42.647]
 [42.647]
 [42.647]] [[2.215]
 [2.518]
 [2.215]
 [2.215]
 [2.215]
 [2.215]
 [2.215]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.07280401148221348, 0.07760956310785476, 0.07635880309570155, 0.08021829113320288, 0.1842501780791798, 0.5087591531018476]
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.743]
 [0.703]
 [0.713]
 [0.64 ]
 [0.655]
 [0.666]] [[66.411]
 [64.662]
 [67.119]
 [66.268]
 [65.91 ]
 [65.129]
 [65.334]] [[2.023]
 [2.077]
 [2.139]
 [2.114]
 [2.026]
 [2.009]
 [2.029]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.07239583755217939, 0.07840343609612861, 0.07713607969096671, 0.07970649268171759, 0.18645826530913362, 0.505899888669874]
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.508671158827646
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.07297301525440332, 0.07906073676007848, 0.07652692834960828, 0.08038117212891506, 0.18112082563272286, 0.509937321874272]
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.07314540108744781, 0.07924753006141295, 0.07670772502900586, 0.08057109043041386, 0.17918440682216796, 0.5111438465695515]
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.07298219631980964, 0.07892636333438616, 0.07767326866644839, 0.0802142661875444, 0.1801980652882499, 0.5100058402035614]
printing an ep nov before normalisation:  44.07456874847412
actions average: 
K:  2  action  0 :  tensor([0.4330, 0.0668, 0.0845, 0.0993, 0.1241, 0.0987, 0.0936],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0030, 0.9579, 0.0029, 0.0201, 0.0031, 0.0033, 0.0096],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2487, 0.0321, 0.1492, 0.1445, 0.1365, 0.1529, 0.1361],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2754, 0.0315, 0.1125, 0.2113, 0.1307, 0.1426, 0.0960],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3459, 0.0136, 0.0840, 0.1176, 0.2160, 0.1129, 0.1098],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2237, 0.0125, 0.1442, 0.1323, 0.1323, 0.2385, 0.1165],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2705, 0.0199, 0.1237, 0.1539, 0.1452, 0.1483, 0.1386],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.761]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.683]
 [0.761]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]]
printing an ep nov before normalisation:  36.39665126800537
actor:  1 policy actor:  1  step number:  64 total reward:  0.1933333333333327  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9950923076923077 -1.0 -0.9950923076923077
probs:  [0.07382087664586007, 0.0793881141652665, 0.07821448030982407, 0.08059434896113792, 0.17207492847136724, 0.5159072514465441]
deleting a thread, now have 4 threads
Frames:  24063 train batches done:  2818 episodes:  754
printing an ep nov before normalisation:  64.616121085203
actor:  1 policy actor:  1  step number:  46 total reward:  0.5533333333333337  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
probs:  [0.07490355975000346, 0.07988908963517302, 0.07781510920294248, 0.08096928777695973, 0.16289063626269734, 0.5235323173722238]
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
probs:  [0.07490355975000346, 0.07988908963517302, 0.07781510920294248, 0.08096928777695973, 0.16289063626269734, 0.5235323173722238]
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
probs:  [0.07490355975000346, 0.07988908963517302, 0.07781510920294248, 0.08096928777695973, 0.16289063626269734, 0.5235323173722238]
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
probs:  [0.07490355975000346, 0.07988908963517302, 0.07781510920294248, 0.08096928777695973, 0.16289063626269734, 0.5235323173722238]
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
probs:  [0.07490355975000346, 0.07988908963517302, 0.07781510920294248, 0.08096928777695973, 0.16289063626269734, 0.5235323173722238]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
using another actor
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
probs:  [0.07490355975000346, 0.07988908963517302, 0.07781510920294248, 0.08096928777695973, 0.16289063626269734, 0.5235323173722238]
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.502]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]] [[51.128]
 [68.48 ]
 [51.128]
 [51.128]
 [51.128]
 [51.128]
 [51.128]] [[1.42 ]
 [2.073]
 [1.42 ]
 [1.42 ]
 [1.42 ]
 [1.42 ]
 [1.42 ]]
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
probs:  [0.07490352643600423, 0.07988910700245642, 0.07781510548681231, 0.08096931612518775, 0.16289086438891945, 0.5235320805606198]
printing an ep nov before normalisation:  52.62609025857673
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
probs:  [0.07490352643600423, 0.07988910700245642, 0.07781510548681231, 0.08096931612518775, 0.16289086438891945, 0.5235320805606198]
printing an ep nov before normalisation:  52.62234765594105
printing an ep nov before normalisation:  69.5055941067106
printing an ep nov before normalisation:  60.13436317443848
printing an ep nov before normalisation:  41.2267742963408
actions average: 
K:  1  action  0 :  tensor([0.5269, 0.0046, 0.0826, 0.0969, 0.1030, 0.0998, 0.0861],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0630, 0.6731, 0.0463, 0.0775, 0.0407, 0.0361, 0.0634],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2568, 0.0179, 0.2687, 0.0997, 0.1118, 0.1459, 0.0992],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.3407, 0.0096, 0.1106, 0.1403, 0.1347, 0.1358, 0.1282],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3167, 0.0262, 0.1082, 0.1243, 0.1908, 0.1247, 0.1091],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2929, 0.0189, 0.1418, 0.1169, 0.1147, 0.2061, 0.1088],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1974, 0.2145, 0.1103, 0.1295, 0.1161, 0.1161, 0.1161],
       grad_fn=<DivBackward0>)
from probs:  [0.07460568594391156, 0.08050623129922747, 0.07843448426336098, 0.08158526621374128, 0.1634177641693994, 0.5214505681103592]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
probs:  [0.0746415156327447, 0.0805740536733471, 0.07749069508645506, 0.08165893910206838, 0.16393513186823158, 0.5216996646371531]
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.545]
 [0.431]
 [0.415]
 [0.413]
 [0.422]
 [0.42 ]] [[40.125]
 [68.59 ]
 [39.078]
 [39.889]
 [38.144]
 [36.265]
 [35.87 ]] [[0.82 ]
 [1.692]
 [0.831]
 [0.836]
 [0.789]
 [0.751]
 [0.739]]
printing an ep nov before normalisation:  45.11061668395996
actor:  1 policy actor:  1  step number:  64 total reward:  0.21999999999999897  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  42.41577932879359
from probs:  [0.06816571364269591, 0.07360934541921389, 0.0698859955460977, 0.07460482437834565, 0.23736737414432602, 0.4763667468693208]
printing an ep nov before normalisation:  69.10929547348412
maxi score, test score, baseline:  -0.995160663507109 -1.0 -0.995160663507109
probs:  [0.06829017081894259, 0.07377050275102229, 0.0700220506070024, 0.07477269308119426, 0.23590819863049006, 0.4772363841113484]
printing an ep nov before normalisation:  42.94208095259613
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  100.74520767340935
maxi score, test score, baseline:  -0.995160663507109 -1.0 -0.995160663507109
probs:  [0.06829854331173554, 0.07380632989554317, 0.07003909924514662, 0.07380632989554317, 0.2367562820447965, 0.4772934156072351]
maxi score, test score, baseline:  -0.995160663507109 -1.0 -0.995160663507109
probs:  [0.06829854331173554, 0.07380632989554317, 0.07003909924514662, 0.07380632989554317, 0.2367562820447965, 0.4772934156072351]
siam score:  -0.86210024
printing an ep nov before normalisation:  68.12009957249317
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.504]
 [0.47 ]
 [0.471]
 [0.476]
 [0.474]
 [0.474]] [[33.878]
 [41.662]
 [31.35 ]
 [31.042]
 [31.416]
 [30.474]
 [29.386]] [[0.48 ]
 [0.504]
 [0.47 ]
 [0.471]
 [0.476]
 [0.474]
 [0.474]]
printing an ep nov before normalisation:  22.731647211564027
printing an ep nov before normalisation:  48.10338266202763
printing an ep nov before normalisation:  47.910061084043946
maxi score, test score, baseline:  -0.995160663507109 -1.0 -0.995160663507109
maxi score, test score, baseline:  -0.995160663507109 -1.0 -0.995160663507109
using another actor
maxi score, test score, baseline:  -0.9951830188679245 -1.0 -0.9951830188679245
probs:  [0.06829850314462264, 0.07380634390070924, 0.07003907619741191, 0.07380634390070924, 0.2367566016855261, 0.47729313117102085]
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.06848149656428341, 0.07400418045035623, 0.07022676030317224, 0.07400418045035623, 0.23470929239354854, 0.47857408983828326]
printing an ep nov before normalisation:  35.963215749516884
printing an ep nov before normalisation:  38.592469692230225
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
printing an ep nov before normalisation:  26.918014458247594
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.0684220323956145, 0.07396677864966127, 0.07017426822481543, 0.07396677864966127, 0.23531388556995697, 0.47815625651029053]
printing an ep nov before normalisation:  45.408267974853516
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.06842199344720323, 0.07396679329819315, 0.07017424621396628, 0.07396679329819315, 0.23531419309914228, 0.4781559806433018]
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.06873826437246433, 0.07331760032754138, 0.07049863247987058, 0.07331760032754138, 0.23375801894862167, 0.4803698835439607]
printing an ep nov before normalisation:  49.19043212537829
from probs:  [0.06853104643348952, 0.07393011937202978, 0.07112540615720367, 0.07393011937202978, 0.23355924131131886, 0.47892406735392845]
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.06847206480480232, 0.07389260273074429, 0.0710767388731121, 0.07389260273074429, 0.23415635901010867, 0.4785096318504882]
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.06847206480480232, 0.07389260273074429, 0.0710767388731121, 0.07389260273074429, 0.23415635901010867, 0.4785096318504882]
Printing some Q and Qe and total Qs values:  [[0.201]
 [0.257]
 [0.237]
 [0.243]
 [0.188]
 [0.351]
 [0.277]] [[30.642]
 [31.129]
 [30.28 ]
 [30.067]
 [33.45 ]
 [30.483]
 [28.539]] [[1.851]
 [1.962]
 [1.845]
 [1.826]
 [2.16 ]
 [1.982]
 [1.685]]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.06859082521922043, 0.07404698445222938, 0.07121261601949744, 0.07404698445222938, 0.23276319086924896, 0.47933939898757444]
printing an ep nov before normalisation:  19.010920524597168
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.06859082521922043, 0.07404698445222938, 0.07121261601949744, 0.07404698445222938, 0.23276319086924896, 0.47933939898757444]
printing an ep nov before normalisation:  78.8241704305013
deleting a thread, now have 3 threads
Frames:  24996 train batches done:  2930 episodes:  785
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.0685323982670314, 0.07401017065429372, 0.07116457460896264, 0.07401017065429372, 0.23335384566127765, 0.47892884015414083]
printing an ep nov before normalisation:  23.670471873249426
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.06854096342657784, 0.0740456749996022, 0.07118608457205708, 0.07306705960884231, 0.2341729876894882, 0.4789872297034323]
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.06854096342657784, 0.0740456749996022, 0.07118608457205708, 0.07306705960884231, 0.2341729876894882, 0.4789872297034323]
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.06854096342657784, 0.0740456749996022, 0.07118608457205708, 0.07306705960884231, 0.2341729876894882, 0.4789872297034323]
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.06854096342657784, 0.0740456749996022, 0.07118608457205708, 0.07306705960884231, 0.2341729876894882, 0.4789872297034323]
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.06871710361846493, 0.07423598904082107, 0.07136903557466204, 0.07325485385462444, 0.23220280726072412, 0.4802202106507034]
printing an ep nov before normalisation:  61.34932313628965
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.09065827231169
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.06895289710441606, 0.07449075662523029, 0.07070418601270774, 0.07350624826597443, 0.23047514746934092, 0.4818707645223306]
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.06895289710441606, 0.07449075662523029, 0.07070418601270774, 0.07350624826597443, 0.23047514746934092, 0.4818707645223306]
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.575]
 [0.499]
 [0.489]
 [0.476]
 [0.489]
 [0.489]] [[58.405]
 [57.848]
 [63.261]
 [57.479]
 [59.831]
 [57.479]
 [57.479]] [[0.746]
 [0.85 ]
 [0.823]
 [0.761]
 [0.769]
 [0.761]
 [0.761]]
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.06895289710441606, 0.07449075662523029, 0.07070418601270774, 0.07350624826597443, 0.23047514746934092, 0.4818707645223306]
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.06895289710441606, 0.07449075662523029, 0.07070418601270774, 0.07350624826597443, 0.23047514746934092, 0.4818707645223306]
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.06895289710441606, 0.07449075662523029, 0.07070418601270774, 0.07350624826597443, 0.23047514746934092, 0.4818707645223306]
printing an ep nov before normalisation:  15.262453023537748
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.06912348778428926, 0.0746751304013041, 0.07087913544949907, 0.0736881717138348, 0.2285691789689506, 0.48306489568212213]
printing an ep nov before normalisation:  49.7414220361756
printing an ep nov before normalisation:  35.62565326690674
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.06918491939305815, 0.07474150540834644, 0.07005240328574029, 0.07375366789451741, 0.2287725872157865, 0.4834949168025512]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.06918491939305815, 0.07474150540834644, 0.07005240328574029, 0.07375366789451741, 0.2287725872157865, 0.4834949168025512]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.06918491939305815, 0.07474150540834644, 0.07005240328574029, 0.07375366789451741, 0.2287725872157865, 0.4834949168025512]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.660601691726804
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.06918491939305815, 0.07474150540834644, 0.07005240328574029, 0.07375366789451741, 0.2287725872157865, 0.4834949168025512]
Printing some Q and Qe and total Qs values:  [[-0.012]
 [-0.019]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.013]
 [-0.013]] [[15.158]
 [18.232]
 [12.844]
 [13.855]
 [13.329]
 [12.855]
 [14.308]] [[0.097]
 [0.15 ]
 [0.049]
 [0.069]
 [0.059]
 [0.05 ]
 [0.079]]
printing an ep nov before normalisation:  35.726301858457944
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.06918491939305815, 0.07474150540834644, 0.07005240328574029, 0.07375366789451741, 0.2287725872157865, 0.4834949168025512]
siam score:  -0.85458857
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.06897571658059205, 0.07534603325079042, 0.07068061145796882, 0.07436318439310266, 0.22859924613601387, 0.48203520818153206]
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.617]
 [0.602]
 [0.586]
 [0.594]
 [0.594]
 [0.594]] [[54.464]
 [53.294]
 [58.688]
 [58.391]
 [54.464]
 [54.464]
 [54.464]] [[2.374]
 [2.357]
 [2.527]
 [2.501]
 [2.374]
 [2.374]
 [2.374]]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.06914630826654694, 0.07556284641079367, 0.06999420794989382, 0.07457286623996703, 0.22749596689795082, 0.4832278042348478]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.06914630826654694, 0.07556284641079367, 0.06999420794989382, 0.07457286623996703, 0.22749596689795082, 0.4832278042348478]
printing an ep nov before normalisation:  55.32705793289408
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.06925570205018758, 0.07571293579221605, 0.07010897936609847, 0.07471667687201737, 0.22621370200583046, 0.48399200391365005]
line 256 mcts: sample exp_bonus 69.02592627245073
printing an ep nov before normalisation:  31.46322727203369
printing an ep nov before normalisation:  20.09100441913296
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.06914486576757525, 0.07565281868291061, 0.07000484525995886, 0.0746487345188303, 0.22733571253033258, 0.4832130232403923]
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]] [[26.173]
 [26.173]
 [26.173]
 [26.173]
 [26.173]
 [26.173]
 [26.173]] [[0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.06914486576757525, 0.07565281868291061, 0.07000484525995886, 0.0746487345188303, 0.22733571253033258, 0.4832130232403923]
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.06907357634024418, 0.07539728107810176, 0.07076742582359889, 0.07442257470873274, 0.227620465574863, 0.4827186764744594]
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.06923660409897466, 0.07557526291548783, 0.07093445913911212, 0.07459825159414557, 0.22579554436625202, 0.48385987788602786]
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.759]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]] [[67.192]
 [71.601]
 [67.192]
 [67.192]
 [67.192]
 [67.192]
 [67.192]] [[1.587]
 [1.845]
 [1.587]
 [1.587]
 [1.587]
 [1.587]
 [1.587]]
printing an ep nov before normalisation:  56.96178582587665
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.824]
 [0.718]
 [0.769]
 [0.811]
 [0.829]
 [0.784]] [[62.555]
 [55.773]
 [59.82 ]
 [59.358]
 [57.746]
 [57.885]
 [58.09 ]] [[2.234]
 [2.169]
 [2.216]
 [2.249]
 [2.231]
 [2.254]
 [2.216]]
printing an ep nov before normalisation:  63.39768363445007
printing an ep nov before normalisation:  50.96410826733803
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
printing an ep nov before normalisation:  54.045041191994464
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
probs:  [0.06978910004708323, 0.07620847987255046, 0.07150857678604766, 0.07331623181931797, 0.2214517955853964, 0.4877258158896042]
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
probs:  [0.06978910004708323, 0.07620847987255046, 0.07150857678604766, 0.07331623181931797, 0.2214517955853964, 0.4877258158896042]
printing an ep nov before normalisation:  57.28146282684187
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
probs:  [0.06984982660112433, 0.07627480289387338, 0.07069969119011232, 0.07338003335538205, 0.22164474290476482, 0.48815090305474307]
printing an ep nov before normalisation:  28.820107913841113
siam score:  -0.8413861
siam score:  -0.8406852
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]] [[34.182]
 [32.869]
 [32.869]
 [32.869]
 [32.869]
 [32.869]
 [32.869]] [[2.272]
 [2.126]
 [2.126]
 [2.126]
 [2.126]
 [2.126]
 [2.126]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.567]
 [0.483]
 [0.487]
 [0.486]
 [0.49 ]
 [0.49 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.477]
 [0.567]
 [0.483]
 [0.487]
 [0.486]
 [0.49 ]
 [0.49 ]]
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
probs:  [0.07152139784012189, 0.07750945670037261, 0.07074841261719794, 0.07481154007102887, 0.21093750803671482, 0.4944716847345639]
Printing some Q and Qe and total Qs values:  [[0.836]
 [0.988]
 [0.837]
 [0.812]
 [0.836]
 [0.836]
 [0.836]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.836]
 [0.988]
 [0.837]
 [0.812]
 [0.836]
 [0.836]
 [0.836]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.168]
 [0.164]
 [0.167]
 [0.169]
 [0.171]
 [0.17 ]] [[20.874]
 [15.958]
 [20.351]
 [18.569]
 [17.302]
 [16.159]
 [15.154]] [[0.171]
 [0.168]
 [0.164]
 [0.167]
 [0.169]
 [0.171]
 [0.17 ]]
printing an ep nov before normalisation:  50.69623424686438
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
probs:  [0.07154139836324672, 0.07663104273872039, 0.0707646654866684, 0.07484749214560567, 0.2116313556531397, 0.4945840456126191]
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
probs:  [0.07154139836324672, 0.07663104273872039, 0.0707646654866684, 0.07484749214560567, 0.2116313556531397, 0.4945840456126191]
printing an ep nov before normalisation:  21.907980832667256
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
printing an ep nov before normalisation:  30.154746450060024
line 256 mcts: sample exp_bonus 27.267939612304136
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.232]
 [0.231]
 [0.232]
 [0.231]
 [0.229]
 [0.231]] [[27.111]
 [31.353]
 [28.124]
 [26.534]
 [25.65 ]
 [25.38 ]
 [25.47 ]] [[0.231]
 [0.232]
 [0.231]
 [0.232]
 [0.231]
 [0.229]
 [0.231]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.598]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]] [[63.705]
 [67.655]
 [63.705]
 [63.705]
 [63.705]
 [63.705]
 [63.705]] [[2.12 ]
 [2.265]
 [2.12 ]
 [2.12 ]
 [2.12 ]
 [2.12 ]
 [2.12 ]]
printing an ep nov before normalisation:  37.93668045058993
printing an ep nov before normalisation:  33.110022942396355
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.07142308511895473, 0.07567836618748817, 0.07063355104117866, 0.07478366606538628, 0.21382079709783047, 0.49366053448916153]
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.07157027045576227, 0.07583433699521161, 0.07077910630265961, 0.07493778967153251, 0.21219902356234083, 0.49467947301249304]
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
actor:  1 policy actor:  1  step number:  41 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.07248130061520612, 0.07641605491904262, 0.07175123776847019, 0.07558874760387699, 0.20224934865338862, 0.5015133104400153]
Printing some Q and Qe and total Qs values:  [[0.211]
 [0.228]
 [0.31 ]
 [0.231]
 [0.31 ]
 [0.31 ]
 [0.31 ]] [[35.203]
 [35.656]
 [37.846]
 [35.497]
 [37.846]
 [37.846]
 [37.846]] [[1.478]
 [1.528]
 [1.777]
 [1.52 ]
 [1.777]
 [1.777]
 [1.777]]
printing an ep nov before normalisation:  32.42591857910156
printing an ep nov before normalisation:  58.537241458185754
printing an ep nov before normalisation:  29.167780811850008
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.551]
 [0.465]
 [0.465]
 [0.53 ]
 [0.464]
 [0.465]] [[27.59 ]
 [47.08 ]
 [27.59 ]
 [27.59 ]
 [43.326]
 [36.496]
 [27.59 ]] [[0.868]
 [1.757]
 [0.868]
 [0.868]
 [1.581]
 [1.234]
 [0.868]]
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.549]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]] [[40.352]
 [38.097]
 [37.854]
 [37.854]
 [37.854]
 [37.854]
 [37.854]] [[2.199]
 [2.044]
 [2.011]
 [2.011]
 [2.011]
 [2.011]
 [2.011]]
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.07248130061520612, 0.07641605491904262, 0.07175123776847019, 0.07558874760387699, 0.20224934865338862, 0.5015133104400153]
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.07274993386282412, 0.0766992981731304, 0.07201716024380345, 0.07586891901045062, 0.19928972041242016, 0.5033749682973712]
printing an ep nov before normalisation:  39.663978446340025
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.07274993386282412, 0.0766992981731304, 0.07201716024380345, 0.07586891901045062, 0.19928972041242016, 0.5033749682973712]
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.07274993386282412, 0.0766992981731304, 0.07201716024380345, 0.07586891901045062, 0.19928972041242016, 0.5033749682973712]
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.571589462156496
printing an ep nov before normalisation:  38.09644362498747
actions average: 
K:  4  action  0 :  tensor([0.5115, 0.0250, 0.0815, 0.0856, 0.1129, 0.0961, 0.0875],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0905, 0.5751, 0.0622, 0.0702, 0.0755, 0.0745, 0.0519],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2870, 0.1066, 0.1273, 0.1033, 0.1263, 0.1422, 0.1074],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2347, 0.0146, 0.1345, 0.1427, 0.1901, 0.1548, 0.1285],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2049, 0.0036, 0.1162, 0.1109, 0.3328, 0.1323, 0.0993],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2255, 0.0583, 0.1392, 0.1429, 0.1372, 0.1586, 0.1383],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2238, 0.0159, 0.1375, 0.1188, 0.1408, 0.1563, 0.2068],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.07294171309983442, 0.07606893542338991, 0.07220700424068582, 0.07606893542338991, 0.19800939237730447, 0.5047040194353954]
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.442]
 [0.502]
 [0.523]
 [0.502]
 [0.502]
 [0.502]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.55 ]
 [0.442]
 [0.502]
 [0.523]
 [0.502]
 [0.502]
 [0.502]]
printing an ep nov before normalisation:  66.73398838784378
siam score:  -0.8410859
printing an ep nov before normalisation:  37.14893417798286
printing an ep nov before normalisation:  29.649436996258217
printing an ep nov before normalisation:  42.29236762651709
siam score:  -0.84343076
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.07301189152929052, 0.07618353241936907, 0.07226674698282627, 0.0753605116820702, 0.19805895388570513, 0.5051183635007387]
line 256 mcts: sample exp_bonus 0.0
line 256 mcts: sample exp_bonus 41.24292561190352
Printing some Q and Qe and total Qs values:  [[-0.005]
 [ 0.002]
 [ 0.002]
 [ 0.002]
 [ 0.002]
 [ 0.002]
 [ 0.002]] [[25.104]
 [23.474]
 [23.474]
 [23.474]
 [23.474]
 [23.474]
 [23.474]] [[1.662]
 [1.561]
 [1.561]
 [1.561]
 [1.561]
 [1.561]
 [1.561]]
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.473]
 [0.269]
 [0.21 ]
 [0.236]
 [0.292]
 [0.203]] [[45.151]
 [39.268]
 [40.75 ]
 [52.619]
 [44.191]
 [38.094]
 [41.309]] [[1.333]
 [1.268]
 [1.12 ]
 [1.516]
 [1.22 ]
 [1.041]
 [1.076]]
printing an ep nov before normalisation:  32.25063767553934
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
printing an ep nov before normalisation:  55.410600841602594
printing an ep nov before normalisation:  60.526108741760254
printing an ep nov before normalisation:  32.824017697150154
siam score:  -0.84652066
printing an ep nov before normalisation:  27.047962640311802
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.0731016297857977, 0.0762910219522639, 0.07235231475873637, 0.07546339487109227, 0.19707554522663837, 0.5057160934054714]
printing an ep nov before normalisation:  32.00516373047502
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]] [[66.581]
 [66.581]
 [66.581]
 [66.581]
 [66.581]
 [66.581]
 [66.581]] [[0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]]
printing an ep nov before normalisation:  57.605308403332124
from probs:  [0.0731016297857977, 0.0762910219522639, 0.07235231475873637, 0.07546339487109227, 0.19707554522663837, 0.5057160934054714]
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.07322890917737748, 0.07642386462709475, 0.07247828711388969, 0.07559479390913013, 0.19567615937272234, 0.5065979857997857]
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.0737594463884689, 0.07694041855664324, 0.0722825664532451, 0.07611497641173724, 0.19567078476889607, 0.5052318074210095]
siam score:  -0.845066
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.0737594463884689, 0.07694041855664324, 0.0722825664532451, 0.07611497641173724, 0.19567078476889607, 0.5052318074210095]
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.0737594463884689, 0.07694041855664324, 0.0722825664532451, 0.07611497641173724, 0.19567078476889607, 0.5052318074210095]
from probs:  [0.0737594463884689, 0.07694041855664324, 0.0722825664532451, 0.07611497641173724, 0.19567078476889607, 0.5052318074210095]
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.574]
 [0.364]
 [0.532]
 [0.501]
 [0.379]
 [0.489]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.479]
 [0.574]
 [0.364]
 [0.532]
 [0.501]
 [0.379]
 [0.489]]
printing an ep nov before normalisation:  42.25562277808468
printing an ep nov before normalisation:  46.717401592366556
from probs:  [0.07375939354741545, 0.07694048281426541, 0.0722824592449494, 0.07611501028299421, 0.1956716080575292, 0.5052310460528463]
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]] [[23.194]
 [23.194]
 [23.194]
 [23.194]
 [23.194]
 [23.194]
 [23.194]] [[1.351]
 [1.351]
 [1.351]
 [1.351]
 [1.351]
 [1.351]
 [1.351]]
printing an ep nov before normalisation:  62.392697948437714
actor:  1 policy actor:  1  step number:  44 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  63.52951263255104
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.95218405837381
printing an ep nov before normalisation:  47.20360912830747
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
probs:  [0.06836241203868929, 0.07132300845820695, 0.0669878494153418, 0.07055475242529413, 0.25460829693320103, 0.46816368072926684]
printing an ep nov before normalisation:  55.5551072930545
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
probs:  [0.06836241203868929, 0.07132300845820695, 0.0669878494153418, 0.07055475242529413, 0.25460829693320103, 0.46816368072926684]
printing an ep nov before normalisation:  24.69756841659546
printing an ep nov before normalisation:  16.708483542393317
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]] [[24.855]
 [24.855]
 [24.855]
 [24.855]
 [24.855]
 [24.855]
 [24.855]] [[1.7]
 [1.7]
 [1.7]
 [1.7]
 [1.7]
 [1.7]
 [1.7]]
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
probs:  [0.06896262506667676, 0.07192831105739422, 0.06692153529659474, 0.07115873431296753, 0.2533269116714447, 0.467701882594922]
printing an ep nov before normalisation:  17.470663712347605
printing an ep nov before normalisation:  58.705573708249354
printing an ep nov before normalisation:  58.10990333557129
printing an ep nov before normalisation:  35.57042121887207
printing an ep nov before normalisation:  35.25216343860848
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 30.357676238628894
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.74 ]
 [0.765]
 [0.621]
 [0.628]
 [0.638]
 [0.667]] [[40.431]
 [56.653]
 [49.203]
 [41.106]
 [40.132]
 [40.048]
 [39.182]] [[1.618]
 [2.236]
 [2.02 ]
 [1.615]
 [1.591]
 [1.598]
 [1.6  ]]
maxi score, test score, baseline:  -0.9955331877729258 -1.0 -0.9955331877729258
probs:  [0.06840898356587795, 0.07211048047945845, 0.06705031646348135, 0.07057435926032254, 0.2532547711396863, 0.4686010890911734]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  52.51925473754859
from probs:  [0.0637168857869443, 0.06717859662117925, 0.062446234280731, 0.06574198662497174, 0.30454819939362876, 0.436368097292545]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.0641345247662285, 0.06759464255451285, 0.062251576946557474, 0.06615869367237484, 0.30485294107086547, 0.435007620989461]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.06417884022551418, 0.06764135385128509, 0.062294588624513265, 0.06551254917766296, 0.3050639335743367, 0.4353087345466878]
Printing some Q and Qe and total Qs values:  [[0.145]
 [0.114]
 [0.137]
 [0.138]
 [0.145]
 [0.129]
 [0.145]] [[35.171]
 [56.138]
 [32.552]
 [27.663]
 [37.043]
 [26.419]
 [37.043]] [[0.145]
 [0.114]
 [0.137]
 [0.138]
 [0.145]
 [0.129]
 [0.145]]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.06417884022551418, 0.06764135385128509, 0.062294588624513265, 0.06551254917766296, 0.3050639335743367, 0.4353087345466878]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.595]
 [0.64 ]
 [0.654]
 [0.654]
 [0.654]
 [0.654]] [[68.913]
 [74.216]
 [72.598]
 [68.913]
 [68.913]
 [68.913]
 [68.913]] [[2.203]
 [2.314]
 [2.308]
 [2.203]
 [2.203]
 [2.203]
 [2.203]]
printing an ep nov before normalisation:  28.32375479978319
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.06417884022551418, 0.06764135385128509, 0.062294588624513265, 0.06551254917766296, 0.3050639335743367, 0.4353087345466878]
siam score:  -0.83704376
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.06434508387498825, 0.0678166458159836, 0.06245590830709774, 0.06568227810411238, 0.303261996651384, 0.43643808724643407]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  33.025956478926034
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.06434508387498825, 0.0678166458159836, 0.06245590830709774, 0.06568227810411238, 0.303261996651384, 0.43643808724643407]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.06428413131657279, 0.06776681750167457, 0.062388902090261586, 0.0656256104397231, 0.3039666274540181, 0.43596791119774975]
printing an ep nov before normalisation:  44.71814322909646
printing an ep nov before normalisation:  38.67121243889541
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
actions average: 
K:  0  action  0 :  tensor([0.4518, 0.0058, 0.1076, 0.1086, 0.1202, 0.1114, 0.0947],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0192, 0.9019, 0.0151, 0.0139, 0.0122, 0.0122, 0.0257],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2800, 0.0105, 0.1526, 0.1264, 0.1504, 0.1625, 0.1176],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1796, 0.0028, 0.1076, 0.3273, 0.1404, 0.1326, 0.1098],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2738, 0.0026, 0.0975, 0.1143, 0.2927, 0.1110, 0.1080],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2588, 0.0103, 0.1434, 0.1404, 0.1447, 0.2065, 0.0959],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2281, 0.0512, 0.1104, 0.1164, 0.0981, 0.1000, 0.2959],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  48 total reward:  0.35333333333333283  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.06544132998106549, 0.06874974275843941, 0.06364093791151781, 0.06671568156938729, 0.2907014324133255, 0.4447508753662644]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.06544132998106549, 0.06874974275843941, 0.06364093791151781, 0.06671568156938729, 0.2907014324133255, 0.4447508753662644]
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.423]
 [0.451]
 [0.511]
 [0.444]
 [0.451]
 [0.475]] [[36.492]
 [39.662]
 [15.025]
 [26.168]
 [14.47 ]
 [14.64 ]
 [15.73 ]] [[0.754]
 [0.644]
 [0.505]
 [0.64 ]
 [0.494]
 [0.501]
 [0.534]]
printing an ep nov before normalisation:  54.997302472616035
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.06544132998106549, 0.06874974275843941, 0.06364093791151781, 0.06671568156938729, 0.2907014324133255, 0.4447508753662644]
maxi score, test score, baseline:  -0.9955896551724138 -1.0 -0.9955896551724138
probs:  [0.06486671580308138, 0.06879196177337998, 0.06368001353299112, 0.06675664904803998, 0.2908802159501479, 0.4450244438923596]
siam score:  -0.8471028
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.43 ]
 [0.413]
 [0.409]
 [0.411]
 [0.414]
 [0.407]] [[40.357]
 [69.459]
 [40.801]
 [39.933]
 [39.413]
 [38.939]
 [40.643]] [[0.875]
 [1.642]
 [0.895]
 [0.868]
 [0.858]
 [0.848]
 [0.885]]
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.06486671199901757, 0.06879199226843959, 0.06367999935942488, 0.06675666175836892, 0.2908802920368778, 0.4450243425778711]
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
printing an ep nov before normalisation:  30.641082188234464
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
Printing some Q and Qe and total Qs values:  [[0.238]
 [0.217]
 [0.242]
 [0.243]
 [0.247]
 [0.247]
 [0.24 ]] [[26.294]
 [26.784]
 [24.83 ]
 [24.803]
 [24.884]
 [24.866]
 [24.704]] [[1.571]
 [1.6  ]
 [1.424]
 [1.423]
 [1.436]
 [1.433]
 [1.41 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.06546617142390368, 0.06938734735707229, 0.06370840290213844, 0.06735414502135524, 0.2888576302324919, 0.4452263030630384]
from probs:  [0.06561986141312096, 0.06955026156677914, 0.06385795789596384, 0.06751227630191935, 0.2871862975595285, 0.44627334526268814]
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.621]
 [0.651]
 [0.676]
 [0.676]
 [0.676]
 [0.676]] [[41.853]
 [55.404]
 [44.72 ]
 [41.853]
 [41.853]
 [41.853]
 [41.853]] [[1.56 ]
 [1.971]
 [1.635]
 [1.56 ]
 [1.56 ]
 [1.56 ]
 [1.56 ]]
printing an ep nov before normalisation:  77.1154225493151
printing an ep nov before normalisation:  25.494006153314235
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.0657716529255815, 0.06971116335627538, 0.06400566549113251, 0.06766845424406373, 0.28553561025357016, 0.44730745372937664]
printing an ep nov before normalisation:  18.76468089722964
printing an ep nov before normalisation:  46.44704909952283
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.0657716529255815, 0.06971116335627538, 0.06400566549113251, 0.06766845424406373, 0.28553561025357016, 0.44730745372937664]
printing an ep nov before normalisation:  28.72710508251089
using explorer policy with actor:  1
printing an ep nov before normalisation:  24.709988701267847
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.06596768449905055, 0.06921873430949359, 0.06419642287818846, 0.06787015068442091, 0.28410405162680286, 0.4486429560020436]
printing an ep nov before normalisation:  51.44356154302484
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.06610332649882945, 0.06937437618699686, 0.06432116839286238, 0.06736387735426957, 0.28332195552510536, 0.4495152960419363]
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.06610332649882945, 0.06937437618699686, 0.06432116839286238, 0.06736387735426957, 0.28332195552510536, 0.4495152960419363]
printing an ep nov before normalisation:  66.13579708441286
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
printing an ep nov before normalisation:  46.95486186922102
UNIT TEST: sample policy line 217 mcts : [0.265 0.102 0.163 0.102 0.102 0.122 0.143]
printing an ep nov before normalisation:  21.328282191798387
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.06599211190775761, 0.0692842345013285, 0.06419847270160518, 0.06726078354137761, 0.28461011668729813, 0.448654280660633]
from probs:  [0.06599211190775761, 0.0692842345013285, 0.06419847270160518, 0.06726078354137761, 0.28461011668729813, 0.448654280660633]
printing an ep nov before normalisation:  27.321429361748745
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
probs:  [0.0660377136992724, 0.06864031386034128, 0.06424281703646628, 0.06730727475345234, 0.2848071472004852, 0.4489647334499824]
printing an ep nov before normalisation:  40.70779035017536
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
printing an ep nov before normalisation:  47.74178504943848
printing an ep nov before normalisation:  37.62974651999093
maxi score, test score, baseline:  -0.9956264957264958 -1.0 -0.9956264957264958
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.06598229600218707, 0.06859324209326481, 0.06418164352558171, 0.06725592824173718, 0.28545144518949295, 0.4485354449477362]
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.06598229600218707, 0.06859324209326481, 0.06418164352558171, 0.06725592824173718, 0.28545144518949295, 0.4485354449477362]
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.06598229600218707, 0.06859324209326481, 0.06418164352558171, 0.06725592824173718, 0.28545144518949295, 0.4485354449477362]
printing an ep nov before normalisation:  0.3854156188950242
actor:  1 policy actor:  1  step number:  58 total reward:  0.13999999999999935  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.34491517970759
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.06330652738846722, 0.06581137801483809, 0.061579044197866604, 0.06452840574279448, 0.31445996722151537, 0.43031467743451823]
printing an ep nov before normalisation:  38.886911732845135
printing an ep nov before normalisation:  40.713539944610865
using explorer policy with actor:  1
printing an ep nov before normalisation:  65.45453071594238
printing an ep nov before normalisation:  16.138629913330078
Printing some Q and Qe and total Qs values:  [[0.693]
 [0.82 ]
 [0.703]
 [0.679]
 [0.66 ]
 [0.66 ]
 [0.66 ]] [[71.261]
 [68.915]
 [71.002]
 [69.305]
 [65.791]
 [65.791]
 [65.791]] [[1.273]
 [1.364]
 [1.278]
 [1.23 ]
 [1.159]
 [1.159]
 [1.159]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.06344018457769968, 0.06596044844635084, 0.061702071564836806, 0.06404747707616985, 0.3136747999866003, 0.4311750183483425]
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.06347868457442452, 0.06600048113638494, 0.0617395145316932, 0.06347868457442452, 0.3138654796521097, 0.4314371555309631]
printing an ep nov before normalisation:  64.1743803024292
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
siam score:  -0.8350148
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.562]
 [0.555]
 [0.562]
 [0.562]
 [0.562]
 [0.562]] [[28.183]
 [28.872]
 [23.9  ]
 [28.183]
 [28.183]
 [28.183]
 [28.183]] [[2.304]
 [2.366]
 [1.906]
 [2.304]
 [2.304]
 [2.304]
 [2.304]]
printing an ep nov before normalisation:  20.086590781823933
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.06367453494112447, 0.06554830783593289, 0.06192998776319938, 0.06367453494112447, 0.3124019811250749, 0.43277065339354404]
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.06367453494112447, 0.06554830783593289, 0.06192998776319938, 0.06367453494112447, 0.3124019811250749, 0.43277065339354404]
printing an ep nov before normalisation:  45.36911964416504
siam score:  -0.83566016
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
probs:  [0.06367453494112447, 0.06554830783593289, 0.06192998776319938, 0.06367453494112447, 0.3124019811250749, 0.43277065339354404]
maxi score, test score, baseline:  -0.9956446808510638 -1.0 -0.9956446808510638
using explorer policy with actor:  0
printing an ep nov before normalisation:  34.03172443595899
deleting a thread, now have 2 threads
Frames:  28607 train batches done:  3350 episodes:  906
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
probs:  [0.0635539629331714, 0.06543923880072357, 0.061798706090967666, 0.0635539629331714, 0.3138045295680058, 0.4318495996739601]
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
printing an ep nov before normalisation:  70.4094244355314
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
probs:  [0.06370724953354748, 0.06559708207366306, 0.06194775027206058, 0.06370724953354748, 0.31214761941836217, 0.4328930491688192]
maxi score, test score, baseline:  -0.9956805907172996 -1.0 -0.9956805907172996
probs:  [0.06385882817014171, 0.06575316661105274, 0.062095133759638335, 0.06385882817014171, 0.3105091710424055, 0.43392487224662]
printing an ep nov before normalisation:  70.06393315393315
siam score:  -0.8380621
from probs:  [0.06385882159668596, 0.06575317568687183, 0.06209511261616807, 0.06385882159668596, 0.31050934626406695, 0.4339247222395211]
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
probs:  [0.06379928849716214, 0.0656994236896292, 0.06203019711107212, 0.06379928849716214, 0.3112025298388945, 0.43346927236607996]
printing an ep nov before normalisation:  29.23465733921777
maxi score, test score, baseline:  -0.99571589958159 -1.0 -0.99571589958159
probs:  [0.06379928191983758, 0.06569943268142722, 0.062030176038357594, 0.06379928191983758, 0.31120270457620997, 0.4334691228643301]
maxi score, test score, baseline:  -0.99571589958159 -1.0 -0.99571589958159
probs:  [0.06379928191983758, 0.06569943268142722, 0.062030176038357594, 0.06379928191983758, 0.31120270457620997, 0.4334691228643301]
printing an ep nov before normalisation:  22.444521206758473
maxi score, test score, baseline:  -0.99571589958159 -1.0 -0.99571589958159
probs:  [0.06379928191983758, 0.06569943268142722, 0.062030176038357594, 0.06379928191983758, 0.31120270457620997, 0.4334691228643301]
using explorer policy with actor:  1
siam score:  -0.83767027
printing an ep nov before normalisation:  35.72290194901511
actor:  1 policy actor:  1  step number:  46 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.06040346616841426, 0.06220228161680785, 0.058728706957840895, 0.06040346616841426, 0.34790629530865474, 0.410355783779868]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
probs:  [0.06040346116348152, 0.06220229113915603, 0.05872868842750871, 0.06040346116348152, 0.3479064458929537, 0.41035565221341863]
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
probs:  [0.060337408649297514, 0.062141412872593005, 0.05865781851036723, 0.060337408649297514, 0.34866738184064855, 0.4098585694777961]
printing an ep nov before normalisation:  23.40307258769318
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
maxi score, test score, baseline:  -0.995750622406639 -1.0 -0.995750622406639
probs:  [0.060337408649297514, 0.062141412872593005, 0.05865781851036723, 0.060337408649297514, 0.34866738184064855, 0.4098585694777961]
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.060337403627860664, 0.062141422302920316, 0.05865780003383961, 0.060337403627860664, 0.34866753211012286, 0.4098584382973958]
maxi score, test score, baseline:  -0.9957677685950413 -1.0 -0.9957677685950413
probs:  [0.060337403627860664, 0.062141422302920316, 0.05865780003383961, 0.060337403627860664, 0.34866753211012286, 0.4098584382973958]
actions average: 
K:  1  action  0 :  tensor([0.4441, 0.0192, 0.1014, 0.1075, 0.1250, 0.1081, 0.0947],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0347, 0.8397, 0.0270, 0.0276, 0.0225, 0.0158, 0.0328],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2438, 0.0153, 0.1358, 0.1534, 0.1748, 0.1546, 0.1223],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2322, 0.0365, 0.1159, 0.2190, 0.1576, 0.1147, 0.1241],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2529, 0.0147, 0.1251, 0.1474, 0.1678, 0.1415, 0.1508],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2145, 0.0068, 0.1528, 0.1411, 0.1715, 0.1788, 0.1346],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2274, 0.0822, 0.0911, 0.1153, 0.1284, 0.1008, 0.2547],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.06033739864795523, 0.06214143165551091, 0.05865778170988615, 0.06033739864795523, 0.3486676811384543, 0.40985830820023816]
printing an ep nov before normalisation:  25.12796314056978
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
printing an ep nov before normalisation:  19.86744245684222
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.060496464678333695, 0.06230527853975485, 0.05881239660045883, 0.060496464678333695, 0.3469486443321048, 0.4109407511710141]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.06058850003144331, 0.06240725266875525, 0.058895178610497714, 0.06058850003144331, 0.3460012006234094, 0.411519368034451]
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.06058850003144331, 0.06240725266875525, 0.058895178610497714, 0.06058850003144331, 0.3460012006234094, 0.411519368034451]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.060523200847023104, 0.0623471673865455, 0.05882502510332985, 0.060523200847023104, 0.3467541072136388, 0.41102729860243953]
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.06055823775239828, 0.06238326251630328, 0.058859076765314315, 0.05997852400386375, 0.3469552088562836, 0.41126569010583675]
printing an ep nov before normalisation:  64.85451763592933
actions average: 
K:  1  action  0 :  tensor([0.4410, 0.0300, 0.1014, 0.1070, 0.1178, 0.1072, 0.0956],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0104, 0.9301, 0.0390, 0.0065, 0.0031, 0.0045, 0.0065],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2251, 0.0667, 0.1467, 0.1415, 0.1374, 0.1527, 0.1300],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1679, 0.1198, 0.1328, 0.1836, 0.1160, 0.1355, 0.1444],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2280, 0.0396, 0.1318, 0.1304, 0.2005, 0.1367, 0.1330],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.3176, 0.0262, 0.1277, 0.1306, 0.1317, 0.1339, 0.1323],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2561, 0.0025, 0.1453, 0.1468, 0.1509, 0.1520, 0.1465],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.06055823775239828, 0.06238326251630328, 0.058859076765314315, 0.05997852400386375, 0.3469552088562836, 0.41126569010583675]
printing an ep nov before normalisation:  63.403177221371685
printing an ep nov before normalisation:  60.09683232464309
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
Printing some Q and Qe and total Qs values:  [[0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]] [[61.955]
 [61.955]
 [61.955]
 [61.955]
 [61.955]
 [61.955]
 [61.955]] [[2.276]
 [2.276]
 [2.276]
 [2.276]
 [2.276]
 [2.276]
 [2.276]]
actions average: 
K:  0  action  0 :  tensor([0.4590, 0.0025, 0.1048, 0.1084, 0.1306, 0.0946, 0.1002],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0212, 0.8786, 0.0225, 0.0235, 0.0131, 0.0100, 0.0312],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.3070, 0.0047, 0.1463, 0.1389, 0.1519, 0.1403, 0.1110],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2647, 0.0298, 0.1266, 0.1730, 0.1504, 0.1228, 0.1328],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2132, 0.0160, 0.1241, 0.1444, 0.2703, 0.1065, 0.1255],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2058, 0.0019, 0.1487, 0.1483, 0.1679, 0.2155, 0.1119],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.3174, 0.0425, 0.1173, 0.1354, 0.1426, 0.1136, 0.1312],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  24.38178888382943
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  69 total reward:  0.05333333333333279  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  56.86958480524895
printing an ep nov before normalisation:  67.7139707108213
printing an ep nov before normalisation:  28.255595572117162
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.05820021673084965, 0.06054277719015554, 0.057109714448069324, 0.05820021673084965, 0.3669293613775084, 0.39901771352256743]
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]] [[35.469]
 [34.771]
 [34.771]
 [34.771]
 [34.771]
 [34.771]
 [34.771]] [[2.213]
 [2.14 ]
 [2.14 ]
 [2.14 ]
 [2.14 ]
 [2.14 ]
 [2.14 ]]
printing an ep nov before normalisation:  42.94850682430763
printing an ep nov before normalisation:  35.80982701583153
printing an ep nov before normalisation:  17.16562189534347
printing an ep nov before normalisation:  15.824038281058606
printing an ep nov before normalisation:  44.82266202060642
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.05835744458239922, 0.060706347987125146, 0.057263989549164736, 0.05835744458239922, 0.36521700119410727, 0.40009777210480435]
line 256 mcts: sample exp_bonus 13.427264631410209
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.05835744458239922, 0.060706347987125146, 0.057263989549164736, 0.05835744458239922, 0.36521700119410727, 0.40009777210480435]
printing an ep nov before normalisation:  38.82283016136453
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.257]
 [0.017]
 [0.072]
 [0.072]
 [0.017]
 [0.017]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.017]
 [0.257]
 [0.017]
 [0.072]
 [0.072]
 [0.017]
 [0.017]]
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
probs:  [0.058357424758963466, 0.06070634611232126, 0.057263961370331386, 0.058357424758963466, 0.36521726993541315, 0.4000975730640074]
printing an ep nov before normalisation:  0.30254797059967586
actor:  1 policy actor:  1  step number:  56 total reward:  0.3800000000000001  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9958349593495935 -1.0 -0.9958349593495935
probs:  [0.059593455018270305, 0.06182630087797551, 0.05855402677323512, 0.059593455018270305, 0.3512874191740154, 0.40914534313823336]
actions average: 
K:  3  action  0 :  tensor([0.5138, 0.0207, 0.0806, 0.0953, 0.1018, 0.0979, 0.0899],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0636, 0.7449, 0.0336, 0.0556, 0.0295, 0.0211, 0.0518],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2237, 0.0095, 0.2298, 0.1410, 0.1529, 0.1435, 0.0996],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2942, 0.0272, 0.1214, 0.1670, 0.1343, 0.1340, 0.1218],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2308, 0.0259, 0.1128, 0.1847, 0.1828, 0.1352, 0.1278],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1465, 0.0161, 0.1566, 0.1661, 0.1390, 0.2700, 0.1057],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2619, 0.0112, 0.1319, 0.1528, 0.1637, 0.1476, 0.1308],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9958349593495935 -1.0 -0.9958349593495935
probs:  [0.059593455018270305, 0.06182630087797551, 0.05855402677323512, 0.059593455018270305, 0.3512874191740154, 0.40914534313823336]
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.350561899253144
siam score:  -0.8428448
printing an ep nov before normalisation:  48.68291883069245
actor:  1 policy actor:  1  step number:  48 total reward:  0.39333333333333276  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9958349593495935 -1.0 -0.9958349593495935
maxi score, test score, baseline:  -0.9958349593495935 -1.0 -0.9958349593495935
probs:  [0.06087397939759519, 0.06300547399547174, 0.05988173191237679, 0.06087397939759519, 0.3369094738867397, 0.4184553614102214]
printing an ep nov before normalisation:  39.87485640719361
maxi score, test score, baseline:  -0.9958349593495935 -1.0 -0.9958349593495935
probs:  [0.060907576121710945, 0.06248757513340936, 0.05991477980784682, 0.060907576121710945, 0.337095750450182, 0.41868674236514]
maxi score, test score, baseline:  -0.9958349593495935 -1.0 -0.9958349593495935
probs:  [0.060907576121710945, 0.06248757513340936, 0.05991477980784682, 0.060907576121710945, 0.337095750450182, 0.41868674236514]
actor:  1 policy actor:  1  step number:  53 total reward:  0.31999999999999984  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  1  action  0 :  tensor([0.5656, 0.0198, 0.0775, 0.0622, 0.1232, 0.0813, 0.0703],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0174, 0.8916, 0.0218, 0.0149, 0.0128, 0.0230, 0.0186],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2563, 0.0174, 0.2643, 0.0966, 0.1221, 0.1300, 0.1133],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.3567, 0.0045, 0.1264, 0.1074, 0.1499, 0.1511, 0.1041],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2695, 0.0264, 0.1157, 0.1040, 0.2432, 0.1360, 0.1052],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2218, 0.0310, 0.1372, 0.1378, 0.1527, 0.1822, 0.1372],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2356, 0.0018, 0.1356, 0.1238, 0.1509, 0.1458, 0.2065],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  52.23666154760469
printing an ep nov before normalisation:  40.75126170586457
printing an ep nov before normalisation:  62.55205610217966
printing an ep nov before normalisation:  56.28740077505906
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9958349593495935 -1.0 -0.9958349593495935
probs:  [0.0587143330662396, 0.05971593096459795, 0.05729729967594852, 0.05823100384784574, 0.36567973433770845, 0.40036169810765976]
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.058714317469007515, 0.05971592309367135, 0.057297273147750155, 0.05823098452222206, 0.3656799909276926, 0.4003615108396562]
actions average: 
K:  1  action  0 :  tensor([0.5625, 0.0091, 0.0704, 0.0831, 0.1234, 0.0772, 0.0743],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0321, 0.8204, 0.0223, 0.0380, 0.0230, 0.0338, 0.0304],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2600, 0.0094, 0.1749, 0.1198, 0.1503, 0.1566, 0.1290],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2325, 0.0655, 0.1167, 0.1984, 0.1220, 0.1234, 0.1414],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2479, 0.0115, 0.1264, 0.1254, 0.2217, 0.1455, 0.1216],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2292, 0.0205, 0.1336, 0.1206, 0.1537, 0.2224, 0.1199],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2592, 0.0316, 0.1106, 0.1396, 0.1729, 0.1291, 0.1569],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  37.73776561806137
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.171]
 [0.17 ]] [[17.738]
 [17.738]
 [17.738]
 [17.738]
 [17.738]
 [17.738]
 [17.926]] [[1.266]
 [1.266]
 [1.266]
 [1.266]
 [1.266]
 [1.266]
 [1.277]]
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.058400085980089576, 0.05888669468086475, 0.057460046444501166, 0.058400085980089576, 0.36535269940810144, 0.40150038750635353]
from probs:  [0.058400085980089576, 0.05888669468086475, 0.057460046444501166, 0.058400085980089576, 0.36535269940810144, 0.40150038750635353]
Printing some Q and Qe and total Qs values:  [[0.213]
 [0.222]
 [0.218]
 [0.212]
 [0.217]
 [0.216]
 [0.216]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.213]
 [0.222]
 [0.218]
 [0.212]
 [0.217]
 [0.216]
 [0.216]]
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.058548722719120046, 0.059036572741830634, 0.05760628517524733, 0.058548722719120046, 0.3637354354180228, 0.4025242612266591]
siam score:  -0.8364595
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.058548722719120046, 0.059036572741830634, 0.05760628517524733, 0.058548722719120046, 0.3637354354180228, 0.4025242612266591]
Starting evaluation
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.058548722719120046, 0.059036572741830634, 0.05760628517524733, 0.058548722719120046, 0.3637354354180228, 0.4025242612266591]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.495]
 [0.469]
 [0.459]
 [0.452]
 [0.452]
 [0.452]] [[49.27 ]
 [56.481]
 [46.862]
 [43.136]
 [38.445]
 [38.445]
 [38.445]] [[0.492]
 [0.495]
 [0.469]
 [0.459]
 [0.452]
 [0.452]
 [0.452]]
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.508]
 [0.45 ]
 [0.441]
 [0.436]
 [0.43 ]
 [0.421]] [[53.657]
 [58.766]
 [55.479]
 [59.202]
 [52.543]
 [48.346]
 [49.682]] [[0.475]
 [0.508]
 [0.45 ]
 [0.441]
 [0.436]
 [0.43 ]
 [0.421]]
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.058548722719120046, 0.059036572741830634, 0.05760628517524733, 0.058548722719120046, 0.3637354354180228, 0.4025242612266591]
printing an ep nov before normalisation:  59.912028699297515
using explorer policy with actor:  0
printing an ep nov before normalisation:  45.07912635803223
printing an ep nov before normalisation:  40.03247304134731
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.531]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]] [[73.85 ]
 [73.409]
 [73.85 ]
 [73.85 ]
 [73.85 ]
 [73.85 ]
 [73.85 ]] [[0.49 ]
 [0.531]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  46.13167045965349
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.058548704521823584, 0.05903655828767941, 0.05760625974687482, 0.058548704521823584, 0.36373569126495153, 0.40252408165684705]
line 256 mcts: sample exp_bonus 37.52237604261469
printing an ep nov before normalisation:  49.57329042602243
printing an ep nov before normalisation:  64.47968123168403
printing an ep nov before normalisation:  43.232032893546226
printing an ep nov before normalisation:  51.29953593341414
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.05854868647116538, 0.059036543950021406, 0.057606234523375335, 0.05854868647116538, 0.36373594505047313, 0.40252390353379947]
printing an ep nov before normalisation:  47.40915347612054
printing an ep nov before normalisation:  71.63254945550676
printing an ep nov before normalisation:  40.456474285662125
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.05854863318150739, 0.05903650162209179, 0.05760616005765116, 0.05854863318150739, 0.3637366942847562, 0.40252337767248614]
printing an ep nov before normalisation:  47.109896822906826
printing an ep nov before normalisation:  50.464816093444824
printing an ep nov before normalisation:  57.22007749828407
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.05854859835663708, 0.05903647396081222, 0.057606111394026, 0.05854859835663708, 0.36373718391173443, 0.4025230340201532]
printing an ep nov before normalisation:  25.22987900541494
actor:  0 policy actor:  1  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]] [[14.462]
 [14.462]
 [14.462]
 [14.462]
 [14.462]
 [14.462]
 [14.462]] [[0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]]
maxi score, test score, baseline:  -0.9895382428940568 -1.0 -0.9895382428940568
probs:  [0.058555796221054404, 0.0590421926288173, 0.05761616679696698, 0.058555796221054404, 0.363636005411342, 0.4025940427207648]
printing an ep nov before normalisation:  10.962878465652466
actor:  0 policy actor:  1  step number:  33 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.983731417624521 -1.0 -0.983731417624521
probs:  [0.0585623668611044, 0.059047415473388674, 0.05762534113282794, 0.0585623668611044, 0.36354368091074435, 0.40265882876083026]
printing an ep nov before normalisation:  41.40176296234131
maxi score, test score, baseline:  -0.9837931297709923 -1.0 -0.9837931297709923
probs:  [0.05856229670355368, 0.05904735969422352, 0.05762524319885059, 0.05856229670355368, 0.3635446665114592, 0.4026581371883592]
maxi score, test score, baseline:  -0.9837931297709923 -1.0 -0.9837931297709923
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]] [[13.455]
 [13.123]
 [13.123]
 [13.123]
 [13.123]
 [13.123]
 [13.123]] [[0.535]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]]
printing an ep nov before normalisation:  9.44568196365907
printing an ep nov before normalisation:  14.275511148852118
maxi score, test score, baseline:  -0.9839151515151514 -1.0 -0.9839151515151514
probs:  [0.05856215800377436, 0.05904724942096475, 0.05762504958420202, 0.05856215800377436, 0.3635466150321293, 0.4026567699551551]
maxi score, test score, baseline:  -0.9842122676579925 -0.8390000000000001 -0.8390000000000001
probs:  [0.05882814259137276, 0.05927821317847803, 0.05795868804810121, 0.05882814259137276, 0.3600994944135389, 0.40500731917713634]
maxi score, test score, baseline:  -0.9842122676579925 -0.8390000000000001 -0.8390000000000001
probs:  [0.05882814259137276, 0.05927821317847803, 0.05795868804810121, 0.05882814259137276, 0.3600994944135389, 0.40500731917713634]
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.455]
 [0.34 ]
 [0.337]
 [0.333]
 [0.325]
 [0.299]] [[39.771]
 [32.526]
 [36.692]
 [36.142]
 [35.61 ]
 [36.171]
 [35.961]] [[0.854]
 [0.807]
 [0.792]
 [0.776]
 [0.759]
 [0.765]
 [0.734]]
maxi score, test score, baseline:  -0.9842703703703702 -0.8390000000000001 -0.8390000000000001
printing an ep nov before normalisation:  67.3790104855727
maxi score, test score, baseline:  -0.9842703703703702 -0.8390000000000001 -0.8390000000000001
probs:  [0.058854592493935536, 0.058854592493935536, 0.057984746145213324, 0.058854592493935536, 0.36026170731680945, 0.4051897690561707]
maxi score, test score, baseline:  -0.9842703703703702 -0.8390000000000001 -0.8390000000000001
probs:  [0.058854592493935536, 0.058854592493935536, 0.057984746145213324, 0.058854592493935536, 0.36026170731680945, 0.4051897690561707]
maxi score, test score, baseline:  -0.9842703703703702 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9842703703703702 -0.8390000000000001 -0.8390000000000001
probs:  [0.058854592493935536, 0.058854592493935536, 0.057984746145213324, 0.058854592493935536, 0.36026170731680945, 0.4051897690561707]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.9842703703703702 -0.8390000000000001 -0.8390000000000001
probs:  [0.058788921775017336, 0.058788921775017336, 0.05791674500473528, 0.058788921775017336, 0.36100354201503493, 0.40471294765517773]
printing an ep nov before normalisation:  54.924467018528055
printing an ep nov before normalisation:  60.84703036654589
maxi score, test score, baseline:  -0.9843280442804427 -0.8390000000000001 -0.8390000000000001
probs:  [0.058788921775017336, 0.058788921775017336, 0.05791674500473528, 0.058788921775017336, 0.36100354201503493, 0.40471294765517773]
maxi score, test score, baseline:  -0.9843280442804427 -0.8390000000000001 -0.8390000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9843280442804427 -0.8390000000000001 -0.8390000000000001
probs:  [0.05865789599811144, 0.05865789599811144, 0.05778106958639971, 0.05865789599811144, 0.36248364561785745, 0.40376159680140855]
maxi score, test score, baseline:  -0.9843280442804427 -0.8390000000000001 -0.8390000000000001
probs:  [0.05865789599811144, 0.05865789599811144, 0.05778106958639971, 0.05865789599811144, 0.36248364561785745, 0.40376159680140855]
using explorer policy with actor:  0
printing an ep nov before normalisation:  42.79853657656577
actions average: 
K:  2  action  0 :  tensor([0.4288, 0.0297, 0.1065, 0.1018, 0.1267, 0.1187, 0.0878],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0454, 0.7454, 0.0541, 0.0392, 0.0287, 0.0389, 0.0484],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2303, 0.1351, 0.1277, 0.1231, 0.1377, 0.1418, 0.1044],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2085, 0.1381, 0.1205, 0.1641, 0.1208, 0.1571, 0.0910],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2868, 0.0365, 0.1200, 0.0997, 0.2350, 0.1278, 0.0942],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2823, 0.0597, 0.1584, 0.1128, 0.1328, 0.1494, 0.1047],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2464, 0.0500, 0.1377, 0.1289, 0.1512, 0.1639, 0.1218],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.984385294117647 -0.8390000000000001 -0.8390000000000001
probs:  [0.05859254043476375, 0.05859254043476375, 0.05771339478524876, 0.05859254043476375, 0.3632219202311413, 0.4032870636793187]
printing an ep nov before normalisation:  44.6894723794804
Printing some Q and Qe and total Qs values:  [[ 0.029]
 [ 0.511]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[20.653]
 [18.413]
 [20.855]
 [20.855]
 [20.855]
 [20.855]
 [20.855]] [[1.411]
 [1.735]
 [1.379]
 [1.379]
 [1.379]
 [1.379]
 [1.379]]
maxi score, test score, baseline:  -0.9844421245421244 -0.8390000000000001 -0.8390000000000001
probs:  [0.058173741980316156, 0.058618565304243635, 0.05773902827738703, 0.058618565304243635, 0.3633835597589108, 0.4034665393748986]
maxi score, test score, baseline:  -0.9844421245421244 -0.8390000000000001 -0.8390000000000001
probs:  [0.058173741980316156, 0.058618565304243635, 0.05773902827738703, 0.058618565304243635, 0.3633835597589108, 0.4034665393748986]
actor:  1 policy actor:  1  step number:  58 total reward:  0.1666666666666663  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  32.17395308348409
Printing some Q and Qe and total Qs values:  [[0.226]
 [0.226]
 [0.226]
 [0.226]
 [0.226]
 [0.226]
 [0.226]] [[33.441]
 [33.441]
 [33.441]
 [33.441]
 [33.441]
 [33.441]
 [33.441]] [[1.703]
 [1.703]
 [1.703]
 [1.703]
 [1.703]
 [1.703]
 [1.703]]
maxi score, test score, baseline:  -0.9844421245421244 -0.8390000000000001 -0.8390000000000001
probs:  [0.05652308006381394, 0.056955252691628704, 0.056100729541176765, 0.056955252691628704, 0.3814698732171251, 0.39199581179462684]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  83.22748000066268
maxi score, test score, baseline:  -0.9844421245421244 -0.8390000000000001 -0.8390000000000001
probs:  [0.05666838879423539, 0.05710167507020107, 0.05624494993363257, 0.05710167507020107, 0.3798777245037249, 0.39300558662800494]
printing an ep nov before normalisation:  42.05592632293701
printing an ep nov before normalisation:  38.10475826263428
printing an ep nov before normalisation:  20.63880868352669
printing an ep nov before normalisation:  28.12438503320276
printing an ep nov before normalisation:  58.13225439166603
maxi score, test score, baseline:  -0.9844421245421244 -0.8390000000000001 -0.8390000000000001
probs:  [0.0567440604591253, 0.057179568038077215, 0.05631845077969503, 0.057179568038077215, 0.37905882519741035, 0.39351952748761493]
actor:  1 policy actor:  1  step number:  67 total reward:  0.02666666666666606  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  57.47723047107273
maxi score, test score, baseline:  -0.9844421245421244 -0.8390000000000001 -0.8390000000000001
probs:  [0.05756343113557231, 0.05798997711139227, 0.056739095092414635, 0.05798997711139227, 0.3732458038220294, 0.3964717157271991]
maxi score, test score, baseline:  -0.9844421245421244 -0.8390000000000001 -0.8390000000000001
probs:  [0.05756343113557231, 0.05798997711139227, 0.056739095092414635, 0.05798997711139227, 0.3732458038220294, 0.3964717157271991]
printing an ep nov before normalisation:  36.0720978188887
maxi score, test score, baseline:  -0.9844421245421244 -0.8390000000000001 -0.8390000000000001
probs:  [0.05756343113557231, 0.05798997711139227, 0.056739095092414635, 0.05798997711139227, 0.3732458038220294, 0.3964717157271991]
siam score:  -0.8292849
printing an ep nov before normalisation:  35.9237611945896
siam score:  -0.8311934
printing an ep nov before normalisation:  40.9331143122604
maxi score, test score, baseline:  -0.9844421245421244 -0.8390000000000001 -0.8390000000000001
probs:  [0.057705407845690455, 0.05813300831182629, 0.056879033911135815, 0.05813300831182629, 0.37169801334660996, 0.3974515282729112]
maxi score, test score, baseline:  -0.9844985401459854 -0.8390000000000001 -0.8390000000000001
probs:  [0.057705407845690455, 0.05813300831182629, 0.056879033911135815, 0.05813300831182629, 0.37169801334660996, 0.3974515282729112]
printing an ep nov before normalisation:  35.91701588597343
printing an ep nov before normalisation:  46.35530923234793
actor:  1 policy actor:  1  step number:  59 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  1.667
from probs:  [0.057705407845690455, 0.05813300831182629, 0.056879033911135815, 0.05813300831182629, 0.37169801334660996, 0.3974515282729112]
maxi score, test score, baseline:  -0.9844985401459854 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9844985401459854 -0.8390000000000001 -0.8390000000000001
probs:  [0.05661175186644359, 0.05702318094717418, 0.05581663049694177, 0.05702318094717418, 0.3900162128781236, 0.3835090428641426]
maxi score, test score, baseline:  -0.9844985401459854 -0.8390000000000001 -0.8390000000000001
probs:  [0.05661175186644359, 0.05702318094717418, 0.05581663049694177, 0.05702318094717418, 0.3900162128781236, 0.3835090428641426]
printing an ep nov before normalisation:  47.563383676911755
maxi score, test score, baseline:  -0.9845545454545453 -0.8390000000000001 -0.8390000000000001
probs:  [0.05661175186644359, 0.05702318094717418, 0.05581663049694177, 0.05702318094717418, 0.3900162128781236, 0.3835090428641426]
printing an ep nov before normalisation:  50.60216103794888
maxi score, test score, baseline:  -0.9845545454545453 -0.8390000000000001 -0.8390000000000001
probs:  [0.05661175186644359, 0.05702318094717418, 0.05581663049694177, 0.05702318094717418, 0.3900162128781236, 0.3835090428641426]
maxi score, test score, baseline:  -0.9845545454545453 -0.8390000000000001 -0.8390000000000001
probs:  [0.05661175186644359, 0.05702318094717418, 0.05581663049694177, 0.05702318094717418, 0.3900162128781236, 0.3835090428641426]
maxi score, test score, baseline:  -0.9845545454545453 -0.8390000000000001 -0.8390000000000001
probs:  [0.05661175186644359, 0.05702318094717418, 0.05581663049694177, 0.05702318094717418, 0.3900162128781236, 0.3835090428641426]
maxi score, test score, baseline:  -0.9845545454545453 -0.8390000000000001 -0.8390000000000001
probs:  [0.05661175186644359, 0.05702318094717418, 0.05581663049694177, 0.05702318094717418, 0.3900162128781236, 0.3835090428641426]
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.477]
 [0.477]
 [0.474]
 [0.477]
 [0.477]
 [0.477]] [[15.793]
 [15.793]
 [15.793]
 [18.206]
 [15.793]
 [15.793]
 [15.793]] [[0.477]
 [0.477]
 [0.477]
 [0.474]
 [0.477]
 [0.477]
 [0.477]]
maxi score, test score, baseline:  -0.9845545454545453 -0.8390000000000001 -0.8390000000000001
probs:  [0.0566350169432253, 0.0566350169432253, 0.05583956804109627, 0.0570466155030479, 0.390176816533933, 0.38366696603547235]
maxi score, test score, baseline:  -0.9846101449275362 -0.8390000000000001 -0.8390000000000001
probs:  [0.0566350169432253, 0.0566350169432253, 0.05583956804109627, 0.0570466155030479, 0.390176816533933, 0.38366696603547235]
maxi score, test score, baseline:  -0.9846101449275362 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9846101449275362 -0.8390000000000001 -0.8390000000000001
probs:  [0.056657771794313, 0.05625536575228639, 0.05586200254266487, 0.05706953611638675, 0.3903338979958327, 0.38382142579851625]
maxi score, test score, baseline:  -0.9846101449275362 -0.8390000000000001 -0.8390000000000001
probs:  [0.056657771794313, 0.05625536575228639, 0.05586200254266487, 0.05706953611638675, 0.3903338979958327, 0.38382142579851625]
Printing some Q and Qe and total Qs values:  [[0.143]
 [0.156]
 [0.299]
 [0.193]
 [0.16 ]
 [0.142]
 [0.113]] [[51.983]
 [58.235]
 [67.84 ]
 [52.069]
 [52.811]
 [60.625]
 [64.956]] [[1.228]
 [1.376]
 [1.727]
 [1.28 ]
 [1.263]
 [1.414]
 [1.478]]
maxi score, test score, baseline:  -0.9846101449275362 -0.8390000000000001 -0.8390000000000001
probs:  [0.056657771794313, 0.05625536575228639, 0.05586200254266487, 0.05706953611638675, 0.3903338979958327, 0.38382142579851625]
maxi score, test score, baseline:  -0.9846101449275362 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9846101449275362 -0.8390000000000001 -0.8390000000000001
probs:  [0.056521049729697896, 0.056116583781688774, 0.05572120695610682, 0.05693492186254445, 0.38934679176581327, 0.3853594459041487]
siam score:  -0.8386067
siam score:  -0.838205
printing an ep nov before normalisation:  39.90891707567571
maxi score, test score, baseline:  -0.9846101449275362 -0.8390000000000001 -0.8390000000000001
probs:  [0.056521049729697896, 0.056116583781688774, 0.05572120695610682, 0.05693492186254445, 0.38934679176581327, 0.3853594459041487]
actor:  1 policy actor:  1  step number:  59 total reward:  0.1466666666666665  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  50.976433753967285
maxi score, test score, baseline:  -0.9846101449275362 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9846101449275362 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9846101449275362 -0.8390000000000001 -0.8390000000000001
probs:  [0.05806489640685119, 0.05730332004979397, 0.05730332004979397, 0.05806489640685119, 0.4004372686138758, 0.3688262984728338]
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.09 ]
 [0.09 ]
 [0.09 ]
 [0.09 ]
 [0.09 ]
 [0.09 ]] [[36.039]
 [31.415]
 [31.415]
 [31.415]
 [31.415]
 [31.415]
 [31.415]] [[1.767]
 [1.408]
 [1.408]
 [1.408]
 [1.408]
 [1.408]
 [1.408]]
printing an ep nov before normalisation:  52.47939109802246
printing an ep nov before normalisation:  52.57645823964902
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.10264252345564273
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.9846101449275362 -0.8390000000000001 -0.8390000000000001
probs:  [0.058432963246670966, 0.057672183354400997, 0.05730447307313717, 0.058432963246670966, 0.4004472787256193, 0.3677101383535006]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9846653429602887 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9846653429602887 -0.8390000000000001 -0.8390000000000001
probs:  [0.058432963246670966, 0.057672183354400997, 0.05730447307313717, 0.058432963246670966, 0.4004472787256193, 0.3677101383535006]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.9846653429602887 -0.8390000000000001 -0.8390000000000001
siam score:  -0.85008186
maxi score, test score, baseline:  -0.9846653429602887 -0.8390000000000001 -0.8390000000000001
probs:  [0.05872578217803613, 0.05796664012314741, 0.05724086694979226, 0.05872578217803613, 0.40000379555895765, 0.3673371330120306]
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.196]
 [0.196]
 [0.183]
 [0.196]
 [0.196]
 [0.18 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.186]
 [0.196]
 [0.196]
 [0.183]
 [0.196]
 [0.196]
 [0.18 ]]
maxi score, test score, baseline:  -0.9846653429602887 -0.8390000000000001 -0.8390000000000001
probs:  [0.05901101807437338, 0.05825347144233289, 0.05717890801318851, 0.05901101807437338, 0.39957179712919927, 0.36697378726653257]
maxi score, test score, baseline:  -0.984720143884892 -0.8390000000000001 -0.8390000000000001
probs:  [0.0590786563592055, 0.05832023946430833, 0.0572444415862205, 0.0590786563592055, 0.40003066861858916, 0.36624733761247097]
maxi score, test score, baseline:  -0.984720143884892 -0.8390000000000001 -0.8390000000000001
printing an ep nov before normalisation:  48.46008819330333
maxi score, test score, baseline:  -0.984720143884892 -0.8390000000000001 -0.8390000000000001
Printing some Q and Qe and total Qs values:  [[0.29]
 [0.4 ]
 [0.29]
 [0.29]
 [0.29]
 [0.29]
 [0.29]] [[61.921]
 [65.279]
 [61.921]
 [61.921]
 [61.921]
 [61.921]
 [61.921]] [[2.121]
 [2.394]
 [2.121]
 [2.121]
 [2.121]
 [2.121]
 [2.121]]
printing an ep nov before normalisation:  32.07634431218891
Printing some Q and Qe and total Qs values:  [[0.998]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]] [[38.061]
 [36.095]
 [36.095]
 [36.095]
 [36.095]
 [36.095]
 [36.095]] [[2.191]
 [1.703]
 [1.703]
 [1.703]
 [1.703]
 [1.703]
 [1.703]]
maxi score, test score, baseline:  -0.984720143884892 -0.8390000000000001 -0.8390000000000001
probs:  [0.05885183538579483, 0.05771491495533055, 0.05699816077090742, 0.05885183538579483, 0.39830387423557184, 0.3692793792666006]
maxi score, test score, baseline:  -0.984720143884892 -0.8390000000000001 -0.8390000000000001
probs:  [0.05885183538579483, 0.05771491495533055, 0.05699816077090742, 0.05885183538579483, 0.39830387423557184, 0.3692793792666006]
printing an ep nov before normalisation:  36.80825216933025
maxi score, test score, baseline:  -0.9847745519713261 -0.8390000000000001 -0.8390000000000001
printing an ep nov before normalisation:  48.91000581179611
maxi score, test score, baseline:  -0.9847745519713261 -0.8390000000000001 -0.8390000000000001
probs:  [0.05885183538579483, 0.05771491495533055, 0.05699816077090742, 0.05885183538579483, 0.39830387423557184, 0.3692793792666006]
printing an ep nov before normalisation:  31.581956624775103
maxi score, test score, baseline:  -0.9847745519713261 -0.8390000000000001 -0.8390000000000001
probs:  [0.05885183538579483, 0.05771491495533055, 0.05699816077090742, 0.05885183538579483, 0.39830387423557184, 0.3692793792666006]
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]] [[34.177]
 [34.177]
 [34.177]
 [34.177]
 [34.177]
 [34.177]
 [34.177]] [[1.964]
 [1.964]
 [1.964]
 [1.964]
 [1.964]
 [1.964]
 [1.964]]
maxi score, test score, baseline:  -0.9847745519713261 -0.8390000000000001 -0.8390000000000001
probs:  [0.05885183538579483, 0.05771491495533055, 0.05699816077090742, 0.05885183538579483, 0.39830387423557184, 0.3692793792666006]
printing an ep nov before normalisation:  28.525538913167594
maxi score, test score, baseline:  -0.9847745519713261 -0.8390000000000001 -0.8390000000000001
probs:  [0.05891962864793662, 0.05778139559194236, 0.0570638138827286, 0.05891962864793662, 0.3987635800717631, 0.36855195315769257]
maxi score, test score, baseline:  -0.9847745519713261 -0.8390000000000001 -0.8390000000000001
probs:  [0.05891962864793662, 0.05778139559194236, 0.0570638138827286, 0.05891962864793662, 0.3987635800717631, 0.36855195315769257]
maxi score, test score, baseline:  -0.9847745519713261 -0.8390000000000001 -0.8390000000000001
probs:  [0.05898712511172184, 0.05784758517686164, 0.05712917956575413, 0.05898712511172184, 0.3992212733194999, 0.36782771171444056]
siam score:  -0.8465487
maxi score, test score, baseline:  -0.9847745519713261 -0.8390000000000001 -0.8390000000000001
probs:  [0.05898712511172184, 0.05784758517686164, 0.05712917956575413, 0.05898712511172184, 0.3992212733194999, 0.36782771171444056]
Printing some Q and Qe and total Qs values:  [[0.323]
 [0.358]
 [0.304]
 [0.323]
 [0.258]
 [0.323]
 [0.323]] [[42.892]
 [54.384]
 [51.228]
 [42.892]
 [62.925]
 [42.892]
 [42.892]] [[0.876]
 [1.358]
 [1.181]
 [0.876]
 [1.59 ]
 [0.876]
 [0.876]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  45.235031424393604
printing an ep nov before normalisation:  49.83452853311803
maxi score, test score, baseline:  -0.9847745519713261 -0.8390000000000001 -0.8390000000000001
probs:  [0.05862138521755193, 0.057870032687358755, 0.05715134765847833, 0.05901001583661737, 0.3993764952323668, 0.36797072336762676]
maxi score, test score, baseline:  -0.9847745519713261 -0.8390000000000001 -0.8390000000000001
probs:  [0.05862138521755193, 0.057870032687358755, 0.05715134765847833, 0.05901001583661737, 0.3993764952323668, 0.36797072336762676]
maxi score, test score, baseline:  -0.9847745519713261 -0.8390000000000001 -0.8390000000000001
probs:  [0.05862138521755193, 0.057870032687358755, 0.05715134765847833, 0.05901001583661737, 0.3993764952323668, 0.36797072336762676]
maxi score, test score, baseline:  -0.9847745519713261 -0.8390000000000001 -0.8390000000000001
probs:  [0.05862138521755193, 0.057870032687358755, 0.05715134765847833, 0.05901001583661737, 0.3993764952323668, 0.36797072336762676]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.237]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]] [[37.977]
 [38.765]
 [35.291]
 [35.291]
 [35.291]
 [35.291]
 [35.291]] [[1.566]
 [1.691]
 [1.461]
 [1.461]
 [1.461]
 [1.461]
 [1.461]]
Printing some Q and Qe and total Qs values:  [[-0.013]
 [-0.005]
 [ 0.007]
 [ 0.026]
 [ 0.059]
 [ 0.094]
 [ 0.017]] [[34.504]
 [34.911]
 [34.024]
 [32.892]
 [32.302]
 [32.156]
 [33.922]] [[1.337]
 [1.376]
 [1.321]
 [1.256]
 [1.245]
 [1.269]
 [1.324]]
maxi score, test score, baseline:  -0.9847745519713261 -0.8390000000000001 -0.8390000000000001
probs:  [0.05868819521687052, 0.05793598442983788, 0.05721647845963275, 0.059077269761887395, 0.3998325438274136, 0.36724952830435775]
maxi score, test score, baseline:  -0.9847745519713261 -0.8390000000000001 -0.8390000000000001
probs:  [0.05875471405645669, 0.058001648752890454, 0.057281325419044495, 0.05914423059278406, 0.400286604950701, 0.36653147622812343]
maxi score, test score, baseline:  -0.9847745519713261 -0.8390000000000001 -0.8390000000000001
probs:  [0.05875471405645669, 0.058001648752890454, 0.057281325419044495, 0.05914423059278406, 0.400286604950701, 0.36653147622812343]
actor:  1 policy actor:  1  step number:  58 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9848285714285714 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9848285714285714 -0.8390000000000001 -0.8390000000000001
probs:  [0.060519671325954705, 0.05981121981084403, 0.05880567572488051, 0.06088611176480505, 0.41097528593550064, 0.34900203543801506]
printing an ep nov before normalisation:  33.56322852567477
maxi score, test score, baseline:  -0.9848285714285714 -0.8390000000000001 -0.8390000000000001
probs:  [0.060519671325954705, 0.05981121981084403, 0.05880567572488051, 0.06088611176480505, 0.41097528593550064, 0.34900203543801506]
maxi score, test score, baseline:  -0.9848285714285714 -0.8390000000000001 -0.8390000000000001
probs:  [0.060519671325954705, 0.05981121981084403, 0.05880567572488051, 0.06088611176480505, 0.41097528593550064, 0.34900203543801506]
maxi score, test score, baseline:  -0.9848285714285714 -0.8390000000000001 -0.8390000000000001
probs:  [0.060519671325954705, 0.05981121981084403, 0.05880567572488051, 0.06088611176480505, 0.41097528593550064, 0.34900203543801506]
maxi score, test score, baseline:  -0.9848285714285714 -0.8390000000000001 -0.8390000000000001
probs:  [0.060519671325954705, 0.05981121981084403, 0.05880567572488051, 0.06088611176480505, 0.41097528593550064, 0.34900203543801506]
maxi score, test score, baseline:  -0.9848285714285714 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9848822064056939 -0.8390000000000001 -0.8390000000000001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9848822064056939 -0.8390000000000001 -0.8390000000000001
probs:  [0.06046651559200835, 0.05975367689744075, 0.05874190584708675, 0.06083522526161227, 0.4105276821659448, 0.34967499423590703]
maxi score, test score, baseline:  -0.9848822064056939 -0.8390000000000001 -0.8390000000000001
probs:  [0.06046651559200835, 0.05975367689744075, 0.05874190584708675, 0.06083522526161227, 0.4105276821659448, 0.34967499423590703]
maxi score, test score, baseline:  -0.9848822064056939 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9848822064056939 -0.8390000000000001 -0.8390000000000001
probs:  [0.06046651559200835, 0.05975367689744075, 0.05874190584708675, 0.06083522526161227, 0.4105276821659448, 0.34967499423590703]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  62 total reward:  0.15333333333333266  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.9848822064056939 -0.8390000000000001 -0.8390000000000001
probs:  [0.06140956514271912, 0.06072516391200829, 0.059753755713580006, 0.06140956514271912, 0.41762137715120357, 0.33908057293776994]
maxi score, test score, baseline:  -0.9848822064056939 -0.8390000000000001 -0.8390000000000001
probs:  [0.06136016369409082, 0.06067158058857655, 0.059694236825911136, 0.06136016369409082, 0.4172035932332813, 0.33971026196404924]
printing an ep nov before normalisation:  37.45451950976999
maxi score, test score, baseline:  -0.9848822064056939 -0.8390000000000001 -0.8390000000000001
probs:  [0.06136016369409082, 0.06067158058857655, 0.059694236825911136, 0.06136016369409082, 0.4172035932332813, 0.33971026196404924]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.9848822064056939 -0.8390000000000001 -0.8390000000000001
probs:  [0.06136016369409082, 0.06067158058857655, 0.059694236825911136, 0.06136016369409082, 0.4172035932332813, 0.33971026196404924]
siam score:  -0.8499852
maxi score, test score, baseline:  -0.9848822064056939 -0.8390000000000001 -0.8390000000000001
printing an ep nov before normalisation:  42.81158447265625
maxi score, test score, baseline:  -0.9848822064056939 -0.8390000000000001 -0.8390000000000001
probs:  [0.06136016369409082, 0.06067158058857655, 0.059694236825911136, 0.06136016369409082, 0.4172035932332813, 0.33971026196404924]
printing an ep nov before normalisation:  32.166485124956836
maxi score, test score, baseline:  -0.9848822064056939 -0.8390000000000001 -0.8390000000000001
probs:  [0.0614221704408871, 0.06073288998220764, 0.05975455642795291, 0.0614221704408871, 0.4176259755686098, 0.3390422371394554]
using another actor
from probs:  [0.0614221704408871, 0.06073288998220764, 0.05975455642795291, 0.0614221704408871, 0.4176259755686098, 0.3390422371394554]
maxi score, test score, baseline:  -0.9848822064056939 -0.8390000000000001 -0.8390000000000001
probs:  [0.061483897728542784, 0.0607939230596096, 0.059814604174672166, 0.061483897728542784, 0.41804645426263254, 0.33837722304600015]
printing an ep nov before normalisation:  67.72902650792012
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.394]
 [0.296]
 [0.316]
 [0.308]
 [0.309]
 [0.305]] [[63.08 ]
 [56.318]
 [63.649]
 [61.175]
 [65.102]
 [65.646]
 [62.113]] [[0.34 ]
 [0.394]
 [0.296]
 [0.316]
 [0.308]
 [0.309]
 [0.305]]
printing an ep nov before normalisation:  44.623987429253994
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.924988347113995
using explorer policy with actor:  0
printing an ep nov before normalisation:  50.599409793231416
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([0.5066, 0.0110, 0.0866, 0.0884, 0.1290, 0.0963, 0.0822],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0243, 0.8208, 0.0240, 0.0456, 0.0234, 0.0205, 0.0414],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2129, 0.0322, 0.2385, 0.1236, 0.1423, 0.1508, 0.0998],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2420, 0.0107, 0.1251, 0.2171, 0.1373, 0.1216, 0.1461],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2036, 0.0148, 0.1629, 0.1662, 0.1569, 0.1455, 0.1501],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2721, 0.0141, 0.1297, 0.1589, 0.1706, 0.1387, 0.1159],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2624, 0.0371, 0.1282, 0.1310, 0.1590, 0.1341, 0.1482],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.546191497141805
maxi score, test score, baseline:  -0.9849883392226148 -0.8390000000000001 -0.8390000000000001
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.889]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]] [[49.005]
 [44.166]
 [49.005]
 [49.005]
 [49.005]
 [49.005]
 [49.005]] [[2.117]
 [2.292]
 [2.117]
 [2.117]
 [2.117]
 [2.117]
 [2.117]]
printing an ep nov before normalisation:  56.62620544433594
maxi score, test score, baseline:  -0.9849883392226148 -0.8390000000000001 -0.8390000000000001
probs:  [0.06128427251306505, 0.060587077681132, 0.059597510822904445, 0.06093175827220003, 0.4165242635473882, 0.34107511716331024]
UNIT TEST: sample policy line 217 mcts : [0.082 0.469 0.082 0.143 0.061 0.082 0.082]
maxi score, test score, baseline:  -0.9849883392226148 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9849883392226148 -0.8390000000000001 -0.8390000000000001
probs:  [0.06128427251306505, 0.060587077681132, 0.059597510822904445, 0.06093175827220003, 0.4165242635473882, 0.34107511716331024]
printing an ep nov before normalisation:  43.46229148633283
maxi score, test score, baseline:  -0.9849883392226148 -0.8390000000000001 -0.8390000000000001
probs:  [0.06130491030231648, 0.060270261212311174, 0.059617579349082024, 0.06095227709186523, 0.4166647908328436, 0.34119018121158146]
maxi score, test score, baseline:  -0.9849883392226148 -0.8390000000000001 -0.8390000000000001
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.387]
 [0.285]
 [0.285]
 [0.285]
 [0.285]
 [0.285]] [[57.837]
 [58.436]
 [57.837]
 [57.837]
 [57.837]
 [57.837]
 [57.837]] [[1.272]
 [1.393]
 [1.272]
 [1.272]
 [1.272]
 [1.272]
 [1.272]]
siam score:  -0.84394073
printing an ep nov before normalisation:  46.350260956203456
printing an ep nov before normalisation:  44.589207132504285
maxi score, test score, baseline:  -0.9849883392226148 -0.8390000000000001 -0.8390000000000001
probs:  [0.06119493318834068, 0.060155126404280605, 0.05949919094193447, 0.06084054211212545, 0.4158347837443525, 0.3424754236089664]
maxi score, test score, baseline:  -0.9849883392226148 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9849883392226148 -0.8390000000000001 -0.8390000000000001
probs:  [0.06121611630267499, 0.06017594879587593, 0.059519785780834236, 0.060514966353647476, 0.4159789961177799, 0.34259418664918756]
printing an ep nov before normalisation:  42.28029607225301
using explorer policy with actor:  1
siam score:  -0.84061193
maxi score, test score, baseline:  -0.9849883392226148 -0.8390000000000001 -0.8390000000000001
probs:  [0.06127828974659868, 0.06023706350147484, 0.05958023260849351, 0.06057642612951521, 0.4164022662645055, 0.34192572174941227]
maxi score, test score, baseline:  -0.9849883392226148 -0.8390000000000001 -0.8390000000000001
probs:  [0.06153180776491786, 0.06049256372219685, 0.05951965440560696, 0.0608312802990837, 0.4159797279474708, 0.34164496586072385]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9849883392226148 -0.8390000000000001 -0.8390000000000001
probs:  [0.06153180776491786, 0.06049256372219685, 0.05951965440560696, 0.0608312802990837, 0.4159797279474708, 0.34164496586072385]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9849883392226148 -0.8390000000000001 -0.8390000000000001
probs:  [0.06148610761364561, 0.06044066604318707, 0.05946195478573653, 0.06078140255504022, 0.41557469760684657, 0.34225517139554407]
maxi score, test score, baseline:  -0.9849883392226148 -0.8390000000000001 -0.8390000000000001
probs:  [0.06148610761364561, 0.06044066604318707, 0.05946195478573653, 0.06078140255504022, 0.41557469760684657, 0.34225517139554407]
maxi score, test score, baseline:  -0.9849883392226148 -0.8390000000000001 -0.8390000000000001
siam score:  -0.8408187
maxi score, test score, baseline:  -0.9849883392226148 -0.8390000000000001 -0.8390000000000001
probs:  [0.06148610761364561, 0.06044066604318707, 0.05946195478573653, 0.06078140255504022, 0.41557469760684657, 0.34225517139554407]
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.39 ]
 [0.38 ]
 [0.396]
 [0.412]
 [0.407]
 [0.404]] [[60.842]
 [60.266]
 [59.832]
 [60.304]
 [62.143]
 [61.212]
 [60.806]] [[2.2  ]
 [2.094]
 [2.06 ]
 [2.102]
 [2.217]
 [2.162]
 [2.136]]
maxi score, test score, baseline:  -0.9849883392226148 -0.8390000000000001 -0.8390000000000001
probs:  [0.06148610761364561, 0.06044066604318707, 0.05946195478573653, 0.06078140255504022, 0.41557469760684657, 0.34225517139554407]
printing an ep nov before normalisation:  11.746963738432369
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.304232597351074
actor:  1 policy actor:  1  step number:  54 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9849883392226148 -0.8390000000000001 -0.8390000000000001
probs:  [0.06078291390940889, 0.059749454627641085, 0.05878196083194357, 0.060086285800958004, 0.4108131310319619, 0.3497862537980866]
printing an ep nov before normalisation:  44.3111931744207
siam score:  -0.83778226
maxi score, test score, baseline:  -0.9849883392226148 -0.8390000000000001 -0.8390000000000001
probs:  [0.06078291390940889, 0.059749454627641085, 0.05878196083194357, 0.060086285800958004, 0.4108131310319619, 0.3497862537980866]
actions average: 
K:  1  action  0 :  tensor([0.5149, 0.0101, 0.0948, 0.0902, 0.1099, 0.0905, 0.0897],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0324, 0.8174, 0.0231, 0.0353, 0.0141, 0.0148, 0.0627],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2490, 0.0192, 0.2345, 0.1198, 0.1173, 0.1454, 0.1148],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2673, 0.0672, 0.1225, 0.1677, 0.1262, 0.1374, 0.1116],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2279, 0.0077, 0.1322, 0.1296, 0.2409, 0.1373, 0.1245],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2372, 0.0159, 0.1501, 0.1583, 0.1428, 0.1693, 0.1263],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2854, 0.0028, 0.1452, 0.1405, 0.1495, 0.1552, 0.1214],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  32.78354589904823
actor:  1 policy actor:  1  step number:  70 total reward:  0.1399999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9849883392226148 -0.8390000000000001 -0.8390000000000001
probs:  [0.061769415154651994, 0.06077733003083525, 0.059848569489389776, 0.06110067629341256, 0.41829022622687856, 0.3382137828048319]
siam score:  -0.8398178
maxi score, test score, baseline:  -0.9849883392226148 -0.8390000000000001 -0.8390000000000001
probs:  [0.06166383059291582, 0.060666895364094406, 0.059733594298814786, 0.0609918224016362, 0.4174841780834868, 0.3394596792590519]
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.229]
 [0.188]
 [0.18 ]
 [0.192]
 [0.151]
 [0.226]] [[41.835]
 [41.63 ]
 [42.988]
 [41.697]
 [42.078]
 [44.698]
 [41.754]] [[1.786]
 [1.7  ]
 [1.719]
 [1.653]
 [1.682]
 [1.759]
 [1.702]]
maxi score, test score, baseline:  -0.9849883392226148 -0.8390000000000001 -0.8390000000000001
probs:  [0.06155854783631442, 0.06055677636613062, 0.059618947755745805, 0.06088327966011645, 0.41668043396508275, 0.34070201441660997]
printing an ep nov before normalisation:  24.922404937561918
printing an ep nov before normalisation:  20.826185623024635
maxi score, test score, baseline:  -0.9850408450704224 -0.8390000000000001 -0.8390000000000001
probs:  [0.06145356559266537, 0.060446971685404124, 0.05950462845307444, 0.060775046736659635, 0.4158789840069548, 0.3419408035252417]
line 256 mcts: sample exp_bonus 53.27423637264739
maxi score, test score, baseline:  -0.9850408450704224 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9850408450704224 -0.8390000000000001 -0.8390000000000001
probs:  [0.06134888257715236, 0.0603374799780796, 0.059390634991713626, 0.060667122306666287, 0.4150798184006255, 0.34317606174576254]
line 256 mcts: sample exp_bonus 40.06174691111098
maxi score, test score, baseline:  -0.9850408450704224 -0.8390000000000001 -0.8390000000000001
siam score:  -0.8449257
printing an ep nov before normalisation:  38.17604146901835
printing an ep nov before normalisation:  38.67037296295166
printing an ep nov before normalisation:  40.36377751554016
line 256 mcts: sample exp_bonus 34.705440040666645
actor:  1 policy actor:  1  step number:  46 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9850929824561403 -0.8390000000000001 -0.8390000000000001
probs:  [0.06044671634713912, 0.059450219302169, 0.05851732845155867, 0.059775003524233344, 0.40896453960146717, 0.3528461927734327]
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.237]
 [0.073]
 [0.071]
 [0.096]
 [0.063]
 [0.073]] [[45.507]
 [44.171]
 [46.264]
 [48.189]
 [46.133]
 [47.505]
 [45.352]] [[1.646]
 [1.687]
 [1.68 ]
 [1.821]
 [1.693]
 [1.763]
 [1.612]]
maxi score, test score, baseline:  -0.9850929824561403 -0.8390000000000001 -0.8390000000000001
probs:  [0.06044671634713912, 0.059450219302169, 0.05851732845155867, 0.059775003524233344, 0.40896453960146717, 0.3528461927734327]
maxi score, test score, baseline:  -0.9850929824561403 -0.8390000000000001 -0.8390000000000001
probs:  [0.06050899880820181, 0.05951147273491947, 0.05857761853865515, 0.059836592343989276, 0.4093867174673916, 0.35217860010684265]
siam score:  -0.8476814
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9850929824561403 -0.8390000000000001 -0.8390000000000001
probs:  [0.06050899880820181, 0.05951147273491947, 0.05857761853865515, 0.059836592343989276, 0.4093867174673916, 0.35217860010684265]
actions average: 
K:  0  action  0 :  tensor([0.4648, 0.0044, 0.0977, 0.1045, 0.1227, 0.1064, 0.0995],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0250, 0.8647, 0.0224, 0.0230, 0.0120, 0.0159, 0.0370],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.3054, 0.0063, 0.1615, 0.1268, 0.1302, 0.1545, 0.1153],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2254, 0.0080, 0.1410, 0.1787, 0.1635, 0.1405, 0.1429],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2876, 0.0034, 0.1173, 0.1274, 0.2062, 0.1469, 0.1111],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.3029, 0.0034, 0.1747, 0.1138, 0.1257, 0.1808, 0.0987],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.3233, 0.0217, 0.1067, 0.1355, 0.1564, 0.1194, 0.1371],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  61 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9850929824561403 -0.8390000000000001 -0.8390000000000001
probs:  [0.05996119178201239, 0.058972716554344766, 0.0580473354901453, 0.05929488625817718, 0.4056734409316062, 0.35805042898371425]
maxi score, test score, baseline:  -0.9850929824561403 -0.8390000000000001 -0.8390000000000001
probs:  [0.05996119178201239, 0.058972716554344766, 0.0580473354901453, 0.05929488625817718, 0.4056734409316062, 0.35805042898371425]
printing an ep nov before normalisation:  38.883628694871106
UNIT TEST: sample policy line 217 mcts : [0.184 0.265 0.163 0.061 0.163 0.163 0.   ]
maxi score, test score, baseline:  -0.9851447552447552 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9851447552447552 -0.8390000000000001 -0.8390000000000001
probs:  [0.05987168472593257, 0.05856158860218625, 0.05794835211873053, 0.059202080040462235, 0.4049794061701842, 0.35943688834250415]
siam score:  -0.8448538
printing an ep nov before normalisation:  34.75517857937946
actor:  1 policy actor:  1  step number:  67 total reward:  0.1599999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9851447552447552 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9851447552447552 -0.8390000000000001 -0.8390000000000001
probs:  [0.060864352457179756, 0.059604934660815685, 0.059015419947623986, 0.06022065002792701, 0.412459310608832, 0.3478353322976215]
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]] [[32.321]
 [32.321]
 [32.321]
 [32.321]
 [32.321]
 [32.321]
 [32.321]] [[0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]]
printing an ep nov before normalisation:  24.201488403654746
maxi score, test score, baseline:  -0.9851447552447552 -0.8390000000000001 -0.8390000000000001
probs:  [0.06076002938406191, 0.059494772792120974, 0.05890252502568053, 0.06011334268151431, 0.411667892743003, 0.3490614373736192]
line 256 mcts: sample exp_bonus 29.519099300014894
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9851961672473867 -0.8390000000000001 -0.8390000000000001
probs:  [0.06065600895004451, 0.059384930500753205, 0.05878995760959557, 0.06000634663151784, 0.41087877076420876, 0.35028398554388007]
maxi score, test score, baseline:  -0.9851961672473867 -0.8390000000000001 -0.8390000000000001
probs:  [0.06065600895004451, 0.059384930500753205, 0.05878995760959557, 0.06000634663151784, 0.41087877076420876, 0.35028398554388007]
maxi score, test score, baseline:  -0.9851961672473867 -0.8390000000000001 -0.8390000000000001
probs:  [0.06055228984011023, 0.059275406398095636, 0.05867771627630156, 0.05989966052530277, 0.4100919346964374, 0.3515029922637524]
maxi score, test score, baseline:  -0.9851961672473867 -0.8390000000000001 -0.8390000000000001
printing an ep nov before normalisation:  30.060740402209422
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.252]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]]
printing an ep nov before normalisation:  60.40417880068592
maxi score, test score, baseline:  -0.9851961672473867 -0.8390000000000001 -0.8390000000000001
probs:  [0.06046801141808715, 0.05918493272955534, 0.058584342705136204, 0.05949512757733226, 0.4094372226562389, 0.3528303629136501]
printing an ep nov before normalisation:  58.23928471696658
printing an ep nov before normalisation:  64.09317809860056
maxi score, test score, baseline:  -0.9852472222222222 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9852472222222222 -0.8390000000000001 -0.8390000000000001
probs:  [0.060486344692199115, 0.058899259909150035, 0.058602103609345105, 0.05951316523182396, 0.40956159340511217, 0.35293753315236953]
printing an ep nov before normalisation:  38.3031425260896
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9852472222222222 -0.8390000000000001 -0.8390000000000001
printing an ep nov before normalisation:  32.88689638890626
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.60036963383678
maxi score, test score, baseline:  -0.9852472222222222 -0.8390000000000001 -0.8390000000000001
probs:  [0.06071972346723809, 0.059135403590988046, 0.058548371384209076, 0.0597482394112518, 0.40918683798414146, 0.35266142416217156]
printing an ep nov before normalisation:  47.360969276189024
actor:  1 policy actor:  1  step number:  62 total reward:  0.0733333333333327  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9852472222222222 -0.8390000000000001 -0.8390000000000001
probs:  [0.060424208554459576, 0.05885619492434022, 0.05799378742777458, 0.0594627232735732, 0.4053048058389862, 0.3579582799808661]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]
 [0.314]] [[51.413]
 [50.483]
 [50.483]
 [50.483]
 [50.483]
 [50.483]
 [50.483]] [[2.122]
 [1.944]
 [1.944]
 [1.944]
 [1.944]
 [1.944]
 [1.944]]
siam score:  -0.84671706
maxi score, test score, baseline:  -0.9852472222222222 -0.8390000000000001 -0.8390000000000001
probs:  [0.060424208554459576, 0.05885619492434022, 0.05799378742777458, 0.0594627232735732, 0.4053048058389862, 0.3579582799808661]
maxi score, test score, baseline:  -0.9852472222222222 -0.8390000000000001 -0.8390000000000001
printing an ep nov before normalisation:  41.367829939846146
printing an ep nov before normalisation:  46.51527813109383
maxi score, test score, baseline:  -0.9852472222222222 -0.8390000000000001 -0.8390000000000001
from probs:  [0.060446737230672974, 0.058868489132544985, 0.05800045267857458, 0.05947897630896372, 0.40535064557598005, 0.35785469907326367]
from probs:  [0.06073139929818862, 0.059154159220927184, 0.058009440649388996, 0.05976425648158216, 0.40541501887078557, 0.35692572547912743]
maxi score, test score, baseline:  -0.9852979238754325 -0.8390000000000001 -0.8390000000000001
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]] [[47.692]
 [47.387]
 [47.387]
 [47.387]
 [47.387]
 [47.387]
 [47.387]] [[2.444]
 [2.321]
 [2.321]
 [2.321]
 [2.321]
 [2.321]
 [2.321]]
actions average: 
K:  1  action  0 :  tensor([0.4912, 0.0102, 0.0830, 0.0978, 0.1082, 0.1018, 0.1078],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0160, 0.9308, 0.0200, 0.0071, 0.0065, 0.0122, 0.0075],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2045, 0.0943, 0.2140, 0.1068, 0.1096, 0.1429, 0.1279],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2722, 0.0593, 0.1189, 0.1477, 0.1429, 0.1468, 0.1121],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2515, 0.0373, 0.0995, 0.1156, 0.2649, 0.1150, 0.1163],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2121, 0.0067, 0.1512, 0.1215, 0.1307, 0.2664, 0.1114],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2359, 0.0738, 0.1145, 0.1362, 0.1300, 0.1255, 0.1842],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  27.73458480834961
maxi score, test score, baseline:  -0.9852979238754325 -0.8390000000000001 -0.8390000000000001
probs:  [0.06081110864376307, 0.058936092420455256, 0.0580855695975115, 0.0598426936712854, 0.4059481165710235, 0.3563764190959613]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9852979238754325 -0.8390000000000001 -0.8390000000000001
probs:  [0.06081110864376307, 0.058936092420455256, 0.0580855695975115, 0.0598426936712854, 0.4059481165710235, 0.3563764190959613]
maxi score, test score, baseline:  -0.9853482758620689 -0.8390000000000001 -0.8390000000000001
probs:  [0.06081110864376307, 0.058936092420455256, 0.0580855695975115, 0.0598426936712854, 0.4059481165710235, 0.3563764190959613]
printing an ep nov before normalisation:  66.79201929552426
maxi score, test score, baseline:  -0.9853482758620689 -0.8390000000000001 -0.8390000000000001
probs:  [0.06081110864376307, 0.058936092420455256, 0.0580855695975115, 0.0598426936712854, 0.4059481165710235, 0.3563764190959613]
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]] [[41.623]
 [42.745]
 [42.745]
 [42.745]
 [42.745]
 [42.745]
 [42.745]] [[0.45 ]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]]
maxi score, test score, baseline:  -0.9853482758620689 -0.8390000000000001 -0.8390000000000001
probs:  [0.06081110864376307, 0.058936092420455256, 0.0580855695975115, 0.0598426936712854, 0.4059481165710235, 0.3563764190959613]
from probs:  [0.06081110864376307, 0.058936092420455256, 0.0580855695975115, 0.0598426936712854, 0.4059481165710235, 0.3563764190959613]
printing an ep nov before normalisation:  38.2558929877453
maxi score, test score, baseline:  -0.9853982817869414 -0.8390000000000001 -0.8390000000000001
probs:  [0.06081110864376307, 0.058936092420455256, 0.0580855695975115, 0.0598426936712854, 0.4059481165710235, 0.3563764190959613]
maxi score, test score, baseline:  -0.9853982817869414 -0.8390000000000001 -0.8390000000000001
probs:  [0.06081110864376307, 0.058936092420455256, 0.0580855695975115, 0.0598426936712854, 0.4059481165710235, 0.3563764190959613]
maxi score, test score, baseline:  -0.9853982817869414 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9853982817869414 -0.8390000000000001 -0.8390000000000001
probs:  [0.06081110864376307, 0.058936092420455256, 0.0580855695975115, 0.0598426936712854, 0.4059481165710235, 0.3563764190959613]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9854479452054794 -0.8390000000000001 -0.8390000000000001
probs:  [0.06087264287906218, 0.05899572517722748, 0.058144339827941635, 0.05990324582426843, 0.4063596587695882, 0.3557243875219122]
maxi score, test score, baseline:  -0.9854479452054794 -0.8390000000000001 -0.8390000000000001
probs:  [0.0605622304639088, 0.059015194975066014, 0.05816352801522885, 0.05992301580038695, 0.40649402524603145, 0.355842005499378]
maxi score, test score, baseline:  -0.9854479452054794 -0.8390000000000001 -0.8390000000000001
probs:  [0.06062321329984699, 0.05907461659692468, 0.05822209016459426, 0.05998335356325491, 0.406904110343982, 0.35519261603139707]
maxi score, test score, baseline:  -0.9854479452054794 -0.8390000000000001 -0.8390000000000001
printing an ep nov before normalisation:  49.483912587503085
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.577]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]] [[54.883]
 [50.248]
 [54.883]
 [54.883]
 [54.883]
 [54.883]
 [54.883]] [[0.568]
 [0.577]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]]
maxi score, test score, baseline:  -0.9854479452054794 -0.8390000000000001 -0.8390000000000001
probs:  [0.06062321329984699, 0.05907461659692468, 0.05822209016459426, 0.05998335356325491, 0.406904110343982, 0.35519261603139707]
maxi score, test score, baseline:  -0.9854479452054794 -0.8390000000000001 -0.8390000000000001
probs:  [0.06052228292793279, 0.05896681389431186, 0.05811050413765869, 0.059879583634920186, 0.40612189568956586, 0.35639891971561055]
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]] [[24.794]
 [24.794]
 [24.794]
 [24.794]
 [24.794]
 [24.794]
 [24.794]] [[0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]]
maxi score, test score, baseline:  -0.9855462585034013 -0.8390000000000001 -0.8390000000000001
UNIT TEST: sample policy line 217 mcts : [0.102 0.306 0.061 0.163 0.163 0.102 0.102]
using another actor
from probs:  [0.06058317821590791, 0.05902614067400524, 0.05816896742928769, 0.059939830835956914, 0.4065312876965802, 0.3557505951482622]
from probs:  [0.060482710882570935, 0.05891880733761571, 0.05805785425204243, 0.05983652656069932, 0.40575238579080763, 0.3569517151762639]
Printing some Q and Qe and total Qs values:  [[-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]]
line 256 mcts: sample exp_bonus 31.568551182746887
maxi score, test score, baseline:  -0.9856432432432432 -0.8390000000000001 -0.8390000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9856432432432432 -0.8390000000000001 -0.8390000000000001
printing an ep nov before normalisation:  36.211488246917725
maxi score, test score, baseline:  -0.9856432432432432 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9856912457912457 -0.8390000000000001 -0.8390000000000001
probs:  [0.06062303314709587, 0.05905549327467094, 0.05819253833459371, 0.059662063051391885, 0.40669551562449147, 0.3557713565677562]
printing an ep nov before normalisation:  78.45780604797787
maxi score, test score, baseline:  -0.9856912457912457 -0.8390000000000001 -0.8390000000000001
probs:  [0.06062303314709587, 0.05905549327467094, 0.05819253833459371, 0.059662063051391885, 0.40669551562449147, 0.3557713565677562]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9856912457912457 -0.8390000000000001 -0.8390000000000001
probs:  [0.06068337485549006, 0.059114271281021515, 0.058250455498788316, 0.059721446142446295, 0.40710108252148164, 0.3551293697007722]
printing an ep nov before normalisation:  54.80171312268567
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9856912457912457 -0.8390000000000001 -0.8390000000000001
probs:  [0.06064400500065186, 0.0590664388484399, 0.05819796428629434, 0.05967688835951323, 0.4067326817403591, 0.35568202176474156]
printing an ep nov before normalisation:  31.45530593513657
maxi score, test score, baseline:  -0.9856912457912457 -0.8390000000000001 -0.8390000000000001
probs:  [0.06064400500065186, 0.0590664388484399, 0.05819796428629434, 0.05967688835951323, 0.4067326817403591, 0.35568202176474156]
maxi score, test score, baseline:  -0.9856912457912457 -0.8390000000000001 -0.8390000000000001
probs:  [0.06062299203529876, 0.058739230127875894, 0.05816309676100086, 0.05965039757222717, 0.4064876916708097, 0.3563365918327876]
maxi score, test score, baseline:  -0.9856912457912457 -0.8390000000000001 -0.8390000000000001
probs:  [0.06052431280750736, 0.05863237771425096, 0.05805374464449214, 0.05954749849305433, 0.4057211241791908, 0.35752094216150426]
maxi score, test score, baseline:  -0.9856912457912457 -0.8390000000000001 -0.8390000000000001
probs:  [0.06058443582166121, 0.058690617191482074, 0.058111408057166115, 0.05960664902890568, 0.40612491169967374, 0.3568819782011111]
maxi score, test score, baseline:  -0.9856912457912457 -0.8390000000000001 -0.8390000000000001
probs:  [0.0604861999291006, 0.05858421523855685, 0.05800250858062423, 0.05950419696387421, 0.4053615153642143, 0.35806136392362986]
printing an ep nov before normalisation:  47.402225678091256
printing an ep nov before normalisation:  54.13654994279418
maxi score, test score, baseline:  -0.9856912457912457 -0.8390000000000001 -0.8390000000000001
probs:  [0.0604861999291006, 0.05858421523855685, 0.05800250858062423, 0.05950419696387421, 0.4053615153642143, 0.35806136392362986]
maxi score, test score, baseline:  -0.9856912457912457 -0.8390000000000001 -0.8390000000000001
probs:  [0.0604861999291006, 0.05858421523855685, 0.05800250858062423, 0.05950419696387421, 0.4053615153642143, 0.35806136392362986]
maxi score, test score, baseline:  -0.9856912457912457 -0.8390000000000001 -0.8390000000000001
probs:  [0.0604861999291006, 0.05858421523855685, 0.05800250858062423, 0.05950419696387421, 0.4053615153642143, 0.35806136392362986]
printing an ep nov before normalisation:  44.26545734817782
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]]
printing an ep nov before normalisation:  41.322951521917126
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.396]
 [0.275]
 [0.275]
 [0.275]
 [0.275]
 [0.275]] [[47.56 ]
 [62.088]
 [47.56 ]
 [47.56 ]
 [47.56 ]
 [47.56 ]
 [47.56 ]] [[1.342]
 [2.099]
 [1.342]
 [1.342]
 [1.342]
 [1.342]
 [1.342]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.23333333333333262  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9857389261744965 -0.8390000000000001 -0.8390000000000001
probs:  [0.061482169442188955, 0.05966297676333456, 0.059106591373581846, 0.060542912352563046, 0.41310124731658465, 0.3461041027517469]
maxi score, test score, baseline:  -0.9857389261744965 -0.8390000000000001 -0.8390000000000001
probs:  [0.061482169442188955, 0.05966297676333456, 0.059106591373581846, 0.060542912352563046, 0.41310124731658465, 0.3461041027517469]
maxi score, test score, baseline:  -0.9857389261744965 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9857389261744965 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9857389261744965 -0.8390000000000001 -0.8390000000000001
probs:  [0.0612397825876633, 0.059738725470530364, 0.05918163210747071, 0.06061978073493448, 0.4136267290070837, 0.34559335009231734]
printing an ep nov before normalisation:  3.9701093862731796
maxi score, test score, baseline:  -0.985786287625418 -0.8390000000000001 -0.8390000000000001
printing an ep nov before normalisation:  53.94482899054207
maxi score, test score, baseline:  -0.985786287625418 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.985786287625418 -0.8390000000000001 -0.8390000000000001
probs:  [0.06129764374763372, 0.05979516529518263, 0.05923754442623172, 0.06067705482162132, 0.4140182616888639, 0.3449743300204667]
using explorer policy with actor:  0
printing an ep nov before normalisation:  43.72720023863774
siam score:  -0.8479902
maxi score, test score, baseline:  -0.985786287625418 -0.8390000000000001 -0.8390000000000001
probs:  [0.06135526503679564, 0.05985137114135302, 0.05929322495335369, 0.060734091471286736, 0.4144081712220419, 0.344357876175169]
maxi score, test score, baseline:  -0.985786287625418 -0.8390000000000001 -0.8390000000000001
probs:  [0.06135526503679564, 0.05985137114135302, 0.05929322495335369, 0.060734091471286736, 0.4144081712220419, 0.344357876175169]
printing an ep nov before normalisation:  49.03641022563854
maxi score, test score, baseline:  -0.9858333333333332 -0.8390000000000001 -0.8390000000000001
probs:  [0.06135526503679564, 0.05985137114135302, 0.05929322495335369, 0.060734091471286736, 0.4144081712220419, 0.344357876175169]
Printing some Q and Qe and total Qs values:  [[0.09 ]
 [0.069]
 [0.086]
 [0.087]
 [0.086]
 [0.085]
 [0.087]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.09 ]
 [0.069]
 [0.086]
 [0.087]
 [0.086]
 [0.085]
 [0.087]]
maxi score, test score, baseline:  -0.9858800664451827 -0.8390000000000001 -0.8390000000000001
probs:  [0.06137368667596095, 0.05986934026061764, 0.05931102612708816, 0.06045166790526666, 0.4145328260645798, 0.3444614529664868]
maxi score, test score, baseline:  -0.9858800664451827 -0.8390000000000001 -0.8390000000000001
probs:  [0.06137368667596095, 0.05986934026061764, 0.05931102612708816, 0.06045166790526666, 0.4145328260645798, 0.3444614529664868]
printing an ep nov before normalisation:  58.18789602151414
maxi score, test score, baseline:  -0.9858800664451827 -0.8390000000000001 -0.8390000000000001
probs:  [0.06137368667596095, 0.05986934026061764, 0.05931102612708816, 0.06045166790526666, 0.4145328260645798, 0.3444614529664868]
Printing some Q and Qe and total Qs values:  [[0.175]
 [0.206]
 [0.063]
 [0.051]
 [0.023]
 [0.013]
 [0.015]] [[ 8.063]
 [ 7.506]
 [ 8.75 ]
 [11.469]
 [11.433]
 [ 7.064]
 [12.081]] [[0.593]
 [0.594]
 [0.517]
 [0.651]
 [0.621]
 [0.377]
 [0.648]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9858800664451827 -0.8390000000000001 -0.8390000000000001
probs:  [0.06068593841609865, 0.059198486250580794, 0.05864644214791437, 0.05977427418561996, 0.40987899747367007, 0.35181586152611616]
maxi score, test score, baseline:  -0.9858800664451827 -0.8390000000000001 -0.8390000000000001
probs:  [0.06090031211728551, 0.05941524381358874, 0.0585969408707354, 0.05999010896340684, 0.40953373771505924, 0.3515636565199242]
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.65 ]
 [0.631]
 [0.635]
 [0.635]
 [0.635]
 [0.635]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.635]
 [0.65 ]
 [0.631]
 [0.635]
 [0.635]
 [0.635]
 [0.635]]
Printing some Q and Qe and total Qs values:  [[0.057]
 [0.088]
 [0.059]
 [0.061]
 [0.063]
 [0.054]
 [0.054]] [[32.679]
 [32.559]
 [33.682]
 [34.643]
 [34.67 ]
 [34.147]
 [34.147]] [[0.696]
 [0.723]
 [0.738]
 [0.779]
 [0.781]
 [0.752]
 [0.752]]
maxi score, test score, baseline:  -0.9858800664451827 -0.8390000000000001 -0.8390000000000001
probs:  [0.06082118294178312, 0.05904962399516809, 0.0585073100319186, 0.0599068299370786, 0.40890531663046226, 0.3528097364635894]
printing an ep nov before normalisation:  73.57409176078168
printing an ep nov before normalisation:  68.59237808715619
maxi score, test score, baseline:  -0.9859264900662251 -0.8390000000000001 -0.8390000000000001
probs:  [0.06082118294178312, 0.05904962399516809, 0.0585073100319186, 0.0599068299370786, 0.40890531663046226, 0.3528097364635894]
printing an ep nov before normalisation:  20.975911159452036
printing an ep nov before normalisation:  61.5362956597321
maxi score, test score, baseline:  -0.9859264900662251 -0.8390000000000001 -0.8390000000000001
probs:  [0.060739509027644044, 0.05928127865228862, 0.05847704250588046, 0.06013740100169084, 0.40869471932806917, 0.352670049484427]
maxi score, test score, baseline:  -0.9859264900662251 -0.8390000000000001 -0.8390000000000001
probs:  [0.060739509027644044, 0.05928127865228862, 0.05847704250588046, 0.06013740100169084, 0.40869471932806917, 0.352670049484427]
UNIT TEST: sample policy line 217 mcts : [0.122 0.184 0.163 0.082 0.082 0.082 0.286]
printing an ep nov before normalisation:  0.08091310844349664
maxi score, test score, baseline:  -0.985972607260726 -0.8390000000000001 -0.8390000000000001
probs:  [0.060759519151744525, 0.059292292116597034, 0.05848309417600055, 0.06015369637594169, 0.40873633721666075, 0.35257506096305546]
UNIT TEST: sample policy line 217 mcts : [0.041 0.714 0.082 0.02  0.082 0.02  0.041]
maxi score, test score, baseline:  -0.9860184210526315 -0.8390000000000001 -0.8390000000000001
probs:  [0.060759519151744525, 0.059292292116597034, 0.05848309417600055, 0.06015369637594169, 0.40873633721666075, 0.35257506096305546]
maxi score, test score, baseline:  -0.9860184210526315 -0.8390000000000001 -0.8390000000000001
probs:  [0.060759519151744525, 0.059292292116597034, 0.05848309417600055, 0.06015369637594169, 0.40873633721666075, 0.35257506096305546]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]]
printing an ep nov before normalisation:  30.638105509468694
using explorer policy with actor:  1
printing an ep nov before normalisation:  66.83126803942483
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.269]
 [0.25 ]
 [0.251]
 [0.253]
 [0.263]
 [0.263]] [[38.63 ]
 [39.013]
 [38.672]
 [39.271]
 [39.989]
 [38.719]
 [38.2  ]] [[1.619]
 [1.672]
 [1.63 ]
 [1.673]
 [1.724]
 [1.646]
 [1.611]]
maxi score, test score, baseline:  -0.9860184210526315 -0.8390000000000001 -0.8390000000000001
probs:  [0.060408132831588406, 0.05869346980333378, 0.05842789909862093, 0.060106991296160164, 0.40834906619915384, 0.35401444077114286]
printing an ep nov before normalisation:  69.71702002090505
maxi score, test score, baseline:  -0.9860184210526315 -0.8390000000000001 -0.8390000000000001
probs:  [0.060408132831588406, 0.05869346980333378, 0.05842789909862093, 0.060106991296160164, 0.40834906619915384, 0.35401444077114286]
using another actor
maxi score, test score, baseline:  -0.9860184210526315 -0.8390000000000001 -0.8390000000000001
probs:  [0.06036956310638249, 0.05864605163229933, 0.05837911046122921, 0.06006686754283383, 0.40800665781792556, 0.3545317494393297]
printing an ep nov before normalisation:  38.136431271084525
printing an ep nov before normalisation:  46.9072181238202
printing an ep nov before normalisation:  49.51685140729261
actions average: 
K:  2  action  0 :  tensor([0.3882, 0.0057, 0.1094, 0.1186, 0.1373, 0.1269, 0.1138],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0257, 0.8460, 0.0262, 0.0324, 0.0142, 0.0162, 0.0392],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2338, 0.0077, 0.2207, 0.1237, 0.1317, 0.1446, 0.1378],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2304, 0.0113, 0.1539, 0.1576, 0.1438, 0.1663, 0.1368],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2470, 0.0565, 0.1391, 0.1311, 0.1591, 0.1048, 0.1624],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2986, 0.0030, 0.1008, 0.1400, 0.1281, 0.2098, 0.1198],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2709, 0.0352, 0.1610, 0.1284, 0.1183, 0.1453, 0.1409],
       grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 57.33430820356189
actions average: 
K:  2  action  0 :  tensor([0.4889, 0.0332, 0.0880, 0.0925, 0.1092, 0.0931, 0.0951],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0480, 0.7920, 0.0386, 0.0309, 0.0232, 0.0283, 0.0390],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1738, 0.0017, 0.3621, 0.1135, 0.1129, 0.1340, 0.1020],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2346, 0.0336, 0.1192, 0.1946, 0.1416, 0.1381, 0.1382],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2488, 0.0216, 0.1139, 0.1393, 0.2154, 0.1276, 0.1334],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2266, 0.1302, 0.1337, 0.1285, 0.1143, 0.1425, 0.1241],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2422, 0.0257, 0.1186, 0.1486, 0.1502, 0.1348, 0.1800],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  58.77469032594863
printing an ep nov before normalisation:  30.025711509820937
maxi score, test score, baseline:  -0.9860639344262294 -0.8390000000000001 -0.8390000000000001
probs:  [0.06029154530225024, 0.05856032904265439, 0.05829219453780115, 0.059689916956716924, 0.4073972599184962, 0.35576875424208115]
actor:  1 policy actor:  1  step number:  60 total reward:  0.0733333333333327  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  43.58026593508397
maxi score, test score, baseline:  -0.9860639344262294 -0.8390000000000001 -0.8390000000000001
probs:  [0.059805249273034704, 0.058088027480256446, 0.05782206046858036, 0.05920848425284936, 0.4041050987187853, 0.36097107980649384]
printing an ep nov before normalisation:  0.053222142666413674
printing an ep nov before normalisation:  38.910823985762725
printing an ep nov before normalisation:  41.98892593383789
printing an ep nov before normalisation:  51.66422841321733
printing an ep nov before normalisation:  31.032211270543975
printing an ep nov before normalisation:  23.342008789377626
maxi score, test score, baseline:  -0.9860639344262294 -0.8390000000000001 -0.8390000000000001
printing an ep nov before normalisation:  42.84030639690458
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]] [[52.94]
 [52.94]
 [52.94]
 [52.94]
 [52.94]
 [52.94]
 [52.94]] [[2.073]
 [2.073]
 [2.073]
 [2.073]
 [2.073]
 [2.073]
 [2.073]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.013863330834596
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.453]
 [0.36 ]
 [0.346]
 [0.335]
 [0.335]
 [0.335]] [[40.837]
 [45.436]
 [41.012]
 [40.517]
 [40.051]
 [40.051]
 [40.051]] [[1.52 ]
 [1.824]
 [1.543]
 [1.509]
 [1.478]
 [1.478]
 [1.478]]
printing an ep nov before normalisation:  51.95474838158686
maxi score, test score, baseline:  -0.9860639344262294 -0.8390000000000001 -0.8390000000000001
probs:  [0.06001233493793973, 0.058288088635283286, 0.0577593197691353, 0.058838889537520755, 0.4036663499145497, 0.36143501720557114]
printing an ep nov before normalisation:  49.940411136175
maxi score, test score, baseline:  -0.9860639344262294 -0.8390000000000001 -0.8390000000000001
probs:  [0.059917007532306346, 0.058185723219638864, 0.05765479603042083, 0.05873877237507431, 0.40293366718044604, 0.3625700336621136]
printing an ep nov before normalisation:  39.9343989232626
printing an ep nov before normalisation:  72.80183753287042
maxi score, test score, baseline:  -0.9860639344262294 -0.8390000000000001 -0.8390000000000001
probs:  [0.05982194762018618, 0.05808364504651909, 0.05755056559059452, 0.058638936146440515, 0.40220304039108695, 0.3637018652051727]
printing an ep nov before normalisation:  34.650998177131
printing an ep nov before normalisation:  25.719189925495414
printing an ep nov before normalisation:  37.43623733520508
printing an ep nov before normalisation:  50.48418045043945
Printing some Q and Qe and total Qs values:  [[0.208]
 [0.381]
 [0.183]
 [0.28 ]
 [0.26 ]
 [0.111]
 [0.223]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.208]
 [0.381]
 [0.183]
 [0.28 ]
 [0.26 ]
 [0.111]
 [0.223]]
printing an ep nov before normalisation:  51.280856132507324
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
probs:  [0.059896502330780095, 0.05788646018026588, 0.05762228321191259, 0.05871201320636993, 0.4027052491527821, 0.36317749191788934]
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.413]
 [0.375]
 [0.375]
 [0.383]
 [0.375]
 [0.375]] [[37.241]
 [34.296]
 [34.249]
 [34.249]
 [36.071]
 [34.249]
 [34.249]] [[1.788]
 [1.622]
 [1.581]
 [1.581]
 [1.716]
 [1.581]
 [1.581]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
probs:  [0.05980189044965885, 0.057783745502152435, 0.05751850359476588, 0.05861262646273543, 0.4019777804424191, 0.3643054535482683]
printing an ep nov before normalisation:  60.03285050392151
actor:  1 policy actor:  1  step number:  67 total reward:  0.10666666666666591  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
probs:  [0.059280598532622665, 0.057280084994692844, 0.05701716035827921, 0.05810172448348545, 0.3984670881499781, 0.36985334348094184]
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
probs:  [0.05918500915397383, 0.0571765562566259, 0.0569125881615459, 0.05800145655375095, 0.3977340779306059, 0.3709903119434976]
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.324]
 [0.294]
 [0.291]
 [0.328]
 [0.328]
 [0.328]] [[49.565]
 [47.047]
 [47.565]
 [47.783]
 [49.565]
 [49.565]
 [49.565]] [[2.215]
 [2.035]
 [2.041]
 [2.053]
 [2.215]
 [2.215]
 [2.215]]
printing an ep nov before normalisation:  25.317972666535464
actions average: 
K:  3  action  0 :  tensor([0.4606, 0.0139, 0.0868, 0.1070, 0.1244, 0.0949, 0.1125],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0767, 0.7157, 0.0353, 0.0520, 0.0325, 0.0319, 0.0559],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2687, 0.0024, 0.2118, 0.1235, 0.1390, 0.1259, 0.1286],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2888, 0.0292, 0.1270, 0.1470, 0.1428, 0.1248, 0.1405],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2990, 0.0188, 0.1021, 0.1136, 0.2490, 0.1103, 0.1073],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1349, 0.0445, 0.1331, 0.1058, 0.0946, 0.3818, 0.1054],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.3538, 0.0196, 0.1162, 0.1157, 0.1527, 0.1342, 0.1078],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  59.2574749986351
printing an ep nov before normalisation:  48.72837543487549
printing an ep nov before normalisation:  29.11199280174813
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
probs:  [0.059265468684258506, 0.05724234003904287, 0.056715811509029185, 0.05807326787547072, 0.3963560182741615, 0.3723470936180372]
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
probs:  [0.059265468684258506, 0.05724234003904287, 0.056715811509029185, 0.05807326787547072, 0.3963560182741615, 0.3723470936180372]
Printing some Q and Qe and total Qs values:  [[-0.022]
 [-0.016]
 [-0.011]
 [-0.022]
 [-0.029]
 [-0.036]
 [-0.029]] [[37.168]
 [36.151]
 [36.906]
 [34.671]
 [37.153]
 [40.861]
 [37.246]] [[0.955]
 [0.91 ]
 [0.952]
 [0.831]
 [0.946]
 [1.122]
 [0.951]]
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
probs:  [0.05917180012777774, 0.05714080865164777, 0.05661223378374409, 0.05797496586505829, 0.39562998383796466, 0.3734702077338075]
actor:  1 policy actor:  1  step number:  53 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
probs:  [0.05677035448076342, 0.0954653359465291, 0.05431490073809622, 0.05562220249923847, 0.3795428070753043, 0.35828439926006844]
printing an ep nov before normalisation:  35.667242643157714
printing an ep nov before normalisation:  29.937365004726814
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.95575374018427
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
Printing some Q and Qe and total Qs values:  [[0.622]
 [1.037]
 [0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.622]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.622]
 [1.037]
 [0.622]
 [0.622]
 [0.622]
 [0.622]
 [0.622]]
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
probs:  [0.056862063790667355, 0.09496269290004734, 0.0544026340700989, 0.05571205267132747, 0.3801571636321726, 0.35790339293568646]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.412]
 [0.315]
 [0.315]
 [0.315]
 [0.315]
 [0.315]] [[43.022]
 [51.198]
 [43.022]
 [43.022]
 [43.022]
 [43.022]
 [43.022]] [[1.328]
 [1.736]
 [1.328]
 [1.328]
 [1.328]
 [1.328]
 [1.328]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.7967304726038
siam score:  -0.8383918
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
probs:  [0.05659076267238984, 0.09521601224667801, 0.0544152772173618, 0.0557324656764608, 0.3802450000057007, 0.35780048218140886]
UNIT TEST: sample policy line 217 mcts : [0.082 0.061 0.122 0.367 0.041 0.245 0.082]
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
Printing some Q and Qe and total Qs values:  [[0.212]
 [0.302]
 [0.211]
 [0.207]
 [0.208]
 [0.204]
 [0.208]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.212]
 [0.302]
 [0.211]
 [0.207]
 [0.208]
 [0.204]
 [0.208]]
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
probs:  [0.05648415254504797, 0.09480314693610967, 0.05428909358273083, 0.055618133188821285, 0.37936000100900136, 0.359445472738289]
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
probs:  [0.05648415254504797, 0.09480314693610967, 0.05428909358273083, 0.055618133188821285, 0.37936000100900136, 0.359445472738289]
printing an ep nov before normalisation:  63.75591324899937
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
probs:  [0.05648415254504797, 0.09480314693610967, 0.05428909358273083, 0.055618133188821285, 0.37936000100900136, 0.359445472738289]
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
probs:  [0.05648415254504797, 0.09480314693610967, 0.05428909358273083, 0.055618133188821285, 0.37936000100900136, 0.359445472738289]
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
probs:  [0.05653794647051672, 0.09489352107919329, 0.05434079205303969, 0.055671100391746484, 0.3797220198497394, 0.3588346201557644]
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
Printing some Q and Qe and total Qs values:  [[0.202]
 [0.183]
 [0.202]
 [0.179]
 [0.202]
 [0.202]
 [0.202]] [[30.842]
 [48.349]
 [30.842]
 [52.2  ]
 [30.842]
 [30.842]
 [30.842]] [[0.432]
 [1.402]
 [0.432]
 [1.616]
 [0.432]
 [0.432]
 [0.432]]
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
probs:  [0.056591533154434846, 0.09498354705513, 0.054392291354556546, 0.05572386353807661, 0.38008264400984654, 0.3582261208879555]
printing an ep nov before normalisation:  68.60296042484872
printing an ep nov before normalisation:  30.304157733917236
printing an ep nov before normalisation:  36.375695174604054
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
probs:  [0.056591533154434846, 0.09498354705513, 0.054392291354556546, 0.05572386353807661, 0.38008264400984654, 0.3582261208879555]
actor:  1 policy actor:  1  step number:  60 total reward:  0.11333333333333284  reward:  1.0 rdn_beta:  1.667
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
printing an ep nov before normalisation:  23.24650320100437
printing an ep nov before normalisation:  31.951835393148652
printing an ep nov before normalisation:  27.400226593017578
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
probs:  [0.057445295665166145, 0.09472258617981096, 0.05530990949610329, 0.05660281909065307, 0.38651398983666185, 0.3494053997316047]
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
probs:  [0.057445295665166145, 0.09472258617981096, 0.05530990949610329, 0.05660281909065307, 0.38651398983666185, 0.3494053997316047]
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
probs:  [0.057445295665166145, 0.09472258617981096, 0.05530990949610329, 0.05660281909065307, 0.38651398983666185, 0.3494053997316047]
printing an ep nov before normalisation:  51.16394592602717
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
probs:  [0.057445295665166145, 0.09472258617981096, 0.05530990949610329, 0.05660281909065307, 0.38651398983666185, 0.3494053997316047]
printing an ep nov before normalisation:  47.22183355068061
maxi score, test score, baseline:  -0.9861091503267974 -0.8390000000000001 -0.8390000000000001
probs:  [0.057445295665166145, 0.09472258617981096, 0.05530990949610329, 0.05660281909065307, 0.38651398983666185, 0.3494053997316047]
maxi score, test score, baseline:  -0.9861540716612377 -0.8390000000000001 -0.8390000000000001
probs:  [0.057445295665166145, 0.09472258617981096, 0.05530990949610329, 0.05660281909065307, 0.38651398983666185, 0.3494053997316047]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9861540716612377 -0.8390000000000001 -0.8390000000000001
using another actor
printing an ep nov before normalisation:  35.56826181567626
maxi score, test score, baseline:  -0.9861987012987012 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9861987012987012 -0.8390000000000001 -0.8390000000000001
probs:  [0.05867544308647716, 0.0941836687482059, 0.056641395737026416, 0.05787294784313917, 0.3958465204980748, 0.33678002408707647]
maxi score, test score, baseline:  -0.9861987012987012 -0.8390000000000001 -0.8390000000000001
probs:  [0.05867544308647716, 0.0941836687482059, 0.056641395737026416, 0.05787294784313917, 0.3958465204980748, 0.33678002408707647]
printing an ep nov before normalisation:  51.39131656149823
printing an ep nov before normalisation:  71.25746505771654
printing an ep nov before normalisation:  20.935240883522745
printing an ep nov before normalisation:  47.399790041163776
siam score:  -0.83437914
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.497]
 [0.497]
 [0.497]
 [0.498]
 [0.5  ]
 [0.497]] [[15.286]
 [14.597]
 [14.597]
 [14.597]
 [15.399]
 [15.218]
 [14.597]] [[0.502]
 [0.497]
 [0.497]
 [0.497]
 [0.498]
 [0.5  ]
 [0.497]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.486666666666667  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9862430420711973 -0.8390000000000001 -0.8390000000000001
probs:  [0.06010238846836384, 0.09375505749229929, 0.05817463451968213, 0.059341829293298, 0.4065925677962291, 0.3220335224301277]
maxi score, test score, baseline:  -0.9862870967741935 -0.8390000000000001 -0.8390000000000001
using explorer policy with actor:  1
siam score:  -0.83437836
maxi score, test score, baseline:  -0.9862870967741935 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9862870967741935 -0.8390000000000001 -0.8390000000000001
probs:  [0.060151238021661224, 0.09383131976576871, 0.05822191376723294, 0.05939005931190631, 0.40692366058917906, 0.32148180854425173]
printing an ep nov before normalisation:  29.696815013885498
maxi score, test score, baseline:  -0.9862870967741935 -0.8390000000000001 -0.8390000000000001
probs:  [0.060151238021661224, 0.09383131976576871, 0.05822191376723294, 0.05939005931190631, 0.40692366058917906, 0.32148180854425173]
printing an ep nov before normalisation:  46.85918277882397
maxi score, test score, baseline:  -0.9862870967741935 -0.8390000000000001 -0.8390000000000001
probs:  [0.06019988977937826, 0.09390727324745556, 0.05826900157749157, 0.05943809404347765, 0.4072534127620679, 0.32093232859012916]
Printing some Q and Qe and total Qs values:  [[0.155]
 [0.227]
 [0.123]
 [0.123]
 [0.123]
 [0.123]
 [0.123]] [[28.316]
 [28.79 ]
 [26.868]
 [26.868]
 [26.868]
 [26.868]
 [26.868]] [[1.54 ]
 [1.655]
 [1.376]
 [1.376]
 [1.376]
 [1.376]
 [1.376]]
printing an ep nov before normalisation:  25.08239507675171
maxi score, test score, baseline:  -0.9862870967741935 -0.8390000000000001 -0.8390000000000001
probs:  [0.06019988977937826, 0.09390727324745556, 0.05826900157749157, 0.05943809404347765, 0.4072534127620679, 0.32093232859012916]
printing an ep nov before normalisation:  15.28475284576416
maxi score, test score, baseline:  -0.9862870967741935 -0.8390000000000001 -0.8390000000000001
probs:  [0.06019988977937826, 0.09390727324745556, 0.05826900157749157, 0.05943809404347765, 0.4072534127620679, 0.32093232859012916]
printing an ep nov before normalisation:  27.87484009870688
siam score:  -0.8411548
maxi score, test score, baseline:  -0.9863308681672025 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9863308681672025 -0.8390000000000001 -0.8390000000000001
probs:  [0.05991624164039609, 0.0940474349960176, 0.05823659408573364, 0.059411347583786245, 0.407025875557333, 0.32136250613673345]
actor:  1 policy actor:  1  step number:  71 total reward:  0.09333333333333305  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  34.27102358203107
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.183]
 [0.287]
 [0.159]
 [0.222]
 [0.183]
 [0.189]
 [0.189]] [[34.072]
 [33.847]
 [32.712]
 [41.696]
 [35.151]
 [34.687]
 [32.696]] [[1.75 ]
 [1.834]
 [1.6  ]
 [2.494]
 [1.85 ]
 [1.813]
 [1.629]]
line 256 mcts: sample exp_bonus 26.24835080230144
Printing some Q and Qe and total Qs values:  [[-0.001]
 [ 0.105]
 [ 0.08 ]
 [-0.003]
 [ 0.049]
 [ 0.037]
 [ 0.046]] [[25.288]
 [25.964]
 [25.666]
 [25.602]
 [24.858]
 [25.764]
 [25.262]] [[1.305]
 [1.482]
 [1.426]
 [1.337]
 [1.31 ]
 [1.393]
 [1.349]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9863308681672025 -0.8390000000000001 -0.8390000000000001
probs:  [0.060439687483594615, 0.09390468690964278, 0.058792824337993994, 0.05994464826423252, 0.41092430073817005, 0.3159938522663661]
using another actor
maxi score, test score, baseline:  -0.9863308681672025 -0.8390000000000001 -0.8390000000000001
probs:  [0.060353957476097876, 0.09394409141273977, 0.05870093627230888, 0.059857067173768444, 0.4102802405895792, 0.3168637070755058]
maxi score, test score, baseline:  -0.9863308681672025 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9863743589743589 -0.8390000000000001 -0.8390000000000001
probs:  [0.06018314111664868, 0.0940226045723845, 0.058517850030799366, 0.05968256254619992, 0.4089969558557852, 0.3185968858781824]
printing an ep nov before normalisation:  35.98581314086914
printing an ep nov before normalisation:  6.443205719385787
maxi score, test score, baseline:  -0.9864175718849839 -0.8390000000000001 -0.8390000000000001
probs:  [0.06018314111664868, 0.0940226045723845, 0.058517850030799366, 0.05968256254619992, 0.4089969558557852, 0.3185968858781824]
maxi score, test score, baseline:  -0.9864175718849839 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9864175718849839 -0.8390000000000001 -0.8390000000000001
probs:  [0.05994535722756582, 0.09404637645564506, 0.05853263245327373, 0.05969763986262919, 0.40910047620135964, 0.31867751779952663]
printing an ep nov before normalisation:  32.326316833496094
printing an ep nov before normalisation:  50.02503951656357
Printing some Q and Qe and total Qs values:  [[0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]] [[46.428]
 [46.428]
 [46.428]
 [46.428]
 [46.428]
 [46.428]
 [46.428]] [[1.701]
 [1.701]
 [1.701]
 [1.701]
 [1.701]
 [1.701]
 [1.701]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.6036915875094
maxi score, test score, baseline:  -0.9864175718849839 -0.8390000000000001 -0.8390000000000001
probs:  [0.05985937470974338, 0.09408558305970696, 0.058441463645482215, 0.05961074794326703, 0.40846145712304527, 0.31954137351875517]
maxi score, test score, baseline:  -0.9864175718849839 -0.8390000000000001 -0.8390000000000001
probs:  [0.05985937470974338, 0.09408558305970696, 0.058441463645482215, 0.05961074794326703, 0.40846145712304527, 0.31954137351875517]
printing an ep nov before normalisation:  37.972436753845464
maxi score, test score, baseline:  -0.9864175718849839 -0.8390000000000001 -0.8390000000000001
probs:  [0.05985937470974338, 0.09408558305970696, 0.058441463645482215, 0.05961074794326703, 0.40846145712304527, 0.31954137351875517]
maxi score, test score, baseline:  -0.9864175718849839 -0.8390000000000001 -0.8390000000000001
probs:  [0.05977360667932633, 0.09412469186106441, 0.05835052226253857, 0.059524072779854866, 0.40782403210765855, 0.3204030743095573]
actor:  1 policy actor:  1  step number:  58 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  2.0
siam score:  -0.84337527
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9864175718849839 -0.8390000000000001 -0.8390000000000001
probs:  [0.05918704747908162, 0.09320029200527277, 0.05777795900360758, 0.058697982406838045, 0.4038144199450987, 0.3273222991601013]
maxi score, test score, baseline:  -0.9864175718849839 -0.8390000000000001 -0.8390000000000001
probs:  [0.05918704747908162, 0.09320029200527277, 0.05777795900360758, 0.058697982406838045, 0.4038144199450987, 0.3273222991601013]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
Printing some Q and Qe and total Qs values:  [[0.212]
 [0.087]
 [0.214]
 [0.221]
 [0.218]
 [0.211]
 [0.176]] [[34.869]
 [32.208]
 [34.002]
 [35.097]
 [35.042]
 [35.364]
 [33.389]] [[1.787]
 [1.443]
 [1.718]
 [1.815]
 [1.807]
 [1.827]
 [1.629]]
printing an ep nov before normalisation:  48.59778646326228
printing an ep nov before normalisation:  38.00772561517914
maxi score, test score, baseline:  -0.98646050955414 -0.8390000000000001 -0.8390000000000001
probs:  [0.059133447274518185, 0.09272520026719985, 0.05771850294265825, 0.0586423497572747, 0.40339749069867914, 0.32838300905966983]
maxi score, test score, baseline:  -0.98646050955414 -0.8390000000000001 -0.8390000000000001
probs:  [0.059133447274518185, 0.09272520026719985, 0.05771850294265825, 0.0586423497572747, 0.40339749069867914, 0.32838300905966983]
printing an ep nov before normalisation:  58.229419137034135
Printing some Q and Qe and total Qs values:  [[0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.161]] [[31.866]
 [31.866]
 [31.866]
 [31.866]
 [31.866]
 [31.866]
 [31.866]] [[1.814]
 [1.814]
 [1.814]
 [1.814]
 [1.814]
 [1.814]
 [1.814]]
maxi score, test score, baseline:  -0.9865031746031745 -0.8390000000000001 -0.8390000000000001
probs:  [0.0591819602328657, 0.09280133405983104, 0.05776585246238084, 0.05869045891046374, 0.4037290749244093, 0.32783131941004934]
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.61750698685874
maxi score, test score, baseline:  -0.9865031746031745 -0.8390000000000001 -0.8390000000000001
probs:  [0.05909553811208751, 0.09283504815796828, 0.0576743699905682, 0.058602280447917585, 0.40308786840105054, 0.3287048948904077]
printing an ep nov before normalisation:  45.411529685102394
printing an ep nov before normalisation:  44.903991084023595
printing an ep nov before normalisation:  39.557271003723145
actions average: 
K:  4  action  0 :  tensor([0.4833, 0.0033, 0.0997, 0.0968, 0.1118, 0.1064, 0.0987],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0338, 0.7597, 0.0288, 0.0360, 0.0216, 0.0221, 0.0980],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1987, 0.0302, 0.1401, 0.1550, 0.1611, 0.1362, 0.1788],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2771, 0.0325, 0.1278, 0.1397, 0.1768, 0.1367, 0.1095],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2195, 0.0171, 0.1363, 0.1767, 0.1589, 0.1450, 0.1465],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2246, 0.0048, 0.1489, 0.1481, 0.1186, 0.2298, 0.1253],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2651, 0.0218, 0.1158, 0.1418, 0.1564, 0.1405, 0.1587],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9865031746031745 -0.8390000000000001 -0.8390000000000001
probs:  [0.05917142628625215, 0.09201871502538156, 0.05774128422276022, 0.058675053954937086, 0.40355589740936515, 0.3288376231013038]
maxi score, test score, baseline:  -0.9865031746031745 -0.8390000000000001 -0.8390000000000001
probs:  [0.05917142628625215, 0.09201871502538156, 0.05774128422276022, 0.058675053954937086, 0.40355589740936515, 0.3288376231013038]
line 256 mcts: sample exp_bonus 49.87034052718711
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.371]
 [0.305]
 [0.303]
 [0.302]
 [0.309]
 [0.276]] [[48.491]
 [54.807]
 [47.787]
 [48.995]
 [49.248]
 [49.531]
 [49.375]] [[1.309]
 [1.586]
 [1.243]
 [1.289]
 [1.297]
 [1.316]
 [1.276]]
maxi score, test score, baseline:  -0.9865031746031745 -0.8390000000000001 -0.8390000000000001
probs:  [0.059048799386340285, 0.0921555678164828, 0.057607359805306284, 0.058548505923507174, 0.40261691271912264, 0.3300228543492409]
maxi score, test score, baseline:  -0.9865031746031745 -0.8390000000000001 -0.8390000000000001
probs:  [0.059048799386340285, 0.0921555678164828, 0.057607359805306284, 0.058548505923507174, 0.40261691271912264, 0.3300228543492409]
maxi score, test score, baseline:  -0.9865031746031745 -0.8390000000000001 -0.8390000000000001
probs:  [0.059048799386340285, 0.0921555678164828, 0.057607359805306284, 0.058548505923507174, 0.40261691271912264, 0.3300228543492409]
printing an ep nov before normalisation:  47.236671447753906
maxi score, test score, baseline:  -0.9865031746031745 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9865031746031745 -0.8390000000000001 -0.8390000000000001
probs:  [0.059048799386340285, 0.0921555678164828, 0.057607359805306284, 0.058548505923507174, 0.40261691271912264, 0.3300228543492409]
maxi score, test score, baseline:  -0.9865031746031745 -0.8390000000000001 -0.8390000000000001
probs:  [0.05901204320518441, 0.09226200078244211, 0.05756436929461559, 0.058509585937255026, 0.4023152921824844, 0.33033670859801845]
maxi score, test score, baseline:  -0.9865031746031745 -0.8390000000000001 -0.8390000000000001
probs:  [0.05901204320518441, 0.09226200078244211, 0.05756436929461559, 0.058509585937255026, 0.4023152921824844, 0.33033670859801845]
maxi score, test score, baseline:  -0.9865031746031745 -0.8390000000000001 -0.8390000000000001
probs:  [0.05901204320518441, 0.09226200078244211, 0.05756436929461559, 0.058509585937255026, 0.4023152921824844, 0.33033670859801845]
printing an ep nov before normalisation:  37.81143250124201
printing an ep nov before normalisation:  41.02819298470206
printing an ep nov before normalisation:  50.98606786311887
maxi score, test score, baseline:  -0.9865455696202531 -0.8390000000000001 -0.8390000000000001
probs:  [0.05907148874247705, 0.09251908758042929, 0.05761520970523717, 0.05856604481546252, 0.40267075571760486, 0.3295574134387892]
printing an ep nov before normalisation:  40.83750604172748
maxi score, test score, baseline:  -0.9865455696202531 -0.8390000000000001 -0.8390000000000001
probs:  [0.059171281620196026, 0.09268932492748305, 0.05748540300560615, 0.058664773170451795, 0.40176233666056, 0.330226880615703]
maxi score, test score, baseline:  -0.9865455696202531 -0.8390000000000001 -0.8390000000000001
probs:  [0.059171281620196026, 0.09268932492748305, 0.05748540300560615, 0.058664773170451795, 0.40176233666056, 0.330226880615703]
maxi score, test score, baseline:  -0.9865455696202531 -0.8390000000000001 -0.8390000000000001
probs:  [0.059087939016189577, 0.09272143750403486, 0.05739625327801369, 0.0585796858636301, 0.40113747687453877, 0.331077207463593]
Printing some Q and Qe and total Qs values:  [[ 0.201]
 [ 0.198]
 [ 0.294]
 [ 0.294]
 [ 0.294]
 [-0.023]
 [ 0.014]] [[53.548]
 [52.819]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [47.609]
 [54.884]] [[0.792]
 [0.78 ]
 [0.284]
 [0.284]
 [0.284]
 [0.501]
 [0.619]]
printing an ep nov before normalisation:  42.77351304760523
maxi score, test score, baseline:  -0.9865455696202531 -0.8390000000000001 -0.8390000000000001
probs:  [0.05930121893824896, 0.09235885647946569, 0.05738813970376867, 0.058793365430126615, 0.40108179989128934, 0.3310766195571008]
maxi score, test score, baseline:  -0.9865455696202531 -0.8390000000000001 -0.8390000000000001
probs:  [0.05930121893824896, 0.09235885647946569, 0.05738813970376867, 0.058793365430126615, 0.40108179989128934, 0.3310766195571008]
printing an ep nov before normalisation:  34.41316398297804
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([0.4414, 0.0167, 0.0868, 0.1138, 0.1364, 0.1063, 0.0987],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0304, 0.8850, 0.0179, 0.0156, 0.0107, 0.0092, 0.0313],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2390, 0.0271, 0.2938, 0.0960, 0.1060, 0.1185, 0.1196],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2287, 0.1647, 0.1063, 0.1207, 0.1436, 0.1243, 0.1117],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2583, 0.0631, 0.0893, 0.1399, 0.1964, 0.1429, 0.1100],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2259, 0.0038, 0.1372, 0.1554, 0.1595, 0.1844, 0.1337],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2198, 0.0719, 0.1177, 0.1450, 0.1589, 0.1277, 0.1590],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9865876971608832 -0.8390000000000001 -0.8390000000000001
probs:  [0.05928115035423096, 0.0926461795747973, 0.057350282049437255, 0.058768574489814075, 0.40081558578038085, 0.3311382277513395]
printing an ep nov before normalisation:  63.743675598106265
printing an ep nov before normalisation:  25.3090398639159
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.352]
 [0.314]
 [0.314]
 [0.307]
 [0.305]
 [0.305]] [[32.655]
 [50.376]
 [35.006]
 [30.704]
 [31.032]
 [36.736]
 [36.736]] [[0.306]
 [0.352]
 [0.314]
 [0.314]
 [0.307]
 [0.305]
 [0.305]]
printing an ep nov before normalisation:  28.040599302380546
printing an ep nov before normalisation:  41.28380154469417
using explorer policy with actor:  1
printing an ep nov before normalisation:  17.55752541075743
maxi score, test score, baseline:  -0.9865876971608832 -0.8390000000000001 -0.8390000000000001
probs:  [0.059118065958081674, 0.0927090118076865, 0.05717412362731746, 0.05860201941151283, 0.39958087514134316, 0.33281590405405836]
maxi score, test score, baseline:  -0.9866295597484276 -0.8390000000000001 -0.8390000000000001
probs:  [0.059118065958081674, 0.0927090118076865, 0.05717412362731746, 0.05860201941151283, 0.39958087514134316, 0.33281590405405836]
printing an ep nov before normalisation:  37.73017497399111
maxi score, test score, baseline:  -0.9866295597484276 -0.8390000000000001 -0.8390000000000001
printing an ep nov before normalisation:  38.87753016537637
printing an ep nov before normalisation:  59.74479631333188
maxi score, test score, baseline:  -0.9866295597484276 -0.8390000000000001 -0.8390000000000001
probs:  [0.0591984351836474, 0.09228656057328628, 0.05725184415294995, 0.05868168550281792, 0.4001251386942691, 0.33245633589302936]
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.3  ]] [[66.596]
 [50.851]
 [50.851]
 [50.851]
 [50.851]
 [50.851]
 [64.706]] [[1.733]
 [0.991]
 [0.991]
 [0.991]
 [0.991]
 [0.991]
 [1.611]]
printing an ep nov before normalisation:  63.01185894848892
maxi score, test score, baseline:  -0.9866295597484276 -0.8390000000000001 -0.8390000000000001
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.315]
 [0.52 ]
 [0.303]
 [0.318]
 [0.292]
 [0.301]] [[35.756]
 [30.568]
 [22.014]
 [28.825]
 [28.828]
 [28.903]
 [29.694]] [[1.652]
 [1.438]
 [1.294]
 [1.355]
 [1.371]
 [1.347]
 [1.389]]
maxi score, test score, baseline:  -0.9866711598746081 -0.8390000000000001 -0.8390000000000001
probs:  [0.05903646894323271, 0.09234633841170088, 0.05707683259703683, 0.05851625620184566, 0.39889846328407846, 0.3341256405621054]
printing an ep nov before normalisation:  39.243880461975046
using explorer policy with actor:  1
printing an ep nov before normalisation:  13.146228998134978
maxi score, test score, baseline:  -0.9866711598746081 -0.8390000000000001 -0.8390000000000001
probs:  [0.05895578309753163, 0.09237611761404772, 0.05698964803709299, 0.058433845182002814, 0.3982873770316685, 0.3349572290376563]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9866711598746081 -0.8390000000000001 -0.8390000000000001
probs:  [0.059058240450257087, 0.09254352293395302, 0.05686334959055022, 0.05853528821793288, 0.3974035403814014, 0.33559605842590545]
maxi score, test score, baseline:  -0.9866711598746081 -0.8390000000000001 -0.8390000000000001
probs:  [0.05880955530214825, 0.09256796371234019, 0.056878353636824946, 0.05855073446040388, 0.39750861087783285, 0.3356847820104499]
maxi score, test score, baseline:  -0.9866711598746081 -0.8390000000000001 -0.8390000000000001
probs:  [0.05880955530214825, 0.09256796371234019, 0.056878353636824946, 0.05855073446040388, 0.39750861087783285, 0.3356847820104499]
maxi score, test score, baseline:  -0.9867535825545171 -0.8390000000000001 -0.8390000000000001
probs:  [0.05872913530579589, 0.09259809631781288, 0.05679160930888219, 0.058469466873219834, 0.3969006145459246, 0.3365110776483647]
printing an ep nov before normalisation:  43.779895367519956
maxi score, test score, baseline:  -0.9867535825545171 -0.8390000000000001 -0.8390000000000001
probs:  [0.05872913530579589, 0.09259809631781288, 0.05679160930888219, 0.058469466873219834, 0.3969006145459246, 0.3365110776483647]
maxi score, test score, baseline:  -0.9867535825545171 -0.8390000000000001 -0.8390000000000001
probs:  [0.05872913530579589, 0.09259809631781288, 0.05679160930888219, 0.058469466873219834, 0.3969006145459246, 0.3365110776483647]
maxi score, test score, baseline:  -0.9867535825545171 -0.8390000000000001 -0.8390000000000001
probs:  [0.058648911964751214, 0.0926281552384177, 0.056705077101472566, 0.058388398014002536, 0.3962941049798849, 0.337335352701471]
printing an ep nov before normalisation:  35.75895912452684
printing an ep nov before normalisation:  36.063820737854236
maxi score, test score, baseline:  -0.9867535825545171 -0.8390000000000001 -0.8390000000000001
printing an ep nov before normalisation:  33.85934839534997
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.438]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]] [[27.792]
 [33.348]
 [27.792]
 [27.792]
 [27.792]
 [27.792]
 [27.792]] [[1.59 ]
 [2.312]
 [1.59 ]
 [1.59 ]
 [1.59 ]
 [1.59 ]
 [1.59 ]]
from probs:  [0.058648911964751214, 0.0926281552384177, 0.056705077101472566, 0.058388398014002536, 0.3962941049798849, 0.337335352701471]
UNIT TEST: sample policy line 217 mcts : [0.265 0.265 0.163 0.061 0.082 0.122 0.041]
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.426]
 [0.113]
 [0.253]
 [0.253]
 [0.172]
 [0.253]] [[25.424]
 [35.549]
 [30.629]
 [25.424]
 [25.424]
 [32.252]
 [25.424]] [[0.938]
 [2.   ]
 [1.255]
 [0.938]
 [0.938]
 [1.457]
 [0.938]]
maxi score, test score, baseline:  -0.9867535825545171 -0.8390000000000001 -0.8390000000000001
probs:  [0.05856888455855756, 0.09265814074410299, 0.056618756237481783, 0.05830752715470205, 0.3956890767328717, 0.338157614572284]
actor:  1 policy actor:  1  step number:  58 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  4  action  0 :  tensor([0.5218, 0.0075, 0.0680, 0.1069, 0.1387, 0.0870, 0.0702],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0384, 0.7016, 0.0610, 0.0584, 0.0352, 0.0638, 0.0416],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2142, 0.0179, 0.1636, 0.1537, 0.1225, 0.2029, 0.1251],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2236, 0.0144, 0.0969, 0.2796, 0.1436, 0.1401, 0.1019],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2533, 0.0134, 0.1098, 0.1835, 0.1605, 0.1548, 0.1247],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2502, 0.0578, 0.1205, 0.1648, 0.1407, 0.1494, 0.1166],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2325, 0.0489, 0.1122, 0.1895, 0.1412, 0.1412, 0.1346],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9867535825545171 -0.8390000000000001 -0.8390000000000001
probs:  [0.05808171488144025, 0.09188677383960034, 0.05614784449978323, 0.05782253637668209, 0.3923913844855324, 0.3436697459169617]
actor:  1 policy actor:  1  step number:  68 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9867944099378881 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9867944099378881 -0.8390000000000001 -0.8390000000000001
probs:  [0.057576831306930774, 0.09108735938854487, 0.0556598100169834, 0.057319910927865654, 0.3889737853867529, 0.34938230297292233]
printing an ep nov before normalisation:  49.51571453391887
printing an ep nov before normalisation:  37.97358989715576
maxi score, test score, baseline:  -0.9867944099378881 -0.8390000000000001 -0.8390000000000001
printing an ep nov before normalisation:  34.42328716381751
printing an ep nov before normalisation:  32.03612247758103
maxi score, test score, baseline:  -0.9867944099378881 -0.8390000000000001 -0.8390000000000001
probs:  [0.05749521935141482, 0.09111179158190856, 0.055572131642860685, 0.057237485947175606, 0.38835926132343634, 0.3502241101532039]
maxi score, test score, baseline:  -0.9867944099378881 -0.8390000000000001 -0.8390000000000001
printing an ep nov before normalisation:  39.42849348484534
from probs:  [0.05749521935141482, 0.09111179158190856, 0.055572131642860685, 0.057237485947175606, 0.38835926132343634, 0.3502241101532039]
maxi score, test score, baseline:  -0.9867944099378881 -0.8390000000000001 -0.8390000000000001
actor:  1 policy actor:  1  step number:  63 total reward:  0.13333333333333264  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9867944099378881 -0.8390000000000001 -0.8390000000000001
printing an ep nov before normalisation:  23.406522575755663
printing an ep nov before normalisation:  60.92359500188547
maxi score, test score, baseline:  -0.9868349845201237 -0.8390000000000001 -0.8390000000000001
probs:  [0.05810177741259728, 0.09102097916432936, 0.056218583854666826, 0.05784939064710145, 0.39288986162987705, 0.34391940729142795]
printing an ep nov before normalisation:  57.156374194327284
maxi score, test score, baseline:  -0.9868349845201237 -0.8390000000000001 -0.8390000000000001
probs:  [0.05813262806671896, 0.09053757831345265, 0.05624843227995171, 0.05788010698189449, 0.39309888529803033, 0.3441023690599518]
maxi score, test score, baseline:  -0.9868349845201237 -0.8390000000000001 -0.8390000000000001
probs:  [0.05805312710478635, 0.09056017064374076, 0.056162995072180255, 0.05779981044062265, 0.39250006776550017, 0.3449238289731698]
maxi score, test score, baseline:  -0.9868349845201237 -0.8390000000000001 -0.8390000000000001
probs:  [0.05805312710478635, 0.09056017064374076, 0.056162995072180255, 0.05779981044062265, 0.39250006776550017, 0.3449238289731698]
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.368]
 [0.262]
 [0.368]
 [0.262]
 [0.262]
 [0.262]] [[29.656]
 [27.365]
 [30.714]
 [32.145]
 [30.714]
 [30.714]
 [30.714]] [[1.791]
 [1.664]
 [1.87 ]
 [2.109]
 [1.87 ]
 [1.87 ]
 [1.87 ]]
printing an ep nov before normalisation:  27.02207112208037
printing an ep nov before normalisation:  43.9649443171289
from probs:  [0.05810169776034602, 0.09063600115999385, 0.05620998069476579, 0.05784816866908269, 0.3928291000078176, 0.3443750517079939]
actor:  1 policy actor:  1  step number:  67 total reward:  0.21333333333333238  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  22.123187257353546
maxi score, test score, baseline:  -0.9868349845201237 -0.8390000000000001 -0.8390000000000001
probs:  [0.05762594677072991, 0.0898932390388161, 0.055749755149419086, 0.057374498409110936, 0.3896062195855872, 0.34975034104633684]
printing an ep nov before normalisation:  35.39510438377249
maxi score, test score, baseline:  -0.9868349845201237 -0.8390000000000001 -0.8390000000000001
probs:  [0.0578504151825682, 0.0901034630554578, 0.05576072456295461, 0.05759907782282087, 0.38968413531602425, 0.3490021840601743]
maxi score, test score, baseline:  -0.9868349845201237 -0.8390000000000001 -0.8390000000000001
probs:  [0.057898929757087096, 0.09017908814829952, 0.0558074826338384, 0.05764738113401595, 0.39001157497648425, 0.3484555433502747]
printing an ep nov before normalisation:  42.55029288891523
siam score:  -0.83230996
maxi score, test score, baseline:  -0.9868753086419753 -0.8390000000000001 -0.8390000000000001
probs:  [0.057898929757087096, 0.09017908814829952, 0.0558074826338384, 0.05764738113401595, 0.39001157497648425, 0.3484555433502747]
printing an ep nov before normalisation:  31.292552013116453
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]] [[28.809]
 [28.809]
 [28.809]
 [28.809]
 [28.809]
 [28.809]
 [28.809]] [[0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]]
printing an ep nov before normalisation:  31.07151427788181
printing an ep nov before normalisation:  40.37992083374241
actions average: 
K:  1  action  0 :  tensor([0.5064, 0.0043, 0.0850, 0.0851, 0.1272, 0.0977, 0.0942],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0059, 0.9678, 0.0045, 0.0072, 0.0050, 0.0045, 0.0052],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1887, 0.0212, 0.2452, 0.1192, 0.1325, 0.1691, 0.1241],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2457, 0.0295, 0.1235, 0.1474, 0.1483, 0.1518, 0.1538],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2745, 0.0962, 0.1100, 0.1194, 0.1487, 0.1335, 0.1177],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.3383, 0.0180, 0.0952, 0.1048, 0.1146, 0.2163, 0.1128],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2207, 0.0817, 0.1179, 0.1253, 0.1152, 0.1198, 0.2194],
       grad_fn=<DivBackward0>)
using another actor
printing an ep nov before normalisation:  29.359704518646996
actor:  1 policy actor:  1  step number:  46 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.9869552147239263 -0.8390000000000001 -0.8390000000000001
actions average: 
K:  2  action  0 :  tensor([0.5089, 0.0036, 0.0828, 0.0876, 0.1290, 0.0978, 0.0902],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0264, 0.8617, 0.0173, 0.0337, 0.0207, 0.0162, 0.0240],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1587, 0.0443, 0.2532, 0.1151, 0.1237, 0.1707, 0.1342],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.3375, 0.0241, 0.1121, 0.1309, 0.1248, 0.1306, 0.1400],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2601, 0.0217, 0.1014, 0.1131, 0.2579, 0.1404, 0.1055],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2087, 0.0871, 0.1455, 0.1015, 0.1290, 0.2090, 0.1191],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2165, 0.0187, 0.1410, 0.1443, 0.1571, 0.1541, 0.1682],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9869948012232416 -0.8390000000000001 -0.8390000000000001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  24.52866819608042
from probs:  [0.05511372496892862, 0.08611333293448153, 0.053105245329031846, 0.10203678436750825, 0.37108725440302065, 0.3325436579970292]
printing an ep nov before normalisation:  12.70377213978179
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]] [[26.014]
 [26.014]
 [26.014]
 [26.014]
 [26.014]
 [26.014]
 [26.014]] [[0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]]
printing an ep nov before normalisation:  16.78881316813321
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]] [[13.83 ]
 [13.824]
 [13.824]
 [13.824]
 [13.824]
 [13.824]
 [13.824]] [[0.571]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]]
printing an ep nov before normalisation:  14.66477766038783
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]] [[20.03 ]
 [25.787]
 [25.787]
 [25.787]
 [25.787]
 [25.787]
 [25.787]] [[0.577]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]]
maxi score, test score, baseline:  -0.9870732522796352 -0.8390000000000001 -0.8390000000000001
probs:  [0.05503078354418648, 0.08612139629400326, 0.05301640766016526, 0.10209159381052915, 0.3704646579093531, 0.33327516078176284]
maxi score, test score, baseline:  -0.9870732522796352 -0.8390000000000001 -0.8390000000000001
probs:  [0.05503078354418648, 0.08612139629400326, 0.05301640766016526, 0.10209159381052915, 0.3704646579093531, 0.33327516078176284]
printing an ep nov before normalisation:  22.56804542257937
printing an ep nov before normalisation:  31.760706172242003
maxi score, test score, baseline:  -0.987189156626506 -0.8390000000000001 -0.8390000000000001
probs:  [0.055074560744831885, 0.08618996621898954, 0.05305857852817215, 0.10217289892050141, 0.370759972934545, 0.3327440226529599]
maxi score, test score, baseline:  -0.987189156626506 -0.8390000000000001 -0.8390000000000001
probs:  [0.055074560744831885, 0.08618996621898954, 0.05305857852817215, 0.10217289892050141, 0.370759972934545, 0.3327440226529599]
Printing some Q and Qe and total Qs values:  [[0.185]
 [0.191]
 [0.386]
 [0.151]
 [0.173]
 [0.386]
 [0.154]] [[46.164]
 [43.56 ]
 [36.228]
 [51.174]
 [49.379]
 [36.228]
 [45.257]] [[1.286]
 [1.174]
 [1.038]
 [1.478]
 [1.419]
 [1.038]
 [1.214]]
maxi score, test score, baseline:  -0.987189156626506 -0.8390000000000001 -0.8390000000000001
probs:  [0.05503558870188057, 0.08626673789853731, 0.05301210739328575, 0.10230912424042876, 0.3704340622481637, 0.332942379517704]
printing an ep nov before normalisation:  14.194192690105936
printing an ep nov before normalisation:  40.14813985896367
printing an ep nov before normalisation:  47.96993899408097
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.321]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]] [[39.127]
 [41.932]
 [42.423]
 [42.423]
 [42.423]
 [42.423]
 [42.423]] [[0.32 ]
 [0.321]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]]
printing an ep nov before normalisation:  31.581343923880386
printing an ep nov before normalisation:  51.75574938021547
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.36 ]
 [0.348]
 [0.282]
 [0.281]
 [0.348]
 [0.284]] [[31.123]
 [40.391]
 [36.433]
 [32.657]
 [32.387]
 [36.433]
 [31.796]] [[0.283]
 [0.36 ]
 [0.348]
 [0.282]
 [0.281]
 [0.348]
 [0.284]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.49 ]
 [0.421]
 [0.223]
 [0.421]
 [0.237]
 [0.421]] [[29.766]
 [34.941]
 [29.766]
 [25.161]
 [29.766]
 [25.294]
 [29.766]] [[1.694]
 [2.141]
 [1.694]
 [1.159]
 [1.694]
 [1.183]
 [1.694]]
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.4  ]
 [0.358]
 [0.35 ]
 [0.336]
 [0.335]
 [0.35 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.335]
 [0.4  ]
 [0.358]
 [0.35 ]
 [0.336]
 [0.335]
 [0.35 ]]
maxi score, test score, baseline:  -0.987189156626506 -0.8390000000000001 -0.8390000000000001
probs:  [0.054953228295484116, 0.08627516864923436, 0.05292386458377286, 0.10236419134156395, 0.36981563334452716, 0.3336679137854175]
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.226]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.278]
 [0.284]] [[17.362]
 [26.829]
 [14.988]
 [14.551]
 [14.405]
 [14.012]
 [14.749]] [[0.278]
 [0.226]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.278]
 [0.284]]
printing an ep nov before normalisation:  35.957705740782664
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  33.00498848955332
actor:  1 policy actor:  1  step number:  49 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.529]
 [0.731]
 [0.554]
 [0.563]
 [0.53 ]
 [0.542]] [[27.636]
 [27.431]
 [17.35 ]
 [24.894]
 [23.597]
 [23.319]
 [25.825]] [[0.574]
 [0.529]
 [0.731]
 [0.554]
 [0.563]
 [0.53 ]
 [0.542]]
maxi score, test score, baseline:  -0.9873404761904762 -0.8390000000000001 -0.8390000000000001
probs:  [0.05439228764433223, 0.08539372910857745, 0.052383689211165645, 0.10131812234794607, 0.36603289193565103, 0.3404792797523275]
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]] [[36.079]
 [34.936]
 [34.936]
 [34.936]
 [34.936]
 [34.936]
 [34.936]] [[0.584]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.96317298632215
siam score:  -0.8305613
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.756]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]] [[38.782]
 [31.622]
 [41.111]
 [41.111]
 [41.111]
 [41.111]
 [41.111]] [[0.695]
 [0.756]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]]
Printing some Q and Qe and total Qs values:  [[0.259]
 [0.26 ]
 [0.259]
 [0.257]
 [0.258]
 [0.259]
 [0.258]] [[43.831]
 [44.552]
 [45.894]
 [46.816]
 [45.532]
 [45.429]
 [45.949]] [[0.787]
 [0.806]
 [0.838]
 [0.86 ]
 [0.828]
 [0.827]
 [0.839]]
maxi score, test score, baseline:  -0.9876325581395348 -0.8390000000000001 -0.8390000000000001
probs:  [0.05439228764433223, 0.08539372910857745, 0.052383689211165645, 0.10131812234794607, 0.36603289193565103, 0.3404792797523275]
printing an ep nov before normalisation:  31.573636531829834
printing an ep nov before normalisation:  30.171985387385902
Printing some Q and Qe and total Qs values:  [[0.865]
 [0.922]
 [0.901]
 [0.88 ]
 [0.86 ]
 [0.868]
 [0.902]] [[35.083]
 [31.206]
 [33.794]
 [36.019]
 [35.104]
 [35.258]
 [38.528]] [[0.865]
 [0.922]
 [0.901]
 [0.88 ]
 [0.86 ]
 [0.868]
 [0.902]]
actor:  0 policy actor:  1  step number:  24 total reward:  0.7133333333333335  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  34.14402495451893
maxi score, test score, baseline:  -0.9827019323671498 -0.8390000000000001 -0.8390000000000001
probs:  [0.054436322658387326, 0.08546292395562916, 0.05242609410743431, 0.10140024095129917, 0.36632984504604627, 0.3399445732812039]
line 256 mcts: sample exp_bonus 25.681030345293802
actor:  0 policy actor:  1  step number:  25 total reward:  0.7066666666666668  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.9778190751445087 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9778190751445087 -0.8390000000000001 -0.8390000000000001
probs:  [0.054436322658387326, 0.08546292395562916, 0.05242609410743431, 0.10140024095129917, 0.36632984504604627, 0.3399445732812039]
actor:  0 policy actor:  1  step number:  29 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  29 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  30 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.9640595441595441 -0.8390000000000001 -0.8390000000000001
maxi score, test score, baseline:  -0.9640595441595441 -0.8390000000000001 -0.8390000000000001
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9640595441595441 -0.8390000000000001 -0.8390000000000001
probs:  [0.054436322658387326, 0.08546292395562916, 0.05242609410743431, 0.10140024095129917, 0.36632984504604627, 0.3399445732812039]
line 256 mcts: sample exp_bonus 18.57453528221561
actor:  0 policy actor:  1  step number:  41 total reward:  0.5600000000000002  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  41.969380374696286
maxi score, test score, baseline:  -0.9597295454545455 -0.8390000000000001 -0.8390000000000001
probs:  [0.054436322658387326, 0.08546292395562916, 0.05242609410743431, 0.10140024095129917, 0.36632984504604627, 0.3399445732812039]
Printing some Q and Qe and total Qs values:  [[0.599]
 [0.614]
 [0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.599]] [[37.914]
 [37.309]
 [37.914]
 [37.914]
 [37.914]
 [37.914]
 [37.914]] [[0.599]
 [0.614]
 [0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.599]]
maxi score, test score, baseline:  -0.9597295454545455 -0.8390000000000001 -0.8390000000000001
probs:  [0.05450222099474908, 0.08506605761764105, 0.0524895529748302, 0.1008109895350785, 0.3667742349564936, 0.3403569439212076]
printing an ep nov before normalisation:  40.41514840467054
printing an ep nov before normalisation:  25.502486340731522
printing an ep nov before normalisation:  33.62870170386017
maxi score, test score, baseline:  -0.9597295454545455 -0.8390000000000001 -0.8390000000000001
probs:  [0.05450222099474908, 0.08506605761764105, 0.0524895529748302, 0.1008109895350785, 0.3667742349564936, 0.3403569439212076]
Printing some Q and Qe and total Qs values:  [[0.774]
 [0.828]
 [0.794]
 [0.791]
 [0.766]
 [0.762]
 [0.753]] [[16.788]
 [17.744]
 [17.401]
 [17.594]
 [18.31 ]
 [18.369]
 [18.069]] [[0.774]
 [0.828]
 [0.794]
 [0.791]
 [0.766]
 [0.762]
 [0.753]]
maxi score, test score, baseline:  -0.9597295454545455 -0.8390000000000001 -0.8390000000000001
probs:  [0.05450222099474908, 0.08506605761764105, 0.0524895529748302, 0.1008109895350785, 0.3667742349564936, 0.3403569439212076]
Printing some Q and Qe and total Qs values:  [[0.143]
 [0.491]
 [0.141]
 [0.14 ]
 [0.141]
 [0.141]
 [0.141]] [[36.452]
 [36.865]
 [28.848]
 [26.639]
 [28.848]
 [28.848]
 [28.848]] [[0.592]
 [0.946]
 [0.488]
 [0.457]
 [0.488]
 [0.488]
 [0.488]]
siam score:  -0.83447814
printing an ep nov before normalisation:  0.06528614908205554
printing an ep nov before normalisation:  12.749226093292236
printing an ep nov before normalisation:  31.270640428076277
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]] [[13.765]
 [11.363]
 [11.363]
 [11.363]
 [11.363]
 [11.363]
 [11.363]] [[0.483]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]
 [0.42 ]]
printing an ep nov before normalisation:  42.58140563964844
maxi score, test score, baseline:  -0.959843342776204 -0.8390000000000001 -0.8390000000000001
probs:  [0.054286687434231984, 0.08426522229375731, 0.05250740329550861, 0.10106598820170845, 0.3668987596047699, 0.3409759391700237]
siam score:  -0.8363507
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.105]
 [0.044]
 [0.066]
 [0.077]
 [0.115]
 [0.051]] [[36.444]
 [36.479]
 [34.106]
 [34.368]
 [35.821]
 [34.487]
 [28.12 ]] [[0.573]
 [0.604]
 [0.477]
 [0.506]
 [0.557]
 [0.559]
 [0.321]]
printing an ep nov before normalisation:  23.928951723394334
printing an ep nov before normalisation:  44.48429337236659
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.959843342776204 -0.8390000000000001 -0.8390000000000001
probs:  [0.054286687434231984, 0.08426522229375731, 0.05250740329550861, 0.10106598820170845, 0.3668987596047699, 0.3409759391700237]
maxi score, test score, baseline:  -0.959843342776204 -0.8390000000000001 -0.8390000000000001
probs:  [0.054286687434231984, 0.08426522229375731, 0.05250740329550861, 0.10106598820170845, 0.3668987596047699, 0.3409759391700237]
Printing some Q and Qe and total Qs values:  [[-0.056]
 [-0.066]
 [-0.061]
 [-0.063]
 [-0.063]
 [-0.065]
 [-0.06 ]] [[24.574]
 [24.236]
 [24.05 ]
 [23.935]
 [23.6  ]
 [23.385]
 [23.685]] [[0.495]
 [0.477]
 [0.478]
 [0.473]
 [0.466]
 [0.459]
 [0.47 ]]
maxi score, test score, baseline:  -0.959843342776204 -0.8390000000000001 -0.8390000000000001
probs:  [0.054286687434231984, 0.08426522229375731, 0.05250740329550861, 0.10106598820170845, 0.3668987596047699, 0.3409759391700237]
Printing some Q and Qe and total Qs values:  [[0.165]
 [0.142]
 [0.174]
 [0.179]
 [0.169]
 [0.175]
 [0.183]] [[29.04 ]
 [27.399]
 [29.69 ]
 [30.704]
 [30.211]
 [30.279]
 [29.599]] [[1.957]
 [1.737]
 [2.044]
 [2.171]
 [2.102]
 [2.116]
 [2.042]]
maxi score, test score, baseline:  -0.959843342776204 -0.8390000000000001 -0.8390000000000001
probs:  [0.054286687434231984, 0.08426522229375731, 0.05250740329550861, 0.10106598820170845, 0.3668987596047699, 0.3409759391700237]
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]] [[12.917]
 [12.056]
 [12.056]
 [12.056]
 [12.056]
 [12.056]
 [12.056]] [[0.388]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]]
printing an ep nov before normalisation:  23.95113107200157
maxi score, test score, baseline:  -0.959843342776204 -0.8390000000000001 -0.8390000000000001
probs:  [0.05431250691098507, 0.08382893265903817, 0.052532374438934916, 0.1011141112774093, 0.3670736273607732, 0.3411384473528593]
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]] [[29.654]
 [29.654]
 [29.654]
 [29.654]
 [29.654]
 [29.654]
 [29.654]] [[49.575]
 [49.575]
 [49.575]
 [49.575]
 [49.575]
 [49.575]
 [49.575]]
maxi score, test score, baseline:  -0.9599564971751413 -0.504 -0.504
probs:  [0.053362737794068776, 0.08255293149194529, 0.05202502435240519, 0.10434636730548641, 0.3635438174520024, 0.34416912160409185]
maxi score, test score, baseline:  -0.9599564971751413 -0.504 -0.504
probs:  [0.053362737794068776, 0.08255293149194529, 0.05202502435240519, 0.10434636730548641, 0.3635438174520024, 0.34416912160409185]
maxi score, test score, baseline:  -0.9600690140845071 -0.504 -0.504
probs:  [0.053362737794068776, 0.08255293149194529, 0.05202502435240519, 0.10434636730548641, 0.3635438174520024, 0.34416912160409185]
printing an ep nov before normalisation:  33.62194994307255
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9601808988764046 -0.504 -0.504
printing an ep nov before normalisation:  34.09806412739119
Printing some Q and Qe and total Qs values:  [[-0.058]
 [-0.069]
 [-0.06 ]
 [-0.06 ]
 [-0.06 ]
 [-0.061]
 [-0.063]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.058]
 [-0.069]
 [-0.06 ]
 [-0.06 ]
 [-0.06 ]
 [-0.061]
 [-0.063]]
maxi score, test score, baseline:  -0.9602921568627452 -0.504 -0.504
probs:  [0.05331198606735365, 0.08272780901303056, 0.05196393260257343, 0.10332195900816697, 0.3631152552978839, 0.34555905801099146]
maxi score, test score, baseline:  -0.9602921568627452 -0.504 -0.504
probs:  [0.05331198606735365, 0.08272780901303056, 0.05196393260257343, 0.10332195900816697, 0.3631152552978839, 0.34555905801099146]
Printing some Q and Qe and total Qs values:  [[-0.121]
 [-0.101]
 [-0.103]
 [-0.103]
 [-0.123]
 [-0.103]
 [-0.103]] [[21.345]
 [42.23 ]
 [35.482]
 [35.482]
 [18.861]
 [35.482]
 [35.482]] [[0.368]
 [1.23 ]
 [0.955]
 [0.955]
 [0.267]
 [0.955]
 [0.955]]
siam score:  -0.8373667
maxi score, test score, baseline:  -0.9602921568627452 -0.504 -0.504
probs:  [0.05331198606735365, 0.08272780901303056, 0.05196393260257343, 0.10332195900816697, 0.3631152552978839, 0.34555905801099146]
maxi score, test score, baseline:  -0.9602921568627452 -0.504 -0.504
probs:  [0.05331198606735365, 0.08272780901303056, 0.05196393260257343, 0.10332195900816697, 0.3631152552978839, 0.34555905801099146]
siam score:  -0.83692926
printing an ep nov before normalisation:  25.31104247565823
printing an ep nov before normalisation:  1.2390686217116809
maxi score, test score, baseline:  -0.9602921568627452 -0.504 -0.504
probs:  [0.053347335541013736, 0.08278271204288101, 0.0519983859857444, 0.10272635373791336, 0.3633565400480801, 0.34578867264436747]
from probs:  [0.053347335541013736, 0.08278271204288101, 0.0519983859857444, 0.10272635373791336, 0.3633565400480801, 0.34578867264436747]
printing an ep nov before normalisation:  33.203670231446786
printing an ep nov before normalisation:  52.43359725937131
printing an ep nov before normalisation:  40.02660986668826
maxi score, test score, baseline:  -0.9602921568627452 -0.504 -0.504
probs:  [0.05326475172886592, 0.08278110560736499, 0.05191209118331292, 0.10277961270508197, 0.362751837308644, 0.34651060146673013]
maxi score, test score, baseline:  -0.9602921568627452 -0.504 -0.504
probs:  [0.05326475172886592, 0.08278110560736499, 0.05191209118331292, 0.10277961270508197, 0.362751837308644, 0.34651060146673013]
printing an ep nov before normalisation:  37.70326614379883
printing an ep nov before normalisation:  10.487717309145626
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9604027932960895 -0.504 -0.504
from probs:  [0.05326475172886592, 0.08278110560736499, 0.05191209118331292, 0.10277961270508197, 0.362751837308644, 0.34651060146673013]
printing an ep nov before normalisation:  25.980986571192233
maxi score, test score, baseline:  -0.9604027932960895 -0.504 -0.504
probs:  [0.053135939837687336, 0.08286281328642588, 0.0519632810129333, 0.10288109138882277, 0.36311033078074745, 0.34604654369338333]
maxi score, test score, baseline:  -0.9605128133704736 -0.504 -0.504
probs:  [0.053053159926953974, 0.08286143363541144, 0.05187729004359126, 0.10293452722452513, 0.3625077565359545, 0.3467658326335636]
printing an ep nov before normalisation:  4.451805936117665
siam score:  -0.83419126
maxi score, test score, baseline:  -0.9606222222222223 -0.504 -0.504
maxi score, test score, baseline:  -0.9606222222222223 -0.504 -0.504
probs:  [0.053053159926953974, 0.08286143363541144, 0.05187729004359126, 0.10293452722452513, 0.3625077565359545, 0.3467658326335636]
maxi score, test score, baseline:  -0.9606222222222223 -0.504 -0.504
probs:  [0.052884403602654596, 0.08287618708860603, 0.051886517895186626, 0.10295286044159825, 0.3625723811164391, 0.3468276498555154]
actions average: 
K:  3  action  0 :  tensor([0.4637, 0.0106, 0.1124, 0.0960, 0.1088, 0.1029, 0.1057],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0264, 0.7719, 0.0319, 0.0605, 0.0199, 0.0196, 0.0698],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2135, 0.0292, 0.1627, 0.1541, 0.1477, 0.1509, 0.1420],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.3020, 0.0025, 0.1253, 0.1279, 0.1743, 0.1342, 0.1338],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2353, 0.0451, 0.1438, 0.1474, 0.1575, 0.1379, 0.1330],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1481, 0.0346, 0.2040, 0.1478, 0.1208, 0.2048, 0.1398],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2084, 0.1145, 0.1361, 0.1156, 0.1224, 0.1053, 0.1976],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.07]
 [-0.07]
 [-0.07]
 [-0.07]
 [-0.07]
 [-0.07]
 [-0.07]] [[30.005]
 [30.005]
 [30.005]
 [30.005]
 [30.005]
 [30.005]
 [30.005]] [[0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]]
printing an ep nov before normalisation:  48.42212055142487
printing an ep nov before normalisation:  44.74510669708252
printing an ep nov before normalisation:  38.96819048400268
maxi score, test score, baseline:  -0.9606222222222223 -0.504 -0.504
probs:  [0.05290638906279691, 0.08249422890340541, 0.05190808745984076, 0.10299571326675376, 0.36272343730244305, 0.3469721440047601]
maxi score, test score, baseline:  -0.9606222222222223 -0.504 -0.504
printing an ep nov before normalisation:  35.709299701742154
UNIT TEST: sample policy line 217 mcts : [0.449 0.306 0.02  0.122 0.02  0.061 0.02 ]
maxi score, test score, baseline:  -0.9606222222222223 -0.504 -0.504
probs:  [0.05282339803430775, 0.08249185192464692, 0.05182237649182575, 0.10304919395414879, 0.3621228247899172, 0.34769035480515365]
maxi score, test score, baseline:  -0.9606222222222223 -0.504 -0.504
probs:  [0.0527832931905898, 0.08255624691963795, 0.051778745795695, 0.10318599713509748, 0.3618169096991802, 0.3478788072597996]
maxi score, test score, baseline:  -0.9606222222222223 -0.504 -0.504
probs:  [0.05274337005355563, 0.08262076793693782, 0.051735298685053666, 0.1033228877545196, 0.3615122807430317, 0.3480653948269016]
printing an ep nov before normalisation:  32.022056579589844
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9606222222222223 -0.504 -0.504
probs:  [0.05280756493597465, 0.08230770939403768, 0.05179826351906548, 0.10344879975055697, 0.3619532353655397, 0.3476844270348256]
maxi score, test score, baseline:  -0.9606222222222223 -0.504 -0.504
probs:  [0.05288488309312664, 0.08242832941095381, 0.05187410017064461, 0.10293633264460827, 0.3624843337571127, 0.3473920209235539]
printing an ep nov before normalisation:  37.51462097546319
printing an ep nov before normalisation:  34.85973809147499
maxi score, test score, baseline:  -0.9606222222222223 -0.504 -0.504
probs:  [0.05272106349416108, 0.08242328653292215, 0.05170484827389357, 0.10304150688448874, 0.36129831004845825, 0.3488109847660763]
printing an ep nov before normalisation:  41.91755364414496
maxi score, test score, baseline:  -0.960731024930748 -0.504 -0.504
probs:  [0.05250495051683517, 0.08248959029308255, 0.051663638670610613, 0.10252283351599385, 0.3610093516835834, 0.34980963531989445]
printing an ep nov before normalisation:  44.606429685651904
printing an ep nov before normalisation:  43.58410508716054
maxi score, test score, baseline:  -0.960731024930748 -0.504 -0.504
maxi score, test score, baseline:  -0.960731024930748 -0.504 -0.504
probs:  [0.05246549607846025, 0.08255389041340089, 0.05162127307705826, 0.10265645380574889, 0.36071229815076766, 0.3499905884745641]
siam score:  -0.83181524
printing an ep nov before normalisation:  44.265742116945255
Printing some Q and Qe and total Qs values:  [[-0.067]
 [-0.069]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]] [[41.069]
 [45.783]
 [41.069]
 [41.069]
 [41.069]
 [41.069]
 [41.069]] [[1.064]
 [1.373]
 [1.064]
 [1.064]
 [1.064]
 [1.064]
 [1.064]]
maxi score, test score, baseline:  -0.960731024930748 -0.504 -0.504
probs:  [0.05250765027166977, 0.08262028112462275, 0.05166274723976325, 0.10273903734315915, 0.3610027483842215, 0.34946753563656363]
maxi score, test score, baseline:  -0.9608392265193371 -0.504 -0.504
probs:  [0.05250765027166977, 0.08262028112462275, 0.05166274723976325, 0.10273903734315915, 0.3610027483842215, 0.34946753563656363]
maxi score, test score, baseline:  -0.9608392265193371 -0.504 -0.504
probs:  [0.05250765027166977, 0.08262028112462275, 0.05166274723976325, 0.10273903734315915, 0.3610027483842215, 0.34946753563656363]
printing an ep nov before normalisation:  54.895219802856445
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  41.01047182333682
maxi score, test score, baseline:  -0.9608392265193371 -0.504 -0.504
probs:  [0.052460206631749225, 0.0826719288265511, 0.051612523285642566, 0.10220738456192148, 0.36065066168724086, 0.3503972950068947]
using another actor
from probs:  [0.052460206631749225, 0.0826719288265511, 0.051612523285642566, 0.10220738456192148, 0.36065066168724086, 0.3503972950068947]
maxi score, test score, baseline:  -0.9609468319559229 -0.504 -0.504
probs:  [0.052460206631749225, 0.0826719288265511, 0.051612523285642566, 0.10220738456192148, 0.36065066168724086, 0.3503972950068947]
siam score:  -0.8298832
printing an ep nov before normalisation:  21.838009357452393
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9609468319559229 -0.504 -0.504
probs:  [0.05237903594886315, 0.08267010424724569, 0.05152912630231093, 0.10225686663386485, 0.3600662599390311, 0.35109860692868444]
maxi score, test score, baseline:  -0.9609468319559229 -0.504 -0.504
probs:  [0.05237903594886315, 0.08267010424724569, 0.05152912630231093, 0.10225686663386485, 0.3600662599390311, 0.35109860692868444]
maxi score, test score, baseline:  -0.9609468319559229 -0.504 -0.504
probs:  [0.05237903594886315, 0.08267010424724569, 0.05152912630231093, 0.10225686663386485, 0.3600662599390311, 0.35109860692868444]
printing an ep nov before normalisation:  39.2627929730036
printing an ep nov before normalisation:  39.355670187162595
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]] [[23.682]
 [24.242]
 [24.242]
 [24.242]
 [24.242]
 [24.242]
 [24.242]] [[0.403]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]]
maxi score, test score, baseline:  -0.9610538461538461 -0.504 -0.504
probs:  [0.05222130626237063, 0.08279864478701958, 0.05136336441885434, 0.10257051477185918, 0.35890432813308826, 0.35214184162680795]
printing an ep nov before normalisation:  28.9975604506682
maxi score, test score, baseline:  -0.9610538461538461 -0.504 -0.504
printing an ep nov before normalisation:  35.56691989091128
printing an ep nov before normalisation:  37.25731566628237
Printing some Q and Qe and total Qs values:  [[-0.07 ]
 [-0.041]
 [-0.06 ]
 [-0.05 ]
 [-0.059]
 [-0.078]
 [-0.05 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.07 ]
 [-0.041]
 [-0.06 ]
 [-0.05 ]
 [-0.059]
 [-0.078]
 [-0.05 ]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  9.387257462157805
maxi score, test score, baseline:  -0.9611602739726028 -0.504 -0.504
probs:  [0.05222130626237063, 0.08279864478701958, 0.05136336441885434, 0.10257051477185918, 0.35890432813308826, 0.35214184162680795]
maxi score, test score, baseline:  -0.9611602739726028 -0.504 -0.504
probs:  [0.05214112461399729, 0.08279718248634753, 0.05128097405561091, 0.10261995385054641, 0.35832697931527163, 0.3528337856782262]
printing an ep nov before normalisation:  45.047343581333486
maxi score, test score, baseline:  -0.9612661202185793 -0.504 -0.504
probs:  [0.05214112461399729, 0.08279718248634753, 0.05128097405561091, 0.10261995385054641, 0.35832697931527163, 0.3528337856782262]
printing an ep nov before normalisation:  40.737444743504014
printing an ep nov before normalisation:  29.46801750863792
maxi score, test score, baseline:  -0.9612661202185793 -0.504 -0.504
probs:  [0.0520611652910232, 0.0827957242403013, 0.05119881214202864, 0.10266925584595926, 0.3577512313538732, 0.3535238111268145]
maxi score, test score, baseline:  -0.9612661202185793 -0.504 -0.504
maxi score, test score, baseline:  -0.9612661202185793 -0.504 -0.504
probs:  [0.0520611652910232, 0.0827957242403013, 0.05119881214202864, 0.10266925584595926, 0.3577512313538732, 0.3535238111268145]
from probs:  [0.0520611652910232, 0.0827957242403013, 0.05119881214202864, 0.10266925584595926, 0.3577512313538732, 0.3535238111268145]
printing an ep nov before normalisation:  28.50793513857147
from probs:  [0.05198142737004219, 0.08279427003204035, 0.05111687772926486, 0.10271842132745922, 0.35717707759989714, 0.35421192594129614]
printing an ep nov before normalisation:  17.715447801677087
siam score:  -0.8326138
maxi score, test score, baseline:  -0.9614760869565218 -0.504 -0.504
probs:  [0.05198142737004219, 0.08279427003204035, 0.05111687772926486, 0.10271842132745922, 0.35717707759989714, 0.35421192594129614]
printing an ep nov before normalisation:  34.59841927107787
printing an ep nov before normalisation:  59.696513147233404
printing an ep nov before normalisation:  33.668131828308105
printing an ep nov before normalisation:  35.317466394363855
maxi score, test score, baseline:  -0.9616837837837838 -0.504 -0.504
maxi score, test score, baseline:  -0.9616837837837838 -0.504 -0.504
probs:  [0.05194407408925077, 0.08286014379058768, 0.051076628094483725, 0.10285104358993799, 0.35689484741473104, 0.3543732630210088]
printing an ep nov before normalisation:  27.442821055253116
printing an ep nov before normalisation:  47.55598656107282
maxi score, test score, baseline:  -0.9616837837837838 -0.504 -0.504
probs:  [0.05194407408925077, 0.08286014379058768, 0.051076628094483725, 0.10285104358993799, 0.35689484741473104, 0.3543732630210088]
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.72 ]
 [0.53 ]
 [0.776]
 [0.53 ]
 [0.53 ]
 [0.53 ]] [[35.409]
 [31.96 ]
 [35.409]
 [30.266]
 [35.409]
 [35.409]
 [35.409]] [[0.53 ]
 [0.72 ]
 [0.53 ]
 [0.776]
 [0.53 ]
 [0.53 ]
 [0.53 ]]
Printing some Q and Qe and total Qs values:  [[0.705]
 [0.263]
 [0.255]
 [0.705]
 [0.705]
 [0.165]
 [0.705]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.705]
 [0.263]
 [0.255]
 [0.705]
 [0.705]
 [0.165]
 [0.705]]
maxi score, test score, baseline:  -0.9619911528150135 -0.504 -0.504
probs:  [0.0520279689520206, 0.08299409958766514, 0.051159118342844964, 0.10301736970870491, 0.3574725346896019, 0.35332890871916256]
from probs:  [0.0520279689520206, 0.08299409958766514, 0.051159118342844964, 0.10301736970870491, 0.3574725346896019, 0.35332890871916256]
printing an ep nov before normalisation:  52.43066551854189
maxi score, test score, baseline:  -0.9619911528150135 -0.504 -0.504
Printing some Q and Qe and total Qs values:  [[-0.084]
 [-0.066]
 [-0.091]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.074]] [[27.129]
 [29.33 ]
 [23.972]
 [27.2  ]
 [26.309]
 [25.95 ]
 [30.196]] [[0.95 ]
 [1.053]
 [0.823]
 [0.967]
 [0.934]
 [0.92 ]
 [1.078]]
maxi score, test score, baseline:  -0.9619911528150135 -0.504 -0.504
probs:  [0.05194879597476371, 0.08299315838064793, 0.05107775033121231, 0.10306701460448454, 0.35690234860796355, 0.35401093210092793]
maxi score, test score, baseline:  -0.9619911528150135 -0.504 -0.504
maxi score, test score, baseline:  -0.9619911528150135 -0.504 -0.504
probs:  [0.05194879597476371, 0.08299315838064793, 0.05107775033121231, 0.10306701460448454, 0.35690234860796355, 0.35401093210092793]
maxi score, test score, baseline:  -0.9619911528150135 -0.504 -0.504
probs:  [0.051869840347661636, 0.08299221975748569, 0.050996605695667524, 0.1031165232122761, 0.35633372783354755, 0.35469108315336156]
siam score:  -0.82827044
maxi score, test score, baseline:  -0.9619911528150135 -0.504 -0.504
maxi score, test score, baseline:  -0.9619911528150135 -0.504 -0.504
probs:  [0.05179110117691821, 0.08299128370755299, 0.050915683517634376, 0.10316589609252856, 0.3557666659294338, 0.35536936957593207]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.962092513368984 -0.504 -0.504
probs:  [0.05179110117691821, 0.08299128370755299, 0.050915683517634376, 0.10316589609252856, 0.3557666659294338, 0.35536936957593207]
from probs:  [0.05179110117691821, 0.08299128370755299, 0.050915683517634376, 0.10316589609252856, 0.3557666659294338, 0.35536936957593207]
maxi score, test score, baseline:  -0.9621933333333333 -0.504 -0.504
probs:  [0.05179110117691821, 0.08299128370755299, 0.050915683517634376, 0.10316589609252856, 0.3557666659294338, 0.35536936957593207]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.193]
 [0.16 ]
 [0.184]
 [0.184]
 [0.184]
 [0.186]
 [0.185]] [[21.419]
 [28.651]
 [16.624]
 [16.442]
 [16.619]
 [16.631]
 [16.207]] [[0.193]
 [0.16 ]
 [0.184]
 [0.184]
 [0.184]
 [0.186]
 [0.185]]
printing an ep nov before normalisation:  52.934339857997514
maxi score, test score, baseline:  -0.9622936170212767 -0.504 -0.504
probs:  [0.051813209348002466, 0.08259911390165366, 0.050937417033671425, 0.10320999128433436, 0.3559188674089591, 0.3555214010233791]
maxi score, test score, baseline:  -0.9622936170212767 -0.504 -0.504
maxi score, test score, baseline:  -0.9622936170212767 -0.504 -0.504
probs:  [0.051813209348002466, 0.08259911390165366, 0.050937417033671425, 0.10320999128433436, 0.3559188674089591, 0.3555214010233791]
maxi score, test score, baseline:  -0.9623933687002653 -0.504 -0.504
probs:  [0.051855101143602214, 0.08266596075250167, 0.05097859891199224, 0.1032935453130697, 0.35620726726102997, 0.3549995266178042]
maxi score, test score, baseline:  -0.9623933687002653 -0.504 -0.504
probs:  [0.051889231234046616, 0.08272042222956905, 0.051012150618517056, 0.10270226865199024, 0.35644223243486445, 0.35523369483101275]
maxi score, test score, baseline:  -0.9623933687002653 -0.504 -0.504
probs:  [0.051889231234046616, 0.08272042222956905, 0.051012150618517056, 0.10270226865199024, 0.35644223243486445, 0.35523369483101275]
maxi score, test score, baseline:  -0.9623933687002653 -0.504 -0.504
maxi score, test score, baseline:  -0.9623933687002653 -0.504 -0.504
probs:  [0.051814678284413954, 0.08272164482277086, 0.05093544201879091, 0.10275260174694704, 0.35587094458815055, 0.35590468853892665]
UNIT TEST: sample policy line 217 mcts : [0.143 0.122 0.224 0.265 0.082 0.102 0.061]
maxi score, test score, baseline:  -0.9623933687002653 -0.504 -0.504
probs:  [0.05184824829633995, 0.0827752906492207, 0.05096844091727006, 0.10217022174698215, 0.3561020162604077, 0.35613578212977937]
maxi score, test score, baseline:  -0.9623933687002653 -0.504 -0.504
probs:  [0.05184824829633995, 0.0827752906492207, 0.05096844091727006, 0.10217022174698215, 0.3561020162604077, 0.35613578212977937]
printing an ep nov before normalisation:  40.33800489756739
maxi score, test score, baseline:  -0.9623933687002653 -0.504 -0.504
probs:  [0.05184824829633995, 0.0827752906492207, 0.05096844091727006, 0.10217022174698215, 0.3561020162604077, 0.35613578212977937]
maxi score, test score, baseline:  -0.9623933687002653 -0.504 -0.504
printing an ep nov before normalisation:  52.85322946982599
maxi score, test score, baseline:  -0.9623933687002653 -0.504 -0.504
probs:  [0.051870111560843896, 0.08238780241131495, 0.050989932230629705, 0.10221335934499802, 0.35625250715391893, 0.35628628729829454]
actor:  1 policy actor:  1  step number:  60 total reward:  0.19333333333333247  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  57 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.023]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]] [[27.675]
 [27.675]
 [27.675]
 [27.675]
 [27.675]
 [27.675]
 [27.675]] [[1.616]
 [1.616]
 [1.616]
 [1.616]
 [1.616]
 [1.616]
 [1.616]]
maxi score, test score, baseline:  -0.9623933687002653 -0.504 -0.504
probs:  [0.05213957118771028, 0.08176215945177688, 0.051285208056986234, 0.10100622064336075, 0.3583572162071439, 0.355449624453022]
maxi score, test score, baseline:  -0.9623933687002653 -0.504 -0.504
printing an ep nov before normalisation:  52.33920867550633
printing an ep nov before normalisation:  19.111948678824717
printing an ep nov before normalisation:  45.61329447560669
maxi score, test score, baseline:  -0.9624925925925927 -0.504 -0.504
printing an ep nov before normalisation:  28.61925400903785
Printing some Q and Qe and total Qs values:  [[-0.018]
 [-0.018]
 [-0.018]
 [-0.055]
 [-0.018]
 [-0.018]
 [-0.018]] [[34.737]
 [34.737]
 [34.737]
 [68.82 ]
 [34.737]
 [34.737]
 [34.737]] [[0.085]
 [0.085]
 [0.085]
 [1.405]
 [0.085]
 [0.085]
 [0.085]]
printing an ep nov before normalisation:  41.510139780544186
actor:  1 policy actor:  1  step number:  57 total reward:  0.2533333333333325  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9625912928759895 -0.504 -0.504
probs:  [0.05296315427440644, 0.08186639847407653, 0.052129538184469214, 0.10064314394464216, 0.3642736539295791, 0.3481241111928265]
printing an ep nov before normalisation:  40.44190986606475
maxi score, test score, baseline:  -0.9625912928759895 -0.504 -0.504
probs:  [0.05281258199629987, 0.08185896851372151, 0.05197483745149425, 0.10072870516714261, 0.3631895935208113, 0.3494353133505306]
maxi score, test score, baseline:  -0.9625912928759895 -0.504 -0.504
probs:  [0.05281258199629987, 0.08185896851372151, 0.05197483745149425, 0.10072870516714261, 0.3631895935208113, 0.3494353133505306]
Printing some Q and Qe and total Qs values:  [[-0.109]
 [-0.097]
 [-0.038]
 [-0.091]
 [-0.038]
 [-0.038]
 [-0.038]] [[46.833]
 [44.718]
 [43.477]
 [43.303]
 [43.477]
 [43.477]
 [43.477]] [[1.304]
 [1.222]
 [1.225]
 [1.165]
 [1.225]
 [1.225]
 [1.225]]
actions average: 
K:  3  action  0 :  tensor([0.4890, 0.0056, 0.1010, 0.1009, 0.1005, 0.1034, 0.0996],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0699, 0.7336, 0.0332, 0.0532, 0.0429, 0.0290, 0.0381],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1263, 0.0290, 0.4117, 0.0985, 0.0776, 0.1359, 0.1210],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2589, 0.0938, 0.1020, 0.1844, 0.1497, 0.0996, 0.1116],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2622, 0.0247, 0.1164, 0.1231, 0.2180, 0.1044, 0.1511],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1906, 0.0802, 0.1198, 0.0961, 0.0904, 0.3205, 0.1024],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.3432, 0.0016, 0.1152, 0.1079, 0.1680, 0.1162, 0.1479],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9625912928759895 -0.504 -0.504
probs:  [0.05281258199629987, 0.08185896851372151, 0.05197483745149425, 0.10072870516714261, 0.3631895935208113, 0.3494353133505306]
Printing some Q and Qe and total Qs values:  [[-0.032]
 [-0.021]
 [-0.032]
 [-0.024]
 [-0.026]
 [-0.027]
 [-0.026]] [[36.322]
 [38.261]
 [36.315]
 [39.364]
 [38.45 ]
 [38.373]
 [37.734]] [[1.122]
 [1.25 ]
 [1.122]
 [1.313]
 [1.257]
 [1.25 ]
 [1.213]]
printing an ep nov before normalisation:  48.73370069123596
siam score:  -0.82313037
maxi score, test score, baseline:  -0.9625912928759895 -0.504 -0.504
probs:  [0.052662783254187585, 0.08185157672335265, 0.051820931463653006, 0.1008138268353894, 0.3621111022629968, 0.3507397794604206]
maxi score, test score, baseline:  -0.9625912928759895 -0.504 -0.504
probs:  [0.052662783254187585, 0.08185157672335265, 0.051820931463653006, 0.1008138268353894, 0.3621111022629968, 0.3507397794604206]
actor:  1 policy actor:  1  step number:  59 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9625912928759895 -0.504 -0.504
probs:  [0.05320048437425789, 0.08187810948260431, 0.052373375502106495, 0.1005082834395426, 0.36598233608736497, 0.34605741111412375]
printing an ep nov before normalisation:  34.01112254489718
maxi score, test score, baseline:  -0.9625912928759895 -0.504 -0.504
printing an ep nov before normalisation:  30.944248880722476
maxi score, test score, baseline:  -0.9626894736842105 -0.504 -0.504
probs:  [0.05320048437425789, 0.08187810948260431, 0.052373375502106495, 0.1005082834395426, 0.36598233608736497, 0.34605741111412375]
printing an ep nov before normalisation:  69.00661082114824
printing an ep nov before normalisation:  31.834118048455377
siam score:  -0.82883716
printing an ep nov before normalisation:  36.36704518672472
printing an ep nov before normalisation:  27.082235471279567
maxi score, test score, baseline:  -0.9627871391076116 -0.504 -0.504
probs:  [0.05312704588697826, 0.08187448567445127, 0.05229792344709614, 0.10055001414076623, 0.36545360823957823, 0.34669692261112983]
printing an ep nov before normalisation:  11.325575911953473
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.9627871391076116 -0.504 -0.504
probs:  [0.05315890371719619, 0.08192362535132411, 0.05232928284103696, 0.10000970012244964, 0.36567322469445046, 0.3469052632735427]
printing an ep nov before normalisation:  43.41950137454821
printing an ep nov before normalisation:  27.890886359195726
Printing some Q and Qe and total Qs values:  [[0.175]
 [0.536]
 [0.175]
 [0.175]
 [0.175]
 [0.175]
 [0.175]] [[32.542]
 [35.464]
 [32.542]
 [32.542]
 [32.542]
 [32.542]
 [32.542]] [[1.749]
 [2.251]
 [1.749]
 [1.749]
 [1.749]
 [1.749]
 [1.749]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.0933333333333326  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]] [[21.249]
 [17.624]
 [17.624]
 [17.624]
 [17.624]
 [17.624]
 [17.624]] [[0.463]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]]
printing an ep nov before normalisation:  44.38814558658292
maxi score, test score, baseline:  -0.9627871391076116 -0.504 -0.504
probs:  [0.052877720150568394, 0.08148990875758497, 0.05205249857245227, 0.0994800770301707, 0.3637348460400125, 0.3503649494492112]
maxi score, test score, baseline:  -0.9628842931937174 -0.504 -0.504
probs:  [0.052715168691719695, 0.08150388156277173, 0.052061415576596264, 0.09949713960143876, 0.36379729370216696, 0.3504251008653067]
maxi score, test score, baseline:  -0.9628842931937174 -0.504 -0.504
probs:  [0.052715168691719695, 0.08150388156277173, 0.052061415576596264, 0.09949713960143876, 0.36379729370216696, 0.3504251008653067]
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.484]
 [0.169]
 [0.169]
 [0.213]
 [0.169]
 [0.338]] [[27.791]
 [31.228]
 [26.534]
 [26.534]
 [28.552]
 [26.534]
 [32.939]] [[1.466]
 [2.087]
 [1.527]
 [1.527]
 [1.676]
 [1.527]
 [2.031]]
printing an ep nov before normalisation:  26.496023740836172
printing an ep nov before normalisation:  22.538596461110675
from probs:  [0.05274605780288222, 0.08155168252084014, 0.052091920642364085, 0.09896853309411173, 0.3640109271165186, 0.35063087882328325]
maxi score, test score, baseline:  -0.9628842931937174 -0.504 -0.504
probs:  [0.05278735697381108, 0.08161559306612724, 0.052132706339262695, 0.09904611523764778, 0.36429655796063737, 0.3501216704225139]
actor:  1 policy actor:  1  step number:  50 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8289258
printing an ep nov before normalisation:  4.477445810709355
actions average: 
K:  3  action  0 :  tensor([0.5049, 0.0189, 0.0827, 0.0909, 0.0937, 0.1082, 0.1008],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0443, 0.7508, 0.0374, 0.0566, 0.0146, 0.0187, 0.0777],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1349, 0.1541, 0.2500, 0.1091, 0.0833, 0.1616, 0.1069],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.3038, 0.1126, 0.0867, 0.1326, 0.1185, 0.1244, 0.1215],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.3430, 0.0375, 0.0863, 0.1020, 0.2053, 0.1196, 0.1062],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2287, 0.0046, 0.0908, 0.1331, 0.1487, 0.2829, 0.1112],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1773, 0.1485, 0.0950, 0.1806, 0.1103, 0.1239, 0.1644],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  28.344194889068604
Printing some Q and Qe and total Qs values:  [[0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]
 [0.237]] [[45.212]
 [45.212]
 [45.212]
 [45.212]
 [45.212]
 [45.212]
 [45.212]] [[2.201]
 [2.201]
 [2.201]
 [2.201]
 [2.201]
 [2.201]
 [2.201]]
using another actor
from probs:  [0.05217888757727845, 0.08067398552175556, 0.051531802063925006, 0.09790308115252307, 0.3600882984860976, 0.3576239451984204]
maxi score, test score, baseline:  -0.9630770833333334 -0.504 -0.504
maxi score, test score, baseline:  -0.9630770833333334 -0.504 -0.504
probs:  [0.05217888757727845, 0.08067398552175556, 0.051531802063925006, 0.09790308115252307, 0.3600882984860976, 0.3576239451984204]
maxi score, test score, baseline:  -0.9630770833333334 -0.504 -0.504
probs:  [0.05217888757727845, 0.08067398552175556, 0.051531802063925006, 0.09790308115252307, 0.3600882984860976, 0.3576239451984204]
printing an ep nov before normalisation:  18.601115982779675
maxi score, test score, baseline:  -0.9630770833333334 -0.504 -0.504
probs:  [0.05217888757727845, 0.08067398552175556, 0.051531802063925006, 0.09790308115252307, 0.3600882984860976, 0.3576239451984204]
printing an ep nov before normalisation:  40.08303928900357
printing an ep nov before normalisation:  49.30860116933183
maxi score, test score, baseline:  -0.9630770833333334 -0.504 -0.504
probs:  [0.05217888757727845, 0.08067398552175556, 0.051531802063925006, 0.09790308115252307, 0.3600882984860976, 0.3576239451984204]
printing an ep nov before normalisation:  0.1415468946700571
maxi score, test score, baseline:  -0.9630770833333334 -0.504 -0.504
probs:  [0.05217888757727845, 0.08067398552175556, 0.051531802063925006, 0.09790308115252307, 0.3600882984860976, 0.3576239451984204]
actor:  1 policy actor:  1  step number:  59 total reward:  0.14666666666666606  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  32.28021419864092
maxi score, test score, baseline:  -0.9630770833333334 -0.504 -0.504
probs:  [0.052747633188939456, 0.08072308948823538, 0.05211234804143796, 0.09763799228429139, 0.364156396272618, 0.35262254072447774]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  79 total reward:  0.10666666666666635  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9630770833333334 -0.504 -0.504
probs:  [0.05239429439656248, 0.08028759191776157, 0.05176087496488358, 0.0971528187503415, 0.36169463313781575, 0.3567097868326351]
printing an ep nov before normalisation:  38.514576770383414
printing an ep nov before normalisation:  31.194868842589557
Printing some Q and Qe and total Qs values:  [[-0.108]
 [-0.105]
 [-0.093]
 [-0.101]
 [-0.096]
 [-0.089]
 [-0.1  ]] [[38.497]
 [41.121]
 [38.257]
 [37.69 ]
 [38.29 ]
 [38.378]
 [39.641]] [[1.335]
 [1.562]
 [1.331]
 [1.274]
 [1.33 ]
 [1.344]
 [1.441]]
maxi score, test score, baseline:  -0.9631727272727273 -0.504 -0.504
probs:  [0.052403576171617665, 0.08040759020221976, 0.05176764251628306, 0.09733975996414586, 0.3617417211618908, 0.3563397099838428]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  57 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9631727272727273 -0.504 -0.504
probs:  [0.052411177260285706, 0.07943852863060165, 0.051797422306008235, 0.09578017526444946, 0.3619531218050579, 0.35861957473359707]
printing an ep nov before normalisation:  58.01956452257064
maxi score, test score, baseline:  -0.9631727272727273 -0.504 -0.504
probs:  [0.05226146594564241, 0.07945106703647217, 0.05180559057014047, 0.0957952973336216, 0.3620103266146827, 0.3586762524994407]
printing an ep nov before normalisation:  37.38933669350901
from probs:  [0.05226146594564241, 0.07945106703647217, 0.05180559057014047, 0.0957952973336216, 0.3620103266146827, 0.3586762524994407]
maxi score, test score, baseline:  -0.9631727272727273 -0.504 -0.504
probs:  [0.05230271926863253, 0.07951383770616577, 0.05184648312180204, 0.09587100252444812, 0.362296709448906, 0.3581692479300455]
printing an ep nov before normalisation:  31.410679817199707
maxi score, test score, baseline:  -0.9631727272727273 -0.504 -0.504
probs:  [0.05230271926863253, 0.07951383770616577, 0.05184648312180204, 0.09587100252444812, 0.362296709448906, 0.3581692479300455]
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.304650810077035
printing an ep nov before normalisation:  33.89794588088989
siam score:  -0.8237412
maxi score, test score, baseline:  -0.9632678756476685 -0.504 -0.504
maxi score, test score, baseline:  -0.9632678756476685 -0.504 -0.504
probs:  [0.05230271926863253, 0.07951383770616577, 0.05184648312180204, 0.09587100252444812, 0.362296709448906, 0.3581692479300455]
printing an ep nov before normalisation:  67.77374267578125
maxi score, test score, baseline:  -0.9632678756476685 -0.504 -0.504
probs:  [0.05230271926863253, 0.07951383770616577, 0.05184648312180204, 0.09587100252444812, 0.362296709448906, 0.3581692479300455]
maxi score, test score, baseline:  -0.9632678756476685 -0.504 -0.504
printing an ep nov before normalisation:  42.263646339344795
maxi score, test score, baseline:  -0.9632678756476685 -0.504 -0.504
printing an ep nov before normalisation:  34.517636474434745
printing an ep nov before normalisation:  41.79620420795794
printing an ep nov before normalisation:  48.38615786848662
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]] [[42.133]
 [43.119]
 [43.119]
 [43.119]
 [43.119]
 [43.119]
 [43.119]] [[0.465]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]]
maxi score, test score, baseline:  -0.9632678756476685 -0.504 -0.504
probs:  [0.05216431038724395, 0.0796423601485402, 0.051861874322024694, 0.09606640907633623, 0.3624042046870729, 0.35786084137878205]
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]] [[21.051]
 [21.24 ]
 [21.24 ]
 [21.24 ]
 [21.24 ]
 [21.24 ]
 [21.24 ]] [[0.482]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]
 [0.495]]
maxi score, test score, baseline:  -0.9632678756476685 -0.504 -0.504
probs:  [0.05216431038724395, 0.0796423601485402, 0.051861874322024694, 0.09606640907633623, 0.3624042046870729, 0.35786084137878205]
printing an ep nov before normalisation:  29.87564584163539
printing an ep nov before normalisation:  44.86693859100342
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.447]
 [0.465]
 [0.462]
 [0.485]
 [0.472]
 [0.469]] [[19.425]
 [25.452]
 [16.074]
 [14.894]
 [14.553]
 [14.642]
 [18.327]] [[0.666]
 [0.447]
 [0.465]
 [0.462]
 [0.485]
 [0.472]
 [0.469]]
printing an ep nov before normalisation:  47.04698539582282
maxi score, test score, baseline:  -0.9632678756476685 -0.504 -0.504
probs:  [0.0519460718162759, 0.07964580727724978, 0.051795933735501495, 0.09611087878604341, 0.3619421096131373, 0.35855919877179215]
Printing some Q and Qe and total Qs values:  [[-0.074]
 [-0.07 ]
 [-0.081]
 [-0.081]
 [-0.081]
 [-0.08 ]
 [-0.081]] [[36.181]
 [37.482]
 [36.659]
 [36.659]
 [36.659]
 [35.458]
 [36.659]] [[1.485]
 [1.597]
 [1.518]
 [1.518]
 [1.518]
 [1.42 ]
 [1.518]]
Printing some Q and Qe and total Qs values:  [[-0.074]
 [-0.08 ]
 [-0.071]
 [-0.081]
 [-0.081]
 [-0.08 ]
 [-0.081]] [[36.181]
 [47.9  ]
 [35.654]
 [36.659]
 [36.659]
 [35.458]
 [36.659]] [[0.651]
 [1.095]
 [0.634]
 [0.663]
 [0.663]
 [0.618]
 [0.663]]
printing an ep nov before normalisation:  50.23859763979976
maxi score, test score, baseline:  -0.9632678756476685 -0.504 -0.504
probs:  [0.0519460718162759, 0.07964580727724978, 0.051795933735501495, 0.09611087878604341, 0.3619421096131373, 0.35855919877179215]
Printing some Q and Qe and total Qs values:  [[-0.052]
 [-0.057]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]] [[41.66 ]
 [46.71 ]
 [45.709]
 [44.677]
 [45.317]
 [44.075]
 [45.036]] [[0.454]
 [0.586]
 [0.563]
 [0.535]
 [0.553]
 [0.519]
 [0.545]]
maxi score, test score, baseline:  -0.9633625322997417 -0.504 -0.504
printing an ep nov before normalisation:  64.59841278400404
printing an ep nov before normalisation:  54.58116812833935
maxi score, test score, baseline:  -0.9633625322997417 -0.504 -0.504
probs:  [0.05189188519777585, 0.07929717606914706, 0.05174134118600981, 0.09617610133939505, 0.3615594887053531, 0.35933400750231914]
printing an ep nov before normalisation:  46.47066187822928
maxi score, test score, baseline:  -0.9633625322997417 -0.504 -0.504
maxi score, test score, baseline:  -0.9633625322997417 -0.504 -0.504
probs:  [0.05189188519777585, 0.07929717606914706, 0.05174134118600981, 0.09617610133939505, 0.3615594887053531, 0.35933400750231914]
actions average: 
K:  0  action  0 :  tensor([0.4648, 0.0051, 0.0952, 0.1185, 0.1303, 0.0994, 0.0867],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0497, 0.8374, 0.0123, 0.0194, 0.0233, 0.0121, 0.0458],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2192, 0.0157, 0.2727, 0.1345, 0.1116, 0.1264, 0.1200],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.3941, 0.0164, 0.1107, 0.1314, 0.1225, 0.1139, 0.1110],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.3032, 0.0261, 0.1167, 0.1634, 0.1384, 0.1391, 0.1132],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2236, 0.0044, 0.0934, 0.1278, 0.1120, 0.3599, 0.0789],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.3063, 0.0147, 0.1269, 0.1262, 0.1299, 0.1392, 0.1568],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9633625322997417 -0.504 -0.504
printing an ep nov before normalisation:  37.31925989081568
maxi score, test score, baseline:  -0.9633625322997417 -0.504 -0.504
probs:  [0.05201380122250122, 0.07948364446481489, 0.051862902608664496, 0.09640232755592561, 0.36241081782422535, 0.35782650632386837]
printing an ep nov before normalisation:  30.616309720454577
printing an ep nov before normalisation:  42.322322010043095
printing an ep nov before normalisation:  24.615465160059745
actor:  1 policy actor:  1  step number:  72 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  1.667
siam score:  -0.81937116
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9633625322997417 -0.504 -0.504
probs:  [0.05259930514316359, 0.07939437446727243, 0.05245376143791774, 0.09638430051326943, 0.3665516930546371, 0.35261656538373976]
printing an ep nov before normalisation:  40.74734279087612
siam score:  -0.81753397
Printing some Q and Qe and total Qs values:  [[-0.083]
 [-0.067]
 [-0.08 ]
 [-0.077]
 [-0.078]
 [-0.08 ]
 [-0.081]] [[30.377]
 [32.864]
 [28.34 ]
 [23.326]
 [23.452]
 [24.215]
 [27.354]] [[0.871]
 [1.074]
 [0.722]
 [0.349]
 [0.357]
 [0.413]
 [0.648]]
printing an ep nov before normalisation:  57.83513552771469
printing an ep nov before normalisation:  39.8743913775383
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9633625322997417 -0.504 -0.504
probs:  [0.05259930514316359, 0.07939437446727243, 0.05245376143791774, 0.09638430051326943, 0.3665516930546371, 0.35261656538373976]
printing an ep nov before normalisation:  36.293101035578104
maxi score, test score, baseline:  -0.9634567010309278 -0.504 -0.504
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9634567010309278 -0.504 -0.504
probs:  [0.05254573419847488, 0.07906024785754645, 0.05239979805631088, 0.0964487892855235, 0.366173484317317, 0.35337194628482727]
printing an ep nov before normalisation:  37.02311652786312
printing an ep nov before normalisation:  31.465269168575716
Printing some Q and Qe and total Qs values:  [[-0.097]
 [-0.088]
 [-0.086]
 [-0.086]
 [-0.086]
 [-0.086]
 [-0.086]] [[36.186]
 [43.816]
 [33.887]
 [33.887]
 [33.887]
 [33.887]
 [33.887]] [[0.821]
 [1.186]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9634567010309278 -0.504 -0.504
probs:  [0.05251390325407403, 0.07911058660699292, 0.0523675148491125, 0.09655301592233465, 0.36594710945520753, 0.3535078699122783]
maxi score, test score, baseline:  -0.9634567010309278 -0.504 -0.504
using explorer policy with actor:  1
from probs:  [0.052636116596233545, 0.07920989871376677, 0.0523463257593764, 0.09663730911963962, 0.36579943923565933, 0.3533709105753244]
siam score:  -0.81878525
actions average: 
K:  2  action  0 :  tensor([0.4414, 0.0103, 0.1128, 0.1089, 0.1107, 0.1184, 0.0975],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0106, 0.9221, 0.0182, 0.0095, 0.0062, 0.0075, 0.0260],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.3321, 0.0145, 0.1694, 0.1117, 0.1091, 0.1684, 0.0947],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1635, 0.2545, 0.1034, 0.1386, 0.1231, 0.1219, 0.0949],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2032, 0.0221, 0.1301, 0.1540, 0.2252, 0.1398, 0.1255],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1734, 0.0624, 0.1872, 0.1052, 0.1269, 0.2454, 0.0994],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2135, 0.0411, 0.1324, 0.1297, 0.1479, 0.1135, 0.2219],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  36.471204841041335
maxi score, test score, baseline:  -0.9634567010309278 -0.504 -0.504
probs:  [0.05256580617632098, 0.07932461248831198, 0.05242206948403312, 0.09623556492136004, 0.36632989847751896, 0.35312204845245493]
maxi score, test score, baseline:  -0.9634567010309278 -0.504 -0.504
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9634567010309278 -0.504 -0.504
siam score:  -0.82417023
printing an ep nov before normalisation:  25.898499488830566
actor:  0 policy actor:  0  step number:  44 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9599342759211653 -0.504 -0.504
maxi score, test score, baseline:  -0.9599342759211653 -0.504 -0.504
probs:  [0.05260559316786302, 0.0793847046491228, 0.05246174740501973, 0.09630848948387238, 0.36660777654570903, 0.352631688748413]
printing an ep nov before normalisation:  32.290100265434965
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9599342759211653 -0.504 -0.504
probs:  [0.05260559316786302, 0.0793847046491228, 0.05246174740501973, 0.09630848948387238, 0.36660777654570903, 0.352631688748413]
siam score:  -0.8204555
maxi score, test score, baseline:  -0.9599342759211653 -0.504 -0.504
probs:  [0.05260559316786302, 0.0793847046491228, 0.05246174740501973, 0.09630848948387238, 0.36660777654570903, 0.352631688748413]
actor:  1 policy actor:  1  step number:  56 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9599342759211653 -0.504 -0.504
probs:  [0.052061299535847455, 0.07856263242914059, 0.051918945879003114, 0.09531086756136412, 0.36280635157913915, 0.35933990301550556]
maxi score, test score, baseline:  -0.9599342759211653 -0.504 -0.504
probs:  [0.052061299535847455, 0.07856263242914059, 0.051918945879003114, 0.09531086756136412, 0.36280635157913915, 0.35933990301550556]
printing an ep nov before normalisation:  26.21179324109451
actor:  1 policy actor:  1  step number:  50 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  63.214328683934895
Printing some Q and Qe and total Qs values:  [[-0.056]
 [-0.059]
 [-0.062]
 [-0.059]
 [-0.063]
 [-0.063]
 [-0.06 ]] [[30.502]
 [ 0.   ]
 [11.866]
 [ 0.   ]
 [10.659]
 [10.189]
 [16.541]] [[ 0.338]
 [-0.124]
 [ 0.051]
 [-0.124]
 [ 0.033]
 [ 0.025]
 [ 0.123]]
printing an ep nov before normalisation:  39.7456169128418
from probs:  [0.05227343228617624, 0.07839089707266043, 0.05213314060217568, 0.09437730830101398, 0.35851738563724717, 0.3643078361007264]
printing an ep nov before normalisation:  40.233911976625336
maxi score, test score, baseline:  -0.9599342759211653 -0.504 -0.504
probs:  [0.05233252633533087, 0.07847959182499742, 0.05219207564920162, 0.094484121542936, 0.3577911046374729, 0.3647205800100613]
maxi score, test score, baseline:  -0.9599342759211653 -0.504 -0.504
probs:  [0.05239133838265858, 0.07856786331843518, 0.052250729453172315, 0.09459042506298604, 0.35706828951237973, 0.365131354270368]
from probs:  [0.05239133838265858, 0.07856786331843518, 0.052250729453172315, 0.09459042506298604, 0.35706828951237973, 0.365131354270368]
siam score:  -0.82625246
maxi score, test score, baseline:  -0.9600367521367521 -0.504 -0.504
probs:  [0.052508852758613334, 0.0786637235548462, 0.052230469247969945, 0.09467303087434578, 0.35693376442810426, 0.36499015913612043]
printing an ep nov before normalisation:  54.47911106153728
siam score:  -0.82405454
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9600367521367521 -0.504 -0.504
probs:  [0.052526701409991446, 0.07834997607984824, 0.052248223031360605, 0.0947052483032109, 0.3570553554981126, 0.36511449567747617]
printing an ep nov before normalisation:  40.412952612816156
from probs:  [0.05248932254441273, 0.07842216236268096, 0.052209662614438226, 0.0948468281022174, 0.3571879500739394, 0.36484407430231125]
printing an ep nov before normalisation:  38.78539573705165
Printing some Q and Qe and total Qs values:  [[ 0.686]
 [ 0.686]
 [ 0.686]
 [-0.098]
 [-0.098]
 [-0.098]
 [ 0.686]] [[ 0.005]
 [ 0.005]
 [ 0.005]
 [64.101]
 [64.101]
 [64.101]
 [ 0.005]] [[0.686]
 [0.686]
 [0.686]
 [1.544]
 [1.544]
 [1.544]
 [0.686]]
from probs:  [0.052663849323639544, 0.07868313514100644, 0.05238325715939282, 0.09516255179288671, 0.35504738445601436, 0.3660598221270601]
maxi score, test score, baseline:  -0.9604414551607445 -0.504 -0.504
probs:  [0.05261606168744206, 0.07867584543992102, 0.0523350327931736, 0.09518091161773128, 0.3554702442030964, 0.36572190425863565]
maxi score, test score, baseline:  -0.9605413502109704 -0.504 -0.504
probs:  [0.05267372654671775, 0.07876214316105885, 0.05239238887546229, 0.0952853440741525, 0.3547628070914552, 0.36612359025115326]
maxi score, test score, baseline:  -0.9605413502109704 -0.504 -0.504
probs:  [0.05267372654671775, 0.07876214316105885, 0.05239238887546229, 0.0952853440741525, 0.3547628070914552, 0.36612359025115326]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9606407407407407 -0.504 -0.504
maxi score, test score, baseline:  -0.9606407407407407 -0.504 -0.504
probs:  [0.0526834846244782, 0.07884100954265238, 0.052401401688780214, 0.0954079804708404, 0.3544795981236639, 0.3661865255495849]
Printing some Q and Qe and total Qs values:  [[-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]] [[50.188]
 [50.188]
 [50.188]
 [50.188]
 [50.188]
 [50.188]
 [50.188]] [[1.949]
 [1.949]
 [1.949]
 [1.949]
 [1.949]
 [1.949]
 [1.949]]
siam score:  -0.8253866
maxi score, test score, baseline:  -0.9606407407407407 -0.504 -0.504
probs:  [0.05269312466860463, 0.07891973552587621, 0.05241029670964557, 0.09553046230436558, 0.35419774499209805, 0.36624863579941]
Printing some Q and Qe and total Qs values:  [[-0.052]
 [-0.028]
 [-0.051]
 [-0.046]
 [-0.05 ]
 [-0.04 ]
 [-0.053]] [[30.149]
 [31.829]
 [31.99 ]
 [32.769]
 [31.567]
 [31.146]
 [29.681]] [[1.264]
 [1.433]
 [1.424]
 [1.497]
 [1.388]
 [1.363]
 [1.222]]
maxi score, test score, baseline:  -0.9606407407407407 -0.504 -0.504
probs:  [0.05269312466860463, 0.07891973552587621, 0.05241029670964557, 0.09553046230436558, 0.35419774499209805, 0.36624863579941]
printing an ep nov before normalisation:  34.650517183524606
printing an ep nov before normalisation:  34.3973818289022
Printing some Q and Qe and total Qs values:  [[-0.138]
 [-0.078]
 [-0.13 ]
 [-0.128]
 [-0.128]
 [-0.129]
 [-0.121]] [[30.958]
 [36.202]
 [29.195]
 [28.209]
 [28.318]
 [28.358]
 [32.94 ]] [[0.355]
 [0.573]
 [0.309]
 [0.281]
 [0.285]
 [0.285]
 [0.431]]
printing an ep nov before normalisation:  0.12049447163889226
printing an ep nov before normalisation:  28.959100384801673
maxi score, test score, baseline:  -0.9606407407407407 -0.504 -0.504
probs:  [0.05267327567705723, 0.0786768492230848, 0.05238916808703038, 0.09570442720696501, 0.3544559851611986, 0.36610029464466404]
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.526]
 [0.319]
 [0.299]
 [0.302]
 [0.311]
 [0.3  ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.333]
 [0.526]
 [0.319]
 [0.299]
 [0.302]
 [0.311]
 [0.3  ]]
printing an ep nov before normalisation:  42.48354700957803
printing an ep nov before normalisation:  33.59022378921509
maxi score, test score, baseline:  -0.9606407407407407 -0.504 -0.504
probs:  [0.052653466817265515, 0.07871067969792214, 0.0523687731792455, 0.09525111937829295, 0.35505868396215723, 0.36595727696511665]
maxi score, test score, baseline:  -0.9607396305625524 -0.504 -0.504
probs:  [0.052586250138544496, 0.07873687606256105, 0.052300535896391775, 0.0948217541133947, 0.356075566023804, 0.36547901776530395]
printing an ep nov before normalisation:  44.73742726699014
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9608380234505862 -0.504 -0.504
probs:  [0.052506473336736095, 0.07883367210939948, 0.052364777052415296, 0.0949383577979555, 0.3554277988025299, 0.3659289209009638]
Printing some Q and Qe and total Qs values:  [[-0.093]
 [-0.082]
 [-0.093]
 [-0.093]
 [-0.093]
 [-0.093]
 [-0.093]] [[28.127]
 [32.476]
 [28.127]
 [28.127]
 [28.127]
 [28.127]
 [28.127]] [[1.148]
 [1.533]
 [1.148]
 [1.148]
 [1.148]
 [1.148]
 [1.148]]
printing an ep nov before normalisation:  38.35235464654474
siam score:  -0.8187216
printing an ep nov before normalisation:  44.80219804454148
Printing some Q and Qe and total Qs values:  [[-0.107]
 [-0.107]
 [-0.107]
 [-0.107]
 [-0.107]
 [-0.107]
 [-0.107]] [[46.382]
 [46.382]
 [46.382]
 [46.382]
 [46.382]
 [46.382]
 [46.382]] [[1.893]
 [1.893]
 [1.893]
 [1.893]
 [1.893]
 [1.893]
 [1.893]]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.54105401895636
maxi score, test score, baseline:  -0.9608380234505862 -0.504 -0.504
probs:  [0.05252530401476362, 0.07899003627801507, 0.05238286750795451, 0.09517885296951967, 0.3548676946481345, 0.36605524458161276]
Printing some Q and Qe and total Qs values:  [[-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.068]] [[42.487]
 [42.487]
 [42.487]
 [42.487]
 [42.487]
 [42.487]
 [40.783]] [[2.111]
 [2.111]
 [2.111]
 [2.111]
 [2.111]
 [2.111]
 [1.932]]
maxi score, test score, baseline:  -0.9608380234505862 -0.504 -0.504
probs:  [0.05259752797117711, 0.07908029528007118, 0.05231507611873565, 0.09528014424067363, 0.3551459572963636, 0.365580999092979]
Printing some Q and Qe and total Qs values:  [[-0.077]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.065]] [[42.11]
 [39.78]
 [39.78]
 [39.78]
 [39.78]
 [39.78]
 [39.78]] [[1.923]
 [1.719]
 [1.719]
 [1.719]
 [1.719]
 [1.719]
 [1.719]]
maxi score, test score, baseline:  -0.9608380234505862 -0.504 -0.504
probs:  [0.052550877082562814, 0.07907379647940771, 0.0522679969881824, 0.09529820698800359, 0.35555801795409026, 0.36525110450775333]
maxi score, test score, baseline:  -0.9608380234505862 -0.504 -0.504
probs:  [0.052550877082562814, 0.07907379647940771, 0.0522679969881824, 0.09529820698800359, 0.35555801795409026, 0.36525110450775333]
maxi score, test score, baseline:  -0.9608380234505862 -0.504 -0.504
maxi score, test score, baseline:  -0.9608380234505862 -0.504 -0.504
probs:  [0.052550877082562814, 0.07907379647940771, 0.0522679969881824, 0.09529820698800359, 0.35555801795409026, 0.36525110450775333]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.31404817743767
printing an ep nov before normalisation:  37.44846142275005
Printing some Q and Qe and total Qs values:  [[-0.082]
 [-0.063]
 [-0.076]
 [-0.069]
 [-0.068]
 [-0.075]
 [-0.071]] [[27.088]
 [28.145]
 [27.68 ]
 [27.741]
 [27.154]
 [25.96 ]
 [26.209]] [[1.458]
 [1.591]
 [1.529]
 [1.542]
 [1.48 ]
 [1.344]
 [1.375]]
actions average: 
K:  1  action  0 :  tensor([0.5147, 0.0057, 0.0753, 0.0864, 0.1429, 0.0925, 0.0825],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0302, 0.8108, 0.0156, 0.0384, 0.0176, 0.0125, 0.0750],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2758, 0.0365, 0.2052, 0.1140, 0.1212, 0.1371, 0.1102],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2999, 0.0046, 0.1278, 0.1469, 0.1456, 0.1503, 0.1249],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3485, 0.0254, 0.1088, 0.1200, 0.1815, 0.1160, 0.0998],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2984, 0.0131, 0.1138, 0.1133, 0.1260, 0.2363, 0.0990],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.3325, 0.0239, 0.1119, 0.1330, 0.1308, 0.1289, 0.1390],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.19327163696289
maxi score, test score, baseline:  -0.9608380234505862 -0.504 -0.504
printing an ep nov before normalisation:  32.50823517358797
Printing some Q and Qe and total Qs values:  [[ 0.084]
 [ 0.03 ]
 [ 0.042]
 [ 0.028]
 [-0.004]
 [ 0.016]
 [ 0.055]] [[42.052]
 [41.324]
 [41.598]
 [44.26 ]
 [47.888]
 [41.771]
 [40.543]] [[1.409]
 [1.317]
 [1.344]
 [1.465]
 [1.617]
 [1.326]
 [1.303]]
printing an ep nov before normalisation:  39.321510883811534
maxi score, test score, baseline:  -0.9608380234505862 -0.504 -0.504
probs:  [0.052634702103966785, 0.07892151068900206, 0.0523506890189429, 0.09555324350777224, 0.3547098124724375, 0.3658300422078784]
Printing some Q and Qe and total Qs values:  [[-0.061]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.07 ]
 [-0.063]
 [-0.063]] [[40.076]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [25.981]
 [ 0.   ]
 [ 0.   ]] [[ 0.413]
 [-0.333]
 [-0.333]
 [-0.333]
 [ 0.141]
 [-0.333]
 [-0.333]]
maxi score, test score, baseline:  -0.9608380234505862 -0.504 -0.504
probs:  [0.052634702103966785, 0.07892151068900206, 0.0523506890189429, 0.09555324350777224, 0.3547098124724375, 0.3658300422078784]
actor:  1 policy actor:  1  step number:  52 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.153]
 [0.172]
 [0.172]
 [0.147]
 [0.172]
 [0.147]
 [0.149]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.153]
 [0.172]
 [0.172]
 [0.147]
 [0.172]
 [0.147]
 [0.149]]
printing an ep nov before normalisation:  27.50607449499545
printing an ep nov before normalisation:  19.914912573274144
maxi score, test score, baseline:  -0.9608380234505862 -0.504 -0.504
probs:  [0.053145920808959995, 0.07907311502819912, 0.052729549228114785, 0.09547731889341202, 0.35108851927943624, 0.3684855767618777]
printing an ep nov before normalisation:  25.103178024291992
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.878]
 [0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]] [[44.873]
 [ 7.63 ]
 [44.873]
 [44.873]
 [44.873]
 [44.873]
 [44.873]] [[0.587]
 [0.878]
 [0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]]
maxi score, test score, baseline:  -0.9609359231411863 -0.504 -0.504
probs:  [0.053145920808959995, 0.07907311502819912, 0.052729549228114785, 0.09547731889341202, 0.35108851927943624, 0.3684855767618777]
printing an ep nov before normalisation:  30.279221099240065
printing an ep nov before normalisation:  53.04125132360631
maxi score, test score, baseline:  -0.9609359231411863 -0.504 -0.504
probs:  [0.0531001069740092, 0.07906660613714835, 0.05268310418480846, 0.09549567834357839, 0.3514943785803939, 0.3681601257800618]
maxi score, test score, baseline:  -0.9609359231411863 -0.504 -0.504
probs:  [0.0531001069740092, 0.07906660613714835, 0.05268310418480846, 0.09549567834357839, 0.3514943785803939, 0.3681601257800618]
actor:  1 policy actor:  1  step number:  62 total reward:  0.11333333333333295  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9609359231411863 -0.504 -0.504
probs:  [0.053390974174215206, 0.07910793039909715, 0.052977978860208746, 0.09537911613321241, 0.3489176207369637, 0.37022637969630273]
printing an ep nov before normalisation:  47.93300037130476
actor:  1 policy actor:  1  step number:  68 total reward:  0.13999999999999957  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.9610333333333333 -0.504 -0.504
probs:  [0.05303375465005137, 0.07857821768267051, 0.052623529449252686, 0.09474026652108805, 0.35328020445431296, 0.36774402724262445]
printing an ep nov before normalisation:  25.547330245890343
printing an ep nov before normalisation:  42.18083971616421
maxi score, test score, baseline:  -0.9610333333333333 -0.504 -0.504
printing an ep nov before normalisation:  46.26586043457484
maxi score, test score, baseline:  -0.9610333333333333 -0.504 -0.504
probs:  [0.05317189622848086, 0.0787088873784468, 0.05263001547331824, 0.09436856396985774, 0.3533305224508915, 0.3677901144990049]
printing an ep nov before normalisation:  32.94173082976221
maxi score, test score, baseline:  -0.9610333333333333 -0.504 -0.504
probs:  [0.05317189622848086, 0.0787088873784468, 0.05263001547331824, 0.09436856396985774, 0.3533305224508915, 0.3677901144990049]
maxi score, test score, baseline:  -0.9611302576891105 -0.504 -0.504
probs:  [0.05317189622848086, 0.0787088873784468, 0.05263001547331824, 0.09436856396985774, 0.3533305224508915, 0.3677901144990049]
printing an ep nov before normalisation:  39.8926238190342
maxi score, test score, baseline:  -0.9611302576891105 -0.504 -0.504
probs:  [0.05317189622848086, 0.0787088873784468, 0.05263001547331824, 0.09436856396985774, 0.3533305224508915, 0.3677901144990049]
maxi score, test score, baseline:  -0.9611302576891105 -0.504 -0.504
probs:  [0.05317189622848086, 0.0787088873784468, 0.05263001547331824, 0.09436856396985774, 0.3533305224508915, 0.3677901144990049]
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]] [[20.084]
 [20.084]
 [20.084]
 [20.084]
 [20.084]
 [20.084]
 [20.084]] [[0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]]
printing an ep nov before normalisation:  32.069554234868335
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]] [[21.233]
 [30.652]
 [30.652]
 [30.652]
 [30.652]
 [30.652]
 [30.652]] [[2.107]
 [4.011]
 [4.011]
 [4.011]
 [4.011]
 [4.011]
 [4.011]]
maxi score, test score, baseline:  -0.9611302576891105 -0.504 -0.504
probs:  [0.0531264619222689, 0.07870187879864175, 0.05258376579455433, 0.09438511863776615, 0.3537367393663199, 0.36746603548044887]
maxi score, test score, baseline:  -0.9611302576891105 -0.504 -0.504
probs:  [0.0531264619222689, 0.07870187879864175, 0.05258376579455433, 0.09438511863776615, 0.3537367393663199, 0.36746603548044887]
maxi score, test score, baseline:  -0.9611302576891105 -0.504 -0.504
probs:  [0.0531264619222689, 0.07870187879864175, 0.05258376579455433, 0.09438511863776615, 0.3537367393663199, 0.36746603548044887]
maxi score, test score, baseline:  -0.9611302576891105 -0.504 -0.504
probs:  [0.0531264619222689, 0.07870187879864175, 0.05258376579455433, 0.09438511863776615, 0.3537367393663199, 0.36746603548044887]
printing an ep nov before normalisation:  30.94747482545735
printing an ep nov before normalisation:  29.995205549925345
Printing some Q and Qe and total Qs values:  [[0.885]
 [0.891]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]] [[10.622]
 [11.836]
 [35.46 ]
 [35.46 ]
 [35.46 ]
 [35.46 ]
 [35.46 ]] [[0.885]
 [0.891]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]]
maxi score, test score, baseline:  -0.9611302576891105 -0.504 -0.504
probs:  [0.053136812922311594, 0.07877754817984388, 0.05259273077486193, 0.09450084225708992, 0.35346341823303706, 0.3675286476328557]
maxi score, test score, baseline:  -0.9612266998341625 -0.504 -0.504
probs:  [0.053154246407201455, 0.07847476669238752, 0.05260998530491908, 0.09453188084368128, 0.35357963256869585, 0.36764948818311477]
maxi score, test score, baseline:  -0.9612266998341625 -0.504 -0.504
probs:  [0.053154246407201455, 0.07847476669238752, 0.05260998530491908, 0.09453188084368128, 0.35357963256869585, 0.36764948818311477]
printing an ep nov before normalisation:  34.51655561041101
Printing some Q and Qe and total Qs values:  [[-0.062]
 [-0.04 ]
 [-0.062]
 [-0.065]
 [-0.062]
 [-0.062]
 [-0.062]] [[33.082]
 [32.064]
 [33.082]
 [31.111]
 [33.082]
 [33.082]
 [33.082]] [[1.15 ]
 [1.108]
 [1.15 ]
 [1.022]
 [1.15 ]
 [1.15 ]
 [1.15 ]]
maxi score, test score, baseline:  -0.9612266998341625 -0.504 -0.504
probs:  [0.053154246407201455, 0.07847476669238752, 0.05260998530491908, 0.09453188084368128, 0.35357963256869585, 0.36764948818311477]
printing an ep nov before normalisation:  0.11484408468049878
maxi score, test score, baseline:  -0.9612266998341625 -0.504 -0.504
probs:  [0.05318032069821089, 0.0785132929245086, 0.052635791943166296, 0.09408692391100572, 0.3537534478687677, 0.36783022265434073]
line 256 mcts: sample exp_bonus 37.382731908169184
maxi score, test score, baseline:  -0.9613226633581472 -0.504 -0.504
probs:  [0.053235873864944584, 0.07859537585423691, 0.052690774856291936, 0.0941853162076471, 0.35307736867283424, 0.36821529054404517]
maxi score, test score, baseline:  -0.9613226633581472 -0.504 -0.504
probs:  [0.05329117566985322, 0.07867708738272498, 0.05274550898782322, 0.09428326330793495, 0.35240434853744723, 0.36859861611421635]
maxi score, test score, baseline:  -0.9613226633581472 -0.504 -0.504
probs:  [0.05329117566985322, 0.07867708738272498, 0.05274550898782322, 0.09428326330793495, 0.35240434853744723, 0.36859861611421635]
actor:  1 policy actor:  1  step number:  63 total reward:  0.11999999999999933  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9613226633581472 -0.504 -0.504
Printing some Q and Qe and total Qs values:  [[-0.045]
 [-0.045]
 [-0.045]
 [-0.055]
 [-0.045]
 [-0.045]
 [-0.045]] [[54.607]
 [54.607]
 [54.607]
 [50.996]
 [54.607]
 [54.607]
 [54.607]] [[2.232]
 [2.232]
 [2.232]
 [1.945]
 [2.232]
 [2.232]
 [2.232]]
printing an ep nov before normalisation:  42.273049383071935
maxi score, test score, baseline:  -0.9613226633581472 -0.504 -0.504
probs:  [0.05224145563253662, 0.07718462028527909, 0.05170530573769204, 0.11243295123951129, 0.34512219136475597, 0.36131347574022493]
actor:  1 policy actor:  1  step number:  68 total reward:  0.05999999999999883  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  24.97698045647965
maxi score, test score, baseline:  -0.9613226633581472 -0.504 -0.504
probs:  [0.05247493016078716, 0.07723099259315631, 0.051942802002445346, 0.11221492081109559, 0.3431587259007093, 0.36297762853180626]
printing an ep nov before normalisation:  45.455331802368164
maxi score, test score, baseline:  -0.9614181518151815 -0.504 -0.504
printing an ep nov before normalisation:  42.1585594157198
Printing some Q and Qe and total Qs values:  [[-0.063]
 [-0.04 ]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]] [[44.619]
 [50.728]
 [44.619]
 [44.619]
 [44.619]
 [44.619]
 [44.619]] [[0.778]
 [0.947]
 [0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]]
Printing some Q and Qe and total Qs values:  [[-0.059]
 [-0.056]
 [-0.061]
 [-0.06 ]
 [-0.059]
 [-0.058]
 [-0.058]] [[69.312]
 [59.046]
 [68.368]
 [72.331]
 [69.604]
 [68.719]
 [68.934]] [[1.343]
 [1.025]
 [1.311]
 [1.436]
 [1.352]
 [1.326]
 [1.333]]
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.709]
 [0.498]
 [0.497]
 [0.496]
 [0.494]
 [0.513]] [[25.853]
 [33.835]
 [26.774]
 [26.272]
 [26.549]
 [26.849]
 [28.976]] [[0.502]
 [0.709]
 [0.498]
 [0.497]
 [0.496]
 [0.494]
 [0.513]]
Printing some Q and Qe and total Qs values:  [[-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]]
maxi score, test score, baseline:  -0.9614181518151815 -0.504 -0.504
using another actor
maxi score, test score, baseline:  -0.9615131687242798 -0.504 -0.504
probs:  [0.05252744371845216, 0.07730834349518412, 0.05199478168480808, 0.11232737050456831, 0.34250039633123786, 0.3633416642657496]
actor:  1 policy actor:  1  step number:  56 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9615131687242798 -0.504 -0.504
probs:  [0.05207783672787353, 0.076646085826391, 0.05154974559132093, 0.11073798522540948, 0.34876345864121183, 0.36022488798779323]
actions average: 
K:  2  action  0 :  tensor([0.5338, 0.0040, 0.0837, 0.0994, 0.1034, 0.0990, 0.0767],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0496, 0.7933, 0.0400, 0.0355, 0.0210, 0.0220, 0.0386],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1983, 0.0485, 0.2819, 0.0967, 0.1183, 0.1535, 0.1028],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2247, 0.0250, 0.1163, 0.2032, 0.1708, 0.1226, 0.1373],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2829, 0.0084, 0.0963, 0.0923, 0.3134, 0.1248, 0.0818],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.3058, 0.0260, 0.1344, 0.1260, 0.1554, 0.1392, 0.1131],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2494, 0.0263, 0.1199, 0.1221, 0.1628, 0.1378, 0.1818],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.16320454391852
maxi score, test score, baseline:  -0.9615131687242798 -0.504 -0.504
probs:  [0.05207783672787353, 0.076646085826391, 0.05154974559132093, 0.11073798522540948, 0.34876345864121183, 0.36022488798779323]
using explorer policy with actor:  0
from probs:  [0.05213061140111355, 0.07672382134386212, 0.051601983734646, 0.11085035742321661, 0.348102492263823, 0.36059073383333873]
maxi score, test score, baseline:  -0.9616077175697865 -0.504 -0.504
probs:  [0.05213061140111355, 0.07672382134386212, 0.051601983734646, 0.11085035742321661, 0.348102492263823, 0.36059073383333873]
printing an ep nov before normalisation:  44.30844756182375
siam score:  -0.8188832
maxi score, test score, baseline:  -0.9616077175697865 -0.504 -0.504
probs:  [0.05213061140111355, 0.07672382134386212, 0.051601983734646, 0.11085035742321661, 0.348102492263823, 0.36059073383333873]
Printing some Q and Qe and total Qs values:  [[-0.092]
 [-0.075]
 [-0.068]
 [-0.072]
 [-0.077]
 [-0.083]
 [-0.087]] [[6.17 ]
 [6.665]
 [8.113]
 [5.242]
 [6.786]
 [7.347]
 [5.231]] [[0.359]
 [0.412]
 [0.526]
 [0.31 ]
 [0.419]
 [0.455]
 [0.295]]
printing an ep nov before normalisation:  35.173444747924805
maxi score, test score, baseline:  -0.9617018018018018 -0.504 -0.504
actor:  0 policy actor:  1  step number:  69 total reward:  0.13333333333333253  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  51.51555483368634
printing an ep nov before normalisation:  46.72381204853857
line 256 mcts: sample exp_bonus 39.49154465458474
printing an ep nov before normalisation:  16.208418607711792
maxi score, test score, baseline:  -0.9590176470588235 -0.504 -0.504
probs:  [0.052153920904539984, 0.07650079063236853, 0.05162381788586015, 0.11103754844986556, 0.3479404421447732, 0.36074347998259254]
printing an ep nov before normalisation:  54.98380110199784
maxi score, test score, baseline:  -0.9590176470588235 -0.504 -0.504
probs:  [0.05210838108846228, 0.07649077324201287, 0.05157750464201466, 0.1110779206754222, 0.3483264590276796, 0.36041896132440837]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9590176470588235 -0.504 -0.504
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9591176039119804 -0.504 -0.504
printing an ep nov before normalisation:  38.0494205262385
maxi score, test score, baseline:  -0.9591176039119804 -0.504 -0.504
probs:  [0.05214766174170671, 0.07660536912456037, 0.05161514546102773, 0.11067810119136018, 0.3482713145034564, 0.3606824079778886]
printing an ep nov before normalisation:  31.827890325399636
siam score:  -0.8111442
maxi score, test score, baseline:  -0.9591176039119804 -0.504 -0.504
probs:  [0.052199921536802536, 0.07668220199781407, 0.05166687022788152, 0.11078916752300899, 0.34761718078932446, 0.36104465792516843]
maxi score, test score, baseline:  -0.9591176039119804 -0.504 -0.504
probs:  [0.052199921536802536, 0.07668220199781407, 0.05166687022788152, 0.11078916752300899, 0.34761718078932446, 0.36104465792516843]
maxi score, test score, baseline:  -0.9591176039119804 -0.504 -0.504
probs:  [0.05225194936178297, 0.07675869382628484, 0.05171836539949647, 0.11089974085483406, 0.3469659506356232, 0.3614052999219785]
printing an ep nov before normalisation:  26.748490793585653
maxi score, test score, baseline:  -0.9591176039119804 -0.504 -0.504
probs:  [0.052206663991789776, 0.07674907638330225, 0.05167230343374417, 0.11093981342168707, 0.3473496017774405, 0.3610825409920363]
printing an ep nov before normalisation:  47.83055317458126
Printing some Q and Qe and total Qs values:  [[-0.049]
 [-0.042]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]] [[29.675]
 [32.327]
 [26.146]
 [26.146]
 [26.146]
 [26.146]
 [26.146]] [[0.498]
 [0.575]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]]
actor:  1 policy actor:  1  step number:  72 total reward:  0.19333333333333313  reward:  1.0 rdn_beta:  1.667
using another actor
from probs:  [0.052206663991789776, 0.07674907638330225, 0.05167230343374417, 0.11093981342168707, 0.3473496017774405, 0.3610825409920363]
printing an ep nov before normalisation:  43.84653343803937
printing an ep nov before normalisation:  55.21872266303571
printing an ep nov before normalisation:  35.715263653442975
maxi score, test score, baseline:  -0.9591176039119804 -0.504 -0.504
maxi score, test score, baseline:  -0.9591176039119804 -0.504 -0.504
maxi score, test score, baseline:  -0.9592170731707317 -0.504 -0.504
probs:  [0.05182328180564235, 0.07587483757944194, 0.051292855390026995, 0.11012401800489309, 0.3524598966669062, 0.35842511055308945]
maxi score, test score, baseline:  -0.9592170731707317 -0.504 -0.504
probs:  [0.05182328180564235, 0.07587483757944194, 0.051292855390026995, 0.11012401800489309, 0.3524598966669062, 0.35842511055308945]
maxi score, test score, baseline:  -0.9592170731707317 -0.504 -0.504
probs:  [0.05182328180564235, 0.07587483757944194, 0.051292855390026995, 0.11012401800489309, 0.3524598966669062, 0.35842511055308945]
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.169]
 [0.206]
 [0.206]
 [0.179]
 [0.179]
 [0.206]] [[16.208]
 [32.704]
 [21.43 ]
 [21.43 ]
 [16.658]
 [16.78 ]
 [21.43 ]] [[0.182]
 [0.169]
 [0.206]
 [0.206]
 [0.179]
 [0.179]
 [0.206]]
maxi score, test score, baseline:  -0.9592170731707317 -0.504 -0.504
probs:  [0.05182328180564235, 0.07587483757944194, 0.051292855390026995, 0.11012401800489309, 0.3524598966669062, 0.35842511055308945]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9592170731707317 -0.504 -0.504
probs:  [0.051870596364435785, 0.07563860573109625, 0.05133968442231439, 0.1096156406492936, 0.3527823993402127, 0.35875307349264723]
printing an ep nov before normalisation:  39.17011379540416
printing an ep nov before normalisation:  59.710941314697266
Printing some Q and Qe and total Qs values:  [[ 0.179]
 [-0.013]
 [ 0.023]
 [ 0.023]
 [-0.033]
 [ 0.023]
 [ 0.023]] [[47.373]
 [37.37 ]
 [38.198]
 [38.198]
 [42.605]
 [38.198]
 [38.198]] [[1.617]
 [0.918]
 [0.996]
 [0.996]
 [1.164]
 [0.996]
 [0.996]]
from probs:  [0.051870596364435785, 0.07563860573109625, 0.05133968442231439, 0.1096156406492936, 0.3527823993402127, 0.35875307349264723]
maxi score, test score, baseline:  -0.9594145631067961 -0.504 -0.504
probs:  [0.05192269873803016, 0.07571464379591133, 0.05139125213841534, 0.10972589545493253, 0.3521312864922127, 0.359114223380498]
Printing some Q and Qe and total Qs values:  [[0.308]
 [0.308]
 [0.308]
 [0.308]
 [0.308]
 [0.308]
 [0.308]] [[22.673]
 [22.673]
 [22.673]
 [22.673]
 [22.673]
 [22.673]
 [22.673]] [[0.308]
 [0.308]
 [0.308]
 [0.308]
 [0.308]
 [0.308]
 [0.308]]
siam score:  -0.8251207
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9594145631067961 -0.504 -0.504
probs:  [0.051832017529973774, 0.07569257616866124, 0.05129903829172355, 0.10980191286269539, 0.3529063767099267, 0.35846807843701933]
using explorer policy with actor:  1
printing an ep nov before normalisation:  20.16260862350464
maxi score, test score, baseline:  -0.9595125907990315 -0.504 -0.504
probs:  [0.0517021392845229, 0.07570293342099062, 0.05130605181453504, 0.10981694563181596, 0.3529547328770636, 0.35851719697107187]
printing an ep nov before normalisation:  28.547732647080228
printing an ep nov before normalisation:  29.312349289063988
maxi score, test score, baseline:  -0.9595125907990315 -0.504 -0.504
probs:  [0.0517021392845229, 0.07570293342099062, 0.05130605181453504, 0.10981694563181596, 0.3529547328770636, 0.35851719697107187]
line 256 mcts: sample exp_bonus 0.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9595125907990315 -0.504 -0.504
probs:  [0.0517021392845229, 0.07570293342099062, 0.05130605181453504, 0.10981694563181596, 0.3529547328770636, 0.35851719697107187]
maxi score, test score, baseline:  -0.9595125907990315 -0.504 -0.504
probs:  [0.0517021392845229, 0.07570293342099062, 0.05130605181453504, 0.10981694563181596, 0.3529547328770636, 0.35851719697107187]
printing an ep nov before normalisation:  36.164700283505624
printing an ep nov before normalisation:  36.726107597351074
line 256 mcts: sample exp_bonus 28.133468627929688
maxi score, test score, baseline:  -0.9595125907990315 -0.504 -0.504
probs:  [0.051708554032630374, 0.0757679616023092, 0.05131149925938014, 0.10996528519712093, 0.3526915191299669, 0.35855518077859244]
Printing some Q and Qe and total Qs values:  [[ 0.103]
 [ 0.29 ]
 [ 0.103]
 [-0.002]
 [ 0.103]
 [ 0.103]
 [ 0.103]] [[64.329]
 [60.778]
 [64.329]
 [61.845]
 [64.329]
 [64.329]
 [64.329]] [[1.77 ]
 [1.829]
 [1.77 ]
 [1.576]
 [1.77 ]
 [1.77 ]
 [1.77 ]]
printing an ep nov before normalisation:  44.96277916486331
maxi score, test score, baseline:  -0.9595125907990315 -0.504 -0.504
probs:  [0.051708554032630374, 0.0757679616023092, 0.05131149925938014, 0.10996528519712093, 0.3526915191299669, 0.35855518077859244]
printing an ep nov before normalisation:  33.74631404876709
Printing some Q and Qe and total Qs values:  [[-0.028]
 [-0.014]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]] [[36.393]
 [38.009]
 [40.318]
 [40.318]
 [40.318]
 [40.318]
 [40.318]] [[1.351]
 [1.48 ]
 [1.636]
 [1.636]
 [1.636]
 [1.636]
 [1.636]]
printing an ep nov before normalisation:  33.908210628560084
maxi score, test score, baseline:  -0.9595125907990315 -0.504 -0.504
Printing some Q and Qe and total Qs values:  [[-0.02 ]
 [-0.008]
 [-0.02 ]
 [-0.017]
 [-0.017]
 [-0.02 ]
 [-0.02 ]] [[32.342]
 [32.383]
 [32.342]
 [31.764]
 [31.634]
 [32.342]
 [32.342]] [[1.433]
 [1.448]
 [1.433]
 [1.391]
 [1.382]
 [1.433]
 [1.433]]
maxi score, test score, baseline:  -0.9595125907990315 -0.504 -0.504
probs:  [0.051708554032630374, 0.0757679616023092, 0.05131149925938014, 0.10996528519712093, 0.3526915191299669, 0.35855518077859244]
printing an ep nov before normalisation:  53.13973349964871
maxi score, test score, baseline:  -0.9595125907990315 -0.504 -0.504
probs:  [0.051708554032630374, 0.0757679616023092, 0.05131149925938014, 0.10996528519712093, 0.3526915191299669, 0.35855518077859244]
maxi score, test score, baseline:  -0.9595125907990315 -0.504 -0.504
probs:  [0.05176016897173331, 0.07584365442752622, 0.05136271683875658, 0.11007520161078412, 0.35204438013682343, 0.35891387801437646]
printing an ep nov before normalisation:  37.421631813049316
printing an ep nov before normalisation:  26.843319310504725
printing an ep nov before normalisation:  35.165089289014546
maxi score, test score, baseline:  -0.9595125907990315 -0.504 -0.504
probs:  [0.05166964958721471, 0.0758221089363142, 0.05127105917128203, 0.11015169355221326, 0.35281385965821765, 0.35827162909475807]
printing an ep nov before normalisation:  37.39581615437678
maxi score, test score, baseline:  -0.9595125907990315 -0.504 -0.504
probs:  [0.05166964958721471, 0.0758221089363142, 0.05127105917128203, 0.11015169355221326, 0.35281385965821765, 0.35827162909475807]
printing an ep nov before normalisation:  51.82405447670041
printing an ep nov before normalisation:  65.36637615889629
maxi score, test score, baseline:  -0.9595125907990315 -0.504 -0.504
probs:  [0.05167597421741758, 0.07588701879246679, 0.05127641696372973, 0.11029987469939306, 0.35255173033787285, 0.35830898498912]
printing an ep nov before normalisation:  40.8247709274292
printing an ep nov before normalisation:  42.825865492068694
actor:  1 policy actor:  1  step number:  60 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  39.75127577545525
printing an ep nov before normalisation:  46.79588794708252
siam score:  -0.81745684
maxi score, test score, baseline:  -0.9596101449275363 -0.504 -0.504
probs:  [0.05217086412394452, 0.07572279405869775, 0.051777231250417506, 0.109925526793916, 0.34858542640050655, 0.36181815737251777]
maxi score, test score, baseline:  -0.9598038461538462 -0.504 -0.504
probs:  [0.05217086412394452, 0.07572279405869775, 0.051777231250417506, 0.109925526793916, 0.34858542640050655, 0.36181815737251777]
printing an ep nov before normalisation:  49.50847843084557
actions average: 
K:  4  action  0 :  tensor([0.4965, 0.0466, 0.0955, 0.0839, 0.1126, 0.0885, 0.0764],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0589, 0.6919, 0.0587, 0.0384, 0.0180, 0.0643, 0.0698],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1847, 0.0994, 0.2298, 0.1011, 0.1201, 0.1192, 0.1458],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.3372, 0.0031, 0.1189, 0.1235, 0.1823, 0.1180, 0.1171],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2215, 0.1187, 0.1166, 0.1182, 0.2112, 0.0975, 0.1163],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.3404, 0.0263, 0.1251, 0.1285, 0.1492, 0.1055, 0.1250],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2953, 0.0076, 0.1222, 0.1247, 0.1092, 0.0957, 0.2452],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9598038461538462 -0.504 -0.504
probs:  [0.05217086412394452, 0.07572279405869775, 0.051777231250417506, 0.109925526793916, 0.34858542640050655, 0.36181815737251777]
printing an ep nov before normalisation:  59.87446154962646
maxi score, test score, baseline:  -0.9598038461538462 -0.504 -0.504
probs:  [0.05217086412394452, 0.07572279405869775, 0.051777231250417506, 0.109925526793916, 0.34858542640050655, 0.36181815737251777]
maxi score, test score, baseline:  -0.9598038461538462 -0.504 -0.504
probs:  [0.05212622728101678, 0.07571185675739425, 0.05173203117350247, 0.10996352885421325, 0.3489649176399542, 0.36150143829391895]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9598038461538462 -0.504 -0.504
probs:  [0.052081659565707025, 0.07570093639433138, 0.05168690109646743, 0.11000147206196337, 0.34934382117373786, 0.361185209707793]
from probs:  [0.052081659565707025, 0.07570093639433138, 0.05168690109646743, 0.11000147206196337, 0.34934382117373786, 0.361185209707793]
printing an ep nov before normalisation:  54.55926946694456
Printing some Q and Qe and total Qs values:  [[-0.048]
 [-0.031]
 [-0.051]
 [-0.049]
 [-0.048]
 [-0.045]
 [-0.05 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.048]
 [-0.031]
 [-0.051]
 [-0.049]
 [-0.048]
 [-0.045]
 [-0.05 ]]
maxi score, test score, baseline:  -0.9598038461538462 -0.504 -0.504
probs:  [0.052183514217906374, 0.07584910107161279, 0.05178798175059967, 0.11021688937693648, 0.34806939370042744, 0.36189311988251727]
printing an ep nov before normalisation:  0.0
printing an ep nov before normalisation:  41.204166412353516
printing an ep nov before normalisation:  28.19065625938583
printing an ep nov before normalisation:  34.70955869269607
from probs:  [0.052139073256438045, 0.0758383873570845, 0.051742977092093946, 0.11025515525839799, 0.3484466374487051, 0.36157776958728055]
actions average: 
K:  3  action  0 :  tensor([0.4760, 0.0595, 0.0809, 0.0791, 0.1065, 0.1082, 0.0898],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0356, 0.8487, 0.0225, 0.0215, 0.0122, 0.0148, 0.0447],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.3435, 0.0344, 0.1014, 0.1192, 0.1410, 0.1536, 0.1070],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2231, 0.0287, 0.1089, 0.2062, 0.1406, 0.1868, 0.1056],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.3856, 0.0109, 0.1022, 0.1103, 0.1408, 0.1519, 0.0983],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2362, 0.0036, 0.1280, 0.1152, 0.1530, 0.2600, 0.1040],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2376, 0.0280, 0.1277, 0.1338, 0.1386, 0.1712, 0.1632],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  26.15782695090052
Printing some Q and Qe and total Qs values:  [[0.269]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]] [[32.989]
 [36.334]
 [36.334]
 [36.334]
 [36.334]
 [36.334]
 [36.334]] [[0.269]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]]
actions average: 
K:  2  action  0 :  tensor([0.4920, 0.0560, 0.0747, 0.0730, 0.1279, 0.0925, 0.0839],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0462, 0.7785, 0.0271, 0.0369, 0.0224, 0.0363, 0.0527],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2610, 0.0199, 0.1861, 0.1111, 0.1472, 0.1687, 0.1059],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2787, 0.0076, 0.1061, 0.1914, 0.1411, 0.1647, 0.1104],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2917, 0.0019, 0.1073, 0.1419, 0.1622, 0.1781, 0.1169],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2689, 0.0085, 0.1201, 0.1288, 0.1650, 0.1728, 0.1360],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2263, 0.0558, 0.1142, 0.1244, 0.1516, 0.1850, 0.1428],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9599 -0.504 -0.504
probs:  [0.05209470076550916, 0.07582769014926055, 0.05169804177262013, 0.11029336218331445, 0.3488232999745679, 0.3612629051547279]
using another actor
siam score:  -0.81622297
maxi score, test score, baseline:  -0.9599956937799043 -0.504 -0.504
probs:  [0.05215938506189477, 0.0759078073808689, 0.051634887690452796, 0.11039589152977732, 0.34908093903009807, 0.3608210893069081]
maxi score, test score, baseline:  -0.9599956937799043 -0.504 -0.504
probs:  [0.05215938506189477, 0.0759078073808689, 0.051634887690452796, 0.11039589152977732, 0.34908093903009807, 0.3608210893069081]
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.186]
 [0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]] [[37.328]
 [56.834]
 [40.849]
 [40.849]
 [40.849]
 [40.849]
 [40.849]] [[0.209]
 [0.186]
 [0.187]
 [0.187]
 [0.187]
 [0.187]
 [0.187]]
maxi score, test score, baseline:  -0.9599956937799043 -0.504 -0.504
probs:  [0.05211533677788543, 0.07589727940507118, 0.051590099091431765, 0.11043404262809961, 0.3494559881442234, 0.36050725395328853]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[-0.049]
 [-0.038]
 [-0.048]
 [-0.044]
 [-0.041]
 [-0.047]
 [-0.046]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.049]
 [-0.038]
 [-0.048]
 [-0.044]
 [-0.041]
 [-0.047]
 [-0.046]]
siam score:  -0.816622
Printing some Q and Qe and total Qs values:  [[0.288]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]] [[14.456]
 [14.756]
 [14.756]
 [14.756]
 [14.756]
 [14.756]
 [14.756]] [[0.288]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]]
maxi score, test score, baseline:  -0.9601857142857143 -0.504 -0.504
printing an ep nov before normalisation:  0.06246654307716426
maxi score, test score, baseline:  -0.9601857142857143 -0.504 -0.504
probs:  [0.051987912509375886, 0.07607052884078415, 0.05159489328348395, 0.11023894314250716, 0.34956737846311725, 0.3605403437607315]
actions average: 
K:  1  action  0 :  tensor([0.4641, 0.0288, 0.0870, 0.1022, 0.1233, 0.0982, 0.0964],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0257, 0.8826, 0.0243, 0.0171, 0.0114, 0.0116, 0.0274],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2171, 0.0235, 0.2818, 0.1068, 0.1135, 0.1604, 0.0968],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.3274, 0.0709, 0.1061, 0.1266, 0.1348, 0.1283, 0.1058],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2641, 0.0102, 0.0957, 0.1296, 0.2649, 0.1326, 0.1029],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2640, 0.0033, 0.1386, 0.1316, 0.1358, 0.2122, 0.1146],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2515, 0.0466, 0.1134, 0.1411, 0.1322, 0.1308, 0.1844],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9602800475059382 -0.504 -0.504
probs:  [0.051987912509375886, 0.07607052884078415, 0.05159489328348395, 0.11023894314250716, 0.34956737846311725, 0.3605403437607315]
siam score:  -0.8136027
maxi score, test score, baseline:  -0.9602800475059382 -0.504 -0.504
probs:  [0.05209583164956025, 0.07616020446742096, 0.05157683686671829, 0.11030273487344781, 0.34944986964547947, 0.3604145224973732]
Printing some Q and Qe and total Qs values:  [[-0.055]
 [-0.063]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.058]
 [-0.058]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.055]
 [-0.063]
 [-0.057]
 [-0.057]
 [-0.057]
 [-0.058]
 [-0.058]]
Printing some Q and Qe and total Qs values:  [[-0.07 ]
 [-0.072]
 [-0.078]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]] [[32.109]
 [42.288]
 [36.311]
 [32.109]
 [32.109]
 [32.109]
 [32.109]] [[0.961]
 [1.503]
 [1.177]
 [0.961]
 [0.961]
 [0.961]
 [0.961]]
maxi score, test score, baseline:  -0.9602800475059382 -0.504 -0.504
printing an ep nov before normalisation:  44.21636533318444
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9602800475059382 -0.504 -0.504
probs:  [0.05262630737894956, 0.07634367400950337, 0.0521147964562601, 0.1099938720621845, 0.34473750582774715, 0.3641838442653552]
maxi score, test score, baseline:  -0.9602800475059382 -0.504 -0.504
probs:  [0.05262630737894956, 0.07634367400950337, 0.0521147964562601, 0.1099938720621845, 0.34473750582774715, 0.3641838442653552]
printing an ep nov before normalisation:  51.778624098569
printing an ep nov before normalisation:  38.54024932132446
maxi score, test score, baseline:  -0.9603739336492891 -0.504 -0.504
probs:  [0.052613994855085854, 0.07637877004426111, 0.05210146147505777, 0.10950638459837915, 0.3453090933249677, 0.3640902957022485]
maxi score, test score, baseline:  -0.9603739336492891 -0.504 -0.504
probs:  [0.052613994855085854, 0.07637877004426111, 0.05210146147505777, 0.10950638459837915, 0.3453090933249677, 0.3640902957022485]
maxi score, test score, baseline:  -0.9603739336492891 -0.504 -0.504
probs:  [0.05277900902056896, 0.07632111122234049, 0.05226486409271022, 0.10985028429765202, 0.3435500531758992, 0.3652346781908291]
maxi score, test score, baseline:  -0.9603739336492891 -0.504 -0.504
probs:  [0.05277900902056896, 0.07632111122234049, 0.05226486409271022, 0.10985028429765202, 0.3435500531758992, 0.3652346781908291]
Printing some Q and Qe and total Qs values:  [[-0.092]
 [-0.092]
 [-0.092]
 [-0.092]
 [-0.092]
 [-0.092]
 [-0.092]] [[53.476]
 [53.476]
 [53.476]
 [53.476]
 [53.476]
 [53.476]
 [53.476]] [[1.172]
 [1.172]
 [1.172]
 [1.172]
 [1.172]
 [1.172]
 [1.172]]
actions average: 
K:  1  action  0 :  tensor([0.5559, 0.0130, 0.0792, 0.0787, 0.1119, 0.0752, 0.0860],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0157, 0.9042, 0.0191, 0.0156, 0.0090, 0.0112, 0.0252],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1889, 0.0481, 0.3299, 0.0970, 0.1026, 0.1310, 0.1025],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2486, 0.0290, 0.1292, 0.1974, 0.1490, 0.1307, 0.1160],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2704, 0.0247, 0.1114, 0.1211, 0.2604, 0.1070, 0.1050],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2537, 0.0122, 0.1311, 0.1149, 0.1301, 0.2557, 0.1023],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2039, 0.1038, 0.1108, 0.1681, 0.1172, 0.0876, 0.2087],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9603739336492891 -0.504 -0.504
probs:  [0.05261063045856316, 0.07632126691329415, 0.05222793830370085, 0.10990212350153557, 0.34396212867309683, 0.36497591214980946]
printing an ep nov before normalisation:  49.88302125798681
line 256 mcts: sample exp_bonus 51.66439127969252
printing an ep nov before normalisation:  34.78496361581362
printing an ep nov before normalisation:  25.09796142578125
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.0011856575650881496
actor:  1 policy actor:  1  step number:  66 total reward:  0.03333333333333266  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  42.51099969519704
maxi score, test score, baseline:  -0.9603739336492891 -0.504 -0.504
probs:  [0.05220458458529929, 0.07597232489590978, 0.051952526542432494, 0.10945069630970566, 0.34737294512127337, 0.3630469225453794]
maxi score, test score, baseline:  -0.9603739336492891 -0.504 -0.504
probs:  [0.05225394101574535, 0.0760442098501031, 0.05200164405678766, 0.10955431412009645, 0.3467549765259519, 0.36339091443131544]
printing an ep nov before normalisation:  26.282856464385986
printing an ep nov before normalisation:  40.15821834697345
siam score:  -0.812791
maxi score, test score, baseline:  -0.9603739336492891 -0.504 -0.504
probs:  [0.05211999465695366, 0.0760879594482974, 0.05199470894096049, 0.10908573158200514, 0.34736941803404675, 0.3633421873377365]
printing an ep nov before normalisation:  15.233022826058525
maxi score, test score, baseline:  -0.9603739336492891 -0.504 -0.504
probs:  [0.05211999465695366, 0.0760879594482974, 0.05199470894096049, 0.10908573158200514, 0.34736941803404675, 0.3633421873377365]
maxi score, test score, baseline:  -0.9603739336492891 -0.504 -0.504
maxi score, test score, baseline:  -0.9603739336492891 -0.504 -0.504
probs:  [0.052076467918960194, 0.07607782019452543, 0.051951007679393486, 0.10912155837524838, 0.34773717508882507, 0.36303597074304755]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.9603739336492891 -0.504 -0.504
probs:  [0.052076467918960194, 0.07607782019452543, 0.051951007679393486, 0.10912155837524838, 0.34773717508882507, 0.36303597074304755]
printing an ep nov before normalisation:  40.95205307006836
maxi score, test score, baseline:  -0.9603739336492891 -0.504 -0.504
probs:  [0.052076467918960194, 0.07607782019452543, 0.051951007679393486, 0.10912155837524838, 0.34773717508882507, 0.36303597074304755]
actor:  0 policy actor:  1  step number:  48 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.957331048069346 -0.504 -0.504
probs:  [0.052076467918960194, 0.07607782019452543, 0.051951007679393486, 0.10912155837524838, 0.34773717508882507, 0.36303597074304755]
printing an ep nov before normalisation:  48.868525297630406
printing an ep nov before normalisation:  51.5834568972624
printing an ep nov before normalisation:  40.90332891803191
from probs:  [0.05203300613738925, 0.07606769607190458, 0.05190737163469643, 0.10915733170295693, 0.3481043833273184, 0.3627302111257344]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  44.072909778739714
maxi score, test score, baseline:  -0.957331048069346 -0.504 -0.504
probs:  [0.051903009080409276, 0.07603741415299467, 0.0517768533452056, 0.10926433208702774, 0.3492027274029065, 0.36181566393145637]
printing an ep nov before normalisation:  25.227627754211426
maxi score, test score, baseline:  -0.957331048069346 -0.504 -0.504
probs:  [0.051783374520340576, 0.07604699877228215, 0.051783374520340576, 0.10927811235571741, 0.349246805317224, 0.36186133451409525]
from probs:  [0.051783374520340576, 0.07604699877228215, 0.051783374520340576, 0.10927811235571741, 0.349246805317224, 0.36186133451409525]
maxi score, test score, baseline:  -0.957331048069346 -0.504 -0.504
probs:  [0.051740001359366766, 0.07603694672678028, 0.051740001359366766, 0.1093136964345421, 0.3496119372181216, 0.3615574169018226]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]]
maxi score, test score, baseline:  -0.9574314465408805 -0.504 -0.504
probs:  [0.051740001359366766, 0.07603694672678028, 0.051740001359366766, 0.1093136964345421, 0.3496119372181216, 0.3615574169018226]
Printing some Q and Qe and total Qs values:  [[ 0.008]
 [ 0.039]
 [-0.003]
 [-0.001]
 [ 0.   ]
 [ 0.   ]
 [-0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.008]
 [ 0.039]
 [-0.003]
 [-0.001]
 [ 0.   ]
 [ 0.   ]
 [-0.001]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.4290182058765
Printing some Q and Qe and total Qs values:  [[-0.097]
 [-0.099]
 [-0.103]
 [-0.104]
 [-0.102]
 [-0.104]
 [-0.097]] [[36.31 ]
 [39.205]
 [36.466]
 [32.156]
 [35.96 ]
 [32.831]
 [37.453]] [[0.363]
 [0.426]
 [0.361]
 [0.263]
 [0.35 ]
 [0.278]
 [0.388]]
printing an ep nov before normalisation:  29.959489229886085
actor:  1 policy actor:  1  step number:  65 total reward:  0.14666666666666617  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  22.068913193918718
Starting evaluation
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.112]
 [0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.012]
 [0.112]
 [0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]]
actions average: 
K:  4  action  0 :  tensor([0.5506, 0.0035, 0.0822, 0.0886, 0.0981, 0.0890, 0.0880],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0393, 0.8894, 0.0237, 0.0098, 0.0090, 0.0109, 0.0178],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2402, 0.1151, 0.2319, 0.1004, 0.0960, 0.1079, 0.1086],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2192, 0.0316, 0.0972, 0.2474, 0.1473, 0.1086, 0.1487],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2729, 0.0748, 0.1218, 0.1371, 0.1453, 0.1241, 0.1240],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1979, 0.0064, 0.1150, 0.1386, 0.1315, 0.3003, 0.1103],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1777, 0.1501, 0.0957, 0.1781, 0.1293, 0.0994, 0.1698],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9575313725490197 -0.504 -0.504
probs:  [0.051373191288091535, 0.0752564665826815, 0.051373191288091535, 0.10808844278472317, 0.3549203737862057, 0.3589883342702065]
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.72 ]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]] [[44.169]
 [49.747]
 [44.169]
 [44.169]
 [44.169]
 [44.169]
 [44.169]] [[0.557]
 [0.72 ]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]]
maxi score, test score, baseline:  -0.9575313725490197 -0.504 -0.504
using explorer policy with actor:  0
printing an ep nov before normalisation:  33.71704993387911
using another actor
maxi score, test score, baseline:  -0.9578283489096574 -0.504 -0.504
probs:  [0.051373191288091535, 0.0752564665826815, 0.051373191288091535, 0.10808844278472317, 0.3549203737862057, 0.3589883342702065]
printing an ep nov before normalisation:  51.13320755318668
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
Printing some Q and Qe and total Qs values:  [[0.78 ]
 [0.761]
 [0.61 ]
 [0.601]
 [0.583]
 [0.587]
 [0.587]] [[48.224]
 [46.985]
 [47.687]
 [49.041]
 [51.011]
 [49.792]
 [45.882]] [[0.78 ]
 [0.761]
 [0.61 ]
 [0.601]
 [0.583]
 [0.587]
 [0.587]]
printing an ep nov before normalisation:  34.83713349274594
printing an ep nov before normalisation:  36.87590915720733
maxi score, test score, baseline:  -0.9579264180264181 -0.504 -0.504
probs:  [0.051373191288091535, 0.0752564665826815, 0.051373191288091535, 0.10808844278472317, 0.3549203737862057, 0.3589883342702065]
printing an ep nov before normalisation:  33.89493736609686
printing an ep nov before normalisation:  37.510481493366306
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.62 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]] [[36.366]
 [29.947]
 [36.634]
 [36.634]
 [36.634]
 [36.634]
 [36.634]] [[0.472]
 [0.62 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]]
actor:  0 policy actor:  1  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.9389322580645161 -0.504 -0.504
probs:  [0.051373191288091535, 0.0752564665826815, 0.051373191288091535, 0.10808844278472317, 0.3549203737862057, 0.3589883342702065]
actor:  0 policy actor:  1  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  28 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  28 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  55.51648953552752
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.9623710248159
actor:  0 policy actor:  1  step number:  32 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  42.027571552519326
printing an ep nov before normalisation:  72.80925021992176
printing an ep nov before normalisation:  35.1686156677138
actor:  1 policy actor:  1  step number:  50 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.8958641255605382 0.41966666666666674 0.41966666666666674
probs:  [0.0694703549088815, 0.0694703549088815, 0.0694703549088815, 0.0694703549088815, 0.24111365708285615, 0.4810049232816179]
printing an ep nov before normalisation:  45.96460968214129
printing an ep nov before normalisation:  54.43658852765219
maxi score, test score, baseline:  -0.8958641255605382 0.41966666666666674 0.41966666666666674
probs:  [0.0694703549088815, 0.0694703549088815, 0.0694703549088815, 0.0694703549088815, 0.24111365708285615, 0.4810049232816179]
maxi score, test score, baseline:  -0.8958641255605382 0.41966666666666674 0.41966666666666674
probs:  [0.0694703549088815, 0.0694703549088815, 0.0694703549088815, 0.0694703549088815, 0.24111365708285615, 0.4810049232816179]
printing an ep nov before normalisation:  45.51728976137936
actions average: 
K:  3  action  0 :  tensor([0.6058, 0.0338, 0.0780, 0.0638, 0.0802, 0.0672, 0.0712],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0339, 0.8607, 0.0208, 0.0207, 0.0156, 0.0155, 0.0327],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2713, 0.0334, 0.1400, 0.1383, 0.1443, 0.1453, 0.1275],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2906, 0.1011, 0.1205, 0.1239, 0.1317, 0.1218, 0.1104],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2315, 0.1537, 0.1086, 0.1007, 0.2069, 0.1070, 0.0915],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2454, 0.0335, 0.1484, 0.1462, 0.1465, 0.1457, 0.1342],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1838, 0.0820, 0.1070, 0.1410, 0.1209, 0.1099, 0.2554],
       grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
Printing some Q and Qe and total Qs values:  [[-0.203]
 [-0.198]
 [-0.204]
 [-0.204]
 [-0.204]
 [-0.204]
 [-0.204]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.203]
 [-0.198]
 [-0.204]
 [-0.204]
 [-0.204]
 [-0.204]
 [-0.204]]
printing an ep nov before normalisation:  45.184879291143154
printing an ep nov before normalisation:  48.28729892726541
printing an ep nov before normalisation:  3.7719851775364077e-06
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
Printing some Q and Qe and total Qs values:  [[-0.192]
 [-0.192]
 [-0.211]
 [-0.192]
 [-0.192]
 [-0.192]
 [-0.192]] [[39.548]
 [39.548]
 [44.362]
 [39.548]
 [39.548]
 [39.548]
 [39.548]] [[0.891]
 [0.891]
 [1.128]
 [0.891]
 [0.891]
 [0.891]
 [0.891]]
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
probs:  [0.06943443091404226, 0.06943443091404226, 0.06943443091404226, 0.06943443091404226, 0.24153657384237023, 0.4807257025014608]
printing an ep nov before normalisation:  35.089056321631496
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
printing an ep nov before normalisation:  43.514458114179064
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.553]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]] [[37.179]
 [38.19 ]
 [37.179]
 [37.179]
 [37.179]
 [37.179]
 [37.179]] [[1.35 ]
 [1.738]
 [1.35 ]
 [1.35 ]
 [1.35 ]
 [1.35 ]
 [1.35 ]]
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
probs:  [0.06943443091404226, 0.06943443091404226, 0.06943443091404226, 0.06943443091404226, 0.24153657384237023, 0.4807257025014608]
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
probs:  [0.06943443091404226, 0.06943443091404226, 0.06943443091404226, 0.06943443091404226, 0.24153657384237023, 0.4807257025014608]
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
probs:  [0.0694705114027113, 0.0694705114027113, 0.0694705114027113, 0.0694705114027113, 0.24114203962103356, 0.48097591476812124]
printing an ep nov before normalisation:  31.177004795151053
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
probs:  [0.0694705114027113, 0.0694705114027113, 0.0694705114027113, 0.0694705114027113, 0.24114203962103356, 0.48097591476812124]
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
probs:  [0.0694705114027113, 0.0694705114027113, 0.0694705114027113, 0.0694705114027113, 0.24114203962103356, 0.48097591476812124]
actor:  1 policy actor:  1  step number:  77 total reward:  0.06666666666666554  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
probs:  [0.06944658457689046, 0.06944658457689046, 0.06944658457689046, 0.06944658457689046, 0.24141370237489676, 0.48079995931754144]
siam score:  -0.80570513
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
probs:  [0.0694226754238341, 0.0694226754238341, 0.0694226754238341, 0.0694226754238341, 0.24168516447398425, 0.48062413383067937]
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.40156590258504
printing an ep nov before normalisation:  39.78029508691341
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
probs:  [0.06939878392396935, 0.06939878392396935, 0.06939878392396935, 0.06939878392396935, 0.2419564261405243, 0.4804484381635982]
printing an ep nov before normalisation:  39.879267216353625
printing an ep nov before normalisation:  14.604465961456299
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
probs:  [0.06937491005775227, 0.06937491005775227, 0.06937491005775227, 0.06937491005775227, 0.24222748759641738, 0.48027287217257364]
printing an ep nov before normalisation:  22.36646996595946
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
probs:  [0.06937491005775227, 0.06937491005775227, 0.06937491005775227, 0.06937491005775227, 0.24222748759641738, 0.48027287217257364]
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
line 256 mcts: sample exp_bonus 36.52741121342906
actor:  1 policy actor:  1  step number:  56 total reward:  0.43333333333333357  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.179]
 [-0.162]
 [-0.179]
 [-0.179]
 [-0.179]
 [-0.179]
 [-0.179]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.179]
 [-0.162]
 [-0.179]
 [-0.179]
 [-0.179]
 [-0.179]
 [-0.179]]
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
probs:  [0.06946072692893387, 0.06946072692893387, 0.06946072692893387, 0.06946072692893387, 0.24125313154402317, 0.4809039607402413]
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
probs:  [0.06943703461541238, 0.06943703461541238, 0.06943703461541238, 0.06943703461541238, 0.2415221316690223, 0.4807297298693281]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.98194408416748
printing an ep nov before normalisation:  31.210624224730807
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
probs:  [0.06947300415817892, 0.06947300415817892, 0.06947300415817892, 0.06947300415817892, 0.24112881790407978, 0.4809791654632045]
Printing some Q and Qe and total Qs values:  [[-0.112]
 [-0.114]
 [-0.104]
 [-0.104]
 [-0.119]
 [-0.104]
 [-0.104]] [[36.561]
 [37.538]
 [37.321]
 [37.321]
 [36.322]
 [37.321]
 [37.321]] [[1.791]
 [1.886]
 [1.875]
 [1.875]
 [1.76 ]
 [1.875]
 [1.875]]
Printing some Q and Qe and total Qs values:  [[-0.118]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]] [[42.061]
 [42.061]
 [42.061]
 [42.061]
 [42.061]
 [42.061]
 [42.061]] [[1.475]
 [1.475]
 [1.475]
 [1.475]
 [1.475]
 [1.475]
 [1.475]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
probs:  [0.06942576914217238, 0.06942576914217238, 0.06942576914217238, 0.06942576914217238, 0.24166517075649935, 0.48063175267481095]
printing an ep nov before normalisation:  40.32570534031267
printing an ep nov before normalisation:  24.01751848595114
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
probs:  [0.06946165695058724, 0.06946165695058724, 0.06946165695058724, 0.06946165695058724, 0.24127276101011827, 0.4808806111875327]
printing an ep nov before normalisation:  18.071219336820587
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
probs:  [0.06946165695058724, 0.06946165695058724, 0.06946165695058724, 0.06946165695058724, 0.24127276101011827, 0.4808806111875327]
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
probs:  [0.06946165695058724, 0.06946165695058724, 0.06946165695058724, 0.06946165695058724, 0.24127276101011827, 0.4808806111875327]
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
probs:  [0.06941458776792948, 0.06941458776792948, 0.06941458776792948, 0.06941458776792948, 0.24180728192744402, 0.4805343670008381]
maxi score, test score, baseline:  -0.8960968680089486 0.41966666666666674 0.41966666666666674
Printing some Q and Qe and total Qs values:  [[-0.09 ]
 [-0.091]
 [-0.091]
 [-0.091]
 [-0.091]
 [-0.091]
 [-0.091]] [[48.664]
 [46.366]
 [46.366]
 [46.366]
 [46.366]
 [46.366]
 [46.366]] [[1.513]
 [1.413]
 [1.413]
 [1.413]
 [1.413]
 [1.413]
 [1.413]]
Printing some Q and Qe and total Qs values:  [[-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]] [[34.207]
 [34.207]
 [34.207]
 [34.207]
 [34.207]
 [34.207]
 [34.207]] [[1.508]
 [1.508]
 [1.508]
 [1.508]
 [1.508]
 [1.508]
 [1.508]]
from probs:  [0.0693910788380424, 0.0693910788380424, 0.0693910788380424, 0.0693910788380424, 0.24207425097298604, 0.4803614336748444]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8963285714285715 0.41966666666666674 0.41966666666666674
Printing some Q and Qe and total Qs values:  [[-0.035]
 [-0.034]
 [-0.034]
 [-0.034]
 [-0.034]
 [-0.034]
 [-0.034]] [[41.745]
 [39.572]
 [39.572]
 [39.572]
 [39.572]
 [39.572]
 [39.572]] [[1.632]
 [1.543]
 [1.543]
 [1.543]
 [1.543]
 [1.543]
 [1.543]]
printing an ep nov before normalisation:  30.712890087067805
from probs:  [0.0694034896889312, 0.0694034896889312, 0.0694034896889312, 0.0694034896889312, 0.24194847404339598, 0.48043756720087927]
printing an ep nov before normalisation:  28.145296514490724
maxi score, test score, baseline:  -0.8963285714285715 0.41966666666666674 0.41966666666666674
probs:  [0.06941583609070055, 0.06941583609070055, 0.06941583609070055, 0.06941583609070055, 0.2418234048530326, 0.48051325078416524]
maxi score, test score, baseline:  -0.8965592427616927 0.41966666666666674 0.41966666666666674
probs:  [0.06939247411162625, 0.06939247411162625, 0.06939247411162625, 0.06939247411162625, 0.2420887558534266, 0.48034134770006837]
actor:  0 policy actor:  1  step number:  41 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  52.76840238919199
printing an ep nov before normalisation:  32.872526637440885
printing an ep nov before normalisation:  39.292406395723084
maxi score, test score, baseline:  -0.8941477876106194 0.41966666666666674 0.41966666666666674
probs:  [0.06936912900913948, 0.06936912900913948, 0.06936912900913948, 0.06936912900913948, 0.24235391516546353, 0.48016956879797856]
maxi score, test score, baseline:  -0.8941477876106194 0.41966666666666674 0.41966666666666674
probs:  [0.06940482102779441, 0.06940482102779441, 0.06940482102779441, 0.06940482102779441, 0.24196368203995602, 0.4804170338488664]
line 256 mcts: sample exp_bonus 23.679157868072807
maxi score, test score, baseline:  -0.8941477876106194 0.41966666666666674 0.41966666666666674
probs:  [0.06940482102779441, 0.06940482102779441, 0.06940482102779441, 0.06940482102779441, 0.24196368203995602, 0.4804170338488664]
printing an ep nov before normalisation:  33.64435072834722
printing an ep nov before normalisation:  0.0007400161609893985
printing an ep nov before normalisation:  30.723469666067583
maxi score, test score, baseline:  -0.8941477876106194 0.41966666666666674 0.41966666666666674
probs:  [0.06935827624303695, 0.06935827624303695, 0.06935827624303695, 0.06935827624303695, 0.24249239950337398, 0.4800744955244781]
UNIT TEST: sample policy line 217 mcts : [0.102 0.388 0.245 0.041 0.    0.204 0.02 ]
Printing some Q and Qe and total Qs values:  [[0.781]
 [0.827]
 [0.776]
 [0.776]
 [0.746]
 [0.776]
 [0.776]] [[30.6  ]
 [34.43 ]
 [35.704]
 [35.704]
 [29.813]
 [35.704]
 [35.704]] [[0.781]
 [0.827]
 [0.776]
 [0.776]
 [0.746]
 [0.776]
 [0.776]]
Printing some Q and Qe and total Qs values:  [[-0.187]
 [-0.193]
 [-0.191]
 [-0.176]
 [-0.192]
 [-0.192]
 [-0.176]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.187]
 [-0.193]
 [-0.191]
 [-0.176]
 [-0.192]
 [-0.192]
 [-0.176]]
actions average: 
K:  2  action  0 :  tensor([0.5249, 0.1000, 0.0704, 0.0656, 0.0993, 0.0693, 0.0705],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0144, 0.8708, 0.0316, 0.0206, 0.0064, 0.0219, 0.0344],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2709, 0.0107, 0.1375, 0.1390, 0.1545, 0.1416, 0.1458],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2737, 0.1093, 0.1193, 0.1252, 0.1294, 0.1402, 0.1029],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2744, 0.0231, 0.1014, 0.1072, 0.2741, 0.1289, 0.0908],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.3563, 0.0628, 0.1342, 0.0925, 0.1101, 0.1649, 0.0792],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2968, 0.1128, 0.1396, 0.1104, 0.1082, 0.1175, 0.1148],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8943812362030905 0.41966666666666674 0.41966666666666674
probs:  [0.06939388745037665, 0.06939388745037665, 0.06939388745037665, 0.06939388745037665, 0.24210306011548802, 0.48032139008300545]
printing an ep nov before normalisation:  53.53157217035399
maxi score, test score, baseline:  -0.8943812362030905 0.41966666666666674 0.41966666666666674
maxi score, test score, baseline:  -0.8943812362030905 0.41966666666666674 0.41966666666666674
probs:  [0.06939388745037665, 0.06939388745037665, 0.06939388745037665, 0.06939388745037665, 0.24210306011548802, 0.48032139008300545]
maxi score, test score, baseline:  -0.8943812362030905 0.41966666666666674 0.41966666666666674
probs:  [0.06937068733793088, 0.06937068733793088, 0.06937068733793088, 0.06937068733793088, 0.24236662300031014, 0.48015062764796634]
printing an ep nov before normalisation:  25.859972806882272
Printing some Q and Qe and total Qs values:  [[-0.192]
 [-0.23 ]
 [-0.17 ]
 [-0.181]
 [-0.178]
 [-0.178]
 [-0.178]] [[48.068]
 [48.641]
 [51.504]
 [43.679]
 [47.16 ]
 [47.16 ]
 [47.16 ]] [[0.804]
 [0.779]
 [0.898]
 [0.724]
 [0.8  ]
 [0.8  ]
 [0.8  ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8943812362030905 0.41966666666666674 0.41966666666666674
probs:  [0.06937068733793088, 0.06937068733793088, 0.06937068733793088, 0.06937068733793088, 0.24236662300031014, 0.48015062764796634]
maxi score, test score, baseline:  -0.8943812362030905 0.41966666666666674 0.41966666666666674
probs:  [0.06937068733793088, 0.06937068733793088, 0.06937068733793088, 0.06937068733793088, 0.24236662300031014, 0.48015062764796634]
from probs:  [0.06937068733793088, 0.06937068733793088, 0.06937068733793088, 0.06937068733793088, 0.24236662300031014, 0.48015062764796634]
maxi score, test score, baseline:  -0.8946136563876652 0.41966666666666674 0.41966666666666674
probs:  [0.06937068733793088, 0.06937068733793088, 0.06937068733793088, 0.06937068733793088, 0.24236662300031014, 0.48015062764796634]
siam score:  -0.7972835
maxi score, test score, baseline:  -0.8946136563876652 0.41966666666666674 0.41966666666666674
probs:  [0.06944148166503969, 0.06944148166503969, 0.06944148166503969, 0.06944148166503969, 0.24159263466834305, 0.4806414386714982]
maxi score, test score, baseline:  -0.8946136563876652 0.41966666666666674 0.41966666666666674
Printing some Q and Qe and total Qs values:  [[-0.194]
 [-0.187]
 [-0.187]
 [-0.187]
 [-0.187]
 [-0.187]
 [-0.187]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.194]
 [-0.187]
 [-0.187]
 [-0.187]
 [-0.187]
 [-0.187]
 [-0.187]]
maxi score, test score, baseline:  -0.8946136563876652 0.41966666666666674 0.41966666666666674
probs:  [0.06947662018023952, 0.06947662018023952, 0.06947662018023952, 0.06947662018023952, 0.24120846828272952, 0.48088505099631224]
printing an ep nov before normalisation:  30.407533206493625
maxi score, test score, baseline:  -0.8946136563876652 0.41966666666666674 0.41966666666666674
probs:  [0.06947662018023952, 0.06947662018023952, 0.06947662018023952, 0.06947662018023952, 0.24120846828272952, 0.48088505099631224]
printing an ep nov before normalisation:  51.42341679505911
actions average: 
K:  4  action  0 :  tensor([0.4798, 0.0346, 0.0872, 0.1089, 0.1125, 0.0880, 0.0889],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0378, 0.7678, 0.0399, 0.0610, 0.0272, 0.0318, 0.0345],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2796, 0.0792, 0.1425, 0.1553, 0.1118, 0.1269, 0.1048],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.3225, 0.1842, 0.0620, 0.1590, 0.0700, 0.0832, 0.1191],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.4830, 0.0372, 0.0972, 0.1040, 0.0965, 0.0932, 0.0888],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2299, 0.0475, 0.1929, 0.1146, 0.1194, 0.1842, 0.1116],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.3163, 0.0536, 0.1006, 0.1722, 0.1283, 0.1098, 0.1191],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]] [[17.446]
 [13.637]
 [13.637]
 [13.637]
 [13.637]
 [13.637]
 [13.637]] [[0.365]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]]
printing an ep nov before normalisation:  14.44876828718363
printing an ep nov before normalisation:  22.9349684715271
actions average: 
K:  4  action  0 :  tensor([0.4814, 0.0247, 0.0929, 0.1024, 0.1106, 0.0849, 0.1032],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0483, 0.7818, 0.0321, 0.0301, 0.0297, 0.0218, 0.0561],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1743, 0.0734, 0.3031, 0.1008, 0.1035, 0.1054, 0.1395],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.3187, 0.2162, 0.0822, 0.1022, 0.0946, 0.0821, 0.1039],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.3034, 0.0099, 0.1198, 0.1447, 0.1509, 0.1293, 0.1421],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1785, 0.0982, 0.1116, 0.1409, 0.1332, 0.2301, 0.1075],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2580, 0.1025, 0.1468, 0.1279, 0.1056, 0.1074, 0.1518],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8953048140043763 0.41966666666666674 0.41966666666666674
probs:  [0.06947662018023952, 0.06947662018023952, 0.06947662018023952, 0.06947662018023952, 0.24120846828272952, 0.48088505099631224]
Printing some Q and Qe and total Qs values:  [[-0.086]
 [-0.104]
 [-0.09 ]
 [-0.089]
 [-0.086]
 [-0.086]
 [-0.076]] [[31.37 ]
 [33.541]
 [27.714]
 [28.693]
 [28.553]
 [28.605]
 [29.22 ]] [[0.693]
 [0.779]
 [0.514]
 [0.561]
 [0.558]
 [0.561]
 [0.6  ]]
maxi score, test score, baseline:  -0.8955331877729258 0.41966666666666674 0.41966666666666674
probs:  [0.06945357693471058, 0.06945357693471058, 0.06945357693471058, 0.06945357693471058, 0.24147032422324521, 0.4807153680379124]
printing an ep nov before normalisation:  34.190598210433826
maxi score, test score, baseline:  -0.8955331877729258 0.41966666666666674 0.41966666666666674
probs:  [0.06945357693471058, 0.06945357693471058, 0.06945357693471058, 0.06945357693471058, 0.24147032422324521, 0.4807153680379124]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8955331877729258 0.41966666666666674 0.41966666666666674
probs:  [0.06943055011009415, 0.06943055011009415, 0.06943055011009415, 0.06943055011009415, 0.2417319935618567, 0.4805458059977667]
Printing some Q and Qe and total Qs values:  [[-0.124]
 [-0.122]
 [-0.131]
 [-0.131]
 [-0.129]
 [-0.129]
 [-0.13 ]] [[34.33 ]
 [33.625]
 [34.32 ]
 [33.7  ]
 [33.572]
 [33.946]
 [34.844]] [[1.378]
 [1.321]
 [1.37 ]
 [1.318]
 [1.309]
 [1.341]
 [1.415]]
Printing some Q and Qe and total Qs values:  [[-0.124]
 [-0.124]
 [-0.124]
 [-0.124]
 [-0.124]
 [-0.124]
 [-0.124]] [[42.436]
 [42.436]
 [42.436]
 [42.436]
 [42.436]
 [42.436]
 [42.436]] [[1.782]
 [1.782]
 [1.782]
 [1.782]
 [1.782]
 [1.782]
 [1.782]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.28666666666666607  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.8955331877729258 0.41966666666666674 0.41966666666666674
probs:  [0.06940753968884388, 0.06940753968884388, 0.06940753968884388, 0.06940753968884388, 0.24199347649795516, 0.48037636474666934]
actions average: 
K:  3  action  0 :  tensor([0.5550, 0.0502, 0.0743, 0.0814, 0.0934, 0.0740, 0.0717],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0233, 0.8774, 0.0199, 0.0183, 0.0122, 0.0100, 0.0389],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1959, 0.0096, 0.2207, 0.1436, 0.1479, 0.1728, 0.1095],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2611, 0.0398, 0.1101, 0.1734, 0.1700, 0.1214, 0.1242],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2706, 0.0034, 0.1191, 0.1073, 0.2519, 0.1316, 0.1160],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2667, 0.0980, 0.2254, 0.0956, 0.0976, 0.1322, 0.0845],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2497, 0.1298, 0.1237, 0.1304, 0.1057, 0.1143, 0.1464],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8955331877729258 0.41966666666666674 0.41966666666666674
probs:  [0.06940753968884388, 0.06940753968884388, 0.06940753968884388, 0.06940753968884388, 0.24199347649795516, 0.48037636474666934]
maxi score, test score, baseline:  -0.8955331877729258 0.41966666666666674 0.41966666666666674
probs:  [0.06940753968884388, 0.06940753968884388, 0.06940753968884388, 0.06940753968884388, 0.24199347649795516, 0.48037636474666934]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.2533333333333331  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.8955331877729258 0.41966666666666674 0.41966666666666674
printing an ep nov before normalisation:  28.455462090687778
maxi score, test score, baseline:  -0.8955331877729258 0.41966666666666674 0.41966666666666674
probs:  [0.06941969858660145, 0.06941969858660145, 0.06941969858660145, 0.06941969858660145, 0.24187046934766246, 0.4804507363059318]
siam score:  -0.80040026
maxi score, test score, baseline:  -0.8955331877729258 0.41966666666666674 0.41966666666666674
probs:  [0.06941969858660145, 0.06941969858660145, 0.06941969858660145, 0.06941969858660145, 0.24187046934766246, 0.4804507363059318]
maxi score, test score, baseline:  -0.8955331877729258 0.41966666666666674 0.41966666666666674
probs:  [0.06941969858660145, 0.06941969858660145, 0.06941969858660145, 0.06941969858660145, 0.24187046934766246, 0.4804507363059318]
actions average: 
K:  0  action  0 :  tensor([0.6186, 0.0043, 0.0600, 0.0583, 0.1351, 0.0576, 0.0660],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0102, 0.9545, 0.0099, 0.0079, 0.0044, 0.0044, 0.0086],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2035, 0.0057, 0.4011, 0.0873, 0.1035, 0.1245, 0.0743],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.3322, 0.0090, 0.1202, 0.1389, 0.1533, 0.1152, 0.1311],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.3029, 0.0017, 0.0957, 0.1127, 0.2840, 0.1110, 0.0919],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.3509, 0.0060, 0.1275, 0.1059, 0.1339, 0.1884, 0.0874],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.3192, 0.0126, 0.1371, 0.1321, 0.1474, 0.1289, 0.1227],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8957605664488018 0.41966666666666674 0.41966666666666674
probs:  [0.06941969858660145, 0.06941969858660145, 0.06941969858660145, 0.06941969858660145, 0.24187046934766246, 0.4804507363059318]
actor:  1 policy actor:  1  step number:  58 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 0.0
using explorer policy with actor:  1
siam score:  -0.7954771
maxi score, test score, baseline:  -0.8957605664488018 0.41966666666666674 0.41966666666666674
probs:  [0.06945468134070884, 0.06945468134070884, 0.06945468134070884, 0.06945468134070884, 0.2414880259202921, 0.48069324871687263]
maxi score, test score, baseline:  -0.8957605664488018 0.41966666666666674 0.41966666666666674
probs:  [0.06945468134070884, 0.06945468134070884, 0.06945468134070884, 0.06945468134070884, 0.2414880259202921, 0.48069324871687263]
printing an ep nov before normalisation:  37.171018702698966
maxi score, test score, baseline:  -0.8957605664488018 0.41966666666666674 0.41966666666666674
probs:  [0.06945468134070884, 0.06945468134070884, 0.06945468134070884, 0.06945468134070884, 0.2414880259202921, 0.48069324871687263]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
actor:  1 policy actor:  1  step number:  64 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.73410828272252
printing an ep nov before normalisation:  25.358578386824266
siam score:  -0.7951099
Printing some Q and Qe and total Qs values:  [[-0.154]
 [-0.165]
 [-0.165]
 [-0.165]
 [-0.165]
 [-0.165]
 [-0.165]] [[16.875]
 [14.074]
 [14.074]
 [14.074]
 [14.074]
 [14.074]
 [14.074]] [[ 0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]]
Printing some Q and Qe and total Qs values:  [[-0.22 ]
 [-0.156]
 [-0.277]
 [-0.194]
 [-0.2  ]
 [-0.253]
 [-0.194]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.22 ]
 [-0.156]
 [-0.277]
 [-0.194]
 [-0.2  ]
 [-0.253]
 [-0.194]]
maxi score, test score, baseline:  -0.8957605664488018 0.41966666666666674 0.41966666666666674
probs:  [0.06945468134070884, 0.06945468134070884, 0.06945468134070884, 0.06945468134070884, 0.2414880259202921, 0.48069324871687263]
maxi score, test score, baseline:  -0.8957605664488018 0.41966666666666674 0.41966666666666674
probs:  [0.06945468134070884, 0.06945468134070884, 0.06945468134070884, 0.06945468134070884, 0.2414880259202921, 0.48069324871687263]
actor:  0 policy actor:  0  step number:  70 total reward:  0.09999999999999964  reward:  1.0 rdn_beta:  0.667
from probs:  [0.06945468134070884, 0.06945468134070884, 0.06945468134070884, 0.06945468134070884, 0.2414880259202921, 0.48069324871687263]
maxi score, test score, baseline:  -0.8935956521739131 0.41966666666666674 0.41966666666666674
actions average: 
K:  0  action  0 :  tensor([0.5416, 0.0102, 0.0762, 0.0833, 0.1101, 0.0929, 0.0856],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0257, 0.8890, 0.0194, 0.0159, 0.0099, 0.0118, 0.0283],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1904, 0.0503, 0.2272, 0.1195, 0.1272, 0.1496, 0.1358],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1525, 0.1067, 0.0831, 0.2702, 0.1544, 0.1129, 0.1201],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2725, 0.0126, 0.0873, 0.1127, 0.3005, 0.1122, 0.1022],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1808, 0.0044, 0.1095, 0.0977, 0.0957, 0.3928, 0.1190],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.3014, 0.1199, 0.0933, 0.1050, 0.0911, 0.0808, 0.2085],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8935956521739131 0.41966666666666674 0.41966666666666674
probs:  [0.06945468134070884, 0.06945468134070884, 0.06945468134070884, 0.06945468134070884, 0.2414880259202921, 0.48069324871687263]
maxi score, test score, baseline:  -0.8935956521739131 0.41966666666666674 0.41966666666666674
probs:  [0.06945468134070884, 0.06945468134070884, 0.06945468134070884, 0.06945468134070884, 0.2414880259202921, 0.48069324871687263]
from probs:  [0.06945468134070884, 0.06945468134070884, 0.06945468134070884, 0.06945468134070884, 0.2414880259202921, 0.48069324871687263]
printing an ep nov before normalisation:  48.7483513580951
printing an ep nov before normalisation:  21.632120609283447
printing an ep nov before normalisation:  21.868653739132032
maxi score, test score, baseline:  -0.8935956521739131 0.41966666666666674 0.41966666666666674
maxi score, test score, baseline:  -0.8935956521739131 0.41966666666666674 0.41966666666666674
probs:  [0.06948949514855866, 0.06948949514855866, 0.06948949514855866, 0.06948949514855866, 0.24110742947116684, 0.4809345899345986]
printing an ep nov before normalisation:  36.02686643600464
printing an ep nov before normalisation:  41.1794665025258
maxi score, test score, baseline:  -0.8935956521739131 0.41966666666666674 0.41966666666666674
probs:  [0.06948949514855866, 0.06948949514855866, 0.06948949514855866, 0.06948949514855866, 0.24110742947116684, 0.4809345899345986]
maxi score, test score, baseline:  -0.8938262472885034 0.41966666666666674 0.41966666666666674
maxi score, test score, baseline:  -0.8938262472885034 0.41966666666666674 0.41966666666666674
printing an ep nov before normalisation:  48.247766703319094
maxi score, test score, baseline:  -0.8942844492440606 0.41966666666666674 0.41966666666666674
probs:  [0.06952414123107026, 0.06952414123107026, 0.06952414123107026, 0.06952414123107026, 0.24072866665277887, 0.4811747684229401]
maxi score, test score, baseline:  -0.8942844492440606 0.41966666666666674 0.41966666666666674
probs:  [0.06952414123107026, 0.06952414123107026, 0.06952414123107026, 0.06952414123107026, 0.24072866665277887, 0.4811747684229401]
printing an ep nov before normalisation:  53.037718027946454
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.194]
 [-0.194]
 [-0.194]
 [-0.194]
 [-0.194]
 [-0.205]
 [-0.194]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.194]
 [-0.194]
 [-0.194]
 [-0.194]
 [-0.194]
 [-0.205]
 [-0.194]]
maxi score, test score, baseline:  -0.8945120689655173 0.41966666666666674 0.41966666666666674
maxi score, test score, baseline:  -0.8945120689655173 0.41966666666666674 0.41966666666666674
probs:  [0.06955862079742701, 0.06955862079742701, 0.06955862079742701, 0.06955862079742701, 0.240351724245922, 0.48141379256436995]
printing an ep nov before normalisation:  47.83357034283433
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8945120689655173 0.41966666666666674 0.41966666666666674
probs:  [0.06955862079742701, 0.06955862079742701, 0.06955862079742701, 0.06955862079742701, 0.240351724245922, 0.48141379256436995]
printing an ep nov before normalisation:  66.91751331603903
printing an ep nov before normalisation:  47.06564695917337
maxi score, test score, baseline:  -0.8947387096774194 0.41966666666666674 0.41966666666666674
maxi score, test score, baseline:  -0.8947387096774194 0.41966666666666674 0.41966666666666674
probs:  [0.06959293504521706, 0.06959293504521706, 0.06959293504521706, 0.06959293504521706, 0.2399765891581536, 0.4816516706609782]
printing an ep nov before normalisation:  51.84108086100671
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.8947387096774194 0.41966666666666674 0.41966666666666674
probs:  [0.06959293504521706, 0.06959293504521706, 0.06959293504521706, 0.06959293504521706, 0.2399765891581536, 0.4816516706609782]
actions average: 
K:  2  action  0 :  tensor([0.6168, 0.0269, 0.0866, 0.0594, 0.0897, 0.0615, 0.0591],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0238, 0.9054, 0.0149, 0.0192, 0.0121, 0.0103, 0.0144],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1970, 0.0563, 0.2497, 0.1158, 0.1142, 0.1659, 0.1011],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1977, 0.0267, 0.1091, 0.2876, 0.1086, 0.0938, 0.1765],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2676, 0.0583, 0.1083, 0.0921, 0.3051, 0.0900, 0.0786],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2692, 0.0362, 0.1736, 0.1294, 0.1439, 0.1211, 0.1266],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2119, 0.1537, 0.1558, 0.1210, 0.0975, 0.1201, 0.1401],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  30.049887118577054
UNIT TEST: sample policy line 217 mcts : [0.082 0.204 0.082 0.082 0.163 0.102 0.286]
actor:  1 policy actor:  1  step number:  73 total reward:  0.10666666666666602  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  53.014914167088634
maxi score, test score, baseline:  -0.8947387096774194 0.41966666666666674 0.41966666666666674
probs:  [0.06959293504521706, 0.06959293504521706, 0.06959293504521706, 0.06959293504521706, 0.2399765891581536, 0.4816516706609782]
maxi score, test score, baseline:  -0.8947387096774194 0.41966666666666674 0.41966666666666674
probs:  [0.06959293504521706, 0.06959293504521706, 0.06959293504521706, 0.06959293504521706, 0.2399765891581536, 0.4816516706609782]
maxi score, test score, baseline:  -0.8947387096774194 0.41966666666666674 0.41966666666666674
probs:  [0.06959293504521706, 0.06959293504521706, 0.06959293504521706, 0.06959293504521706, 0.2399765891581536, 0.4816516706609782]
Printing some Q and Qe and total Qs values:  [[-0.137]
 [-0.129]
 [-0.137]
 [-0.137]
 [-0.137]
 [-0.137]
 [-0.137]] [[38.061]
 [52.514]
 [38.061]
 [38.061]
 [38.061]
 [38.061]
 [38.061]] [[0.479]
 [1.081]
 [0.479]
 [0.479]
 [0.479]
 [0.479]
 [0.479]]
printing an ep nov before normalisation:  53.706468646845224
maxi score, test score, baseline:  -0.8947387096774194 0.41966666666666674 0.41966666666666674
probs:  [0.06959293504521706, 0.06959293504521706, 0.06959293504521706, 0.06959293504521706, 0.2399765891581536, 0.4816516706609782]
printing an ep nov before normalisation:  35.55390018457092
actor:  0 policy actor:  1  step number:  80 total reward:  0.0066666666666662655  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.182]
 [-0.133]
 [-0.173]
 [-0.182]
 [-0.182]
 [-0.182]
 [-0.182]] [[49.522]
 [52.663]
 [61.736]
 [49.522]
 [49.522]
 [49.522]
 [49.522]] [[0.833]
 [1.009]
 [1.332]
 [0.833]
 [0.833]
 [0.833]
 [0.833]]
actions average: 
K:  4  action  0 :  tensor([0.6346, 0.0367, 0.0727, 0.0653, 0.0719, 0.0610, 0.0579],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0880, 0.6928, 0.0377, 0.0582, 0.0376, 0.0282, 0.0576],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2225, 0.1345, 0.1921, 0.1103, 0.1195, 0.1128, 0.1083],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1980, 0.0575, 0.1266, 0.2228, 0.1253, 0.1298, 0.1400],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2989, 0.0564, 0.1376, 0.1345, 0.1497, 0.1173, 0.1056],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.3132, 0.1318, 0.1495, 0.0897, 0.0966, 0.1282, 0.0910],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2889, 0.0578, 0.1430, 0.1345, 0.1104, 0.1429, 0.1225],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8932618233618235 0.41966666666666674 0.41966666666666674
probs:  [0.06959293504521706, 0.06959293504521706, 0.06959293504521706, 0.06959293504521706, 0.2399765891581536, 0.4816516706609782]
printing an ep nov before normalisation:  51.471182800171285
maxi score, test score, baseline:  -0.8932618233618235 0.41966666666666674 0.41966666666666674
probs:  [0.06962708516057182, 0.06962708516057182, 0.06962708516057182, 0.06962708516057182, 0.23960324842227923, 0.4818884109354335]
maxi score, test score, baseline:  -0.8932618233618235 0.41966666666666674 0.41966666666666674
probs:  [0.06962708516057182, 0.06962708516057182, 0.06962708516057182, 0.06962708516057182, 0.23960324842227923, 0.4818884109354335]
maxi score, test score, baseline:  -0.8932618233618235 0.41966666666666674 0.41966666666666674
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8932618233618235 0.41966666666666674 0.41966666666666674
maxi score, test score, baseline:  -0.8932618233618235 0.41966666666666674 0.41966666666666674
probs:  [0.06962708516057182, 0.06962708516057182, 0.06962708516057182, 0.06962708516057182, 0.23960324842227923, 0.4818884109354335]
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.1496281575635
printing an ep nov before normalisation:  55.19478428264738
siam score:  -0.7962309
maxi score, test score, baseline:  -0.8932618233618235 0.41966666666666674 0.41966666666666674
probs:  [0.06962708516057182, 0.06962708516057182, 0.06962708516057182, 0.06962708516057182, 0.23960324842227923, 0.4818884109354335]
maxi score, test score, baseline:  -0.8932618233618235 0.41966666666666674 0.41966666666666674
probs:  [0.06962708516057182, 0.06962708516057182, 0.06962708516057182, 0.06962708516057182, 0.23960324842227923, 0.4818884109354335]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.922]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]] [[48.738]
 [ 0.001]
 [48.738]
 [48.738]
 [48.738]
 [48.738]
 [48.738]] [[0.433]
 [0.922]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]]
printing an ep nov before normalisation:  21.193834306827128
actions average: 
K:  1  action  0 :  tensor([0.5125, 0.0230, 0.0864, 0.0983, 0.0990, 0.0877, 0.0932],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0141, 0.9019, 0.0160, 0.0224, 0.0070, 0.0080, 0.0305],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1007, 0.0589, 0.5150, 0.0790, 0.0689, 0.0917, 0.0857],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1852, 0.0955, 0.0965, 0.3222, 0.0630, 0.0724, 0.1652],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3718, 0.0235, 0.1052, 0.1214, 0.1491, 0.1060, 0.1230],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1782, 0.0445, 0.1243, 0.1529, 0.1169, 0.2577, 0.1254],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1817, 0.1012, 0.1216, 0.1510, 0.0963, 0.1242, 0.2241],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  58 total reward:  0.23333333333333317  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  23.975151603204665
from probs:  [0.06962708516057182, 0.06962708516057182, 0.06962708516057182, 0.06962708516057182, 0.23960324842227923, 0.4818884109354335]
actor:  1 policy actor:  1  step number:  76 total reward:  0.00666666666666571  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  1  action  0 :  tensor([0.5372, 0.0025, 0.0864, 0.0984, 0.0977, 0.0956, 0.0821],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0174, 0.9053, 0.0099, 0.0147, 0.0100, 0.0088, 0.0339],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2017, 0.1168, 0.1954, 0.1236, 0.1124, 0.1181, 0.1320],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.3436, 0.1006, 0.0938, 0.1202, 0.1130, 0.1146, 0.1142],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3353, 0.0766, 0.0445, 0.0686, 0.3711, 0.0489, 0.0550],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.3212, 0.0026, 0.1208, 0.1057, 0.0920, 0.2495, 0.1082],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2386, 0.0875, 0.1111, 0.1467, 0.1287, 0.1148, 0.1727],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8937156028368795 0.41966666666666674 0.41966666666666674
probs:  [0.06962708516057182, 0.06962708516057182, 0.06962708516057182, 0.06962708516057182, 0.23960324842227923, 0.4818884109354335]
printing an ep nov before normalisation:  69.7838702773484
siam score:  -0.7897593
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.147]
 [0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[33.111]
 [34.284]
 [33.111]
 [33.111]
 [33.111]
 [33.111]
 [33.111]] [[1.441]
 [1.637]
 [1.441]
 [1.441]
 [1.441]
 [1.441]
 [1.441]]
maxi score, test score, baseline:  -0.8937156028368795 0.41966666666666674 0.41966666666666674
probs:  [0.06962708516057182, 0.06962708516057182, 0.06962708516057182, 0.06962708516057182, 0.23960324842227923, 0.4818884109354335]
Printing some Q and Qe and total Qs values:  [[-0.017]
 [ 0.101]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]] [[32.121]
 [30.578]
 [32.121]
 [32.121]
 [32.121]
 [32.121]
 [32.121]] [[1.511]
 [1.507]
 [1.511]
 [1.511]
 [1.511]
 [1.511]
 [1.511]]
printing an ep nov before normalisation:  34.51736901650926
printing an ep nov before normalisation:  38.7793804839481
maxi score, test score, baseline:  -0.8937156028368795 0.41966666666666674 0.41966666666666674
maxi score, test score, baseline:  -0.8937156028368795 0.41966666666666674 0.41966666666666674
probs:  [0.06962708516057182, 0.06962708516057182, 0.06962708516057182, 0.06962708516057182, 0.23960324842227923, 0.4818884109354335]
printing an ep nov before normalisation:  29.71700470522187
maxi score, test score, baseline:  -0.8937156028368795 0.41966666666666674 0.41966666666666674
probs:  [0.06962708516057182, 0.06962708516057182, 0.06962708516057182, 0.06962708516057182, 0.23960324842227923, 0.4818884109354335]
actor:  1 policy actor:  1  step number:  61 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  49.412521719932556
Printing some Q and Qe and total Qs values:  [[-0.146]
 [-0.146]
 [-0.146]
 [-0.146]
 [-0.146]
 [-0.146]
 [-0.146]] [[40.955]
 [38.37 ]
 [38.37 ]
 [38.37 ]
 [38.37 ]
 [38.37 ]
 [38.37 ]] [[1.513]
 [1.303]
 [1.303]
 [1.303]
 [1.303]
 [1.303]
 [1.303]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  8.183668340955462
maxi score, test score, baseline:  -0.8937156028368795 0.41966666666666674 0.41966666666666674
probs:  [0.06966107231830275, 0.06966107231830275, 0.06966107231830275, 0.06966107231830275, 0.23923168919485854, 0.4821240215319305]
maxi score, test score, baseline:  -0.8937156028368795 0.41966666666666674 0.41966666666666674
probs:  [0.06966107231830275, 0.06966107231830275, 0.06966107231830275, 0.06966107231830275, 0.23923168919485854, 0.4821240215319305]
maxi score, test score, baseline:  -0.8937156028368795 0.41966666666666674 0.41966666666666674
printing an ep nov before normalisation:  34.46922411569907
printing an ep nov before normalisation:  39.188305051816485
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  59 total reward:  0.33333333333333326  reward:  1.0 rdn_beta:  2.0
using another actor
maxi score, test score, baseline:  -0.8937156028368795 0.41966666666666674 0.41966666666666674
printing an ep nov before normalisation:  25.577355290697188
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8937156028368795 0.41966666666666674 0.41966666666666674
Printing some Q and Qe and total Qs values:  [[0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]] [[38.969]
 [38.969]
 [38.969]
 [38.969]
 [38.969]
 [38.969]
 [38.969]] [[0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]]
maxi score, test score, baseline:  -0.8937156028368795 0.41966666666666674 0.41966666666666674
probs:  [0.06966107231830275, 0.06966107231830275, 0.06966107231830275, 0.06966107231830275, 0.23923168919485854, 0.4821240215319305]
printing an ep nov before normalisation:  39.879594855945626
printing an ep nov before normalisation:  44.21888468655326
from probs:  [0.06966107231830275, 0.06966107231830275, 0.06966107231830275, 0.06966107231830275, 0.23923168919485854, 0.4821240215319305]
printing an ep nov before normalisation:  39.3189621430901
printing an ep nov before normalisation:  22.150076508577897
siam score:  -0.7877767
maxi score, test score, baseline:  -0.8937156028368795 0.41966666666666674 0.41966666666666674
maxi score, test score, baseline:  -0.8937156028368795 0.41966666666666674 0.41966666666666674
probs:  [0.06969489768203611, 0.06969489768203611, 0.06969489768203611, 0.06969489768203611, 0.23886189875473132, 0.48235851051712425]
actor:  0 policy actor:  1  step number:  59 total reward:  0.06666666666666621  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  29.859372892019564
maxi score, test score, baseline:  -0.8919056497175141 0.41966666666666674 0.41966666666666674
probs:  [0.06969489768203611, 0.06969489768203611, 0.06969489768203611, 0.06969489768203611, 0.23886189875473132, 0.48235851051712425]
maxi score, test score, baseline:  -0.8919056497175141 0.41966666666666674 0.41966666666666674
probs:  [0.06969489768203611, 0.06969489768203611, 0.06969489768203611, 0.06969489768203611, 0.23886189875473132, 0.48235851051712425]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.212]
 [0.211]
 [0.217]
 [0.217]
 [0.215]
 [0.211]] [[18.458]
 [37.952]
 [24.432]
 [18.439]
 [18.381]
 [18.56 ]
 [18.763]] [[0.222]
 [0.212]
 [0.211]
 [0.217]
 [0.217]
 [0.215]
 [0.211]]
printing an ep nov before normalisation:  46.56711063414533
printing an ep nov before normalisation:  21.336704476258543
printing an ep nov before normalisation:  57.21666922765172
maxi score, test score, baseline:  -0.8919056497175141 0.41966666666666674 0.41966666666666674
probs:  [0.06969489768203611, 0.06969489768203611, 0.06969489768203611, 0.06969489768203611, 0.23886189875473132, 0.48235851051712425]
printing an ep nov before normalisation:  30.497443675994873
printing an ep nov before normalisation:  46.63813310251834
printing an ep nov before normalisation:  30.804756172657473
printing an ep nov before normalisation:  30.58582960486053
maxi score, test score, baseline:  -0.8923613220815753 0.41966666666666674 0.41966666666666674
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  52.39927374208756
printing an ep nov before normalisation:  47.86171410953624
maxi score, test score, baseline:  -0.8923613220815753 0.41966666666666674 0.41966666666666674
probs:  [0.06979541448050983, 0.06979541448050983, 0.06979541448050983, 0.06979541448050983, 0.2377630147503854, 0.4830553273275752]
siam score:  -0.7796072
maxi score, test score, baseline:  -0.8923613220815753 0.41966666666666674 0.41966666666666674
probs:  [0.06979541448050983, 0.06979541448050983, 0.06979541448050983, 0.06979541448050983, 0.2377630147503854, 0.4830553273275752]
printing an ep nov before normalisation:  33.20510833383668
maxi score, test score, baseline:  -0.8923613220815753 0.41966666666666674 0.41966666666666674
probs:  [0.06979541448050983, 0.06979541448050983, 0.06979541448050983, 0.06979541448050983, 0.2377630147503854, 0.4830553273275752]
printing an ep nov before normalisation:  49.482912235818546
maxi score, test score, baseline:  -0.8923613220815753 0.41966666666666674 0.41966666666666674
probs:  [0.06982860408541869, 0.06982860408541869, 0.06982860408541869, 0.06982860408541869, 0.23740017464307303, 0.48328540901525224]
Printing some Q and Qe and total Qs values:  [[-0.172]
 [-0.155]
 [-0.172]
 [-0.172]
 [-0.158]
 [-0.172]
 [-0.172]] [[29.207]
 [40.653]
 [31.055]
 [31.206]
 [40.35 ]
 [31.358]
 [30.959]] [[0.65 ]
 [1.342]
 [0.758]
 [0.767]
 [1.321]
 [0.776]
 [0.753]]
maxi score, test score, baseline:  -0.8925877192982455 0.41966666666666674 0.41966666666666674
maxi score, test score, baseline:  -0.8925877192982455 0.41966666666666674 0.41966666666666674
probs:  [0.06982860408541869, 0.06982860408541869, 0.06982860408541869, 0.06982860408541869, 0.23740017464307303, 0.48328540901525224]
maxi score, test score, baseline:  -0.8925877192982455 0.41966666666666674 0.41966666666666674
probs:  [0.06982860408541869, 0.06982860408541869, 0.06982860408541869, 0.06982860408541869, 0.23740017464307303, 0.48328540901525224]
printing an ep nov before normalisation:  53.87085894525023
maxi score, test score, baseline:  -0.8925877192982455 0.41966666666666674 0.41966666666666674
probs:  [0.06986163755126497, 0.06986163755126497, 0.06986163755126497, 0.06986163755126497, 0.23703904150137153, 0.48351440829356856]
printing an ep nov before normalisation:  37.58349154874721
using another actor
maxi score, test score, baseline:  -0.8925877192982455 0.41966666666666674 0.41966666666666674
probs:  [0.06986163755126497, 0.06986163755126497, 0.06986163755126497, 0.06986163755126497, 0.23703904150137153, 0.48351440829356856]
maxi score, test score, baseline:  -0.8925877192982455 0.41966666666666674 0.41966666666666674
printing an ep nov before normalisation:  28.476841185874324
printing an ep nov before normalisation:  25.593881736823466
Printing some Q and Qe and total Qs values:  [[-0.189]
 [-0.187]
 [-0.197]
 [-0.192]
 [-0.197]
 [-0.197]
 [-0.199]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.189]
 [-0.187]
 [-0.197]
 [-0.192]
 [-0.197]
 [-0.197]
 [-0.199]]
printing an ep nov before normalisation:  31.702893724202
printing an ep nov before normalisation:  60.4016727871365
printing an ep nov before normalisation:  35.45692443847656
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.534]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]] [[34.203]
 [43.468]
 [32.755]
 [32.755]
 [32.755]
 [32.755]
 [32.755]] [[0.466]
 [0.534]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]]
printing an ep nov before normalisation:  47.7388449328465
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [ 0.2041],
        [-0.0000],
        [-0.4104],
        [-0.6105],
        [-0.5733],
        [-0.5974],
        [-0.0000],
        [-0.1789],
        [-0.4692]], dtype=torch.float64)
-0.026333999999998844 -0.026333999999998844
-0.032346567066 0.1717380558995394
-0.006599999999997514 -0.006599999999997514
-0.058614567066 -0.46902075574295193
-0.032346567066 -0.6428511062499765
-0.032346567066 -0.6056076662548396
-0.032346567066 -0.6297939274098173
-0.558188532 -0.558188532
-0.058354513866000005 -0.2372202035371025
-0.09703970119800001 -0.5662888761773471
printing an ep nov before normalisation:  34.27261829376221
printing an ep nov before normalisation:  38.00099703634737
Printing some Q and Qe and total Qs values:  [[-0.192]
 [-0.164]
 [-0.195]
 [-0.193]
 [-0.184]
 [-0.188]
 [-0.184]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.192]
 [-0.164]
 [-0.195]
 [-0.193]
 [-0.184]
 [-0.188]
 [-0.184]]
maxi score, test score, baseline:  -0.8925877192982455 0.41966666666666674 0.41966666666666674
probs:  [0.06986163755126497, 0.06986163755126497, 0.06986163755126497, 0.06986163755126497, 0.23703904150137153, 0.48351440829356856]
printing an ep nov before normalisation:  29.167510300715076
maxi score, test score, baseline:  -0.8925877192982455 0.41966666666666674 0.41966666666666674
probs:  [0.06986163755126497, 0.06986163755126497, 0.06986163755126497, 0.06986163755126497, 0.23703904150137153, 0.48351440829356856]
using explorer policy with actor:  1
printing an ep nov before normalisation:  27.234022547561306
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8925877192982455 0.41966666666666674 0.41966666666666674
probs:  [0.06986163755126497, 0.06986163755126497, 0.06986163755126497, 0.06986163755126497, 0.23703904150137153, 0.48351440829356856]
actor:  0 policy actor:  1  step number:  65 total reward:  0.1466666666666664  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  47 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.667
UNIT TEST: sample policy line 217 mcts : [0.163 0.306 0.02  0.02  0.041 0.408 0.041]
printing an ep nov before normalisation:  63.32070753508291
maxi score, test score, baseline:  -0.8904042016806722 0.41966666666666674 0.41966666666666674
probs:  [0.06979318117918518, 0.06979318117918518, 0.06979318117918518, 0.06979318117918518, 0.2377874299639598, 0.48303984531929955]
printing an ep nov before normalisation:  47.475995715451155
maxi score, test score, baseline:  -0.8904042016806722 0.41966666666666674 0.41966666666666674
probs:  [0.06979318117918518, 0.06979318117918518, 0.06979318117918518, 0.06979318117918518, 0.2377874299639598, 0.48303984531929955]
printing an ep nov before normalisation:  42.331119765319144
actions average: 
K:  2  action  0 :  tensor([0.5556, 0.0477, 0.0881, 0.0812, 0.0928, 0.0656, 0.0691],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0369, 0.7862, 0.0350, 0.0397, 0.0184, 0.0191, 0.0648],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1466, 0.2273, 0.2023, 0.1176, 0.0862, 0.0816, 0.1385],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2096, 0.0161, 0.1832, 0.1632, 0.1823, 0.1225, 0.1231],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2681, 0.0148, 0.1319, 0.1276, 0.2553, 0.1068, 0.0954],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1874, 0.0513, 0.2377, 0.0992, 0.0878, 0.2446, 0.0919],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2232, 0.1049, 0.1406, 0.1394, 0.1609, 0.1097, 0.1212],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.138]
 [0.176]
 [0.138]
 [0.138]
 [0.138]
 [0.138]
 [0.138]] [[25.398]
 [34.42 ]
 [25.398]
 [25.398]
 [25.398]
 [25.398]
 [25.398]] [[1.44 ]
 [1.949]
 [1.44 ]
 [1.44 ]
 [1.44 ]
 [1.44 ]
 [1.44 ]]
maxi score, test score, baseline:  -0.8904042016806722 0.41966666666666674 0.41966666666666674
probs:  [0.06982609513275301, 0.06982609513275301, 0.06982609513275301, 0.06982609513275301, 0.23742760337134364, 0.48326801609764425]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  51 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.8904042016806722 0.41966666666666674 0.41966666666666674
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.097]
 [-0.077]
 [-0.097]
 [-0.097]
 [-0.097]
 [-0.097]
 [-0.097]] [[53.293]
 [61.402]
 [53.293]
 [53.293]
 [53.293]
 [53.293]
 [53.293]] [[0.912]
 [1.244]
 [0.912]
 [0.912]
 [0.912]
 [0.912]
 [0.912]]
maxi score, test score, baseline:  -0.8904042016806722 0.41966666666666674 0.41966666666666674
probs:  [0.06985885555136954, 0.06985885555136954, 0.06985885555136954, 0.06985885555136954, 0.2370694552753038, 0.483495122519218]
maxi score, test score, baseline:  -0.8904042016806722 0.41966666666666674 0.41966666666666674
probs:  [0.06985885555136954, 0.06985885555136954, 0.06985885555136954, 0.06985885555136954, 0.2370694552753038, 0.483495122519218]
Printing some Q and Qe and total Qs values:  [[-0.194]
 [-0.198]
 [-0.194]
 [-0.194]
 [-0.194]
 [-0.194]
 [-0.195]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.194]
 [-0.198]
 [-0.194]
 [-0.194]
 [-0.194]
 [-0.194]
 [-0.195]]
maxi score, test score, baseline:  -0.8904042016806722 0.41966666666666674 0.41966666666666674
actions average: 
K:  2  action  0 :  tensor([0.5851, 0.0144, 0.0763, 0.0950, 0.0824, 0.0764, 0.0705],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0335, 0.8747, 0.0082, 0.0194, 0.0156, 0.0073, 0.0413],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1642, 0.0629, 0.2642, 0.1420, 0.1025, 0.1340, 0.1302],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2181, 0.1053, 0.1319, 0.1692, 0.1227, 0.1239, 0.1289],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1964, 0.1885, 0.0999, 0.0992, 0.2381, 0.0796, 0.0982],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1611, 0.1115, 0.1286, 0.1357, 0.1210, 0.2144, 0.1277],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.3676, 0.0398, 0.0529, 0.2324, 0.1520, 0.0605, 0.0947],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8904042016806722 0.41966666666666674 0.41966666666666674
maxi score, test score, baseline:  -0.8904042016806722 0.41966666666666674 0.41966666666666674
maxi score, test score, baseline:  -0.8904042016806722 0.41966666666666674 0.41966666666666674
probs:  [0.06989146350683537, 0.06989146350683537, 0.06989146350683537, 0.06989146350683537, 0.23671297395854982, 0.48372117201410875]
maxi score, test score, baseline:  -0.8904042016806722 0.41966666666666674 0.41966666666666674
probs:  [0.069923920060998, 0.069923920060998, 0.069923920060998, 0.069923920060998, 0.2363581478126013, 0.48394617194340667]
Printing some Q and Qe and total Qs values:  [[-0.151]
 [-0.13 ]
 [-0.151]
 [-0.151]
 [-0.151]
 [-0.151]
 [-0.151]] [[47.491]
 [47.825]
 [47.491]
 [47.491]
 [47.491]
 [47.491]
 [47.491]] [[0.131]
 [0.156]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]]
maxi score, test score, baseline:  -0.8904042016806722 0.41966666666666674 0.41966666666666674
probs:  [0.069923920060998, 0.069923920060998, 0.069923920060998, 0.069923920060998, 0.2363581478126013, 0.48394617194340667]
siam score:  -0.7618231
printing an ep nov before normalisation:  28.635280782243946
printing an ep nov before normalisation:  22.540111541748047
Printing some Q and Qe and total Qs values:  [[-0.115]
 [-0.1  ]
 [-0.127]
 [-0.127]
 [-0.127]
 [-0.127]
 [-0.127]] [[25.803]
 [42.396]
 [22.539]
 [22.539]
 [22.539]
 [22.539]
 [22.539]] [[ 0.008]
 [ 0.162]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.75765455
printing an ep nov before normalisation:  37.834437030919055
maxi score, test score, baseline:  -0.8904042016806722 0.41966666666666674 0.41966666666666674
maxi score, test score, baseline:  -0.8904042016806722 0.41966666666666674 0.41966666666666674
maxi score, test score, baseline:  -0.8906337526205451 0.41966666666666674 0.41966666666666674
probs:  [0.06995622626586746, 0.06995622626586746, 0.06995622626586746, 0.06995622626586746, 0.23600496533652546, 0.4841701296000048]
printing an ep nov before normalisation:  53.289896217361274
maxi score, test score, baseline:  -0.8906337526205451 0.41966666666666674 0.41966666666666674
probs:  [0.06995622626586746, 0.06995622626586746, 0.06995622626586746, 0.06995622626586746, 0.23600496533652546, 0.4841701296000048]
Printing some Q and Qe and total Qs values:  [[-0.177]
 [-0.213]
 [-0.182]
 [-0.182]
 [-0.182]
 [-0.182]
 [-0.182]] [[67.479]
 [61.756]
 [59.047]
 [59.047]
 [59.047]
 [59.047]
 [59.047]] [[1.093]
 [0.873]
 [0.818]
 [0.818]
 [0.818]
 [0.818]
 [0.818]]
maxi score, test score, baseline:  -0.8906337526205451 0.41966666666666674 0.41966666666666674
probs:  [0.06995622626586746, 0.06995622626586746, 0.06995622626586746, 0.06995622626586746, 0.23600496533652546, 0.4841701296000048]
printing an ep nov before normalisation:  36.7515019005619
Printing some Q and Qe and total Qs values:  [[0.098]
 [0.299]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]] [[33.253]
 [38.94 ]
 [33.253]
 [33.253]
 [33.253]
 [33.253]
 [33.253]] [[1.052]
 [1.538]
 [1.052]
 [1.052]
 [1.052]
 [1.052]
 [1.052]]
maxi score, test score, baseline:  -0.8908623430962344 0.41966666666666674 0.41966666666666674
probs:  [0.06998838316372971, 0.06998838316372971, 0.06998838316372971, 0.06998838316372971, 0.23565341513569552, 0.4843930522093856]
printing an ep nov before normalisation:  43.374680723557795
printing an ep nov before normalisation:  49.34274353987984
maxi score, test score, baseline:  -0.8908623430962344 0.41966666666666674 0.41966666666666674
probs:  [0.06998838316372971, 0.06998838316372971, 0.06998838316372971, 0.06998838316372971, 0.23565341513569552, 0.4843930522093856]
maxi score, test score, baseline:  -0.8908623430962344 0.41966666666666674 0.41966666666666674
probs:  [0.06998838316372971, 0.06998838316372971, 0.06998838316372971, 0.06998838316372971, 0.23565341513569552, 0.4843930522093856]
printing an ep nov before normalisation:  40.0743114496009
maxi score, test score, baseline:  -0.8908623430962344 0.41966666666666674 0.41966666666666674
probs:  [0.06998838316372971, 0.06998838316372971, 0.06998838316372971, 0.06998838316372971, 0.23565341513569552, 0.4843930522093856]
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.297]
 [0.196]
 [0.196]
 [0.196]
 [0.196]
 [0.196]] [[32.122]
 [27.811]
 [27.256]
 [27.256]
 [27.256]
 [27.256]
 [27.256]] [[1.254]
 [1.327]
 [1.205]
 [1.205]
 [1.205]
 [1.205]
 [1.205]]
printing an ep nov before normalisation:  31.348142623901367
printing an ep nov before normalisation:  32.09163605389274
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]] [[44.939]
 [44.939]
 [44.939]
 [44.939]
 [44.939]
 [44.939]
 [44.939]] [[1.669]
 [1.669]
 [1.669]
 [1.669]
 [1.669]
 [1.669]
 [1.669]]
maxi score, test score, baseline:  -0.8908623430962344 0.41966666666666674 0.41966666666666674
probs:  [0.07005225315962765, 0.07005225315962765, 0.07005225315962765, 0.07005225315962765, 0.23495516650546064, 0.4848358208560289]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8908623430962344 0.41966666666666674 0.41966666666666674
Printing some Q and Qe and total Qs values:  [[-0.21 ]
 [-0.184]
 [-0.21 ]
 [-0.21 ]
 [-0.21 ]
 [-0.21 ]
 [-0.21 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.21 ]
 [-0.184]
 [-0.21 ]
 [-0.21 ]
 [-0.21 ]
 [-0.21 ]
 [-0.21 ]]
actor:  1 policy actor:  1  step number:  72 total reward:  0.033333333333333104  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.8908623430962344 0.41966666666666674 0.41966666666666674
probs:  [0.07005225315962765, 0.07005225315962765, 0.07005225315962765, 0.07005225315962765, 0.23495516650546064, 0.4848358208560289]
Printing some Q and Qe and total Qs values:  [[-0.199]
 [-0.206]
 [-0.2  ]
 [-0.199]
 [-0.2  ]
 [-0.2  ]
 [-0.201]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.199]
 [-0.206]
 [-0.2  ]
 [-0.199]
 [-0.2  ]
 [-0.2  ]
 [-0.201]]
actions average: 
K:  4  action  0 :  tensor([0.6214, 0.0343, 0.0626, 0.0864, 0.0706, 0.0513, 0.0734],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0374, 0.8329, 0.0197, 0.0326, 0.0228, 0.0148, 0.0397],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2307, 0.0557, 0.1784, 0.1451, 0.1511, 0.1180, 0.1210],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1743, 0.0476, 0.1200, 0.1885, 0.2226, 0.1072, 0.1398],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2148, 0.1797, 0.1134, 0.1221, 0.1574, 0.0970, 0.1156],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1781, 0.0278, 0.1241, 0.1404, 0.1161, 0.3121, 0.1014],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2511, 0.0772, 0.0926, 0.1180, 0.0935, 0.0954, 0.2722],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8908623430962344 0.41966666666666674 0.41966666666666674
probs:  [0.07005225315962765, 0.07005225315962765, 0.07005225315962765, 0.07005225315962765, 0.23495516650546064, 0.4848358208560289]
printing an ep nov before normalisation:  27.94461965560913
line 256 mcts: sample exp_bonus 43.748308550963785
printing an ep nov before normalisation:  47.137254626040225
maxi score, test score, baseline:  -0.8908623430962344 0.41966666666666674 0.41966666666666674
probs:  [0.07005225315962765, 0.07005225315962765, 0.07005225315962765, 0.07005225315962765, 0.23495516650546064, 0.4848358208560289]
maxi score, test score, baseline:  -0.8908623430962344 0.41966666666666674 0.41966666666666674
probs:  [0.07005225315962765, 0.07005225315962765, 0.07005225315962765, 0.07005225315962765, 0.23495516650546064, 0.4848358208560289]
printing an ep nov before normalisation:  43.40782642364502
maxi score, test score, baseline:  -0.8908623430962344 0.41966666666666674 0.41966666666666674
probs:  [0.07005225315962765, 0.07005225315962765, 0.07005225315962765, 0.07005225315962765, 0.23495516650546064, 0.4848358208560289]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8908623430962344 0.41966666666666674 0.41966666666666674
probs:  [0.07005225315962765, 0.07005225315962765, 0.07005225315962765, 0.07005225315962765, 0.23495516650546064, 0.4848358208560289]
printing an ep nov before normalisation:  25.890605449676514
printing an ep nov before normalisation:  34.517438164472075
maxi score, test score, baseline:  -0.8908623430962344 0.41966666666666674 0.41966666666666674
probs:  [0.07005225315962765, 0.07005225315962765, 0.07005225315962765, 0.07005225315962765, 0.23495516650546064, 0.4848358208560289]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8908623430962344 0.41966666666666674 0.41966666666666674
probs:  [0.07005225315962765, 0.07005225315962765, 0.07005225315962765, 0.07005225315962765, 0.23495516650546064, 0.4848358208560289]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.1228],
        [-0.6016],
        [-0.6277],
        [ 0.0205],
        [-0.6338],
        [-0.3966],
        [-0.0000],
        [-0.4294],
        [-0.6277],
        [-0.0000]], dtype=torch.float64)
-0.032346567066 -0.15515620812575803
-0.032346567066 -0.6339847084275094
-0.032346567066 -0.6600161318793174
-0.09703970119800001 -0.07654321362971614
-0.032346567066 -0.6661287755290387
-0.032346567066 -0.42892023272797664
0.99 0.99
-0.084359833866 -0.513803971694537
-0.032346567066 -0.6600161318793174
-0.934428 -0.934428
maxi score, test score, baseline:  -0.8908623430962344 0.41966666666666674 0.41966666666666674
probs:  [0.07005225315962765, 0.07005225315962765, 0.07005225315962765, 0.07005225315962765, 0.23495516650546064, 0.4848358208560289]
maxi score, test score, baseline:  -0.8908623430962344 0.41966666666666674 0.41966666666666674
Printing some Q and Qe and total Qs values:  [[-0.286]
 [-0.285]
 [-0.255]
 [-0.283]
 [-0.255]
 [-0.255]
 [-0.255]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.286]
 [-0.285]
 [-0.255]
 [-0.283]
 [-0.255]
 [-0.255]
 [-0.255]]
maxi score, test score, baseline:  -0.8910899791231732 0.41966666666666674 0.41966666666666674
probs:  [0.07008396829461652, 0.07008396829461652, 0.07008396829461652, 0.07008396829461652, 0.23460844580738682, 0.48505568101414714]
printing an ep nov before normalisation:  57.98201377182614
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.91814377762356
Printing some Q and Qe and total Qs values:  [[-0.146]
 [-0.14 ]
 [-0.15 ]
 [-0.154]
 [-0.149]
 [-0.149]
 [-0.15 ]] [[49.272]
 [44.764]
 [48.317]
 [48.429]
 [47.853]
 [47.855]
 [48.317]] [[1.399]
 [1.142]
 [1.339]
 [1.341]
 [1.313]
 [1.313]
 [1.339]]
maxi score, test score, baseline:  -0.8913166666666666 0.41966666666666674 0.41966666666666674
probs:  [0.07008396829461652, 0.07008396829461652, 0.07008396829461652, 0.07008396829461652, 0.23460844580738682, 0.48505568101414714]
printing an ep nov before normalisation:  37.95780728240351
maxi score, test score, baseline:  -0.8913166666666666 0.41966666666666674 0.41966666666666674
probs:  [0.07011553819672108, 0.07011553819672108, 0.07011553819672108, 0.07011553819672108, 0.2342633128448548, 0.48527453436826085]
printing an ep nov before normalisation:  39.25797939300537
maxi score, test score, baseline:  -0.8913166666666666 0.41966666666666674 0.41966666666666674
probs:  [0.07011553819672108, 0.07011553819672108, 0.07011553819672108, 0.07011553819672108, 0.2342633128448548, 0.48527453436826085]
printing an ep nov before normalisation:  28.11166524887085
maxi score, test score, baseline:  -0.8913166666666666 0.41966666666666674 0.41966666666666674
probs:  [0.07014696386125828, 0.07014696386125828, 0.07014696386125828, 0.07014696386125828, 0.23391975673671953, 0.4854923878182475]
maxi score, test score, baseline:  -0.8913166666666666 0.41966666666666674 0.41966666666666674
probs:  [0.07014696386125828, 0.07014696386125828, 0.07014696386125828, 0.07014696386125828, 0.23391975673671953, 0.4854923878182475]
actor:  1 policy actor:  1  step number:  57 total reward:  0.2666666666666664  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  43.96785005257192
printing an ep nov before normalisation:  41.36423393581654
Printing some Q and Qe and total Qs values:  [[-0.052]
 [-0.053]
 [-0.065]
 [-0.062]
 [-0.054]
 [-0.061]
 [-0.064]] [[48.411]
 [37.46 ]
 [38.155]
 [38.335]
 [37.563]
 [37.854]
 [36.967]] [[1.071]
 [0.652]
 [0.667]
 [0.677]
 [0.655]
 [0.659]
 [0.623]]
maxi score, test score, baseline:  -0.8913166666666666 0.41966666666666674 0.41966666666666674
probs:  [0.07014696386125828, 0.07014696386125828, 0.07014696386125828, 0.07014696386125828, 0.23391975673671953, 0.4854923878182475]
siam score:  -0.76976013
using explorer policy with actor:  1
from probs:  [0.07014696386125828, 0.07014696386125828, 0.07014696386125828, 0.07014696386125828, 0.23391975673671953, 0.4854923878182475]
maxi score, test score, baseline:  -0.8915424116424117 0.41966666666666674 0.41966666666666674
probs:  [0.07014696386125828, 0.07014696386125828, 0.07014696386125828, 0.07014696386125828, 0.23391975673671953, 0.4854923878182475]
maxi score, test score, baseline:  -0.8915424116424117 0.41966666666666674 0.41966666666666674
probs:  [0.07014696386125828, 0.07014696386125828, 0.07014696386125828, 0.07014696386125828, 0.23391975673671953, 0.4854923878182475]
maxi score, test score, baseline:  -0.8915424116424117 0.41966666666666674 0.41966666666666674
probs:  [0.07014696386125828, 0.07014696386125828, 0.07014696386125828, 0.07014696386125828, 0.23391975673671953, 0.4854923878182475]
actor:  1 policy actor:  1  step number:  56 total reward:  0.1799999999999996  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.8915424116424117 0.41966666666666674 0.41966666666666674
maxi score, test score, baseline:  -0.8917672199170125 0.41966666666666674 0.41966666666666674
probs:  [0.07014696386125828, 0.07014696386125828, 0.07014696386125828, 0.07014696386125828, 0.23391975673671953, 0.4854923878182475]
siam score:  -0.7631439
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
UNIT TEST: sample policy line 217 mcts : [0.408 0.163 0.122 0.245 0.02  0.02  0.02 ]
maxi score, test score, baseline:  -0.8917672199170125 0.41966666666666674 0.41966666666666674
printing an ep nov before normalisation:  38.047428131103516
printing an ep nov before normalisation:  39.990456130352484
maxi score, test score, baseline:  -0.8917672199170125 0.41966666666666674 0.41966666666666674
printing an ep nov before normalisation:  28.224366756150445
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.343]
 [0.292]
 [0.313]
 [0.298]
 [0.29 ]
 [0.297]] [[48.988]
 [47.647]
 [49.661]
 [50.788]
 [53.028]
 [51.343]
 [49.935]] [[0.302]
 [0.343]
 [0.292]
 [0.313]
 [0.298]
 [0.29 ]
 [0.297]]
Printing some Q and Qe and total Qs values:  [[-0.207]
 [-0.181]
 [-0.203]
 [-0.2  ]
 [-0.206]
 [-0.206]
 [-0.205]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.207]
 [-0.181]
 [-0.203]
 [-0.2  ]
 [-0.206]
 [-0.206]
 [-0.205]]
printing an ep nov before normalisation:  39.174580574035645
printing an ep nov before normalisation:  45.52261532347676
maxi score, test score, baseline:  -0.8919910973084886 0.41966666666666674 0.41966666666666674
probs:  [0.0701782462744708, 0.0701782462744708, 0.0701782462744708, 0.0701782462744708, 0.23357776670103808, 0.4857092482010785]
Printing some Q and Qe and total Qs values:  [[-0.113]
 [-0.124]
 [-0.124]
 [-0.124]
 [-0.124]
 [-0.124]
 [-0.124]] [[52.706]
 [36.927]
 [36.927]
 [36.927]
 [36.927]
 [36.927]
 [36.927]] [[0.666]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]]
maxi score, test score, baseline:  -0.8919910973084886 0.41966666666666674 0.41966666666666674
printing an ep nov before normalisation:  43.31597328186035
printing an ep nov before normalisation:  52.93901998350503
maxi score, test score, baseline:  -0.8919910973084886 0.41966666666666674 0.41966666666666674
probs:  [0.07024038524713992, 0.07024038524713992, 0.07024038524713992, 0.07024038524713992, 0.2328984422085214, 0.4861400168029189]
maxi score, test score, baseline:  -0.8919910973084886 0.41966666666666674 0.41966666666666674
probs:  [0.07024038524713992, 0.07024038524713992, 0.07024038524713992, 0.07024038524713992, 0.2328984422085214, 0.4861400168029189]
printing an ep nov before normalisation:  44.04549131420948
printing an ep nov before normalisation:  32.04262971878052
maxi score, test score, baseline:  -0.8919910973084886 0.41966666666666674 0.41966666666666674
probs:  [0.07024038524713992, 0.07024038524713992, 0.07024038524713992, 0.07024038524713992, 0.2328984422085214, 0.4861400168029189]
siam score:  -0.7521584
maxi score, test score, baseline:  -0.8919910973084886 0.41966666666666674 0.41966666666666674
probs:  [0.07024038524713992, 0.07024038524713992, 0.07024038524713992, 0.07024038524713992, 0.2328984422085214, 0.4861400168029189]
Printing some Q and Qe and total Qs values:  [[-0.11 ]
 [-0.108]
 [-0.11 ]
 [-0.11 ]
 [-0.11 ]
 [-0.11 ]
 [-0.11 ]] [[48.771]
 [46.835]
 [48.771]
 [48.771]
 [48.771]
 [48.771]
 [48.771]] [[1.397]
 [1.304]
 [1.397]
 [1.397]
 [1.397]
 [1.397]
 [1.397]]
maxi score, test score, baseline:  -0.8919910973084886 0.41966666666666674 0.41966666666666674
line 256 mcts: sample exp_bonus 40.3850468646555
actor:  1 policy actor:  1  step number:  66 total reward:  0.12666666666666626  reward:  1.0 rdn_beta:  2.0
from probs:  [0.07024038524713992, 0.07024038524713992, 0.07024038524713992, 0.07024038524713992, 0.2328984422085214, 0.4861400168029189]
maxi score, test score, baseline:  -0.8919910973084886 0.41966666666666674 0.41966666666666674
probs:  [0.07024038524713992, 0.07024038524713992, 0.07024038524713992, 0.07024038524713992, 0.2328984422085214, 0.4861400168029189]
maxi score, test score, baseline:  -0.8919910973084886 0.41966666666666674 0.41966666666666674
probs:  [0.07024038524713992, 0.07024038524713992, 0.07024038524713992, 0.07024038524713992, 0.2328984422085214, 0.4861400168029189]
maxi score, test score, baseline:  -0.8919910973084886 0.41966666666666674 0.41966666666666674
probs:  [0.07024038524713992, 0.07024038524713992, 0.07024038524713992, 0.07024038524713992, 0.2328984422085214, 0.4861400168029189]
maxi score, test score, baseline:  -0.8919910973084886 0.41966666666666674 0.41966666666666674
probs:  [0.07024038524713992, 0.07024038524713992, 0.07024038524713992, 0.07024038524713992, 0.2328984422085214, 0.4861400168029189]
maxi score, test score, baseline:  -0.8919910973084886 0.41966666666666674 0.41966666666666674
probs:  [0.07024038524713992, 0.07024038524713992, 0.07024038524713992, 0.07024038524713992, 0.2328984422085214, 0.4861400168029189]
printing an ep nov before normalisation:  0.209524412657629
actor:  1 policy actor:  1  step number:  59 total reward:  0.18666666666666631  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  71 total reward:  0.11999999999999911  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8922140495867769 0.41966666666666674 0.41966666666666674
probs:  [0.07030196282707324, 0.07030196282707324, 0.07030196282707324, 0.07030196282707324, 0.2322252550533142, 0.48656689363839284]
maxi score, test score, baseline:  -0.8922140495867769 0.41966666666666674 0.41966666666666674
probs:  [0.07030196282707324, 0.07030196282707324, 0.07030196282707324, 0.07030196282707324, 0.2322252550533142, 0.48656689363839284]
printing an ep nov before normalisation:  51.082588152875616
printing an ep nov before normalisation:  43.30985013995569
maxi score, test score, baseline:  -0.8922140495867769 0.41966666666666674 0.41966666666666674
probs:  [0.07030196282707324, 0.07030196282707324, 0.07030196282707324, 0.07030196282707324, 0.2322252550533142, 0.48656689363839284]
maxi score, test score, baseline:  -0.8922140495867769 0.41966666666666674 0.41966666666666674
probs:  [0.07030196282707324, 0.07030196282707324, 0.07030196282707324, 0.07030196282707324, 0.2322252550533142, 0.48656689363839284]
maxi score, test score, baseline:  -0.8922140495867769 0.41966666666666674 0.41966666666666674
probs:  [0.07030196282707324, 0.07030196282707324, 0.07030196282707324, 0.07030196282707324, 0.2322252550533142, 0.48656689363839284]
maxi score, test score, baseline:  -0.8922140495867769 0.41966666666666674 0.41966666666666674
printing an ep nov before normalisation:  31.117194939515322
maxi score, test score, baseline:  -0.8922140495867769 0.41966666666666674 0.41966666666666674
Printing some Q and Qe and total Qs values:  [[-0.142]
 [-0.11 ]
 [-0.142]
 [-0.142]
 [-0.142]
 [-0.142]
 [-0.142]] [[33.439]
 [46.136]
 [33.439]
 [33.439]
 [33.439]
 [33.439]
 [33.439]] [[0.493]
 [1.171]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]]
maxi score, test score, baseline:  -0.8922140495867769 0.41966666666666674 0.41966666666666674
probs:  [0.07030196282707324, 0.07030196282707324, 0.07030196282707324, 0.07030196282707324, 0.2322252550533142, 0.48656689363839284]
printing an ep nov before normalisation:  30.743378522963297
siam score:  -0.7588506
printing an ep nov before normalisation:  43.87854980050808
printing an ep nov before normalisation:  26.69726096963941
Printing some Q and Qe and total Qs values:  [[-0.108]
 [-0.099]
 [-0.104]
 [-0.108]
 [-0.108]
 [-0.108]
 [-0.108]] [[28.401]
 [46.498]
 [41.306]
 [28.401]
 [28.401]
 [28.401]
 [28.401]] [[0.421]
 [1.377]
 [1.1  ]
 [0.421]
 [0.421]
 [0.421]
 [0.421]]
maxi score, test score, baseline:  -0.8924360824742268 0.41966666666666674 0.41966666666666674
probs:  [0.07039329311571495, 0.07039329311571495, 0.07039329311571495, 0.07039329311571495, 0.2312268011150216, 0.4872000264221187]
printing an ep nov before normalisation:  37.68392057823985
maxi score, test score, baseline:  -0.8924360824742268 0.41966666666666674 0.41966666666666674
probs:  [0.07042346396758117, 0.07042346396758117, 0.07042346396758117, 0.07042346396758117, 0.23089696304798485, 0.4874091810816905]
line 256 mcts: sample exp_bonus 32.54136301623631
actor:  1 policy actor:  1  step number:  60 total reward:  0.09999999999999953  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8926572016460905 0.41966666666666674 0.41966666666666674
probs:  [0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.23056859829883572, 0.48761740149099825]
actions average: 
K:  2  action  0 :  tensor([0.5872, 0.0444, 0.0646, 0.0671, 0.0920, 0.0635, 0.0811],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0107, 0.9303, 0.0129, 0.0172, 0.0053, 0.0055, 0.0182],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2444, 0.0672, 0.2238, 0.1196, 0.1323, 0.1178, 0.0949],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2278, 0.0878, 0.1409, 0.1826, 0.1093, 0.1055, 0.1461],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2997, 0.0146, 0.1265, 0.1535, 0.1749, 0.1416, 0.0892],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1808, 0.0087, 0.1124, 0.0991, 0.1059, 0.4193, 0.0739],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2164, 0.0797, 0.1183, 0.1372, 0.1074, 0.0914, 0.2496],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8926572016460905 0.41966666666666674 0.41966666666666674
probs:  [0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.23056859829883572, 0.48761740149099825]
maxi score, test score, baseline:  -0.8926572016460905 0.41966666666666674 0.41966666666666674
probs:  [0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.23056859829883572, 0.48761740149099825]
maxi score, test score, baseline:  -0.8926572016460905 0.41966666666666674 0.41966666666666674
probs:  [0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.23056859829883572, 0.48761740149099825]
maxi score, test score, baseline:  -0.8926572016460905 0.41966666666666674 0.41966666666666674
probs:  [0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.23056859829883572, 0.48761740149099825]
maxi score, test score, baseline:  -0.8928774127310062 0.41966666666666674 0.41966666666666674
maxi score, test score, baseline:  -0.8928774127310062 0.41966666666666674 0.41966666666666674
probs:  [0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.23056859829883572, 0.48761740149099825]
printing an ep nov before normalisation:  31.790875455911145
maxi score, test score, baseline:  -0.8928774127310062 0.41966666666666674 0.41966666666666674
probs:  [0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.23056859829883572, 0.48761740149099825]
maxi score, test score, baseline:  -0.8928774127310062 0.41966666666666674 0.41966666666666674
probs:  [0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.23056859829883572, 0.48761740149099825]
actor:  1 policy actor:  1  step number:  60 total reward:  0.15333333333333243  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8928774127310062 0.41966666666666674 0.41966666666666674
using explorer policy with actor:  0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.138]
 [-0.138]
 [-0.149]
 [-0.138]
 [-0.144]
 [-0.15 ]
 [-0.141]] [[38.704]
 [40.975]
 [28.559]
 [37.841]
 [35.243]
 [28.555]
 [36.718]] [[ 0.071]
 [ 0.095]
 [-0.049]
 [ 0.061]
 [ 0.028]
 [-0.05 ]
 [ 0.046]]
printing an ep nov before normalisation:  77.55861455088974
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  52.31161899047838
siam score:  -0.741951
maxi score, test score, baseline:  -0.8928774127310062 0.41966666666666674 0.41966666666666674
maxi score, test score, baseline:  -0.8928774127310062 0.41966666666666674 0.41966666666666674
probs:  [0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.23056859829883572, 0.48761740149099825]
printing an ep nov before normalisation:  33.705341775614066
printing an ep nov before normalisation:  21.390704209869323
maxi score, test score, baseline:  -0.8930967213114754 0.41966666666666674 0.41966666666666674
probs:  [0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.23056859829883572, 0.48761740149099825]
maxi score, test score, baseline:  -0.8930967213114754 0.41966666666666674 0.41966666666666674
probs:  [0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.23056859829883572, 0.48761740149099825]
maxi score, test score, baseline:  -0.8930967213114754 0.41966666666666674 0.41966666666666674
probs:  [0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.23056859829883572, 0.48761740149099825]
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.627]
 [0.581]
 [0.592]
 [0.544]
 [0.577]
 [0.551]] [[42.539]
 [50.187]
 [54.5  ]
 [57.749]
 [60.402]
 [57.762]
 [65.473]] [[0.683]
 [0.627]
 [0.581]
 [0.592]
 [0.544]
 [0.577]
 [0.551]]
maxi score, test score, baseline:  -0.8930967213114754 0.41966666666666674 0.41966666666666674
probs:  [0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.23056859829883572, 0.48761740149099825]
actor:  1 policy actor:  1  step number:  62 total reward:  0.03333333333333266  reward:  1.0 rdn_beta:  2.0
siam score:  -0.7484529
maxi score, test score, baseline:  -0.8930967213114754 0.41966666666666674 0.41966666666666674
probs:  [0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.23056859829883572, 0.48761740149099825]
from probs:  [0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.23056859829883572, 0.48761740149099825]
printing an ep nov before normalisation:  41.79594039916992
maxi score, test score, baseline:  -0.8933151329243354 0.41966666666666674 0.41966666666666674
probs:  [0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.23056859829883572, 0.48761740149099825]
maxi score, test score, baseline:  -0.8935326530612245 0.41966666666666674 0.41966666666666674
probs:  [0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.07045350005254153, 0.23056859829883572, 0.48761740149099825]
printing an ep nov before normalisation:  51.577139360989094
printing an ep nov before normalisation:  30.143922203050995
maxi score, test score, baseline:  -0.8935326530612245 0.41966666666666674 0.41966666666666674
probs:  [0.07048340227154731, 0.07048340227154731, 0.07048340227154731, 0.07048340227154731, 0.23024169701806618, 0.48782469389574457]
actor:  1 policy actor:  1  step number:  66 total reward:  0.12666666666666626  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.8935326530612245 0.41966666666666674 0.41966666666666674
probs:  [0.07048340227154731, 0.07048340227154731, 0.07048340227154731, 0.07048340227154731, 0.23024169701806618, 0.48782469389574457]
printing an ep nov before normalisation:  23.509369578572887
printing an ep nov before normalisation:  32.567974889280244
printing an ep nov before normalisation:  33.43230655486848
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8935326530612245 0.41966666666666674 0.41966666666666674
probs:  [0.07051317151753692, 0.07051317151753692, 0.07051317151753692, 0.07051317151753692, 0.22991624944376862, 0.48803106448608363]
printing an ep nov before normalisation:  27.17505931854248
maxi score, test score, baseline:  -0.8935326530612245 0.41966666666666674 0.41966666666666674
probs:  [0.07051317151753692, 0.07051317151753692, 0.07051317151753692, 0.07051317151753692, 0.22991624944376862, 0.48803106448608363]
printing an ep nov before normalisation:  26.325113318059945
maxi score, test score, baseline:  -0.8935326530612245 0.41966666666666674 0.41966666666666674
probs:  [0.07051317151753692, 0.07051317151753692, 0.07051317151753692, 0.07051317151753692, 0.22991624944376862, 0.48803106448608363]
maxi score, test score, baseline:  -0.8935326530612245 0.41966666666666674 0.41966666666666674
probs:  [0.07051317151753692, 0.07051317151753692, 0.07051317151753692, 0.07051317151753692, 0.22991624944376862, 0.48803106448608363]
printing an ep nov before normalisation:  23.112404346466064
from probs:  [0.07054280867552472, 0.07054280867552472, 0.07054280867552472, 0.07054280867552472, 0.22959224590066357, 0.48823651939723756]
maxi score, test score, baseline:  -0.8939650406504065 0.41966666666666674 0.41966666666666674
Printing some Q and Qe and total Qs values:  [[-0.198]
 [-0.193]
 [-0.179]
 [-0.179]
 [-0.179]
 [-0.179]
 [-0.179]] [[54.975]
 [46.953]
 [50.986]
 [50.986]
 [50.986]
 [50.986]
 [50.986]] [[1.106]
 [0.806]
 [0.972]
 [0.972]
 [0.972]
 [0.972]
 [0.972]]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.8941799188640973 0.41966666666666674 0.41966666666666674
probs:  [0.07054280867552472, 0.07054280867552472, 0.07054280867552472, 0.07054280867552472, 0.22959224590066357, 0.48823651939723756]
printing an ep nov before normalisation:  23.53704041508696
printing an ep nov before normalisation:  29.50642480723206
maxi score, test score, baseline:  -0.8941799188640973 0.41966666666666674 0.41966666666666674
printing an ep nov before normalisation:  47.67626621436525
maxi score, test score, baseline:  -0.8941799188640973 0.41966666666666674 0.41966666666666674
printing an ep nov before normalisation:  39.39636411903642
maxi score, test score, baseline:  -0.8941799188640973 0.41966666666666674 0.41966666666666674
printing an ep nov before normalisation:  21.48048778409475
maxi score, test score, baseline:  -0.8941799188640973 0.41966666666666674 0.41966666666666674
probs:  [0.07054280867552472, 0.07054280867552472, 0.07054280867552472, 0.07054280867552472, 0.22959224590066357, 0.48823651939723756]
maxi score, test score, baseline:  -0.894393927125506 0.41966666666666674 0.41966666666666674
probs:  [0.07054280867552472, 0.07054280867552472, 0.07054280867552472, 0.07054280867552472, 0.22959224590066357, 0.48823651939723756]
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.973202887363065
printing an ep nov before normalisation:  25.766516281751684
printing an ep nov before normalisation:  47.84912533874354
line 256 mcts: sample exp_bonus 37.67512964142991
maxi score, test score, baseline:  -0.894393927125506 0.41966666666666674 0.41966666666666674
probs:  [0.07054280867552472, 0.07054280867552472, 0.07054280867552472, 0.07054280867552472, 0.22959224590066357, 0.48823651939723756]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.118]
 [-0.111]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]] [[34.678]
 [48.674]
 [34.678]
 [34.678]
 [34.678]
 [34.678]
 [34.678]] [[0.656]
 [1.535]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]]
printing an ep nov before normalisation:  24.49321746826172
siam score:  -0.7456479
printing an ep nov before normalisation:  36.56566457901149
Printing some Q and Qe and total Qs values:  [[-0.192]
 [-0.158]
 [-0.174]
 [-0.18 ]
 [-0.176]
 [-0.168]
 [-0.173]] [[55.034]
 [51.617]
 [53.528]
 [52.356]
 [52.248]
 [55.657]
 [52.202]] [[1.288]
 [1.147]
 [1.229]
 [1.163]
 [1.161]
 [1.344]
 [1.162]]
printing an ep nov before normalisation:  42.967220524991035
maxi score, test score, baseline:  -0.8946070707070707 0.41966666666666674 0.41966666666666674
maxi score, test score, baseline:  -0.8946070707070707 0.41966666666666674 0.41966666666666674
probs:  [0.07063093635459379, 0.07063093635459379, 0.07063093635459379, 0.07063093635459379, 0.22862880398508717, 0.48884745059653767]
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.518]
 [0.389]
 [0.386]
 [0.388]
 [0.371]
 [0.389]] [[39.822]
 [42.51 ]
 [38.731]
 [39.506]
 [39.732]
 [34.392]
 [40.612]] [[0.381]
 [0.518]
 [0.389]
 [0.386]
 [0.388]
 [0.371]
 [0.389]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.33333333333333326  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.94273668736508
maxi score, test score, baseline:  -0.8948193548387097 0.41966666666666674 0.41966666666666674
probs:  [0.07063093635459379, 0.07063093635459379, 0.07063093635459379, 0.07063093635459379, 0.22862880398508717, 0.48884745059653767]
maxi score, test score, baseline:  -0.8948193548387097 0.41966666666666674 0.41966666666666674
probs:  [0.07063093635459379, 0.07063093635459379, 0.07063093635459379, 0.07063093635459379, 0.22862880398508717, 0.48884745059653767]
Printing some Q and Qe and total Qs values:  [[-0.106]
 [-0.106]
 [-0.106]
 [-0.106]
 [-0.106]
 [-0.106]
 [-0.106]] [[53.463]
 [53.463]
 [53.463]
 [53.463]
 [53.463]
 [53.463]
 [53.463]] [[1.894]
 [1.894]
 [1.894]
 [1.894]
 [1.894]
 [1.894]
 [1.894]]
maxi score, test score, baseline:  -0.8948193548387097 0.41966666666666674 0.41966666666666674
probs:  [0.07063093635459379, 0.07063093635459379, 0.07063093635459379, 0.07063093635459379, 0.22862880398508717, 0.48884745059653767]
actor:  1 policy actor:  1  step number:  52 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.8948193548387097 0.41966666666666674 0.41966666666666674
probs:  [0.07063093635459379, 0.07063093635459379, 0.07063093635459379, 0.07063093635459379, 0.22862880398508717, 0.48884745059653767]
printing an ep nov before normalisation:  28.838958740234375
maxi score, test score, baseline:  -0.8948193548387097 0.41966666666666674 0.41966666666666674
probs:  [0.07063093635459379, 0.07063093635459379, 0.07063093635459379, 0.07063093635459379, 0.22862880398508717, 0.48884745059653767]
Printing some Q and Qe and total Qs values:  [[-0.044]
 [-0.003]
 [-0.044]
 [-0.044]
 [-0.044]
 [-0.044]
 [-0.044]] [[35.008]
 [41.576]
 [35.008]
 [35.008]
 [35.008]
 [35.008]
 [35.008]] [[1.217]
 [1.631]
 [1.217]
 [1.217]
 [1.217]
 [1.217]
 [1.217]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  34.55688384395869
printing an ep nov before normalisation:  61.12539542925061
printing an ep nov before normalisation:  25.25867516459592
maxi score, test score, baseline:  -0.8950307847082495 0.41966666666666674 0.41966666666666674
maxi score, test score, baseline:  -0.8950307847082495 0.41966666666666674 0.41966666666666674
probs:  [0.07063093635459379, 0.07063093635459379, 0.07063093635459379, 0.07063093635459379, 0.22862880398508717, 0.48884745059653767]
printing an ep nov before normalisation:  56.295752251765386
maxi score, test score, baseline:  -0.8950307847082495 0.41966666666666674 0.41966666666666674
probs:  [0.07063093635459379, 0.07063093635459379, 0.07063093635459379, 0.07063093635459379, 0.22862880398508717, 0.48884745059653767]
printing an ep nov before normalisation:  39.11044188900628
maxi score, test score, baseline:  -0.8950307847082495 0.41966666666666674 0.41966666666666674
probs:  [0.07063093635459379, 0.07063093635459379, 0.07063093635459379, 0.07063093635459379, 0.22862880398508717, 0.48884745059653767]
printing an ep nov before normalisation:  47.069488412232715
Printing some Q and Qe and total Qs values:  [[-0.192]
 [-0.207]
 [-0.192]
 [-0.192]
 [-0.192]
 [-0.192]
 [-0.192]] [[53.056]
 [53.48 ]
 [55.084]
 [55.084]
 [55.084]
 [55.084]
 [55.084]] [[0.085]
 [0.074]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]]
printing an ep nov before normalisation:  36.82633253867996
actor:  0 policy actor:  1  step number:  56 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  2.0
from probs:  [0.07066005385528233, 0.07066005385528233, 0.07066005385528233, 0.07066005385528233, 0.22831048151323155, 0.4890493030656392]
printing an ep nov before normalisation:  35.61537981033325
printing an ep nov before normalisation:  35.51891565322876
printing an ep nov before normalisation:  37.1194939947313
maxi score, test score, baseline:  -0.8927915662650602 0.41966666666666674 0.41966666666666674
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[-0.126]
 [-0.123]
 [-0.142]
 [-0.126]
 [-0.134]
 [-0.139]
 [-0.136]] [[20.777]
 [21.526]
 [10.881]
 [22.613]
 [19.523]
 [17.243]
 [18.224]] [[ 0.039]
 [ 0.05 ]
 [-0.085]
 [ 0.059]
 [ 0.017]
 [-0.012]
 [ 0.001]]
maxi score, test score, baseline:  -0.8927915662650602 0.41966666666666674 0.41966666666666674
probs:  [0.07066005385528233, 0.07066005385528233, 0.07066005385528233, 0.07066005385528233, 0.22831048151323155, 0.4890493030656392]
maxi score, test score, baseline:  -0.8927915662650602 0.41966666666666674 0.41966666666666674
probs:  [0.07066005385528233, 0.07066005385528233, 0.07066005385528233, 0.07066005385528233, 0.22831048151323155, 0.4890493030656392]
from probs:  [0.07066005385528233, 0.07066005385528233, 0.07066005385528233, 0.07066005385528233, 0.22831048151323155, 0.4890493030656392]
actor:  0 policy actor:  1  step number:  45 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  46 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.89042 0.41966666666666674 0.41966666666666674
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
probs:  [0.07104279755886644, 0.07104279755886644, 0.07104279755886644, 0.07104279755886644, 0.2239445346932546, 0.49188427507127963]
printing an ep nov before normalisation:  57.81229495912244
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
probs:  [0.0710710662987372, 0.0710710662987372, 0.0710710662987372, 0.0710710662987372, 0.2236354187437059, 0.49208031606134517]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.544]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]] [[28.967]
 [31.918]
 [28.967]
 [28.967]
 [28.967]
 [28.967]
 [28.967]] [[1.372]
 [1.768]
 [1.372]
 [1.372]
 [1.372]
 [1.372]
 [1.372]]
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
probs:  [0.0710710662987372, 0.0710710662987372, 0.0710710662987372, 0.0710710662987372, 0.2236354187437059, 0.49208031606134517]
printing an ep nov before normalisation:  12.903979132482322
actor:  1 policy actor:  1  step number:  65 total reward:  0.2133333333333326  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
probs:  [0.07115512892542654, 0.07115512892542654, 0.07115512892542654, 0.07115512892542654, 0.22271620201209996, 0.4926632822861938]
Printing some Q and Qe and total Qs values:  [[-0.214]
 [-0.214]
 [-0.198]
 [-0.214]
 [-0.214]
 [-0.214]
 [-0.214]] [[56.207]
 [56.207]
 [57.108]
 [56.207]
 [56.207]
 [56.207]
 [56.207]] [[1.643]
 [1.643]
 [1.7  ]
 [1.643]
 [1.643]
 [1.643]
 [1.643]]
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
probs:  [0.07115512892542654, 0.07115512892542654, 0.07115512892542654, 0.07115512892542654, 0.22271620201209996, 0.4926632822861938]
line 256 mcts: sample exp_bonus 30.480094128067062
printing an ep nov before normalisation:  34.81000900268555
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
printing an ep nov before normalisation:  0.0034297560205232003
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
probs:  [0.07115512892542654, 0.07115512892542654, 0.07115512892542654, 0.07115512892542654, 0.22271620201209996, 0.4926632822861938]
actor:  1 policy actor:  1  step number:  57 total reward:  0.1599999999999996  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  52.672177051653115
Printing some Q and Qe and total Qs values:  [[-0.141]
 [-0.13 ]
 [-0.155]
 [-0.154]
 [-0.144]
 [-0.153]
 [-0.141]] [[34.858]
 [33.671]
 [32.12 ]
 [33.298]
 [33.58 ]
 [33.001]
 [32.886]] [[0.485]
 [0.446]
 [0.357]
 [0.407]
 [0.428]
 [0.396]
 [0.402]]
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
probs:  [0.07115512892542654, 0.07115512892542654, 0.07115512892542654, 0.07115512892542654, 0.22271620201209996, 0.4926632822861938]
Printing some Q and Qe and total Qs values:  [[ 0.132]
 [ 0.277]
 [-0.009]
 [ 0.208]
 [ 0.234]
 [ 0.001]
 [ 0.191]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.132]
 [ 0.277]
 [-0.009]
 [ 0.208]
 [ 0.234]
 [ 0.001]
 [ 0.191]]
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
probs:  [0.0711829046473766, 0.0711829046473766, 0.0711829046473766, 0.0711829046473766, 0.2224124771660599, 0.4928559042444336]
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
Printing some Q and Qe and total Qs values:  [[-0.233]
 [-0.193]
 [-0.256]
 [-0.193]
 [-0.193]
 [-0.258]
 [-0.197]] [[37.433]
 [49.498]
 [46.033]
 [56.336]
 [69.511]
 [48.8  ]
 [43.171]] [[0.472]
 [0.877]
 [0.708]
 [1.083]
 [1.48 ]
 [0.79 ]
 [0.682]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
actions average: 
K:  1  action  0 :  tensor([0.4654, 0.0132, 0.1045, 0.0976, 0.1162, 0.1046, 0.0984],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0179, 0.9127, 0.0134, 0.0153, 0.0087, 0.0074, 0.0246],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2798, 0.0075, 0.3388, 0.0765, 0.1057, 0.1147, 0.0771],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1370, 0.1905, 0.1084, 0.2486, 0.1019, 0.0990, 0.1146],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2138, 0.0109, 0.1153, 0.1061, 0.3095, 0.1191, 0.1253],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1779, 0.0077, 0.1726, 0.0899, 0.1286, 0.3257, 0.0976],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2492, 0.0666, 0.1031, 0.0931, 0.1542, 0.1156, 0.2181],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
probs:  [0.07121055913011953, 0.07121055913011953, 0.07121055913011953, 0.07121055913011953, 0.22211007805908037, 0.4930476854204415]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
probs:  [0.07121055913011953, 0.07121055913011953, 0.07121055913011953, 0.07121055913011953, 0.22211007805908037, 0.4930476854204415]
siam score:  -0.73829055
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
probs:  [0.07121055913011953, 0.07121055913011953, 0.07121055913011953, 0.07121055913011953, 0.22211007805908037, 0.4930476854204415]
printing an ep nov before normalisation:  59.19770709956566
printing an ep nov before normalisation:  53.36444422185955
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
probs:  [0.07121055913011953, 0.07121055913011953, 0.07121055913011953, 0.07121055913011953, 0.22211007805908037, 0.4930476854204415]
printing an ep nov before normalisation:  17.30091589013533
printing an ep nov before normalisation:  51.445580766116166
actor:  1 policy actor:  1  step number:  74 total reward:  0.03333333333333288  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
probs:  [0.07121055913011953, 0.07121055913011953, 0.07121055913011953, 0.07121055913011953, 0.22211007805908037, 0.4930476854204415]
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
probs:  [0.07121055913011953, 0.07121055913011953, 0.07121055913011953, 0.07121055913011953, 0.22211007805908037, 0.4930476854204415]
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
printing an ep nov before normalisation:  36.14825763074486
from probs:  [0.07123809316572842, 0.07123809316572842, 0.07123809316572842, 0.07123809316572842, 0.22180899602991844, 0.493238631307168]
printing an ep nov before normalisation:  41.44630566081622
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
probs:  [0.07123809316572842, 0.07123809316572842, 0.07123809316572842, 0.07123809316572842, 0.22180899602991844, 0.493238631307168]
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
probs:  [0.07123809316572842, 0.07123809316572842, 0.07123809316572842, 0.07123809316572842, 0.22180899602991844, 0.493238631307168]
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
probs:  [0.07123809316572842, 0.07123809316572842, 0.07123809316572842, 0.07123809316572842, 0.22180899602991844, 0.493238631307168]
printing an ep nov before normalisation:  60.479009436979126
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
printing an ep nov before normalisation:  31.984071731567383
actor:  1 policy actor:  1  step number:  70 total reward:  0.20666666666666633  reward:  1.0 rdn_beta:  2.0
from probs:  [0.07123809316572842, 0.07123809316572842, 0.07123809316572842, 0.07123809316572842, 0.22180899602991844, 0.493238631307168]
printing an ep nov before normalisation:  0.35192668554373085
from probs:  [0.07123809316572842, 0.07123809316572842, 0.07123809316572842, 0.07123809316572842, 0.22180899602991844, 0.493238631307168]
Starting evaluation
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
probs:  [0.0712655075393917, 0.0712655075393917, 0.0712655075393917, 0.0712655075393917, 0.2215092224926136, 0.49342874734981956]
printing an ep nov before normalisation:  49.19712387079878
printing an ep nov before normalisation:  45.811295955829394
printing an ep nov before normalisation:  56.269519820992286
printing an ep nov before normalisation:  55.08104736776137
printing an ep nov before normalisation:  49.12886865962039
printing an ep nov before normalisation:  49.43344384244655
printing an ep nov before normalisation:  44.8408262996588
maxi score, test score, baseline:  -0.89242 0.41966666666666674 0.41966666666666674
probs:  [0.0712655075393917, 0.0712655075393917, 0.0712655075393917, 0.0712655075393917, 0.2215092224926136, 0.49342874734981956]
printing an ep nov before normalisation:  38.686025299478985
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.662]
 [0.414]
 [0.516]
 [0.466]
 [0.482]
 [0.53 ]] [[35.945]
 [35.159]
 [33.884]
 [36.531]
 [39.716]
 [35.704]
 [31.688]] [[0.715]
 [0.662]
 [0.414]
 [0.516]
 [0.466]
 [0.482]
 [0.53 ]]
printing an ep nov before normalisation:  46.241135597229004
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  36.04740091103098
Printing some Q and Qe and total Qs values:  [[0.836]
 [0.919]
 [0.756]
 [0.814]
 [0.814]
 [0.814]
 [0.721]] [[38.733]
 [35.464]
 [38.483]
 [51.897]
 [51.897]
 [51.897]
 [38.743]] [[0.836]
 [0.919]
 [0.756]
 [0.814]
 [0.814]
 [0.814]
 [0.721]]
printing an ep nov before normalisation:  35.42734458524408
printing an ep nov before normalisation:  48.39070796966553
Printing some Q and Qe and total Qs values:  [[0.994]
 [1.016]
 [0.994]
 [0.994]
 [0.994]
 [0.994]
 [0.897]] [[36.299]
 [40.563]
 [36.299]
 [36.299]
 [36.299]
 [36.299]
 [36.838]] [[0.994]
 [1.016]
 [0.994]
 [0.994]
 [0.994]
 [0.994]
 [0.897]]
printing an ep nov before normalisation:  36.90508062539998
printing an ep nov before normalisation:  37.919694195512
printing an ep nov before normalisation:  38.43813244995347
printing an ep nov before normalisation:  34.079726659509234
printing an ep nov before normalisation:  36.90508062539998
printing an ep nov before normalisation:  39.391516198425606
printing an ep nov before normalisation:  33.386431204781566
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  38.821704387664795
actor:  0 policy actor:  0  step number:  27 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  42.56855835805394
actor:  0 policy actor:  0  step number:  29 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  29 total reward:  0.6133333333333334  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  47.89123948052618
actor:  0 policy actor:  0  step number:  30 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.841]
 [0.841]
 [0.841]
 [0.841]
 [0.841]
 [0.841]
 [0.841]] [[39.009]
 [39.009]
 [39.009]
 [39.009]
 [39.009]
 [39.009]
 [39.009]] [[0.841]
 [0.841]
 [0.841]
 [0.841]
 [0.841]
 [0.841]
 [0.841]]
actor:  0 policy actor:  0  step number:  31 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.82562 0.6700000000000002 0.6700000000000002
maxi score, test score, baseline:  -0.82562 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  81 total reward:  0.0933333333333326  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  44.23481975429009
actor:  0 policy actor:  0  step number:  55 total reward:  0.17333333333333267  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.27678488760489
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  36.152074337005615
line 256 mcts: sample exp_bonus 0.1196800550814089
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.02455001605075
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
actor:  1 policy actor:  1  step number:  62 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 27.02050540980587
printing an ep nov before normalisation:  57.47037571592854
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.6200895218722
printing an ep nov before normalisation:  42.74918404963092
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.151]
 [-0.126]
 [-0.151]
 [-0.151]
 [-0.151]
 [-0.151]
 [-0.151]] [[39.68]
 [46.66]
 [39.68]
 [39.68]
 [39.68]
 [39.68]
 [39.68]] [[0.527]
 [0.784]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]]
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8232733333333332 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.109]
 [-0.064]
 [-0.109]
 [-0.109]
 [-0.109]
 [-0.109]
 [-0.109]] [[70.619]
 [78.426]
 [70.619]
 [70.619]
 [70.619]
 [70.619]
 [70.619]] [[1.23 ]
 [1.533]
 [1.23 ]
 [1.23 ]
 [1.23 ]
 [1.23 ]
 [1.23 ]]
maxi score, test score, baseline:  -0.8232733333333332 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  35.42381374136457
maxi score, test score, baseline:  -0.8232733333333332 0.6700000000000002 0.6700000000000002
maxi score, test score, baseline:  -0.8232733333333332 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8232733333333332 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.40518439770819
printing an ep nov before normalisation:  62.781706314025385
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.09333333333333271  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.093]
 [-0.091]
 [-0.093]
 [-0.093]
 [-0.093]
 [-0.093]
 [-0.093]] [[57.204]
 [65.624]
 [57.204]
 [57.204]
 [57.204]
 [57.204]
 [57.204]] [[1.243]
 [1.545]
 [1.243]
 [1.243]
 [1.243]
 [1.243]
 [1.243]]
printing an ep nov before normalisation:  36.43341206488926
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.134]
 [-0.097]
 [-0.132]
 [-0.128]
 [-0.121]
 [-0.121]
 [-0.121]] [[34.799]
 [36.489]
 [35.213]
 [34.608]
 [32.592]
 [32.592]
 [32.592]] [[0.126]
 [0.187]
 [0.134]
 [0.129]
 [0.107]
 [0.107]
 [0.107]]
Printing some Q and Qe and total Qs values:  [[-0.14 ]
 [-0.122]
 [-0.126]
 [-0.132]
 [-0.134]
 [-0.135]
 [-0.13 ]] [[32.501]
 [35.108]
 [33.662]
 [31.5  ]
 [31.257]
 [31.07 ]
 [31.834]] [[0.043]
 [0.099]
 [0.074]
 [0.036]
 [0.031]
 [0.027]
 [0.042]]
maxi score, test score, baseline:  -0.8232733333333332 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8232733333333332 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8232733333333332 0.6700000000000002 0.6700000000000002
maxi score, test score, baseline:  -0.8232733333333332 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8232733333333332 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8232733333333332 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.49720832803915
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8232733333333332 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.88426672234968
printing an ep nov before normalisation:  33.59493970870972
printing an ep nov before normalisation:  53.84165606739029
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.4052, 0.0878, 0.0891, 0.1037, 0.1318, 0.0812, 0.1012],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0327, 0.7962, 0.0390, 0.0251, 0.0146, 0.0222, 0.0701],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1835, 0.0927, 0.2394, 0.1112, 0.1128, 0.1404, 0.1200],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1507, 0.0725, 0.1329, 0.2780, 0.1197, 0.1082, 0.1378],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2557, 0.0215, 0.1086, 0.1547, 0.2527, 0.1022, 0.1046],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1690, 0.0752, 0.1997, 0.1200, 0.1006, 0.1735, 0.1620],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1984, 0.0856, 0.1336, 0.0872, 0.1218, 0.1521, 0.2213],
       grad_fn=<DivBackward0>)
siam score:  -0.74773526
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
printing an ep nov before normalisation:  37.08778428160109
printing an ep nov before normalisation:  32.736025011171414
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.316963785097656
printing an ep nov before normalisation:  0.009751177962584734
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
Printing some Q and Qe and total Qs values:  [[-0.186]
 [-0.186]
 [-0.186]
 [-0.186]
 [-0.201]
 [-0.186]
 [-0.186]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [61.209]
 [ 0.   ]
 [ 0.   ]] [[-1.041]
 [-1.041]
 [-1.041]
 [-1.041]
 [ 1.243]
 [-1.041]
 [-1.041]]
printing an ep nov before normalisation:  50.07489510109143
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.287881420439092
printing an ep nov before normalisation:  22.638748427655653
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.810515283255754
Printing some Q and Qe and total Qs values:  [[0.194]
 [0.26 ]
 [0.194]
 [0.194]
 [0.194]
 [0.194]
 [0.194]] [[47.244]
 [51.487]
 [47.244]
 [47.244]
 [47.244]
 [47.244]
 [47.244]] [[0.787]
 [0.927]
 [0.787]
 [0.787]
 [0.787]
 [0.787]
 [0.787]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  75 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 41.14079073886354
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
line 256 mcts: sample exp_bonus 40.44061993321196
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.07661408256501
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.35681955225369
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.296826331624814
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  72 total reward:  0.21999999999999886  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.681]
 [0.363]
 [0.451]
 [0.498]
 [0.372]
 [0.661]] [[32.223]
 [32.983]
 [33.831]
 [27.728]
 [33.443]
 [29.748]
 [32.536]] [[0.428]
 [0.681]
 [0.363]
 [0.451]
 [0.498]
 [0.372]
 [0.661]]
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.59832747354106
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.388]
 [0.356]
 [0.36 ]
 [0.356]
 [0.352]
 [0.351]] [[12.469]
 [33.23 ]
 [12.16 ]
 [15.348]
 [12.064]
 [12.123]
 [13.312]] [[0.358]
 [0.388]
 [0.356]
 [0.36 ]
 [0.356]
 [0.352]
 [0.351]]
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.476]
 [0.37 ]
 [0.363]
 [0.367]
 [0.352]
 [0.356]] [[14.834]
 [32.656]
 [21.519]
 [16.305]
 [18.947]
 [11.885]
 [13.82 ]] [[0.361]
 [0.476]
 [0.37 ]
 [0.363]
 [0.367]
 [0.352]
 [0.356]]
maxi score, test score, baseline:  -0.8232733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  5.429672569334798e-06
actor:  0 policy actor:  0  step number:  50 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.82094 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.145]
 [0.377]
 [0.167]
 [0.236]
 [0.225]
 [0.22 ]
 [0.218]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.145]
 [0.377]
 [0.167]
 [0.236]
 [0.225]
 [0.22 ]
 [0.218]]
maxi score, test score, baseline:  -0.82094 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.82094 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.15724246687866
Printing some Q and Qe and total Qs values:  [[0.36]
 [0.36]
 [0.36]
 [0.36]
 [0.36]
 [0.36]
 [0.36]] [[29.527]
 [29.527]
 [29.527]
 [29.527]
 [29.527]
 [29.527]
 [29.527]] [[0.36]
 [0.36]
 [0.36]
 [0.36]
 [0.36]
 [0.36]
 [0.36]]
printing an ep nov before normalisation:  45.58017112700405
maxi score, test score, baseline:  -0.82094 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.82094 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  69 total reward:  0.03999999999999959  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.81886 0.6700000000000002 0.6700000000000002
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.198]
 [-0.167]
 [-0.185]
 [-0.2  ]
 [-0.2  ]
 [-0.199]
 [-0.2  ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.198]
 [-0.167]
 [-0.185]
 [-0.2  ]
 [-0.2  ]
 [-0.199]
 [-0.2  ]]
maxi score, test score, baseline:  -0.81886 0.6700000000000002 0.6700000000000002
maxi score, test score, baseline:  -0.81886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.07874336318505
printing an ep nov before normalisation:  43.64976538121452
printing an ep nov before normalisation:  45.95087315605932
printing an ep nov before normalisation:  41.634604977598144
maxi score, test score, baseline:  -0.81886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.81886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.95979575951627
printing an ep nov before normalisation:  37.45735213426882
printing an ep nov before normalisation:  37.28642069302018
Printing some Q and Qe and total Qs values:  [[-0.09 ]
 [-0.058]
 [-0.093]
 [-0.095]
 [-0.09 ]
 [-0.09 ]
 [-0.09 ]] [[45.37 ]
 [47.785]
 [42.52 ]
 [43.722]
 [45.37 ]
 [45.37 ]
 [45.37 ]] [[1.283]
 [1.43 ]
 [1.144]
 [1.2  ]
 [1.283]
 [1.283]
 [1.283]]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.81886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  67 total reward:  0.0666666666666661  reward:  1.0 rdn_beta:  1.0
siam score:  -0.76363343
Printing some Q and Qe and total Qs values:  [[-0.16 ]
 [-0.159]
 [-0.162]
 [-0.162]
 [-0.162]
 [-0.162]
 [-0.189]] [[47.019]
 [48.281]
 [44.684]
 [44.684]
 [44.684]
 [44.684]
 [38.274]] [[0.541]
 [0.572]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.304]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  12.335756398922763
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]] [[43.399]
 [42.335]
 [42.335]
 [42.335]
 [42.335]
 [42.335]
 [42.335]] [[0.597]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]]
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.22065795613778
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.394]
 [0.383]
 [0.352]
 [0.352]
 [0.352]
 [0.352]] [[25.172]
 [29.98 ]
 [26.87 ]
 [29.658]
 [29.658]
 [29.658]
 [29.658]] [[0.396]
 [0.394]
 [0.383]
 [0.352]
 [0.352]
 [0.352]
 [0.352]]
printing an ep nov before normalisation:  18.728260811006447
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.91068818015975
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.073]
 [0.071]] [[31.685]
 [27.679]
 [27.679]
 [27.679]
 [27.679]
 [29.055]
 [27.679]] [[0.041]
 [0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.073]
 [0.071]]
siam score:  -0.76610976
printing an ep nov before normalisation:  22.371302528506785
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
siam score:  -0.7589075
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.39845611504086
printing an ep nov before normalisation:  41.796836853027344
siam score:  -0.7603158
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.60026810574477
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76068
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.862699060469442
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.520452544801486
actor:  1 policy actor:  1  step number:  65 total reward:  0.09333333333333238  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  90 total reward:  0.019999999999998352  reward:  1.0 rdn_beta:  1.667
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.5737],
        [-0.0000],
        [ 0.3033]], dtype=torch.float64)
-0.6479657180039999 -0.6479657180039999
-0.07801990019999874 -0.07801990019999874
-0.620796 -0.620796
-0.22222199999999964 -0.22222199999999964
-0.7752715212 -0.7752715212
-0.6006 -0.6006
-0.6923433000000002 -0.6923433000000002
-0.032346567066 -0.6060642248606175
0.9734999999999999 0.9734999999999999
-0.070771701198 0.23251096247838668
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.18689775466919
printing an ep nov before normalisation:  38.481580320219024
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
actions average: 
K:  2  action  0 :  tensor([0.4146, 0.0542, 0.1040, 0.1055, 0.1224, 0.1153, 0.0840],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0254, 0.8619, 0.0204, 0.0304, 0.0133, 0.0102, 0.0384],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1457, 0.0605, 0.4220, 0.0772, 0.0695, 0.1430, 0.0821],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1631, 0.0116, 0.1210, 0.2905, 0.1488, 0.1381, 0.1270],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1701, 0.0296, 0.1132, 0.1040, 0.3930, 0.1102, 0.0799],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1833, 0.1329, 0.1717, 0.1303, 0.1217, 0.1418, 0.1183],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1620, 0.0729, 0.1435, 0.1841, 0.1165, 0.1135, 0.2075],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]] [[19.096]
 [19.096]
 [19.096]
 [19.096]
 [19.096]
 [19.096]
 [19.096]] [[0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]]
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76468545
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.481315203760346
actions average: 
K:  4  action  0 :  tensor([0.4274, 0.1607, 0.0619, 0.0989, 0.1156, 0.0617, 0.0738],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0380, 0.8023, 0.0335, 0.0346, 0.0171, 0.0157, 0.0587],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1627, 0.1771, 0.1347, 0.1375, 0.1207, 0.1242, 0.1431],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1353, 0.1661, 0.1077, 0.2286, 0.0828, 0.1539, 0.1256],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1642, 0.2298, 0.0823, 0.1006, 0.2815, 0.0827, 0.0589],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1450, 0.1238, 0.1085, 0.1163, 0.1119, 0.2676, 0.1269],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2573, 0.0518, 0.1342, 0.1391, 0.0950, 0.0999, 0.2226],
       grad_fn=<DivBackward0>)
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.8167266666666666 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.451015090522674
actor:  0 policy actor:  0  step number:  65 total reward:  0.0666666666666661  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.8145933333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.71595918215967
actor:  0 policy actor:  0  step number:  51 total reward:  0.23999999999999932  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  37.79553230726195
printing an ep nov before normalisation:  53.83174364957945
printing an ep nov before normalisation:  37.66613483428955
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  8.147925989968437
Printing some Q and Qe and total Qs values:  [[-0.197]
 [-0.197]
 [-0.197]
 [-0.197]
 [-0.197]
 [-0.197]
 [-0.197]] [[48.348]
 [48.348]
 [48.348]
 [48.348]
 [48.348]
 [48.348]
 [48.348]] [[0.079]
 [0.079]
 [0.079]
 [0.079]
 [0.079]
 [0.079]
 [0.079]]
actions average: 
K:  1  action  0 :  tensor([0.4003, 0.0152, 0.1036, 0.1005, 0.1652, 0.1109, 0.1042],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0113, 0.9345, 0.0135, 0.0087, 0.0055, 0.0091, 0.0173],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0732, 0.0421, 0.4726, 0.0738, 0.0543, 0.2084, 0.0755],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2618, 0.0129, 0.1464, 0.1476, 0.1477, 0.1595, 0.1241],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1383, 0.0321, 0.0999, 0.1230, 0.4156, 0.1068, 0.0844],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1820, 0.0249, 0.1511, 0.0942, 0.0954, 0.3579, 0.0945],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1831, 0.0295, 0.2024, 0.1397, 0.1391, 0.1641, 0.1421],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7782168
actor:  1 policy actor:  1  step number:  78 total reward:  0.07333333333333214  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.49670100276459
Printing some Q and Qe and total Qs values:  [[-0.177]
 [-0.177]
 [-0.177]
 [-0.177]
 [-0.177]
 [-0.177]
 [-0.177]] [[34.871]
 [34.871]
 [34.871]
 [34.871]
 [34.871]
 [34.871]
 [34.871]] [[0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]]
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.58080772321956
printing an ep nov before normalisation:  28.42418429116358
printing an ep nov before normalisation:  41.752149092217856
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.43476095239165
Printing some Q and Qe and total Qs values:  [[-0.16]
 [-0.16]
 [-0.16]
 [-0.16]
 [-0.16]
 [-0.16]
 [-0.16]] [[35.94]
 [35.94]
 [35.94]
 [35.94]
 [35.94]
 [35.94]
 [35.94]] [[1.84]
 [1.84]
 [1.84]
 [1.84]
 [1.84]
 [1.84]
 [1.84]]
actions average: 
K:  3  action  0 :  tensor([0.5366, 0.0627, 0.0927, 0.0712, 0.0897, 0.0613, 0.0858],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0196, 0.8893, 0.0240, 0.0177, 0.0118, 0.0137, 0.0239],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1525, 0.0883, 0.1990, 0.1591, 0.1030, 0.1156, 0.1824],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2109, 0.0513, 0.1513, 0.2814, 0.0872, 0.1083, 0.1094],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2903, 0.0174, 0.1331, 0.1376, 0.2092, 0.1256, 0.0867],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2094, 0.1131, 0.1530, 0.1124, 0.1001, 0.1941, 0.1178],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1739, 0.2390, 0.0748, 0.1180, 0.0696, 0.0537, 0.2710],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  53 total reward:  0.3866666666666668  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
printing an ep nov before normalisation:  0.06725304218434758
actions average: 
K:  2  action  0 :  tensor([0.4908, 0.0133, 0.0947, 0.1041, 0.1283, 0.0777, 0.0912],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0148, 0.9038, 0.0214, 0.0156, 0.0072, 0.0124, 0.0248],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1187, 0.0598, 0.2794, 0.1258, 0.0894, 0.1578, 0.1692],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2075, 0.0079, 0.1414, 0.1945, 0.1933, 0.1447, 0.1107],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2554, 0.0100, 0.1087, 0.1347, 0.2788, 0.1060, 0.1065],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1171, 0.0673, 0.0962, 0.0767, 0.0851, 0.5069, 0.0508],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2025, 0.0465, 0.1112, 0.2066, 0.1347, 0.0835, 0.2150],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.148]
 [-0.117]
 [-0.148]
 [-0.148]
 [-0.148]
 [-0.148]
 [-0.148]] [[63.979]
 [63.851]
 [63.979]
 [63.979]
 [63.979]
 [63.979]
 [63.979]] [[1.319]
 [1.346]
 [1.319]
 [1.319]
 [1.319]
 [1.319]
 [1.319]]
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
printing an ep nov before normalisation:  63.65243168004502
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.275492262161265
actor:  1 policy actor:  1  step number:  46 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  40.86637617822689
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.712407285213537
printing an ep nov before normalisation:  42.284603118896484
printing an ep nov before normalisation:  51.53115749359131
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.729]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.444]
 [0.729]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]]
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
Printing some Q and Qe and total Qs values:  [[0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]
 [0.236]] [[40.054]
 [40.054]
 [40.054]
 [40.054]
 [40.054]
 [40.054]
 [40.054]] [[0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]]
printing an ep nov before normalisation:  18.732250357023972
actor:  1 policy actor:  1  step number:  68 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  53.558401725786815
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.87722490853072
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
Printing some Q and Qe and total Qs values:  [[-0.199]
 [-0.235]
 [-0.184]
 [-0.193]
 [-0.145]
 [-0.178]
 [-0.243]] [[33.061]
 [37.243]
 [32.04 ]
 [32.729]
 [37.247]
 [35.794]
 [34.154]] [[0.731]
 [0.929]
 [0.688]
 [0.717]
 [1.02 ]
 [0.905]
 [0.748]]
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 46.58880678941773
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.893]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]] [[32.381]
 [34.474]
 [32.381]
 [32.381]
 [32.381]
 [32.381]
 [32.381]] [[0.676]
 [0.893]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]]
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.10225147671169
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.697328753864646
Printing some Q and Qe and total Qs values:  [[0.257]
 [0.356]
 [0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.257]] [[25.401]
 [31.033]
 [25.401]
 [25.401]
 [25.401]
 [25.401]
 [25.401]] [[0.833]
 [1.315]
 [0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.833]]
printing an ep nov before normalisation:  28.18379902927528
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.379063342099805
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.02 ]
 [-0.078]
 [-0.072]
 [-0.071]
 [-0.02 ]
 [-0.02 ]
 [-0.09 ]] [[32.272]
 [32.981]
 [30.633]
 [29.811]
 [32.272]
 [32.272]
 [31.838]] [[1.588]
 [1.589]
 [1.401]
 [1.334]
 [1.588]
 [1.588]
 [1.482]]
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
printing an ep nov before normalisation:  42.41153687069368
printing an ep nov before normalisation:  33.994174434891576
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.645]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]] [[38.457]
 [42.723]
 [38.457]
 [38.457]
 [38.457]
 [38.457]
 [38.457]] [[0.548]
 [0.869]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]]
siam score:  -0.77863157
actor:  1 policy actor:  1  step number:  66 total reward:  0.1799999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.201]
 [-0.204]
 [-0.198]
 [-0.197]
 [-0.197]
 [-0.197]
 [-0.199]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.201]
 [-0.204]
 [-0.198]
 [-0.197]
 [-0.197]
 [-0.197]
 [-0.199]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.17 ]
 [-0.145]
 [-0.17 ]
 [-0.17 ]
 [-0.17 ]
 [-0.17 ]
 [-0.17 ]] [[40.074]
 [46.606]
 [40.074]
 [40.074]
 [40.074]
 [40.074]
 [40.074]] [[0.083]
 [0.188]
 [0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]]
Printing some Q and Qe and total Qs values:  [[-0.118]
 [-0.062]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]
 [-0.118]] [[33.261]
 [38.29 ]
 [43.87 ]
 [43.87 ]
 [43.87 ]
 [43.87 ]
 [43.87 ]] [[0.094]
 [0.207]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]]
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.8976638109838
printing an ep nov before normalisation:  41.66156738316466
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.902177333831787
printing an ep nov before normalisation:  50.71393035930925
printing an ep nov before normalisation:  42.7094744654503
actor:  1 policy actor:  1  step number:  59 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.58256327117946
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
printing an ep nov before normalisation:  49.133630645830294
printing an ep nov before normalisation:  34.05965260797952
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8121133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  47 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  44.12923794183118
maxi score, test score, baseline:  -0.8094733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  44.30540729608779
Printing some Q and Qe and total Qs values:  [[-0.176]
 [-0.159]
 [-0.188]
 [-0.188]
 [-0.188]
 [-0.188]
 [-0.188]] [[55.013]
 [50.848]
 [48.229]
 [48.229]
 [48.229]
 [48.229]
 [48.229]] [[0.932]
 [0.813]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]]
printing an ep nov before normalisation:  49.692438976704935
printing an ep nov before normalisation:  32.13566427131537
maxi score, test score, baseline:  -0.8094733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.46210277317898
printing an ep nov before normalisation:  33.442198992068775
maxi score, test score, baseline:  -0.8094733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.34 ]
 [0.073]
 [0.073]
 [0.073]
 [0.073]
 [0.073]] [[40.955]
 [38.721]
 [40.955]
 [40.955]
 [40.955]
 [40.955]
 [40.955]] [[0.357]
 [0.593]
 [0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]]
siam score:  -0.75839156
maxi score, test score, baseline:  -0.8094733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  70 total reward:  0.1666666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8094733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8094733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.02400588989258
printing an ep nov before normalisation:  41.00775172533736
maxi score, test score, baseline:  -0.8094733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8094733333333333 0.6700000000000002 0.6700000000000002
actor:  1 policy actor:  1  step number:  51 total reward:  0.3066666666666664  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.25 ]
 [0.254]
 [0.251]
 [0.237]
 [0.24 ]
 [0.245]
 [0.24 ]] [[32.333]
 [36.4  ]
 [28.66 ]
 [29.261]
 [29.853]
 [28.92 ]
 [29.311]] [[0.25 ]
 [0.254]
 [0.251]
 [0.237]
 [0.24 ]
 [0.245]
 [0.24 ]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8094733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.3647697128821
printing an ep nov before normalisation:  20.701828308803506
maxi score, test score, baseline:  -0.8094733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  51.84151086065531
printing an ep nov before normalisation:  37.603854912881694
maxi score, test score, baseline:  -0.8094733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.068]
 [-0.054]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.068]] [[40.978]
 [37.299]
 [40.978]
 [40.978]
 [40.978]
 [40.978]
 [40.978]] [[1.466]
 [1.234]
 [1.466]
 [1.466]
 [1.466]
 [1.466]
 [1.466]]
actor:  0 policy actor:  0  step number:  63 total reward:  0.27999999999999914  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.789611533761075
maxi score, test score, baseline:  -0.8069133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.17390640179779
siam score:  -0.76425153
maxi score, test score, baseline:  -0.8069133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.93771171569824
maxi score, test score, baseline:  -0.8069133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.914066314697266
maxi score, test score, baseline:  -0.8069133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8069133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.77150878465044
printing an ep nov before normalisation:  44.224124816646366
maxi score, test score, baseline:  -0.8069133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.968]
 [0.987]
 [0.978]
 [1.005]
 [1.005]
 [1.005]
 [0.831]] [[41.206]
 [41.984]
 [39.075]
 [39.058]
 [39.058]
 [39.058]
 [39.331]] [[0.968]
 [0.987]
 [0.978]
 [1.005]
 [1.005]
 [1.005]
 [0.831]]
maxi score, test score, baseline:  -0.8069133333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.45124065421986
actor:  0 policy actor:  0  step number:  37 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8039799999999999 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8039799999999999 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8039799999999999 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8039799999999999 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8039799999999999 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8039799999999999 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8039799999999999 0.6700000000000002 0.6700000000000002
maxi score, test score, baseline:  -0.8039799999999999 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.652852394955715
printing an ep nov before normalisation:  36.59933938997399
maxi score, test score, baseline:  -0.8039799999999999 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.127535395501198
actor:  1 policy actor:  1  step number:  57 total reward:  0.3466666666666658  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  47.90209192474301
maxi score, test score, baseline:  -0.8039799999999999 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.70506002839767
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.18 ]
 [-0.122]
 [-0.139]
 [-0.163]
 [-0.163]
 [-0.163]
 [-0.147]] [[34.004]
 [44.987]
 [49.574]
 [32.28 ]
 [32.28 ]
 [32.28 ]
 [43.661]] [[0.306]
 [0.593]
 [0.672]
 [0.287]
 [0.287]
 [0.287]
 [0.541]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8039799999999999 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.66361226270728
actor:  0 policy actor:  0  step number:  43 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  58.63222288662299
printing an ep nov before normalisation:  42.77728771655383
maxi score, test score, baseline:  -0.80134 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.5773494081368
maxi score, test score, baseline:  -0.80134 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.80134 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.63201117515564
printing an ep nov before normalisation:  22.55272304404741
UNIT TEST: sample policy line 217 mcts : [0.041 0.551 0.02  0.02  0.122 0.02  0.224]
maxi score, test score, baseline:  -0.80134 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  70 total reward:  0.0066666666666661545  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  44.0070673688264
maxi score, test score, baseline:  -0.80134 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.80134 0.6700000000000002 0.6700000000000002
actor:  1 policy actor:  1  step number:  60 total reward:  0.11333333333333284  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  39.31496858596802
Printing some Q and Qe and total Qs values:  [[-0.175]
 [-0.169]
 [-0.175]
 [-0.175]
 [-0.175]
 [-0.175]
 [-0.175]] [[43.654]
 [44.62 ]
 [43.654]
 [43.654]
 [43.654]
 [43.654]
 [43.654]] [[0.789]
 [0.831]
 [0.789]
 [0.789]
 [0.789]
 [0.789]
 [0.789]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.80134 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.80134 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  66 total reward:  0.03333333333333266  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.80134 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76566416
printing an ep nov before normalisation:  25.486491504175802
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.80134 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.948948523130188
maxi score, test score, baseline:  -0.80134 0.6700000000000002 0.6700000000000002
maxi score, test score, baseline:  -0.80134 0.6700000000000002 0.6700000000000002
printing an ep nov before normalisation:  0.056838130136327436
maxi score, test score, baseline:  -0.80134 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.950024679165317
maxi score, test score, baseline:  -0.80134 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.80134 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.80134 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.111]
 [-0.111]
 [-0.113]
 [-0.116]
 [-0.112]
 [-0.112]
 [-0.11 ]] [[48.434]
 [49.494]
 [46.232]
 [45.957]
 [46.999]
 [48.097]
 [47.073]] [[0.499]
 [0.524]
 [0.445]
 [0.435]
 [0.464]
 [0.489]
 [0.468]]
maxi score, test score, baseline:  -0.80134 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76686233
maxi score, test score, baseline:  -0.80134 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.80134 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.03678958116868
printing an ep nov before normalisation:  39.676671146040924
actor:  0 policy actor:  0  step number:  53 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  66 total reward:  0.2733333333333332  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.79886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.206466674804688
maxi score, test score, baseline:  -0.79886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  1.333
siam score:  -0.7694357
maxi score, test score, baseline:  -0.79886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.135]
 [-0.117]
 [-0.135]
 [-0.135]
 [-0.135]
 [-0.135]
 [-0.135]] [[36.038]
 [33.625]
 [36.038]
 [36.038]
 [36.038]
 [36.038]
 [36.038]] [[1.865]
 [1.638]
 [1.865]
 [1.865]
 [1.865]
 [1.865]
 [1.865]]
maxi score, test score, baseline:  -0.79886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.79886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.79886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.29707375504103
printing an ep nov before normalisation:  45.5335791890203
actor:  1 policy actor:  1  step number:  61 total reward:  0.02666666666666606  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.79886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.79886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.79886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.437546590109648
printing an ep nov before normalisation:  59.55236919780997
Printing some Q and Qe and total Qs values:  [[-0.156]
 [-0.126]
 [-0.155]
 [-0.166]
 [-0.169]
 [-0.171]
 [-0.159]] [[41.553]
 [42.931]
 [42.842]
 [40.632]
 [43.682]
 [42.448]
 [42.806]] [[0.566]
 [0.651]
 [0.618]
 [0.518]
 [0.639]
 [0.586]
 [0.613]]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.79886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.565754740939724
maxi score, test score, baseline:  -0.79886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.30205558256019
maxi score, test score, baseline:  -0.79886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  12.065903400086272
maxi score, test score, baseline:  -0.79886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.79886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.79886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.02 ]
 [ 0.464]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]] [[34.974]
 [35.893]
 [34.974]
 [34.974]
 [34.974]
 [34.974]
 [34.974]] [[1.161]
 [1.706]
 [1.161]
 [1.161]
 [1.161]
 [1.161]
 [1.161]]
Printing some Q and Qe and total Qs values:  [[-0.017]
 [ 0.19 ]
 [-0.001]
 [ 0.017]
 [-0.017]
 [-0.017]
 [-0.017]] [[42.926]
 [41.87 ]
 [38.413]
 [42.073]
 [42.926]
 [42.926]
 [42.926]] [[1.761]
 [1.885]
 [1.422]
 [1.729]
 [1.761]
 [1.761]
 [1.761]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[-0.004]
 [ 0.174]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[37.239]
 [41.088]
 [37.239]
 [37.239]
 [37.239]
 [37.239]
 [37.239]] [[1.586]
 [2.06 ]
 [1.586]
 [1.586]
 [1.586]
 [1.586]
 [1.586]]
printing an ep nov before normalisation:  26.729190349578857
maxi score, test score, baseline:  -0.79886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.5797, 0.0110, 0.0693, 0.0789, 0.1516, 0.0506, 0.0589],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0258, 0.8720, 0.0242, 0.0246, 0.0139, 0.0122, 0.0273],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1255, 0.1145, 0.3139, 0.1174, 0.1052, 0.1253, 0.0981],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2150, 0.1812, 0.1514, 0.1108, 0.0987, 0.0905, 0.1525],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2243, 0.0198, 0.1596, 0.1528, 0.2359, 0.1080, 0.0996],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2040, 0.0465, 0.1178, 0.0771, 0.0620, 0.4333, 0.0593],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1813, 0.2456, 0.1043, 0.1233, 0.0596, 0.0656, 0.2203],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  60 total reward:  0.2066666666666659  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.79886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.79886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 34.45403814315796
siam score:  -0.7754057
maxi score, test score, baseline:  -0.79886 0.6700000000000002 0.6700000000000002
maxi score, test score, baseline:  -0.79886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.450734908181836
printing an ep nov before normalisation:  35.74657478922642
printing an ep nov before normalisation:  36.319851875305176
maxi score, test score, baseline:  -0.79886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.829]
 [0.546]
 [0.546]
 [0.546]
 [0.552]
 [0.546]] [[34.949]
 [35.152]
 [34.236]
 [34.236]
 [34.236]
 [34.943]
 [34.236]] [[0.553]
 [0.829]
 [0.546]
 [0.546]
 [0.546]
 [0.552]
 [0.546]]
Printing some Q and Qe and total Qs values:  [[0.828]
 [0.822]
 [0.828]
 [0.828]
 [0.828]
 [0.828]
 [0.828]] [[35.6  ]
 [37.859]
 [35.6  ]
 [35.6  ]
 [35.6  ]
 [35.6  ]
 [35.6  ]] [[0.828]
 [0.822]
 [0.828]
 [0.828]
 [0.828]
 [0.828]
 [0.828]]
maxi score, test score, baseline:  -0.79886 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  50 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.7964733333333333 0.6700000000000002 0.6700000000000002
maxi score, test score, baseline:  -0.7964733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.70132531060113
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]]
printing an ep nov before normalisation:  37.692209301341286
printing an ep nov before normalisation:  41.88954944635107
printing an ep nov before normalisation:  53.83624626010404
actions average: 
K:  1  action  0 :  tensor([0.3428, 0.0643, 0.1264, 0.0870, 0.1821, 0.0933, 0.1041],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0166, 0.9106, 0.0133, 0.0139, 0.0081, 0.0072, 0.0304],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1248, 0.0103, 0.5075, 0.0706, 0.0793, 0.1277, 0.0796],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1461, 0.0364, 0.1465, 0.2151, 0.1387, 0.1540, 0.1631],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2538, 0.0074, 0.0918, 0.0942, 0.3727, 0.1047, 0.0754],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1745, 0.0597, 0.1803, 0.1325, 0.1464, 0.1492, 0.1574],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1167, 0.1381, 0.1009, 0.1410, 0.1030, 0.1054, 0.2950],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.7964733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.329]
 [0.333]
 [0.33 ]
 [0.329]
 [0.33 ]
 [0.331]
 [0.331]] [[28.434]
 [35.963]
 [30.078]
 [32.202]
 [31.125]
 [32.188]
 [28.479]] [[0.329]
 [0.333]
 [0.33 ]
 [0.329]
 [0.33 ]
 [0.331]
 [0.331]]
siam score:  -0.7676223
maxi score, test score, baseline:  -0.7964733333333334 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7964733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7964733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7964733333333333 0.6700000000000002 0.6700000000000002
siam score:  -0.76582354
printing an ep nov before normalisation:  36.35095914029392
printing an ep nov before normalisation:  45.37220744669016
maxi score, test score, baseline:  -0.7964733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  15.05652889895384
printing an ep nov before normalisation:  43.09888292035728
maxi score, test score, baseline:  -0.7964733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.14987283027373
actor:  1 policy actor:  1  step number:  61 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7964733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.622137977460785
Printing some Q and Qe and total Qs values:  [[-0.104]
 [-0.066]
 [-0.104]
 [-0.12 ]
 [-0.121]
 [-0.104]
 [-0.104]] [[39.499]
 [47.335]
 [39.499]
 [33.862]
 [32.479]
 [39.499]
 [39.499]] [[0.813]
 [1.177]
 [0.813]
 [0.563]
 [0.505]
 [0.813]
 [0.813]]
printing an ep nov before normalisation:  44.45954230141823
printing an ep nov before normalisation:  43.400474371093374
siam score:  -0.76126975
printing an ep nov before normalisation:  38.38243228356668
printing an ep nov before normalisation:  39.779833134246395
printing an ep nov before normalisation:  36.326086037925336
maxi score, test score, baseline:  -0.7964733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 31.834007085119218
printing an ep nov before normalisation:  35.22696480316545
actor:  1 policy actor:  1  step number:  55 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.527]
 [0.419]
 [0.589]
 [0.493]
 [0.589]
 [0.589]] [[32.583]
 [36.592]
 [35.629]
 [37.131]
 [38.428]
 [37.131]
 [37.131]] [[0.498]
 [0.527]
 [0.419]
 [0.589]
 [0.493]
 [0.589]
 [0.589]]
printing an ep nov before normalisation:  57.89747926981401
maxi score, test score, baseline:  -0.7964733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7964733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.74259747507457
printing an ep nov before normalisation:  50.16070753485952
maxi score, test score, baseline:  -0.7964733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.76005054
printing an ep nov before normalisation:  35.532770662150284
printing an ep nov before normalisation:  42.82712609405938
actor:  1 policy actor:  1  step number:  46 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.7964733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  23.72670649088457
Printing some Q and Qe and total Qs values:  [[-0.   ]
 [-0.   ]
 [-0.277]
 [-0.277]
 [-0.277]
 [-0.277]
 [-0.277]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.   ]
 [-0.   ]
 [-0.277]
 [-0.277]
 [-0.277]
 [-0.277]
 [-0.277]]
printing an ep nov before normalisation:  51.52415118522653
printing an ep nov before normalisation:  34.869141578674316
maxi score, test score, baseline:  -0.7964733333333333 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.544]
 [0.364]
 [0.407]
 [0.306]
 [0.306]
 [0.306]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.38 ]
 [0.544]
 [0.364]
 [0.407]
 [0.306]
 [0.306]
 [0.306]]
actor:  0 policy actor:  0  step number:  44 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  33.978703022003174
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.66120192485844
maxi score, test score, baseline:  -0.7938466666666667 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.3403, 0.1390, 0.1130, 0.1082, 0.1103, 0.0869, 0.1024],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0095, 0.9527, 0.0088, 0.0067, 0.0034, 0.0033, 0.0156],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1705, 0.0893, 0.2984, 0.1060, 0.0964, 0.1459, 0.0936],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1385, 0.1466, 0.0944, 0.2825, 0.0892, 0.0846, 0.1642],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2272, 0.0111, 0.1181, 0.1173, 0.3203, 0.0863, 0.1196],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1324, 0.0041, 0.1168, 0.1394, 0.1051, 0.4173, 0.0850],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2352, 0.0479, 0.1354, 0.1384, 0.1423, 0.1223, 0.1785],
       grad_fn=<DivBackward0>)
siam score:  -0.7561002
maxi score, test score, baseline:  -0.7938466666666667 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7938466666666667 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.96748432517807
printing an ep nov before normalisation:  48.024968100059965
Printing some Q and Qe and total Qs values:  [[-0.096]
 [-0.095]
 [-0.096]
 [-0.096]
 [-0.094]
 [-0.093]
 [-0.093]] [[28.929]
 [30.905]
 [30.096]
 [29.489]
 [28.121]
 [28.036]
 [27.393]] [[1.242]
 [1.335]
 [1.296]
 [1.268]
 [1.206]
 [1.203]
 [1.173]]
printing an ep nov before normalisation:  37.478945026864565
siam score:  -0.7640721
maxi score, test score, baseline:  -0.7938466666666667 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.166]
 [-0.168]
 [-0.17 ]
 [-0.169]
 [-0.169]
 [-0.168]
 [-0.168]] [[20.618]
 [41.465]
 [21.174]
 [20.589]
 [20.699]
 [20.965]
 [20.596]] [[-0.073]
 [ 0.414]
 [-0.064]
 [-0.077]
 [-0.074]
 [-0.067]
 [-0.075]]
printing an ep nov before normalisation:  34.426389852076454
printing an ep nov before normalisation:  44.42201355511616
printing an ep nov before normalisation:  30.951820395554712
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7938466666666667 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.50383499263418
Printing some Q and Qe and total Qs values:  [[0.299]
 [0.558]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]] [[31.03 ]
 [35.492]
 [31.03 ]
 [31.03 ]
 [31.03 ]
 [31.03 ]
 [31.03 ]] [[1.452]
 [1.876]
 [1.452]
 [1.452]
 [1.452]
 [1.452]
 [1.452]]
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.65 ]
 [0.835]
 [0.545]
 [0.294]
 [0.835]
 [0.835]] [[53.973]
 [58.846]
 [55.013]
 [53.798]
 [57.275]
 [55.013]
 [55.013]] [[1.167]
 [1.65 ]
 [1.743]
 [1.424]
 [1.256]
 [1.743]
 [1.743]]
actor:  1 policy actor:  1  step number:  63 total reward:  0.14666666666666628  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  42.97343120758223
printing an ep nov before normalisation:  19.935437440872192
printing an ep nov before normalisation:  25.521278118545972
maxi score, test score, baseline:  -0.7938466666666667 0.6700000000000002 0.6700000000000002
maxi score, test score, baseline:  -0.7938466666666667 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7938466666666667 0.6700000000000002 0.6700000000000002
maxi score, test score, baseline:  -0.7938466666666667 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7938466666666667 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.04406415642336
printing an ep nov before normalisation:  16.80480665753617
maxi score, test score, baseline:  -0.7938466666666667 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7938466666666667 0.6700000000000002 0.6700000000000002
maxi score, test score, baseline:  -0.7938466666666667 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.037]
 [ 0.362]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]] [[39.578]
 [39.598]
 [39.578]
 [39.578]
 [39.578]
 [39.578]
 [39.578]] [[1.026]
 [1.426]
 [1.026]
 [1.026]
 [1.026]
 [1.026]
 [1.026]]
printing an ep nov before normalisation:  34.00446500297562
maxi score, test score, baseline:  -0.7938466666666667 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7938466666666667 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.3066666666666664  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  17.173123359680176
maxi score, test score, baseline:  -0.7938466666666667 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7938466666666667 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7938466666666667 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  37 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  49.26456784545381
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([0.5220, 0.0401, 0.0609, 0.0814, 0.1639, 0.0699, 0.0618],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0154, 0.9153, 0.0197, 0.0111, 0.0078, 0.0104, 0.0203],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1732, 0.0358, 0.3151, 0.1221, 0.1090, 0.1218, 0.1232],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1147, 0.1690, 0.1161, 0.2436, 0.1000, 0.1188, 0.1379],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1586, 0.0384, 0.1388, 0.1452, 0.2610, 0.1317, 0.1263],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1138, 0.1428, 0.1547, 0.1298, 0.1129, 0.2474, 0.0986],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1674, 0.0816, 0.1376, 0.1653, 0.1525, 0.1519, 0.1437],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.79086 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.79086 0.6700000000000002 0.6700000000000002
Printing some Q and Qe and total Qs values:  [[-0.21 ]
 [-0.153]
 [-0.191]
 [-0.179]
 [-0.186]
 [-0.181]
 [-0.175]] [[23.714]
 [26.848]
 [23.849]
 [22.904]
 [22.824]
 [22.983]
 [23.086]] [[-0.006]
 [ 0.093]
 [ 0.014]
 [ 0.013]
 [ 0.006]
 [ 0.012]
 [ 0.02 ]]
printing an ep nov before normalisation:  31.658945083618164
maxi score, test score, baseline:  -0.79086 0.6700000000000002 0.6700000000000002
maxi score, test score, baseline:  -0.79086 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.493578773680905
actor:  1 policy actor:  1  step number:  52 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.79086 0.6700000000000002 0.6700000000000002
maxi score, test score, baseline:  -0.79086 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.22 ]
 [-0.147]
 [-0.183]
 [-0.179]
 [-0.196]
 [-0.183]
 [-0.183]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.22 ]
 [-0.147]
 [-0.183]
 [-0.179]
 [-0.196]
 [-0.183]
 [-0.183]]
maxi score, test score, baseline:  -0.79086 0.6700000000000002 0.6700000000000002
printing an ep nov before normalisation:  50.47580200268269
maxi score, test score, baseline:  -0.79086 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.59779167175293
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.064]
 [-0.064]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.062]
 [-0.059]] [[14.337]
 [16.762]
 [11.951]
 [12.168]
 [12.189]
 [14.567]
 [12.11 ]] [[0.152]
 [0.189]
 [0.116]
 [0.118]
 [0.118]
 [0.158]
 [0.124]]
maxi score, test score, baseline:  -0.79086 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.79086 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.79086 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.79086 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.291457722940564
printing an ep nov before normalisation:  38.15016385402985
maxi score, test score, baseline:  -0.79086 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.434]
 [0.355]
 [0.272]
 [0.332]
 [0.261]
 [0.308]] [[31.198]
 [27.772]
 [29.215]
 [30.162]
 [29.922]
 [31.497]
 [30.404]] [[1.001]
 [1.049]
 [1.002]
 [0.94 ]
 [0.994]
 [0.959]
 [0.981]]
Printing some Q and Qe and total Qs values:  [[-0.197]
 [-0.192]
 [-0.197]
 [-0.197]
 [-0.194]
 [-0.197]
 [-0.197]] [[19.034]
 [52.303]
 [19.034]
 [19.034]
 [22.268]
 [19.034]
 [19.034]] [[0.05 ]
 [0.896]
 [0.05 ]
 [0.05 ]
 [0.135]
 [0.05 ]
 [0.05 ]]
printing an ep nov before normalisation:  13.753392696380615
maxi score, test score, baseline:  -0.79086 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.52746737986967
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([0.4046, 0.0279, 0.1049, 0.1185, 0.1245, 0.1062, 0.1135],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0202, 0.8648, 0.0194, 0.0308, 0.0131, 0.0150, 0.0367],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1655, 0.0609, 0.2571, 0.1361, 0.1304, 0.1540, 0.0961],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1654, 0.1630, 0.1047, 0.1796, 0.1182, 0.1231, 0.1460],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2695, 0.0152, 0.0952, 0.1272, 0.2877, 0.1105, 0.0947],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0987, 0.1546, 0.1080, 0.1256, 0.0811, 0.3377, 0.0944],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2644, 0.0358, 0.1165, 0.1662, 0.1497, 0.1347, 0.1328],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.79086 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.614008520521466
printing an ep nov before normalisation:  36.53075546758678
Printing some Q and Qe and total Qs values:  [[0.085]
 [0.205]
 [0.086]
 [0.081]
 [0.081]
 [0.222]
 [0.351]] [[26.722]
 [29.124]
 [26.289]
 [26.513]
 [26.449]
 [29.805]
 [31.955]] [[0.555]
 [0.764]
 [0.54 ]
 [0.544]
 [0.541]
 [0.807]
 [1.015]]
maxi score, test score, baseline:  -0.79086 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.79086 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.027]
 [-0.024]
 [-0.034]
 [-0.03 ]
 [-0.028]
 [-0.028]
 [-0.034]] [[35.195]
 [34.259]
 [35.521]
 [35.329]
 [34.988]
 [34.794]
 [35.028]] [[0.963]
 [0.914]
 [0.975]
 [0.968]
 [0.951]
 [0.94 ]
 [0.947]]
printing an ep nov before normalisation:  30.99404238512413
maxi score, test score, baseline:  -0.79086 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.79086 0.6700000000000002 0.6700000000000002
actor:  1 policy actor:  1  step number:  78 total reward:  0.00666666666666571  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.79086 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]] [[35.931]
 [35.931]
 [35.931]
 [35.931]
 [35.931]
 [35.931]
 [35.931]] [[0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]]
printing an ep nov before normalisation:  37.382345431403685
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.68 ]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]] [[37.15 ]
 [46.267]
 [37.15 ]
 [37.15 ]
 [37.15 ]
 [37.15 ]
 [37.15 ]] [[0.654]
 [0.68 ]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]]
printing an ep nov before normalisation:  43.20365905761719
printing an ep nov before normalisation:  44.79709283293204
maxi score, test score, baseline:  -0.79086 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.79086 0.6700000000000002 0.6700000000000002
Starting evaluation
printing an ep nov before normalisation:  21.269445419311523
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.79086 0.6700000000000002 0.6700000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.397559347971274
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  35.945267291388404
printing an ep nov before normalisation:  27.21157872222214
printing an ep nov before normalisation:  43.67485856157578
maxi score, test score, baseline:  -0.79086 0.6700000000000002 0.6700000000000002
printing an ep nov before normalisation:  37.5791776712535
Printing some Q and Qe and total Qs values:  [[0.723]
 [0.848]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]] [[34.583]
 [30.136]
 [34.583]
 [34.583]
 [34.583]
 [34.583]
 [34.583]] [[0.723]
 [0.848]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]]
Printing some Q and Qe and total Qs values:  [[-0.175]
 [-0.135]
 [-0.125]
 [-0.108]
 [-0.175]
 [-0.174]
 [-0.181]] [[34.917]
 [38.148]
 [37.884]
 [39.601]
 [40.066]
 [38.84 ]
 [26.246]] [[0.857]
 [0.993]
 [0.996]
 [1.063]
 [1.01 ]
 [0.975]
 [0.595]]
printing an ep nov before normalisation:  32.07223420567001
printing an ep nov before normalisation:  35.49945744460481
printing an ep nov before normalisation:  32.05188741025942
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  41.40979562486921
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[-0.068]
 [-0.051]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.068]] [[34.278]
 [37.985]
 [34.278]
 [34.278]
 [34.278]
 [34.278]
 [34.278]] [[0.415]
 [0.532]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]]
actor:  0 policy actor:  0  step number:  27 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.72338 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([0.3767, 0.0239, 0.1320, 0.1142, 0.1506, 0.1010, 0.1016],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0164, 0.9051, 0.0096, 0.0217, 0.0122, 0.0074, 0.0277],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1599, 0.0110, 0.3005, 0.1480, 0.1138, 0.1682, 0.0986],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1822, 0.1544, 0.1238, 0.1693, 0.1378, 0.1155, 0.1169],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2165, 0.0037, 0.1082, 0.1607, 0.2880, 0.1141, 0.1088],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1353, 0.0271, 0.1524, 0.1185, 0.1196, 0.3381, 0.1090],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1340, 0.2797, 0.1027, 0.1618, 0.0987, 0.0733, 0.1498],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.72338 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.13999999999999901  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.72338 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.022022025060153
printing an ep nov before normalisation:  36.97938919067383
maxi score, test score, baseline:  -0.72338 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.72338 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.454]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]] [[33.131]
 [32.344]
 [33.131]
 [33.131]
 [33.131]
 [33.131]
 [33.131]] [[1.768]
 [1.859]
 [1.768]
 [1.768]
 [1.768]
 [1.768]
 [1.768]]
printing an ep nov before normalisation:  36.25197271486607
actor:  1 policy actor:  1  step number:  55 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.72338 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.78057694102575
maxi score, test score, baseline:  -0.72338 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.415239165471654
maxi score, test score, baseline:  -0.72338 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  13.083980083465576
printing an ep nov before normalisation:  22.20710476933753
maxi score, test score, baseline:  -0.72338 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  13.191601437623712
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.72338 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.2911, 0.0568, 0.1366, 0.1271, 0.1780, 0.0989, 0.1116],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0293, 0.8393, 0.0188, 0.0291, 0.0200, 0.0204, 0.0432],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1453, 0.1215, 0.2213, 0.1232, 0.1020, 0.1221, 0.1646],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1959, 0.1150, 0.1137, 0.1650, 0.1530, 0.1051, 0.1523],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1717, 0.0167, 0.0823, 0.0927, 0.5138, 0.0541, 0.0687],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1024, 0.2002, 0.1189, 0.1527, 0.0847, 0.2348, 0.1063],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1581, 0.1004, 0.1396, 0.1393, 0.1521, 0.1369, 0.1736],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.72338 0.687 0.687
maxi score, test score, baseline:  -0.72338 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.72338 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.897934970727604
maxi score, test score, baseline:  -0.72338 0.687 0.687
maxi score, test score, baseline:  -0.72338 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  37 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.7204466666666668 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.655448755809104
siam score:  -0.7729092
printing an ep nov before normalisation:  31.404131480097405
maxi score, test score, baseline:  -0.7204466666666668 0.687 0.687
maxi score, test score, baseline:  -0.7204466666666668 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.54623222351074
printing an ep nov before normalisation:  30.006450556135665
maxi score, test score, baseline:  -0.7204466666666668 0.687 0.687
maxi score, test score, baseline:  -0.7204466666666668 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.91467718912374
actor:  0 policy actor:  0  step number:  60 total reward:  0.17999999999999972  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  21.148131214662037
maxi score, test score, baseline:  -0.7180866666666668 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.85985352481675
printing an ep nov before normalisation:  41.67367882198757
maxi score, test score, baseline:  -0.7180866666666668 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.910448760680396
printing an ep nov before normalisation:  51.857878705269435
maxi score, test score, baseline:  -0.7180866666666668 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7180866666666668 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.719]
 [0.633]
 [0.35 ]
 [0.658]
 [0.376]
 [0.536]] [[30.096]
 [29.575]
 [34.309]
 [32.356]
 [30.096]
 [32.906]
 [30.504]] [[1.875]
 [1.914]
 [2.02 ]
 [1.657]
 [1.875]
 [1.706]
 [1.769]]
maxi score, test score, baseline:  -0.7180866666666668 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  0.4999999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]] [[25.142]
 [25.142]
 [25.142]
 [25.142]
 [25.142]
 [25.142]
 [25.142]] [[0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]
 [0.129]]
printing an ep nov before normalisation:  32.364636244732665
actor:  0 policy actor:  0  step number:  59 total reward:  0.013333333333332753  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7160600000000001 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.166]
 [-0.166]
 [-0.166]
 [-0.166]
 [-0.166]
 [-0.166]
 [-0.166]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.166]
 [-0.166]
 [-0.166]
 [-0.166]
 [-0.166]
 [-0.166]
 [-0.166]]
maxi score, test score, baseline:  -0.7160600000000001 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 24.94434106349945
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7160600000000001 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7160600000000001 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7160600000000001 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.244429018243615
maxi score, test score, baseline:  -0.7160600000000001 0.687 0.687
maxi score, test score, baseline:  -0.7160600000000001 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.84736306645583
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.184]
 [-0.208]
 [-0.184]
 [-0.184]
 [-0.184]
 [-0.184]
 [-0.184]] [[36.636]
 [40.271]
 [36.636]
 [36.636]
 [36.636]
 [36.636]
 [36.636]] [[0.968]
 [1.134]
 [0.968]
 [0.968]
 [0.968]
 [0.968]
 [0.968]]
printing an ep nov before normalisation:  28.637706336017143
printing an ep nov before normalisation:  38.408339226623454
maxi score, test score, baseline:  -0.71606 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.71606 0.687 0.687
printing an ep nov before normalisation:  39.037961376651744
printing an ep nov before normalisation:  34.79214015153737
printing an ep nov before normalisation:  34.2158820864187
actor:  0 policy actor:  0  step number:  67 total reward:  0.013333333333332531  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.068615867284464
maxi score, test score, baseline:  -0.7140333333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7140333333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7140333333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7140333333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  2.0352129581624467e-05
maxi score, test score, baseline:  -0.7140333333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.639]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]] [[47.578]
 [42.65 ]
 [47.578]
 [47.578]
 [47.578]
 [47.578]
 [47.578]] [[0.496]
 [0.639]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]]
maxi score, test score, baseline:  -0.7140333333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7140333333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  54 total reward:  0.17999999999999938  reward:  1.0 rdn_beta:  1.0
siam score:  -0.7847209
printing an ep nov before normalisation:  35.67569968664665
printing an ep nov before normalisation:  42.77701746015086
printing an ep nov before normalisation:  32.610497062556334
actor:  0 policy actor:  0  step number:  48 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.7092066666666668 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7092066666666668 0.687 0.687
maxi score, test score, baseline:  -0.7092066666666668 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7092066666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.257]] [[37.783]
 [37.783]
 [37.783]
 [37.783]
 [37.783]
 [37.783]
 [37.783]] [[0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.257]]
maxi score, test score, baseline:  -0.7092066666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  13.589077025537767
maxi score, test score, baseline:  -0.7092066666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7092066666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.3186, 0.0132, 0.1444, 0.1307, 0.1479, 0.1252, 0.1199],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0206, 0.8965, 0.0207, 0.0173, 0.0091, 0.0132, 0.0226],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2248, 0.1172, 0.1247, 0.1486, 0.1306, 0.1372, 0.1171],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1444, 0.1013, 0.1297, 0.2399, 0.1004, 0.1709, 0.1135],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2049, 0.0023, 0.0957, 0.0983, 0.4220, 0.0947, 0.0822],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1458, 0.0217, 0.1750, 0.1223, 0.1287, 0.3084, 0.0981],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2279, 0.1126, 0.1008, 0.1242, 0.1152, 0.0917, 0.2276],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  48 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.7092066666666668 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.793942315237864
siam score:  -0.78860795
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.7092066666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.95933912869446
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.593]
 [0.511]
 [0.516]
 [0.497]
 [0.509]
 [0.521]] [[24.78 ]
 [29.466]
 [23.729]
 [24.204]
 [16.816]
 [20.417]
 [24.634]] [[0.516]
 [0.593]
 [0.511]
 [0.516]
 [0.497]
 [0.509]
 [0.521]]
printing an ep nov before normalisation:  47.20000432767525
printing an ep nov before normalisation:  53.39593365385583
printing an ep nov before normalisation:  40.848635448857905
Printing some Q and Qe and total Qs values:  [[-0.071]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]] [[49.552]
 [49.552]
 [49.552]
 [49.552]
 [49.552]
 [49.552]
 [49.552]] [[0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]
 [0.918]]
maxi score, test score, baseline:  -0.7092066666666668 0.687 0.687
maxi score, test score, baseline:  -0.7092066666666668 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.103]
 [-0.103]
 [-0.103]
 [-0.103]
 [-0.103]
 [-0.103]
 [-0.103]] [[40.745]
 [40.745]
 [40.745]
 [40.745]
 [40.745]
 [40.745]
 [40.745]] [[0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]
 [0.214]]
maxi score, test score, baseline:  -0.7092066666666668 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7092066666666668 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7092066666666668 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.823608667608827
printing an ep nov before normalisation:  47.113148339081086
maxi score, test score, baseline:  -0.7092066666666668 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 17.281537119607414
maxi score, test score, baseline:  -0.7092066666666668 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.5254619895506
actions average: 
K:  2  action  0 :  tensor([0.3381, 0.0119, 0.1401, 0.1347, 0.1372, 0.1331, 0.1049],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0336, 0.8654, 0.0204, 0.0285, 0.0123, 0.0124, 0.0274],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1773, 0.1146, 0.2231, 0.1137, 0.1202, 0.1385, 0.1126],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1302, 0.0401, 0.1084, 0.2802, 0.1276, 0.1424, 0.1711],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2635, 0.0228, 0.1328, 0.1603, 0.1964, 0.1194, 0.1048],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1812, 0.0014, 0.1717, 0.1388, 0.1208, 0.2639, 0.1222],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2256, 0.0356, 0.1396, 0.1664, 0.1261, 0.1173, 0.1893],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  22.532205783078666
printing an ep nov before normalisation:  38.672332763671875
maxi score, test score, baseline:  -0.7092066666666668 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.35728579821363
maxi score, test score, baseline:  -0.7092066666666668 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  52 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  42.977926276594665
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.488]
 [0.355]
 [0.426]
 [0.426]
 [0.426]
 [0.426]] [[38.655]
 [36.571]
 [38.429]
 [33.097]
 [33.097]
 [33.097]
 [33.097]] [[0.376]
 [0.488]
 [0.355]
 [0.426]
 [0.426]
 [0.426]
 [0.426]]
maxi score, test score, baseline:  -0.70666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.77443469919937
maxi score, test score, baseline:  -0.70666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.70666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.4193, 0.0204, 0.0889, 0.0931, 0.2233, 0.0703, 0.0847],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0161, 0.9290, 0.0088, 0.0122, 0.0091, 0.0053, 0.0196],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1431, 0.0230, 0.2720, 0.1547, 0.1404, 0.1456, 0.1211],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1480, 0.0113, 0.1120, 0.3535, 0.1374, 0.1079, 0.1298],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2315, 0.0665, 0.1216, 0.1208, 0.2554, 0.0944, 0.1098],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1239, 0.0064, 0.2464, 0.1076, 0.1213, 0.2978, 0.0967],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1618, 0.0337, 0.1223, 0.1377, 0.1784, 0.1246, 0.2416],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.70666 0.687 0.687
printing an ep nov before normalisation:  18.524446814801635
printing an ep nov before normalisation:  50.51085482511123
printing an ep nov before normalisation:  49.2982014678956
maxi score, test score, baseline:  -0.70666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.48051830984555
Printing some Q and Qe and total Qs values:  [[-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]] [[45.367]
 [45.367]
 [45.367]
 [45.367]
 [45.367]
 [45.367]
 [45.367]] [[60.423]
 [60.423]
 [60.423]
 [60.423]
 [60.423]
 [60.423]
 [60.423]]
printing an ep nov before normalisation:  0.0
printing an ep nov before normalisation:  30.54104458915935
printing an ep nov before normalisation:  36.48291349411011
maxi score, test score, baseline:  -0.70666 0.687 0.687
maxi score, test score, baseline:  -0.70666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.070013999938965
maxi score, test score, baseline:  -0.70666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.70666 0.687 0.687
Printing some Q and Qe and total Qs values:  [[-0.088]
 [-0.078]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]] [[47.469]
 [50.698]
 [47.597]
 [47.597]
 [47.597]
 [47.597]
 [47.597]] [[0.403]
 [0.484]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]]
Printing some Q and Qe and total Qs values:  [[0.245]
 [0.355]
 [0.237]
 [0.241]
 [0.232]
 [0.237]
 [0.237]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.245]
 [0.355]
 [0.237]
 [0.241]
 [0.232]
 [0.237]
 [0.237]]
Printing some Q and Qe and total Qs values:  [[-0.061]
 [-0.051]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]] [[53.854]
 [39.701]
 [53.854]
 [53.854]
 [53.854]
 [53.854]
 [53.854]] [[0.543]
 [0.353]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]]
maxi score, test score, baseline:  -0.70666 0.687 0.687
maxi score, test score, baseline:  -0.70666 0.687 0.687
maxi score, test score, baseline:  -0.70666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.70666 0.687 0.687
Printing some Q and Qe and total Qs values:  [[-0.112]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]
 [-0.112]] [[30.101]
 [25.988]
 [25.988]
 [25.988]
 [25.988]
 [25.988]
 [25.988]] [[0.613]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]]
printing an ep nov before normalisation:  27.460977733564338
printing an ep nov before normalisation:  32.882394790649414
maxi score, test score, baseline:  -0.70666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.941049102741275
printing an ep nov before normalisation:  37.226104736328125
maxi score, test score, baseline:  -0.70666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.70666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.549466496066145
maxi score, test score, baseline:  -0.70666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.048]
 [-0.048]
 [-0.046]
 [-0.044]
 [-0.042]
 [-0.047]
 [-0.042]] [[43.445]
 [39.627]
 [42.303]
 [43.664]
 [43.466]
 [41.333]
 [43.37 ]] [[1.502]
 [1.236]
 [1.425]
 [1.522]
 [1.51 ]
 [1.357]
 [1.503]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.70666 0.687 0.687
printing an ep nov before normalisation:  0.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.70666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.70666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.70666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.041 0.714 0.02  0.041 0.102 0.02  0.061]
actor:  0 policy actor:  0  step number:  47 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.70386 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.849]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]] [[31.682]
 [39.332]
 [31.682]
 [31.682]
 [31.682]
 [31.682]
 [31.682]] [[0.6  ]
 [0.849]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.6  ]]
printing an ep nov before normalisation:  37.850680351257324
printing an ep nov before normalisation:  19.56957598849302
maxi score, test score, baseline:  -0.70386 0.687 0.687
actor:  0 policy actor:  0  step number:  50 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.154]
 [-0.122]
 [-0.214]
 [-0.14 ]
 [-0.148]
 [-0.21 ]
 [-0.144]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.154]
 [-0.122]
 [-0.214]
 [-0.14 ]
 [-0.148]
 [-0.21 ]
 [-0.144]]
maxi score, test score, baseline:  -0.7012866666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7012866666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.11 ]
 [-0.115]
 [-0.113]
 [-0.114]
 [-0.113]
 [-0.114]
 [-0.11 ]] [[39.493]
 [39.147]
 [40.967]
 [40.224]
 [41.287]
 [40.224]
 [39.66 ]] [[1.636]
 [1.602]
 [1.763]
 [1.697]
 [1.792]
 [1.697]
 [1.651]]
printing an ep nov before normalisation:  55.30443267296745
Printing some Q and Qe and total Qs values:  [[-0.116]
 [-0.098]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]] [[39.961]
 [42.786]
 [39.961]
 [39.961]
 [39.961]
 [39.961]
 [39.961]] [[0.121]
 [0.174]
 [0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]]
maxi score, test score, baseline:  -0.7012866666666666 0.687 0.687
maxi score, test score, baseline:  -0.7012866666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.92729080398456
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.755]
 [0.589]
 [0.576]
 [0.558]
 [0.691]
 [0.558]] [[24.152]
 [26.792]
 [28.033]
 [27.209]
 [30.118]
 [27.818]
 [30.118]] [[2.166]
 [2.512]
 [2.427]
 [2.36 ]
 [2.533]
 [2.515]
 [2.533]]
maxi score, test score, baseline:  -0.7012866666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.984133272438847
printing an ep nov before normalisation:  30.78506823514358
maxi score, test score, baseline:  -0.7012866666666666 0.687 0.687
maxi score, test score, baseline:  -0.7012866666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 40.344935871641326
actor:  1 policy actor:  1  step number:  60 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.7012866666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7012866666666666 0.687 0.687
maxi score, test score, baseline:  -0.7012866666666666 0.687 0.687
maxi score, test score, baseline:  -0.7012866666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.723411346188065
printing an ep nov before normalisation:  35.101586185684624
maxi score, test score, baseline:  -0.7012866666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7782819
maxi score, test score, baseline:  -0.7012866666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.189678084408015
actor:  1 policy actor:  1  step number:  48 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  32.12117287124844
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.96628527816432
actor:  1 policy actor:  1  step number:  50 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7012866666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7012866666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.377]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.357]] [[34.375]
 [37.844]
 [34.375]
 [34.375]
 [34.375]
 [34.375]
 [34.244]] [[0.558]
 [0.377]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.357]]
printing an ep nov before normalisation:  39.066312788294596
maxi score, test score, baseline:  -0.7012866666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7012866666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7012866666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.0560423083737
actions average: 
K:  1  action  0 :  tensor([0.3151, 0.0703, 0.1188, 0.1220, 0.1442, 0.1033, 0.1264],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0151, 0.9211, 0.0117, 0.0168, 0.0071, 0.0062, 0.0220],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1602, 0.0242, 0.3663, 0.1122, 0.0958, 0.1450, 0.0963],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2099, 0.1429, 0.1326, 0.1768, 0.1227, 0.0978, 0.1173],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1119, 0.0054, 0.0970, 0.0914, 0.5265, 0.0952, 0.0726],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0978, 0.0523, 0.1973, 0.0812, 0.0668, 0.4396, 0.0650],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1782, 0.0764, 0.1501, 0.1192, 0.0940, 0.1012, 0.2809],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7012866666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.84630584716797
printing an ep nov before normalisation:  44.85184736201721
maxi score, test score, baseline:  -0.7012866666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  64 total reward:  0.20666666666666633  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  39.051363620327464
maxi score, test score, baseline:  -0.7012866666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.288]
 [0.186]
 [0.122]
 [0.186]
 [0.096]
 [0.108]] [[35.658]
 [37.443]
 [36.714]
 [34.173]
 [36.714]
 [34.441]
 [34.649]] [[1.336]
 [1.633]
 [1.483]
 [1.255]
 [1.483]
 [1.246]
 [1.272]]
maxi score, test score, baseline:  -0.7012866666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7012866666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7012866666666666 0.687 0.687
actor:  0 policy actor:  0  step number:  37 total reward:  0.5466666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6981933333333333 0.687 0.687
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6981933333333333 0.687 0.687
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6981933333333333 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6981933333333333 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  0.24666666666666648  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  38.07931900024414
maxi score, test score, baseline:  -0.6981933333333333 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.545663833618164
maxi score, test score, baseline:  -0.6981933333333333 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6981933333333333 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.579079062718495
maxi score, test score, baseline:  -0.6981933333333333 0.687 0.687
printing an ep nov before normalisation:  37.88136487939077
maxi score, test score, baseline:  -0.6981933333333333 0.687 0.687
maxi score, test score, baseline:  -0.6981933333333333 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.19 ]
 [-0.172]
 [-0.178]
 [-0.182]
 [-0.189]
 [-0.194]
 [-0.191]] [[43.963]
 [40.928]
 [41.332]
 [41.455]
 [40.654]
 [40.152]
 [43.498]] [[0.65 ]
 [0.565]
 [0.572]
 [0.573]
 [0.539]
 [0.517]
 [0.632]]
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.413]
 [0.318]
 [0.318]
 [0.318]
 [0.318]
 [0.318]] [[38.233]
 [38.076]
 [38.233]
 [38.233]
 [38.233]
 [38.233]
 [38.233]] [[1.53 ]
 [1.616]
 [1.53 ]
 [1.53 ]
 [1.53 ]
 [1.53 ]
 [1.53 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6981933333333333 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  50 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.04 ]
 [ 0.281]
 [-0.005]
 [ 0.069]
 [-0.023]
 [-0.013]
 [ 0.018]] [[35.329]
 [38.842]
 [40.874]
 [34.337]
 [30.208]
 [35.102]
 [38.421]] [[0.837]
 [1.327]
 [1.139]
 [0.898]
 [0.607]
 [0.853]
 [1.043]]
maxi score, test score, baseline:  -0.6955666666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6955666666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.15779628726746
maxi score, test score, baseline:  -0.6955666666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6955666666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6955666666666667 0.687 0.687
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.137215615200354
printing an ep nov before normalisation:  28.504556746293467
maxi score, test score, baseline:  -0.6955666666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.884335769627995
maxi score, test score, baseline:  -0.6955666666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.61512797271226
actor:  0 policy actor:  0  step number:  44 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.087]
 [-0.059]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.087]
 [-0.059]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]]
maxi score, test score, baseline:  -0.6927266666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.52976576644051
printing an ep nov before normalisation:  42.42262246921324
siam score:  -0.8029689
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.369]
 [0.103]
 [0.104]
 [0.059]
 [0.036]
 [0.192]] [[34.07 ]
 [34.181]
 [34.98 ]
 [33.738]
 [32.635]
 [31.967]
 [33.527]] [[1.288]
 [1.444]
 [1.217]
 [1.156]
 [1.056]
 [1.   ]
 [1.233]]
siam score:  -0.803947
maxi score, test score, baseline:  -0.6927266666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.168]
 [-0.201]
 [-0.168]
 [-0.168]
 [-0.168]
 [-0.168]
 [-0.168]] [[ 0.   ]
 [50.934]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-1.436]
 [ 1.132]
 [-1.436]
 [-1.436]
 [-1.436]
 [-1.436]
 [-1.436]]
actor:  1 policy actor:  1  step number:  33 total reward:  0.5333333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6927266666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.82236375438127
maxi score, test score, baseline:  -0.6927266666666666 0.687 0.687
siam score:  -0.7996235
maxi score, test score, baseline:  -0.6927266666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6927266666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.174]
 [-0.174]
 [-0.174]
 [-0.174]
 [-0.174]
 [-0.174]
 [-0.174]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.174]
 [-0.174]
 [-0.174]
 [-0.174]
 [-0.174]
 [-0.174]
 [-0.174]]
maxi score, test score, baseline:  -0.6927266666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.597808237924625
printing an ep nov before normalisation:  36.03866578727826
maxi score, test score, baseline:  -0.6927266666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  52 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6903933333333333 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.23877255948689
maxi score, test score, baseline:  -0.6903933333333333 0.687 0.687
maxi score, test score, baseline:  -0.6903933333333333 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.072]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]] [[33.019]
 [33.019]
 [33.019]
 [33.019]
 [33.019]
 [33.019]
 [33.019]] [[1.243]
 [1.243]
 [1.243]
 [1.243]
 [1.243]
 [1.243]
 [1.243]]
siam score:  -0.79598755
actor:  1 policy actor:  1  step number:  63 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 33.07246527312709
maxi score, test score, baseline:  -0.6903933333333333 0.687 0.687
maxi score, test score, baseline:  -0.6903933333333333 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6903933333333333 0.687 0.687
maxi score, test score, baseline:  -0.6903933333333333 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.918689586575734
line 256 mcts: sample exp_bonus 41.19723588230667
printing an ep nov before normalisation:  28.078021093530936
maxi score, test score, baseline:  -0.6903933333333333 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.18666666666666643  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.593]
 [0.441]
 [0.504]
 [0.533]
 [0.437]
 [0.449]] [[35.513]
 [37.683]
 [32.759]
 [31.743]
 [35.41 ]
 [31.844]
 [31.278]] [[0.558]
 [0.593]
 [0.441]
 [0.504]
 [0.533]
 [0.437]
 [0.449]]
maxi score, test score, baseline:  -0.6903933333333333 0.687 0.687
maxi score, test score, baseline:  -0.6903933333333333 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.051270368983296066
printing an ep nov before normalisation:  35.73856284182445
maxi score, test score, baseline:  -0.6903933333333333 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.6903933333333333 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.920537234455615
maxi score, test score, baseline:  -0.6903933333333333 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.2897499082305
maxi score, test score, baseline:  -0.6903933333333333 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  51 total reward:  0.21333333333333304  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  37 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6879666666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.65203779994002
printing an ep nov before normalisation:  33.086576291897586
maxi score, test score, baseline:  -0.6879666666666666 0.687 0.687
maxi score, test score, baseline:  -0.6879666666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  66 total reward:  0.03333333333333266  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6879666666666666 0.687 0.687
printing an ep nov before normalisation:  40.94832420349121
maxi score, test score, baseline:  -0.6879666666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.43187834563359
printing an ep nov before normalisation:  29.69935586457692
printing an ep nov before normalisation:  33.93220629273425
maxi score, test score, baseline:  -0.6879666666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6879666666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.89279101619317
actor:  0 policy actor:  0  step number:  45 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6851133333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.204]
 [-0.225]
 [-0.21 ]
 [-0.212]
 [-0.211]
 [-0.207]
 [-0.211]] [[37.886]
 [38.445]
 [36.122]
 [37.419]
 [37.487]
 [37.712]
 [37.214]] [[0.641]
 [0.645]
 [0.556]
 [0.612]
 [0.617]
 [0.631]
 [0.604]]
siam score:  -0.7864458
UNIT TEST: sample policy line 217 mcts : [0.184 0.265 0.143 0.041 0.061 0.265 0.041]
line 256 mcts: sample exp_bonus 52.10857087777932
maxi score, test score, baseline:  -0.6851133333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6851133333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6851133333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  44.059291077783996
printing an ep nov before normalisation:  23.427274227142334
maxi score, test score, baseline:  -0.6851133333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  67 total reward:  0.10666666666666591  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6851133333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  42 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  48.240900422873224
siam score:  -0.7836171
maxi score, test score, baseline:  -0.6823266666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.535768866477795
printing an ep nov before normalisation:  35.09133506645761
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.03961706161499
maxi score, test score, baseline:  -0.6823266666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.116]
 [-0.109]
 [-0.079]
 [-0.123]
 [-0.115]
 [-0.108]
 [-0.115]] [[38.016]
 [39.793]
 [31.03 ]
 [42.352]
 [42.573]
 [40.699]
 [41.057]] [[0.502]
 [0.582]
 [0.25 ]
 [0.674]
 [0.691]
 [0.621]
 [0.628]]
maxi score, test score, baseline:  -0.6823266666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6823266666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.557]
 [0.427]
 [0.438]
 [0.434]
 [0.421]
 [0.437]] [[51.965]
 [44.216]
 [54.672]
 [54.461]
 [53.748]
 [52.045]
 [58.875]] [[0.436]
 [0.557]
 [0.427]
 [0.438]
 [0.434]
 [0.421]
 [0.437]]
maxi score, test score, baseline:  -0.6823266666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6823266666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.558]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]] [[32.259]
 [31.341]
 [32.259]
 [32.259]
 [32.259]
 [32.259]
 [32.259]] [[0.979]
 [1.182]
 [0.979]
 [0.979]
 [0.979]
 [0.979]
 [0.979]]
maxi score, test score, baseline:  -0.6823266666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6823266666666667 0.687 0.687
actor:  0 policy actor:  0  step number:  58 total reward:  0.12666666666666582  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  45.26309048168895
actor:  0 policy actor:  0  step number:  60 total reward:  0.19333333333333313  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.0001667051913045725
printing an ep nov before normalisation:  46.275890549505355
Printing some Q and Qe and total Qs values:  [[-0.111]
 [-0.099]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]] [[32.894]
 [39.925]
 [32.894]
 [32.894]
 [32.894]
 [32.894]
 [32.894]] [[0.633]
 [0.901]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.29919935760164
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]] [[32.944]
 [32.944]
 [32.944]
 [32.944]
 [32.944]
 [32.944]
 [32.944]] [[0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]]
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.047199091570505
printing an ep nov before normalisation:  31.137332253082263
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.2999999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  66 total reward:  0.019999999999998908  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.10790006941312
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.684903087061386
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
printing an ep nov before normalisation:  34.74513111114547
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
printing an ep nov before normalisation:  32.98626224793744
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.941777453643077
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  21.829283517395005
actor:  1 policy actor:  1  step number:  50 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.921432082894896
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]] [[49.111]
 [49.111]
 [49.111]
 [49.111]
 [49.111]
 [49.111]
 [49.111]] [[1.468]
 [1.468]
 [1.468]
 [1.468]
 [1.468]
 [1.468]
 [1.468]]
siam score:  -0.7878557
actor:  1 policy actor:  1  step number:  52 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  30.874142698147097
printing an ep nov before normalisation:  30.026123356326817
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.124]
 [-0.11 ]
 [-0.128]
 [-0.123]
 [-0.126]
 [-0.131]
 [-0.128]] [[39.279]
 [46.834]
 [41.878]
 [42.908]
 [43.317]
 [41.129]
 [43.947]] [[0.396]
 [0.637]
 [0.47 ]
 [0.506]
 [0.516]
 [0.444]
 [0.532]]
siam score:  -0.78555316
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.93038939859735
printing an ep nov before normalisation:  41.99875423512637
printing an ep nov before normalisation:  38.13756676193172
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.071]
 [-0.06 ]
 [-0.066]
 [-0.066]
 [-0.067]
 [-0.066]
 [-0.066]] [[35.574]
 [38.315]
 [34.235]
 [34.663]
 [34.012]
 [34.235]
 [34.235]] [[0.423]
 [0.506]
 [0.393]
 [0.404]
 [0.386]
 [0.393]
 [0.393]]
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]] [[40.659]
 [40.659]
 [40.659]
 [40.659]
 [40.659]
 [40.659]
 [40.659]] [[1.057]
 [1.057]
 [1.057]
 [1.057]
 [1.057]
 [1.057]
 [1.057]]
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.04188383407894
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0
actor:  1 policy actor:  1  step number:  74 total reward:  0.09999999999999976  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  59 total reward:  0.29333333333333256  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.696655426723392
actor:  1 policy actor:  1  step number:  61 total reward:  0.21333333333333304  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  24.295550816483363
printing an ep nov before normalisation:  37.71812282106975
printing an ep nov before normalisation:  42.324366296635766
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.096]
 [-0.095]
 [-0.116]
 [-0.11 ]
 [-0.116]
 [-0.116]
 [-0.116]] [[44.89 ]
 [45.49 ]
 [40.777]
 [45.519]
 [40.777]
 [40.777]
 [40.777]] [[0.772]
 [0.794]
 [0.609]
 [0.78 ]
 [0.609]
 [0.609]
 [0.609]]
printing an ep nov before normalisation:  42.8578023924927
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.495]
 [0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]] [[31.064]
 [37.244]
 [31.064]
 [31.064]
 [31.064]
 [31.064]
 [31.064]] [[1.421]
 [2.   ]
 [1.421]
 [1.421]
 [1.421]
 [1.421]
 [1.421]]
siam score:  -0.8079615
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  52.10250307418539
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  43.01416181093256
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.566720962524414
actor:  1 policy actor:  1  step number:  54 total reward:  0.5266666666666671  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
printing an ep nov before normalisation:  29.250938403142467
printing an ep nov before normalisation:  29.321281798343843
actions average: 
K:  1  action  0 :  tensor([0.2387, 0.0050, 0.1486, 0.1252, 0.1785, 0.1617, 0.1423],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0196, 0.9147, 0.0083, 0.0137, 0.0130, 0.0064, 0.0243],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1514, 0.2356, 0.2814, 0.0839, 0.0866, 0.0906, 0.0704],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1771, 0.0566, 0.1309, 0.2240, 0.1401, 0.1554, 0.1160],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1858, 0.0113, 0.1285, 0.1194, 0.2974, 0.1343, 0.1232],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1742, 0.0172, 0.1672, 0.1266, 0.1558, 0.2358, 0.1232],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1514, 0.1843, 0.1301, 0.1024, 0.1138, 0.1152, 0.2028],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  28.256537914276123
printing an ep nov before normalisation:  2.4826217668305617e-05
actor:  1 policy actor:  1  step number:  60 total reward:  0.11333333333333262  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  41.432513643510866
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.63835332107343
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]] [[25.022]
 [25.022]
 [25.022]
 [25.022]
 [25.022]
 [25.022]
 [25.022]] [[0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]]
printing an ep nov before normalisation:  34.605700477328696
actor:  1 policy actor:  1  step number:  57 total reward:  0.09333333333333282  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.140647471976244
actor:  1 policy actor:  1  step number:  60 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([0.4620, 0.0083, 0.0982, 0.1114, 0.1384, 0.0977, 0.0841],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0233, 0.8544, 0.0252, 0.0299, 0.0192, 0.0187, 0.0294],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1810, 0.1135, 0.2201, 0.1184, 0.1200, 0.1405, 0.1064],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1560, 0.0404, 0.0924, 0.3607, 0.1328, 0.1035, 0.1141],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2522, 0.0326, 0.1545, 0.1059, 0.1945, 0.1051, 0.1553],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1895, 0.0180, 0.2101, 0.0980, 0.1002, 0.2674, 0.1168],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1184, 0.2862, 0.0855, 0.1004, 0.0823, 0.0676, 0.2597],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  37.449233872538066
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.15133285522461
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.545]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]] [[36.866]
 [35.573]
 [36.866]
 [36.866]
 [36.866]
 [36.866]
 [36.866]] [[2.286]
 [2.321]
 [2.286]
 [2.286]
 [2.286]
 [2.286]
 [2.286]]
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  66 total reward:  0.11333333333333284  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.598]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]] [[36.237]
 [34.952]
 [36.237]
 [36.237]
 [36.237]
 [36.237]
 [36.237]] [[0.854]
 [0.873]
 [0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]]
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.7943481
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.76700300409694
line 256 mcts: sample exp_bonus 47.346702673256864
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.246477755450805
printing an ep nov before normalisation:  41.4184045791626
Printing some Q and Qe and total Qs values:  [[-0.055]
 [ 0.027]
 [-0.055]
 [-0.055]
 [-0.056]
 [-0.059]
 [-0.055]] [[27.702]
 [37.422]
 [27.702]
 [27.702]
 [39.838]
 [38.123]
 [27.702]] [[0.318]
 [0.764]
 [0.318]
 [0.318]
 [0.772]
 [0.705]
 [0.318]]
UNIT TEST: sample policy line 217 mcts : [0.041 0.531 0.02  0.306 0.02  0.061 0.02 ]
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.061]
 [ 0.061]
 [-0.062]
 [-0.069]
 [-0.069]
 [-0.062]
 [-0.062]] [[42.27 ]
 [52.696]
 [38.001]
 [40.542]
 [43.396]
 [38.001]
 [38.001]] [[0.512]
 [0.851]
 [0.422]
 [0.468]
 [0.528]
 [0.422]
 [0.422]]
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.2533333333333331  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  45.935542692347354
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.757]
 [0.649]
 [0.692]
 [0.692]
 [0.692]
 [0.692]] [[36.587]
 [35.978]
 [32.405]
 [36.587]
 [36.587]
 [36.587]
 [36.587]] [[1.692]
 [1.725]
 [1.426]
 [1.692]
 [1.692]
 [1.692]
 [1.692]]
maxi score, test score, baseline:  -0.6776866666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  23.84169578552246
printing an ep nov before normalisation:  19.118385528557596
actor:  1 policy actor:  1  step number:  49 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  1.0
siam score:  -0.7984813
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  41 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6748333333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.900768094112244
maxi score, test score, baseline:  -0.6748333333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.5252013429647
actor:  1 policy actor:  1  step number:  47 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  68 total reward:  0.03333333333333288  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6748333333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.08 ]
 [-0.086]
 [-0.084]
 [-0.081]
 [-0.094]
 [-0.082]
 [-0.094]] [[44.126]
 [35.594]
 [43.822]
 [46.203]
 [42.443]
 [45.848]
 [42.443]] [[1.18 ]
 [0.801]
 [1.163]
 [1.271]
 [1.093]
 [1.254]
 [1.093]]
maxi score, test score, baseline:  -0.6748333333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.334203284885426
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6748333333333334 0.687 0.687
maxi score, test score, baseline:  -0.6748333333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6748333333333334 0.687 0.687
printing an ep nov before normalisation:  31.551015586837906
actor:  1 policy actor:  1  step number:  67 total reward:  0.13333333333333297  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6748333333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6748333333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.59809780884673
maxi score, test score, baseline:  -0.6748333333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.16933646105254
printing an ep nov before normalisation:  31.71525478363037
printing an ep nov before normalisation:  41.780166910569044
actor:  1 policy actor:  1  step number:  56 total reward:  0.23333333333333306  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  29.816640083304346
printing an ep nov before normalisation:  20.83603360832427
Printing some Q and Qe and total Qs values:  [[-0.063]
 [-0.06 ]
 [-0.073]
 [-0.066]
 [-0.073]
 [-0.073]
 [-0.073]] [[37.218]
 [37.449]
 [31.955]
 [39.31 ]
 [31.955]
 [31.955]
 [31.955]] [[0.231]
 [0.236]
 [0.156]
 [0.253]
 [0.156]
 [0.156]
 [0.156]]
printing an ep nov before normalisation:  41.22509815784777
maxi score, test score, baseline:  -0.6748333333333334 0.687 0.687
printing an ep nov before normalisation:  35.32511072156291
printing an ep nov before normalisation:  10.190613269805908
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]] [[38.662]
 [38.662]
 [38.662]
 [38.662]
 [38.662]
 [38.662]
 [38.662]] [[2.351]
 [2.351]
 [2.351]
 [2.351]
 [2.351]
 [2.351]
 [2.351]]
maxi score, test score, baseline:  -0.6748333333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.18691699955571
maxi score, test score, baseline:  -0.6748333333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6748333333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.547589088675004
actor:  1 policy actor:  1  step number:  56 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 38.29848408908679
actor:  0 policy actor:  0  step number:  47 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6722733333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  64 total reward:  0.09999999999999931  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6700733333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6700733333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  44 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  34.32764784921823
maxi score, test score, baseline:  -0.6671533333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.69 ]
 [0.831]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.674]] [[27.615]
 [36.155]
 [27.615]
 [27.615]
 [27.615]
 [27.615]
 [29.563]] [[0.69 ]
 [0.831]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.674]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.046]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]] [[28.148]
 [28.148]
 [28.148]
 [28.148]
 [28.148]
 [28.148]
 [28.148]] [[1.121]
 [1.121]
 [1.121]
 [1.121]
 [1.121]
 [1.121]
 [1.121]]
maxi score, test score, baseline:  -0.6671533333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.81668454
actor:  0 policy actor:  0  step number:  54 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6645800000000001 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.18996123985127
maxi score, test score, baseline:  -0.6645800000000001 0.687 0.687
actor:  1 policy actor:  1  step number:  60 total reward:  0.206666666666666  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6645800000000001 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.083]
 [-0.074]
 [-0.086]
 [-0.086]
 [-0.086]
 [-0.086]
 [-0.086]] [[44.789]
 [35.303]
 [36.098]
 [36.098]
 [36.098]
 [36.098]
 [36.098]] [[0.727]
 [0.462]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]]
printing an ep nov before normalisation:  37.87656197972068
siam score:  -0.8208444
maxi score, test score, baseline:  -0.6645800000000001 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.081]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]] [[22.276]
 [22.342]
 [22.342]
 [22.342]
 [22.342]
 [22.342]
 [22.342]] [[0.648]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]]
printing an ep nov before normalisation:  52.20957419227324
maxi score, test score, baseline:  -0.6645800000000001 0.687 0.687
maxi score, test score, baseline:  -0.6645800000000001 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 33.5730130283444
printing an ep nov before normalisation:  40.44323970324821
maxi score, test score, baseline:  -0.6645800000000001 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.370163878547686
printing an ep nov before normalisation:  43.75675650096208
actor:  0 policy actor:  0  step number:  46 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6617666666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  4  action  0 :  tensor([0.3417, 0.0491, 0.1072, 0.1127, 0.1717, 0.0963, 0.1213],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0195, 0.8851, 0.0138, 0.0247, 0.0076, 0.0089, 0.0403],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2231, 0.0590, 0.2974, 0.1008, 0.0959, 0.1181, 0.1057],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1966, 0.2104, 0.1368, 0.1076, 0.0904, 0.1180, 0.1402],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1901, 0.0501, 0.1244, 0.1230, 0.2963, 0.1033, 0.1127],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0659, 0.0210, 0.2214, 0.2074, 0.0686, 0.3515, 0.0641],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1414, 0.1477, 0.1444, 0.1285, 0.0717, 0.1143, 0.2519],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  34.74864394095168
maxi score, test score, baseline:  -0.6617666666666668 0.687 0.687
maxi score, test score, baseline:  -0.6617666666666668 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6617666666666667 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  45 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6590733333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.471194179456866
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.674]
 [0.586]
 [0.577]
 [0.577]
 [0.651]
 [0.584]] [[38.152]
 [38.072]
 [43.938]
 [41.352]
 [41.212]
 [40.775]
 [41.11 ]] [[0.674]
 [0.674]
 [0.586]
 [0.577]
 [0.577]
 [0.651]
 [0.584]]
maxi score, test score, baseline:  -0.6590733333333334 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6590733333333334 0.687 0.687
Printing some Q and Qe and total Qs values:  [[-0.071]
 [-0.067]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]] [[36.361]
 [36.761]
 [36.361]
 [36.361]
 [36.361]
 [36.361]
 [36.361]] [[0.987]
 [1.015]
 [0.987]
 [0.987]
 [0.987]
 [0.987]
 [0.987]]
actor:  0 policy actor:  0  step number:  60 total reward:  0.04666666666666608  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.157]
 [-0.122]
 [-0.168]
 [-0.137]
 [-0.169]
 [-0.166]
 [-0.165]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.157]
 [-0.122]
 [-0.168]
 [-0.137]
 [-0.169]
 [-0.166]
 [-0.165]]
Printing some Q and Qe and total Qs values:  [[0.792]
 [0.811]
 [0.739]
 [0.73 ]
 [0.733]
 [0.736]
 [0.761]] [[27.257]
 [27.52 ]
 [31.272]
 [30.987]
 [30.205]
 [30.629]
 [30.708]] [[0.792]
 [0.811]
 [0.739]
 [0.73 ]
 [0.733]
 [0.736]
 [0.761]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.94690104984167
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.65698 0.687 0.687
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.148]
 [-0.152]
 [-0.153]
 [-0.15 ]
 [-0.15 ]
 [-0.15 ]
 [-0.153]] [[37.457]
 [33.56 ]
 [38.068]
 [36.844]
 [36.51 ]
 [36.451]
 [34.574]] [[0.143]
 [0.089]
 [0.146]
 [0.133]
 [0.129]
 [0.128]
 [0.1  ]]
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.29333333333333245  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.52432809976085
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8297038
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.33704428826606
actions average: 
K:  0  action  0 :  tensor([0.5077, 0.0127, 0.0923, 0.0703, 0.1257, 0.0868, 0.1044],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0032, 0.9631, 0.0046, 0.0050, 0.0013, 0.0019, 0.0209],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1844, 0.0170, 0.1235, 0.1481, 0.1882, 0.2107, 0.1282],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1717, 0.0070, 0.1090, 0.2942, 0.1366, 0.1320, 0.1495],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2167, 0.0058, 0.1418, 0.1464, 0.1637, 0.1825, 0.1431],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1473, 0.0157, 0.1990, 0.1022, 0.1256, 0.3060, 0.1041],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1647, 0.2015, 0.1548, 0.1087, 0.1172, 0.1151, 0.1380],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.65698 0.687 0.687
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.65698 0.687 0.687
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.65698 0.687 0.687
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.109]
 [-0.072]
 [-0.106]
 [-0.106]
 [-0.106]
 [-0.106]
 [-0.106]] [[49.349]
 [52.404]
 [47.364]
 [47.364]
 [47.364]
 [47.364]
 [47.364]] [[0.339]
 [0.434]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]]
maxi score, test score, baseline:  -0.65698 0.687 0.687
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8214558
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 26.727815778485287
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.82347072080959
printing an ep nov before normalisation:  31.97085344805385
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.64442772906972
Printing some Q and Qe and total Qs values:  [[-0.142]
 [-0.155]
 [-0.142]
 [-0.159]
 [-0.142]
 [-0.142]
 [-0.142]] [[ 0.   ]
 [34.155]
 [ 0.   ]
 [12.075]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.38 ]
 [ 0.28 ]
 [-0.38 ]
 [-0.159]
 [-0.38 ]
 [-0.38 ]
 [-0.38 ]]
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.845269829927524
Printing some Q and Qe and total Qs values:  [[-0.165]
 [-0.097]
 [-0.221]
 [-0.157]
 [-0.154]
 [-0.207]
 [-0.157]] [[23.454]
 [32.982]
 [18.531]
 [25.192]
 [40.67 ]
 [20.193]
 [27.662]] [[ 0.288]
 [ 0.903]
 [-0.052]
 [ 0.395]
 [ 1.289]
 [ 0.058]
 [ 0.538]]
actor:  1 policy actor:  1  step number:  65 total reward:  0.10666666666666635  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.458]
 [0.335]
 [0.335]
 [0.335]
 [0.335]
 [0.335]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.335]
 [0.458]
 [0.335]
 [0.335]
 [0.335]
 [0.335]
 [0.335]]
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.07666635513306
printing an ep nov before normalisation:  39.719796650874635
printing an ep nov before normalisation:  40.4975674865956
printing an ep nov before normalisation:  33.11752671205785
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.08]
 [-0.08]
 [-0.08]
 [-0.08]
 [-0.08]
 [-0.08]
 [-0.08]] [[38.522]
 [38.522]
 [38.522]
 [38.522]
 [38.522]
 [38.522]
 [38.522]] [[0.86]
 [0.86]
 [0.86]
 [0.86]
 [0.86]
 [0.86]
 [0.86]]
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.65698 0.687 0.687
printing an ep nov before normalisation:  37.01443892502075
printing an ep nov before normalisation:  27.075295448303223
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.48560905456543
printing an ep nov before normalisation:  35.21719593889067
printing an ep nov before normalisation:  44.249185687343605
printing an ep nov before normalisation:  29.52259042113812
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.82268977165222
maxi score, test score, baseline:  -0.65698 0.687 0.687
actor:  1 policy actor:  1  step number:  60 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.65698 0.687 0.687
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.673418045043945
maxi score, test score, baseline:  -0.65698 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.159]
 [-0.122]
 [-0.158]
 [-0.158]
 [-0.158]
 [-0.158]
 [-0.158]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.159]
 [-0.122]
 [-0.158]
 [-0.158]
 [-0.158]
 [-0.158]
 [-0.158]]
Starting evaluation
printing an ep nov before normalisation:  13.715069961225401
maxi score, test score, baseline:  -0.65698 0.687 0.687
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.569]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.372]
 [0.569]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]]
actions average: 
K:  3  action  0 :  tensor([0.3601, 0.0720, 0.1149, 0.1180, 0.1442, 0.0848, 0.1060],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0156, 0.9225, 0.0104, 0.0183, 0.0063, 0.0073, 0.0195],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2463, 0.0442, 0.1354, 0.1558, 0.1569, 0.1223, 0.1392],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2203, 0.0543, 0.2162, 0.1132, 0.1317, 0.1264, 0.1378],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2110, 0.0261, 0.0915, 0.1053, 0.3992, 0.0726, 0.0942],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1926, 0.0299, 0.1654, 0.1251, 0.1324, 0.2191, 0.1355],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2658, 0.1005, 0.1250, 0.1275, 0.1596, 0.1144, 0.1072],
       grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  35.52281110754507
printing an ep nov before normalisation:  34.05667504404283
actor:  0 policy actor:  0  step number:  43 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  50.60410843397515
printing an ep nov before normalisation:  35.54136636958215
maxi score, test score, baseline:  -0.6542066666666666 0.687 0.687
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.56531143517846
printing an ep nov before normalisation:  31.330188455838353
printing an ep nov before normalisation:  1.804243767070517
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.58626 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.58626 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.58626 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.472]
 [0.39 ]
 [0.247]
 [0.39 ]
 [0.39 ]
 [0.39 ]] [[36.393]
 [41.591]
 [36.393]
 [39.366]
 [36.393]
 [36.393]
 [36.393]] [[1.239]
 [1.595]
 [1.239]
 [1.253]
 [1.239]
 [1.239]
 [1.239]]
printing an ep nov before normalisation:  40.13868250786084
maxi score, test score, baseline:  -0.58626 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.58626 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.58626 0.6986666666666668 0.6986666666666668
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.611]
 [0.575]
 [0.351]
 [0.317]
 [0.513]
 [0.37 ]] [[28.455]
 [32.644]
 [32.574]
 [27.679]
 [27.481]
 [29.143]
 [27.749]] [[0.987]
 [1.384]
 [1.346]
 [0.998]
 [0.959]
 [1.197]
 [1.019]]
printing an ep nov before normalisation:  43.723625760864635
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.816885973619357
printing an ep nov before normalisation:  21.588142624793747
siam score:  -0.8154036
printing an ep nov before normalisation:  20.211016832120976
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.58626 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.2166428565979
maxi score, test score, baseline:  -0.58626 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.977077827632165
using explorer policy with actor:  1
printing an ep nov before normalisation:  22.97964096069336
maxi score, test score, baseline:  -0.58626 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.55366874254556
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.3842, 0.0871, 0.0874, 0.1074, 0.1858, 0.0719, 0.0761],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0374, 0.8413, 0.0250, 0.0320, 0.0162, 0.0119, 0.0364],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1488, 0.0110, 0.3339, 0.1541, 0.1337, 0.1320, 0.0866],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1737, 0.0152, 0.1924, 0.1918, 0.1027, 0.2120, 0.1122],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1585, 0.0669, 0.1242, 0.1354, 0.3142, 0.1166, 0.0841],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1349, 0.0242, 0.1480, 0.1214, 0.0952, 0.4004, 0.0760],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1676, 0.1692, 0.1637, 0.1389, 0.1279, 0.1501, 0.0825],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.58626 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.809198
maxi score, test score, baseline:  -0.58626 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.58626 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.58626 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.98425401936587
Printing some Q and Qe and total Qs values:  [[-0.053]
 [-0.027]
 [-0.053]
 [-0.053]
 [-0.053]
 [-0.053]
 [-0.053]] [[35.633]
 [36.989]
 [35.633]
 [35.633]
 [35.633]
 [35.633]
 [35.633]] [[0.141]
 [0.181]
 [0.141]
 [0.141]
 [0.141]
 [0.141]
 [0.141]]
maxi score, test score, baseline:  -0.58626 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.108]
 [ 0.245]
 [-0.002]
 [ 0.024]
 [-0.009]
 [ 0.176]
 [ 0.088]] [[35.422]
 [37.361]
 [37.402]
 [37.254]
 [37.933]
 [38.743]
 [36.897]] [[0.3  ]
 [0.459]
 [0.213]
 [0.238]
 [0.212]
 [0.407]
 [0.298]]
Printing some Q and Qe and total Qs values:  [[-0.012]
 [ 0.261]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.012]] [[41.171]
 [40.271]
 [41.171]
 [41.171]
 [41.171]
 [41.171]
 [41.171]] [[0.848]
 [1.095]
 [0.848]
 [0.848]
 [0.848]
 [0.848]
 [0.848]]
maxi score, test score, baseline:  -0.58626 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.2199999999999993  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  1  action  0 :  tensor([0.2982, 0.0114, 0.1136, 0.1215, 0.2194, 0.1314, 0.1045],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0073, 0.9370, 0.0083, 0.0163, 0.0059, 0.0062, 0.0190],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1511, 0.0564, 0.3241, 0.1361, 0.1133, 0.1123, 0.1067],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1249, 0.1096, 0.1200, 0.2886, 0.1066, 0.1155, 0.1347],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1662, 0.0989, 0.1316, 0.1403, 0.2194, 0.1282, 0.1154],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1148, 0.0347, 0.1828, 0.1020, 0.0898, 0.3807, 0.0952],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1427, 0.0778, 0.1691, 0.1792, 0.1086, 0.1262, 0.1964],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]] [[54.272]
 [54.272]
 [54.272]
 [54.272]
 [54.272]
 [54.272]
 [54.272]] [[1.448]
 [1.448]
 [1.448]
 [1.448]
 [1.448]
 [1.448]
 [1.448]]
printing an ep nov before normalisation:  41.96028589244354
maxi score, test score, baseline:  -0.58626 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.58626 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.58626 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7964103
actor:  0 policy actor:  0  step number:  46 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  23.73526096343994
maxi score, test score, baseline:  -0.5836066666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.00089235163752
siam score:  -0.79776835
actor:  1 policy actor:  1  step number:  55 total reward:  0.2533333333333332  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5836066666666667 0.6986666666666668 0.6986666666666668
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.693]
 [0.43 ]
 [0.43 ]
 [0.201]
 [0.43 ]
 [0.43 ]] [[43.945]
 [32.775]
 [43.945]
 [43.945]
 [46.597]
 [43.945]
 [43.945]] [[0.924]
 [0.906]
 [0.924]
 [0.924]
 [0.761]
 [0.924]
 [0.924]]
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.626]
 [0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]] [[41.21 ]
 [41.198]
 [41.21 ]
 [41.21 ]
 [41.21 ]
 [41.21 ]
 [41.21 ]] [[0.772]
 [0.92 ]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]]
printing an ep nov before normalisation:  40.217113661394585
maxi score, test score, baseline:  -0.5836066666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5836066666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  75 total reward:  0.09333333333333294  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  28.513742258575448
UNIT TEST: sample policy line 217 mcts : [0.122 0.347 0.061 0.224 0.041 0.041 0.163]
printing an ep nov before normalisation:  18.661173352220022
siam score:  -0.7904795
maxi score, test score, baseline:  -0.5836066666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5836066666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5836066666666667 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.5836066666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7922658
printing an ep nov before normalisation:  42.58414861067101
maxi score, test score, baseline:  -0.5836066666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.73097968671338
maxi score, test score, baseline:  -0.5836066666666667 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  41.48239145490459
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.342]
 [0.329]
 [0.328]
 [0.329]
 [0.328]
 [0.325]] [[29.103]
 [41.08 ]
 [25.445]
 [25.72 ]
 [25.239]
 [25.083]
 [25.269]] [[0.33 ]
 [0.342]
 [0.329]
 [0.328]
 [0.329]
 [0.328]
 [0.325]]
maxi score, test score, baseline:  -0.5836066666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5836066666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5836066666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.13999999999999946  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5836066666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.161]
 [-0.16 ]
 [-0.168]
 [-0.164]
 [-0.167]
 [-0.164]
 [-0.168]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.161]
 [-0.16 ]
 [-0.168]
 [-0.164]
 [-0.167]
 [-0.164]
 [-0.168]]
actions average: 
K:  2  action  0 :  tensor([0.3633, 0.0395, 0.1085, 0.1275, 0.1565, 0.1183, 0.0864],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0160, 0.8545, 0.0327, 0.0175, 0.0083, 0.0223, 0.0486],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1960, 0.0372, 0.2215, 0.1134, 0.1375, 0.1963, 0.0980],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2162, 0.0597, 0.0881, 0.3353, 0.1087, 0.0822, 0.1097],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1967, 0.0089, 0.0828, 0.1110, 0.4448, 0.0914, 0.0644],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1235, 0.0233, 0.1631, 0.1317, 0.1212, 0.3278, 0.1093],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1505, 0.0897, 0.1348, 0.1470, 0.1406, 0.1815, 0.1559],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5836066666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5836066666666667 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.5836066666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  39 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.58078 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.79778254
actions average: 
K:  2  action  0 :  tensor([0.2629, 0.0499, 0.1542, 0.1418, 0.1248, 0.1437, 0.1226],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0157, 0.8542, 0.0153, 0.0174, 0.0086, 0.0150, 0.0736],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1301, 0.1736, 0.2730, 0.1043, 0.0940, 0.1107, 0.1143],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1708, 0.2197, 0.1107, 0.1609, 0.0988, 0.1110, 0.1282],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2146, 0.0958, 0.0832, 0.0885, 0.3533, 0.0917, 0.0729],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2053, 0.0455, 0.1354, 0.1342, 0.1231, 0.2382, 0.1182],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1247, 0.2081, 0.0837, 0.1467, 0.0960, 0.0850, 0.2558],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.58078 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.061 0.837 0.02  0.02  0.02  0.02  0.02 ]
printing an ep nov before normalisation:  31.432380677375413
maxi score, test score, baseline:  -0.58078 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.78389085576168
printing an ep nov before normalisation:  32.60662085945218
maxi score, test score, baseline:  -0.58078 0.6986666666666668 0.6986666666666668
Printing some Q and Qe and total Qs values:  [[-0.114]
 [-0.095]
 [-0.114]
 [-0.114]
 [-0.114]
 [-0.114]
 [-0.114]] [[22.698]
 [33.342]
 [22.698]
 [22.698]
 [22.698]
 [22.698]
 [22.698]] [[0.036]
 [0.194]
 [0.036]
 [0.036]
 [0.036]
 [0.036]
 [0.036]]
actor:  1 policy actor:  1  step number:  74 total reward:  0.09999999999999964  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  11.931793689727783
actor:  1 policy actor:  1  step number:  49 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.58078 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.58078 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  13.269839116439178
printing an ep nov before normalisation:  0.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.182]
 [0.182]
 [0.182]
 [0.182]
 [0.182]
 [0.182]] [[33.465]
 [33.465]
 [33.465]
 [33.465]
 [33.465]
 [33.465]
 [33.465]] [[0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]]
maxi score, test score, baseline:  -0.58078 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.58078 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.58078 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.313714529081786
maxi score, test score, baseline:  -0.58078 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.58078 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 24.978334925665582
maxi score, test score, baseline:  -0.58078 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.58078 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.58078 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.1982005883894
actions average: 
K:  2  action  0 :  tensor([0.3739, 0.0858, 0.0961, 0.1132, 0.1490, 0.1036, 0.0785],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0054, 0.9574, 0.0046, 0.0104, 0.0035, 0.0033, 0.0154],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1299, 0.0400, 0.3067, 0.1055, 0.1086, 0.1863, 0.1231],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1163, 0.0568, 0.1124, 0.3478, 0.1348, 0.1194, 0.1125],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2269, 0.0554, 0.1075, 0.1184, 0.2854, 0.1074, 0.0990],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1522, 0.0614, 0.1673, 0.1122, 0.1175, 0.2643, 0.1251],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1875, 0.1097, 0.0997, 0.1403, 0.0993, 0.0909, 0.2726],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.58078 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.58078 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  69 total reward:  0.06666666666666565  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.58078 0.6986666666666668 0.6986666666666668
Printing some Q and Qe and total Qs values:  [[-0.016]
 [ 0.042]
 [-0.016]
 [-0.037]
 [-0.016]
 [-0.042]
 [-0.016]] [[44.846]
 [46.5  ]
 [44.846]
 [47.507]
 [44.846]
 [47.608]
 [44.846]] [[0.789]
 [0.896]
 [0.789]
 [0.847]
 [0.789]
 [0.845]
 [0.789]]
actor:  1 policy actor:  1  step number:  64 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.58078 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.835492362662947
printing an ep nov before normalisation:  28.086083168620004
printing an ep nov before normalisation:  39.75249236723051
printing an ep nov before normalisation:  30.532724857330322
actor:  0 policy actor:  0  step number:  37 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.57782 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.57782 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.81108857283777
actor:  1 policy actor:  1  step number:  74 total reward:  0.05999999999999894  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.57782 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.117]
 [-0.099]
 [-0.117]
 [-0.117]
 [-0.117]
 [-0.117]
 [-0.117]] [[40.564]
 [40.795]
 [40.564]
 [40.564]
 [40.564]
 [40.564]
 [40.564]] [[1.063]
 [1.096]
 [1.063]
 [1.063]
 [1.063]
 [1.063]
 [1.063]]
printing an ep nov before normalisation:  41.449448113866296
printing an ep nov before normalisation:  51.529007906767795
printing an ep nov before normalisation:  34.504637902587085
printing an ep nov before normalisation:  33.052684107299044
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.57782 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.635571726237984
printing an ep nov before normalisation:  48.113149052581235
printing an ep nov before normalisation:  33.47746856043573
maxi score, test score, baseline:  -0.57782 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.752206325531006
maxi score, test score, baseline:  -0.57782 0.6986666666666668 0.6986666666666668
siam score:  -0.8016966
maxi score, test score, baseline:  -0.57782 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.57782 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.57782 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.7998523
Printing some Q and Qe and total Qs values:  [[0.127]
 [0.359]
 [0.146]
 [0.126]
 [0.126]
 [0.127]
 [0.346]] [[34.427]
 [39.528]
 [33.946]
 [32.822]
 [33.611]
 [33.602]
 [38.643]] [[0.279]
 [0.553]
 [0.294]
 [0.263]
 [0.27 ]
 [0.271]
 [0.533]]
maxi score, test score, baseline:  -0.57782 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  0.11999999999999955  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  73 total reward:  0.11999999999999889  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.57782 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8005208
maxi score, test score, baseline:  -0.57782 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.586799148076594
printing an ep nov before normalisation:  40.10695565778533
maxi score, test score, baseline:  -0.57782 0.6986666666666668 0.6986666666666668
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.57782 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.80509293
actor:  1 policy actor:  1  step number:  53 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.57782 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  62 total reward:  0.13999999999999924  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.57554 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.23729690525522
maxi score, test score, baseline:  -0.57554 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  37.26934357794009
actor:  1 policy actor:  1  step number:  62 total reward:  0.13999999999999935  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.57554 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.80858153
printing an ep nov before normalisation:  15.625619088034188
actions average: 
K:  2  action  0 :  tensor([0.4566, 0.0541, 0.1132, 0.0965, 0.1067, 0.0848, 0.0882],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0178, 0.8993, 0.0170, 0.0168, 0.0111, 0.0131, 0.0249],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1246, 0.0281, 0.3013, 0.1290, 0.1147, 0.1934, 0.1090],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1334, 0.1463, 0.0692, 0.3329, 0.1220, 0.0807, 0.1155],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2295, 0.0685, 0.0871, 0.1003, 0.3570, 0.0821, 0.0755],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1351, 0.0125, 0.2080, 0.1497, 0.1496, 0.2099, 0.1352],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1682, 0.1016, 0.1134, 0.1546, 0.1244, 0.1065, 0.2314],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  61 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]]
maxi score, test score, baseline:  -0.57306 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.57306 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.99899797503162
printing an ep nov before normalisation:  27.187966644103312
actor:  0 policy actor:  0  step number:  49 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5705266666666666 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.8738020843627
printing an ep nov before normalisation:  55.53958803388541
maxi score, test score, baseline:  -0.5705266666666666 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.5705266666666666 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5705266666666666 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  53 total reward:  0.17333333333333267  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  31.32994571799417
maxi score, test score, baseline:  -0.56818 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.89428013618776
printing an ep nov before normalisation:  37.395762753174694
maxi score, test score, baseline:  -0.56818 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8129469
Printing some Q and Qe and total Qs values:  [[-0.038]
 [-0.041]
 [-0.043]
 [-0.041]
 [-0.042]
 [-0.058]
 [-0.045]] [[34.153]
 [34.744]
 [32.006]
 [31.829]
 [31.674]
 [33.724]
 [32.102]] [[0.457]
 [0.469]
 [0.395]
 [0.392]
 [0.387]
 [0.426]
 [0.395]]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  50.00098010526933
printing an ep nov before normalisation:  41.76569220907674
actor:  0 policy actor:  0  step number:  52 total reward:  0.2733333333333331  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  46.49694337780053
printing an ep nov before normalisation:  36.84451109034843
printing an ep nov before normalisation:  51.661795856840214
maxi score, test score, baseline:  -0.5656333333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5656333333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  78 total reward:  0.006666666666665377  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  57 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5656333333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.294]
 [0.288]
 [0.294]
 [0.294]
 [0.294]
 [0.335]] [[25.214]
 [25.688]
 [25.067]
 [25.688]
 [25.688]
 [25.688]
 [24.576]] [[0.39 ]
 [0.294]
 [0.288]
 [0.294]
 [0.294]
 [0.294]
 [0.335]]
maxi score, test score, baseline:  -0.5656333333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5656333333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5656333333333333 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  29.618852738078864
actor:  1 policy actor:  1  step number:  56 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  29.449731762933176
maxi score, test score, baseline:  -0.5656333333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5656333333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5656333333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5656333333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.343213081359863
actor:  0 policy actor:  0  step number:  35 total reward:  0.5333333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5625666666666668 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.3840, 0.0151, 0.1345, 0.1077, 0.1209, 0.1070, 0.1308],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0041, 0.9566, 0.0041, 0.0153, 0.0026, 0.0029, 0.0143],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2275, 0.0323, 0.2243, 0.1233, 0.1170, 0.1496, 0.1259],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1702, 0.0180, 0.1264, 0.3400, 0.1190, 0.1009, 0.1254],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1813, 0.0109, 0.1119, 0.1391, 0.3299, 0.1026, 0.1243],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2124, 0.0039, 0.1495, 0.1391, 0.1612, 0.1510, 0.1829],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1226, 0.1167, 0.0966, 0.2545, 0.0816, 0.0761, 0.2520],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5625666666666668 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5625666666666668 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.5625666666666668 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.66807989391765
maxi score, test score, baseline:  -0.5625666666666668 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.00523990712902
actor:  1 policy actor:  1  step number:  42 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.629245884754624
maxi score, test score, baseline:  -0.5625666666666668 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5625666666666668 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  69 total reward:  0.1733333333333329  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  3  action  0 :  tensor([0.2721, 0.1217, 0.1133, 0.1129, 0.1643, 0.0948, 0.1208],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0077, 0.9139, 0.0178, 0.0140, 0.0059, 0.0114, 0.0293],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1349, 0.0874, 0.3038, 0.1182, 0.1128, 0.1115, 0.1314],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1581, 0.0793, 0.1166, 0.2432, 0.1207, 0.1306, 0.1515],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1929, 0.1622, 0.0763, 0.0832, 0.3282, 0.0735, 0.0837],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1472, 0.0543, 0.1368, 0.1515, 0.1388, 0.2332, 0.1382],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1472, 0.2495, 0.1016, 0.1328, 0.0909, 0.0937, 0.1842],
       grad_fn=<DivBackward0>)
UNIT TEST: sample policy line 217 mcts : [0.367 0.143 0.061 0.102 0.061 0.122 0.143]
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.569]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]] [[24.763]
 [29.059]
 [24.763]
 [24.763]
 [24.763]
 [24.763]
 [24.763]] [[1.053]
 [1.395]
 [1.053]
 [1.053]
 [1.053]
 [1.053]
 [1.053]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.08666666666666611  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5625666666666668 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5625666666666668 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.711540115386086
printing an ep nov before normalisation:  39.743103896666405
maxi score, test score, baseline:  -0.5625666666666668 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.821]
 [0.915]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]] [[45.33 ]
 [43.173]
 [45.33 ]
 [45.33 ]
 [45.33 ]
 [45.33 ]
 [45.33 ]] [[0.821]
 [0.915]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]]
actor:  0 policy actor:  0  step number:  55 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  23.05601648168922
printing an ep nov before normalisation:  29.1222987995097
actor:  0 policy actor:  0  step number:  40 total reward:  0.48666666666666647  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5569533333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5569533333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.70428072494839
maxi score, test score, baseline:  -0.5569533333333333 0.6986666666666668 0.6986666666666668
Printing some Q and Qe and total Qs values:  [[-0.052]
 [-0.048]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]] [[27.796]
 [29.555]
 [27.796]
 [27.796]
 [27.796]
 [27.796]
 [27.796]] [[1.264]
 [1.46 ]
 [1.264]
 [1.264]
 [1.264]
 [1.264]
 [1.264]]
maxi score, test score, baseline:  -0.5569533333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.27699763553652
printing an ep nov before normalisation:  38.63067860277094
maxi score, test score, baseline:  -0.5569533333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5569533333333333 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  26.753180428494797
maxi score, test score, baseline:  -0.5569533333333333 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  40.306574075086786
actor:  1 policy actor:  1  step number:  69 total reward:  0.0933333333333326  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5569533333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 33.129338710825074
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.705]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]] [[27.748]
 [28.389]
 [27.748]
 [27.748]
 [27.748]
 [27.748]
 [27.748]] [[0.871]
 [0.964]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]]
printing an ep nov before normalisation:  15.291470289230347
maxi score, test score, baseline:  -0.5569533333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.361]
 [0.376]
 [0.369]
 [0.368]
 [0.381]
 [0.411]] [[12.943]
 [25.21 ]
 [12.816]
 [13.376]
 [13.768]
 [13.138]
 [14.371]] [[0.363]
 [0.361]
 [0.376]
 [0.369]
 [0.368]
 [0.381]
 [0.411]]
actor:  0 policy actor:  0  step number:  52 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.05189810889754
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.238]
 [ 0.27 ]
 [ 0.173]
 [ 0.118]
 [-0.011]
 [ 0.07 ]
 [ 0.139]] [[34.994]
 [37.979]
 [37.736]
 [36.411]
 [36.186]
 [37.442]
 [38.104]] [[0.912]
 [1.066]
 [0.959]
 [0.85 ]
 [0.712]
 [0.844]
 [0.94 ]]
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
Printing some Q and Qe and total Qs values:  [[-0.025]
 [ 0.171]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]
 [-0.025]] [[37.399]
 [44.168]
 [37.399]
 [37.399]
 [37.399]
 [37.399]
 [37.399]] [[0.712]
 [1.083]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]]
printing an ep nov before normalisation:  40.86751335174924
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
siam score:  -0.8227699
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8235132
printing an ep nov before normalisation:  33.9502795094215
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.072]
 [-0.078]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]
 [-0.072]] [[39.917]
 [42.474]
 [39.917]
 [39.917]
 [39.917]
 [39.917]
 [39.917]] [[1.66 ]
 [1.854]
 [1.66 ]
 [1.66 ]
 [1.66 ]
 [1.66 ]
 [1.66 ]]
printing an ep nov before normalisation:  42.56558723674876
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.73118782043457
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
Printing some Q and Qe and total Qs values:  [[-0.083]
 [-0.105]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.098]
 [-0.098]] [[31.777]
 [33.114]
 [32.533]
 [32.533]
 [32.533]
 [32.533]
 [32.533]] [[0.847]
 [0.893]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]]
siam score:  -0.82109916
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [0.351]
 [0.1  ]
 [0.121]
 [0.021]
 [0.1  ]
 [0.1  ]] [[36.025]
 [34.365]
 [36.025]
 [30.861]
 [31.114]
 [36.025]
 [36.025]] [[1.241]
 [1.411]
 [1.241]
 [1.01 ]
 [0.923]
 [1.241]
 [1.241]]
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 41.87987210298825
printing an ep nov before normalisation:  42.684162408983894
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.145118072162084
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.98229444733047
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.425025021976104
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.044]
 [-0.006]
 [-0.044]
 [-0.044]
 [-0.044]
 [-0.044]
 [-0.044]] [[31.136]
 [37.737]
 [31.136]
 [31.136]
 [31.136]
 [31.136]
 [31.136]] [[0.987]
 [1.465]
 [0.987]
 [0.987]
 [0.987]
 [0.987]
 [0.987]]
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.537]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]] [[36.004]
 [35.538]
 [36.004]
 [36.004]
 [36.004]
 [36.004]
 [36.004]] [[1.901]
 [2.012]
 [1.901]
 [1.901]
 [1.901]
 [1.901]
 [1.901]]
printing an ep nov before normalisation:  23.024871534547803
Printing some Q and Qe and total Qs values:  [[-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]
 [-0.074]] [[22.044]
 [16.155]
 [16.155]
 [16.155]
 [16.155]
 [16.155]
 [16.155]] [[0.663]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]]
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  35.86045304129113
printing an ep nov before normalisation:  0.04058633283932522
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  2.5603680667041813
Printing some Q and Qe and total Qs values:  [[0.866]
 [0.922]
 [0.842]
 [0.766]
 [0.842]
 [0.842]
 [0.748]] [[35.534]
 [29.378]
 [30.805]
 [37.244]
 [30.805]
 [30.805]
 [34.557]] [[0.866]
 [0.922]
 [0.842]
 [0.766]
 [0.842]
 [0.842]
 [0.748]]
maxi score, test score, baseline:  -0.5545133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  46 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5519666666666667 0.6986666666666668 0.6986666666666668
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5519666666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.82316405
actor:  1 policy actor:  1  step number:  56 total reward:  0.24666666666666637  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.658]
 [0.455]
 [0.459]
 [0.455]
 [0.455]
 [0.457]] [[33.059]
 [28.852]
 [33.024]
 [33.627]
 [33.024]
 [33.024]
 [33.536]] [[0.462]
 [0.658]
 [0.455]
 [0.459]
 [0.455]
 [0.455]
 [0.457]]
printing an ep nov before normalisation:  23.20484752654816
maxi score, test score, baseline:  -0.5519666666666666 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.101]
 [-0.021]
 [-0.083]
 [-0.084]
 [-0.076]
 [-0.097]
 [-0.09 ]] [[34.158]
 [34.6  ]
 [36.857]
 [41.654]
 [39.179]
 [42.038]
 [36.305]] [[0.098]
 [0.183]
 [0.147]
 [0.2  ]
 [0.18 ]
 [0.192]
 [0.133]]
actor:  1 policy actor:  1  step number:  63 total reward:  0.18666666666666598  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5519666666666666 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5519666666666666 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  58 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.38160705596156
maxi score, test score, baseline:  -0.5495800000000001 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.4040, 0.0085, 0.1120, 0.1247, 0.1347, 0.1143, 0.1018],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0057, 0.9487, 0.0157, 0.0075, 0.0030, 0.0057, 0.0136],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1741, 0.0040, 0.2431, 0.1416, 0.1415, 0.1631, 0.1327],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1566, 0.0140, 0.1167, 0.3173, 0.1338, 0.1391, 0.1226],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1710, 0.0072, 0.0925, 0.1029, 0.4417, 0.0957, 0.0889],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1612, 0.0035, 0.2406, 0.1424, 0.1322, 0.1951, 0.1250],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1825, 0.0222, 0.1598, 0.1516, 0.1369, 0.1505, 0.1965],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5495800000000001 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  40 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5467133333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.82373756
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  44.88191336708429
maxi score, test score, baseline:  -0.5467133333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.261026513634974
maxi score, test score, baseline:  -0.5467133333333334 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  26.30945430979814
printing an ep nov before normalisation:  27.31547544224792
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.024]
 [-0.026]
 [-0.036]
 [-0.026]
 [-0.036]
 [-0.036]] [[26.397]
 [26.171]
 [25.591]
 [24.73 ]
 [25.704]
 [24.754]
 [24.793]] [[1.164]
 [1.144]
 [1.089]
 [0.998]
 [1.099]
 [1.   ]
 [1.004]]
printing an ep nov before normalisation:  33.23446683774215
actor:  1 policy actor:  1  step number:  62 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5467133333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.411524894512155
actor:  1 policy actor:  1  step number:  57 total reward:  0.10666666666666613  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5467133333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5467133333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.015341012878757
maxi score, test score, baseline:  -0.5467133333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5467133333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.936544073249877
actor:  1 policy actor:  1  step number:  62 total reward:  0.11333333333333295  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  31.12995365493082
printing an ep nov before normalisation:  24.243240697043284
siam score:  -0.8263776
maxi score, test score, baseline:  -0.5467133333333334 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  34.20441009672612
maxi score, test score, baseline:  -0.5467133333333334 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  36.006988032556094
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5467133333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  70 total reward:  0.25999999999999923  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5467133333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  68 total reward:  0.09999999999999931  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5467133333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.121]
 [-0.109]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.121]
 [-0.109]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]]
Printing some Q and Qe and total Qs values:  [[-0.129]
 [-0.126]
 [-0.163]
 [-0.163]
 [-0.163]
 [-0.163]
 [-0.163]] [[41.885]
 [39.346]
 [38.044]
 [38.044]
 [38.044]
 [38.044]
 [38.044]] [[1.637]
 [1.486]
 [1.371]
 [1.371]
 [1.371]
 [1.371]
 [1.371]]
maxi score, test score, baseline:  -0.5467133333333333 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  39.183608978387454
maxi score, test score, baseline:  -0.5467133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5467133333333333 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  12.586041654509867
line 256 mcts: sample exp_bonus 35.08474987823844
Printing some Q and Qe and total Qs values:  [[-0.137]
 [-0.124]
 [-0.138]
 [-0.137]
 [-0.137]
 [-0.139]
 [-0.137]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.137]
 [-0.124]
 [-0.138]
 [-0.137]
 [-0.137]
 [-0.139]
 [-0.137]]
maxi score, test score, baseline:  -0.5467133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.084589958190918
maxi score, test score, baseline:  -0.5467133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5467133333333333 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.5467133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5467133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  43 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  24.003185698920763
printing an ep nov before normalisation:  29.691027616789242
maxi score, test score, baseline:  -0.5440200000000001 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5440200000000001 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.08590332018344
maxi score, test score, baseline:  -0.5440200000000001 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.5440200000000001 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.53232955932617
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.508]
 [0.347]
 [0.347]
 [0.3  ]
 [0.347]
 [0.347]] [[46.05 ]
 [36.04 ]
 [46.05 ]
 [46.05 ]
 [41.303]
 [46.05 ]
 [46.05 ]] [[1.655]
 [1.283]
 [1.655]
 [1.655]
 [1.355]
 [1.655]
 [1.655]]
maxi score, test score, baseline:  -0.5440200000000001 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.60343214634359
maxi score, test score, baseline:  -0.5440200000000001 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5440200000000001 0.6986666666666668 0.6986666666666668
Printing some Q and Qe and total Qs values:  [[-0.045]
 [-0.029]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]] [[44.144]
 [49.479]
 [44.144]
 [44.144]
 [44.144]
 [44.144]
 [44.144]] [[0.232]
 [0.29 ]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]]
printing an ep nov before normalisation:  37.4120375230052
printing an ep nov before normalisation:  38.4201301450288
printing an ep nov before normalisation:  43.199653214993496
Printing some Q and Qe and total Qs values:  [[-0.053]
 [-0.053]
 [-0.053]
 [-0.053]
 [-0.053]
 [-0.053]
 [-0.053]] [[41.257]
 [41.257]
 [41.257]
 [41.257]
 [41.257]
 [41.257]
 [41.257]] [[0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]]
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]] [[25.399]
 [25.399]
 [25.399]
 [25.399]
 [25.399]
 [25.399]
 [25.399]] [[0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]
 [0.206]]
printing an ep nov before normalisation:  25.90101346492213
printing an ep nov before normalisation:  38.05903686552741
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.82945959990247
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.77 ]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]] [[26.489]
 [31.854]
 [26.489]
 [26.489]
 [26.489]
 [26.489]
 [26.489]] [[0.727]
 [0.77 ]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]]
actor:  0 policy actor:  0  step number:  44 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5412333333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  70 total reward:  0.019999999999998463  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5412333333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]] [[33.456]
 [33.456]
 [33.456]
 [33.456]
 [33.456]
 [33.456]
 [33.456]] [[0.99]
 [0.99]
 [0.99]
 [0.99]
 [0.99]
 [0.99]
 [0.99]]
actor:  0 policy actor:  0  step number:  65 total reward:  0.01333333333333242  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5392066666666666 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5392066666666666 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.679999828338623
printing an ep nov before normalisation:  51.39751587843736
printing an ep nov before normalisation:  33.275545040910345
printing an ep nov before normalisation:  52.45631635307444
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.75810670852661
maxi score, test score, baseline:  -0.5392066666666666 0.6986666666666668 0.6986666666666668
line 256 mcts: sample exp_bonus 29.922093916228494
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  12.311313162488165
maxi score, test score, baseline:  -0.5392066666666666 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.44402422415559
maxi score, test score, baseline:  -0.5425533333333334 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.5425533333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.45199328512903
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.590840593736935
printing an ep nov before normalisation:  27.64839108295636
maxi score, test score, baseline:  -0.5425533333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.3329, 0.0064, 0.1193, 0.1382, 0.1659, 0.1043, 0.1329],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0090, 0.9524, 0.0064, 0.0063, 0.0027, 0.0028, 0.0204],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1330, 0.0130, 0.4352, 0.1027, 0.0901, 0.1354, 0.0905],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1536, 0.0542, 0.0909, 0.3320, 0.1461, 0.1022, 0.1211],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2112, 0.0130, 0.1060, 0.1120, 0.3589, 0.0788, 0.1201],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1510, 0.0535, 0.2021, 0.1334, 0.1140, 0.2022, 0.1438],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1097, 0.0955, 0.0782, 0.1880, 0.0818, 0.0691, 0.3777],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  49 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  50 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  25.48881085378475
maxi score, test score, baseline:  -0.5425533333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.022]
 [-0.04 ]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.022]] [[39.561]
 [50.606]
 [39.561]
 [39.561]
 [39.561]
 [39.561]
 [39.561]] [[1.325]
 [1.96 ]
 [1.325]
 [1.325]
 [1.325]
 [1.325]
 [1.325]]
printing an ep nov before normalisation:  43.700240539967155
printing an ep nov before normalisation:  34.78892225649632
printing an ep nov before normalisation:  40.97664185979799
printing an ep nov before normalisation:  26.48722936471292
UNIT TEST: sample policy line 217 mcts : [0.02  0.612 0.02  0.02  0.286 0.02  0.02 ]
printing an ep nov before normalisation:  48.74982476142699
printing an ep nov before normalisation:  20.735504627227783
using explorer policy with actor:  1
printing an ep nov before normalisation:  27.47671127319336
printing an ep nov before normalisation:  0.5945904375522559
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.095]
 [-0.095]
 [-0.095]
 [-0.095]
 [-0.095]
 [-0.095]
 [-0.095]] [[38.721]
 [38.721]
 [38.721]
 [38.721]
 [38.721]
 [38.721]
 [38.721]] [[0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]]
maxi score, test score, baseline:  -0.5425533333333334 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.5425533333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.9600084665202
maxi score, test score, baseline:  -0.5425533333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.48685625929157
siam score:  -0.8370686
printing an ep nov before normalisation:  56.57713675305494
actions average: 
K:  1  action  0 :  tensor([0.3073, 0.0122, 0.1452, 0.1349, 0.1680, 0.1192, 0.1132],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0080, 0.9137, 0.0077, 0.0329, 0.0043, 0.0029, 0.0304],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1249, 0.0044, 0.3858, 0.1191, 0.1173, 0.1560, 0.0925],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1890, 0.0097, 0.1471, 0.2406, 0.1462, 0.1373, 0.1301],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1551, 0.0071, 0.1380, 0.1569, 0.3028, 0.1306, 0.1093],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1346, 0.0261, 0.1641, 0.1397, 0.1336, 0.2640, 0.1379],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1447, 0.1671, 0.1272, 0.1686, 0.1346, 0.1344, 0.1235],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  47.695886642818195
printing an ep nov before normalisation:  44.01722122273903
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.004]
 [ 0.011]
 [-0.009]
 [-0.001]
 [-0.009]
 [-0.009]
 [-0.009]] [[38.08 ]
 [34.454]
 [34.893]
 [36.763]
 [34.893]
 [34.893]
 [34.893]] [[1.625]
 [1.359]
 [1.374]
 [1.526]
 [1.374]
 [1.374]
 [1.374]]
maxi score, test score, baseline:  -0.5425533333333334 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  43.99143966065262
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.273]
 [0.246]
 [0.246]
 [0.202]
 [0.246]
 [0.2  ]] [[37.854]
 [33.967]
 [37.854]
 [37.854]
 [38.994]
 [37.854]
 [38.18 ]] [[2.077]
 [1.751]
 [2.077]
 [2.077]
 [2.138]
 [2.077]
 [2.061]]
printing an ep nov before normalisation:  55.16001082855986
Printing some Q and Qe and total Qs values:  [[-0.079]
 [-0.071]
 [-0.077]
 [-0.079]
 [-0.076]
 [-0.077]
 [-0.07 ]] [[27.198]
 [25.758]
 [26.774]
 [26.279]
 [23.038]
 [24.677]
 [23.969]] [[1.004]
 [0.955]
 [0.989]
 [0.968]
 [0.841]
 [0.906]
 [0.884]]
printing an ep nov before normalisation:  39.93848521665296
maxi score, test score, baseline:  -0.5425533333333334 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.5425533333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.013490761075527
printing an ep nov before normalisation:  18.842698833663405
maxi score, test score, baseline:  -0.5425533333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.976889054086286
printing an ep nov before normalisation:  25.37851032832572
maxi score, test score, baseline:  -0.5425533333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  0.07333333333333292  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5425533333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.45435905456543
maxi score, test score, baseline:  -0.5425533333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.049]
 [-0.03 ]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]] [[11.744]
 [27.56 ]
 [11.744]
 [11.744]
 [11.744]
 [11.744]
 [11.744]] [[-0.046]
 [ 0.202]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]]
printing an ep nov before normalisation:  35.73371528221733
maxi score, test score, baseline:  -0.5425533333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  55 total reward:  0.34666666666666657  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.53986 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8373256
maxi score, test score, baseline:  -0.53986 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  47.34562027004745
Printing some Q and Qe and total Qs values:  [[-0.023]
 [-0.057]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]] [[47.826]
 [43.645]
 [47.826]
 [47.826]
 [47.826]
 [47.826]
 [47.826]] [[1.31 ]
 [1.102]
 [1.31 ]
 [1.31 ]
 [1.31 ]
 [1.31 ]
 [1.31 ]]
actor:  1 policy actor:  1  step number:  63 total reward:  0.18666666666666598  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.53986 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.528]
 [0.507]
 [0.476]
 [0.42 ]
 [0.496]
 [0.478]] [[29.874]
 [28.769]
 [26.172]
 [26.477]
 [30.053]
 [26.048]
 [30.327]] [[1.343]
 [1.398]
 [1.298]
 [1.276]
 [1.328]
 [1.283]
 [1.395]]
actor:  1 policy actor:  1  step number:  41 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.53986 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.42750957529673
printing an ep nov before normalisation:  47.43482866347534
maxi score, test score, baseline:  -0.53986 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.53986 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  27.23411960936169
printing an ep nov before normalisation:  31.509487360778053
printing an ep nov before normalisation:  23.49190439467919
actor:  1 policy actor:  1  step number:  82 total reward:  0.20666666666666578  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.53986 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.14666666666666617  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.59706256942246
printing an ep nov before normalisation:  35.47174605968599
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.064]
 [0.383]
 [0.107]
 [0.068]
 [0.064]
 [0.064]
 [0.064]] [[30.778]
 [29.645]
 [30.298]
 [27.825]
 [30.778]
 [30.778]
 [30.778]] [[1.801]
 [2.054]
 [1.816]
 [1.635]
 [1.801]
 [1.801]
 [1.801]]
printing an ep nov before normalisation:  26.936828168694575
siam score:  -0.83548474
maxi score, test score, baseline:  -0.53986 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.53986 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.301634311676025
maxi score, test score, baseline:  -0.53986 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.83306295
printing an ep nov before normalisation:  14.725905656814575
actions average: 
K:  3  action  0 :  tensor([0.2298, 0.0639, 0.1211, 0.1389, 0.1838, 0.1270, 0.1355],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0178, 0.9273, 0.0100, 0.0118, 0.0100, 0.0064, 0.0166],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2108, 0.0586, 0.1660, 0.1316, 0.1391, 0.1692, 0.1248],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1636, 0.0327, 0.1188, 0.2262, 0.1581, 0.1087, 0.1920],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1570, 0.0451, 0.1010, 0.2279, 0.2201, 0.1201, 0.1288],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1496, 0.0315, 0.1315, 0.1845, 0.1714, 0.1389, 0.1925],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1984, 0.0268, 0.1776, 0.1441, 0.1762, 0.1409, 0.1360],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.29104288412335
printing an ep nov before normalisation:  23.223853234503196
maxi score, test score, baseline:  -0.53986 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.791]
 [0.917]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]] [[38.222]
 [37.714]
 [38.222]
 [38.222]
 [38.222]
 [38.222]
 [38.222]] [[0.791]
 [0.917]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]]
printing an ep nov before normalisation:  35.16923917681083
maxi score, test score, baseline:  -0.53986 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  64 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  46.751134114068435
printing an ep nov before normalisation:  37.69230795406549
maxi score, test score, baseline:  -0.5407000000000001 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5407000000000001 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.086]
 [-0.052]
 [-0.085]
 [-0.086]
 [-0.086]
 [-0.086]
 [-0.09 ]] [[32.275]
 [34.412]
 [32.736]
 [32.682]
 [32.662]
 [33.629]
 [33.389]] [[0.85 ]
 [1.009]
 [0.878]
 [0.874]
 [0.873]
 [0.93 ]
 [0.912]]
printing an ep nov before normalisation:  29.583027414106315
rdn beta is 0 so we're just using the maxi policy
UNIT TEST: sample policy line 217 mcts : [0.143 0.408 0.061 0.184 0.102 0.041 0.061]
printing an ep nov before normalisation:  32.689512359061034
maxi score, test score, baseline:  -0.5407000000000001 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  43 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.82663465415414
maxi score, test score, baseline:  -0.5379 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  93 total reward:  0.06666666666666599  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5357666666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5357666666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5357666666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.65656360352688
maxi score, test score, baseline:  -0.5357666666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5357666666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  72 total reward:  0.11333333333333295  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  49 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5357666666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.008887717882004
maxi score, test score, baseline:  -0.5357666666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.945622543049364
printing an ep nov before normalisation:  44.983937353596524
printing an ep nov before normalisation:  35.45749849689191
printing an ep nov before normalisation:  39.014220237731934
printing an ep nov before normalisation:  32.49759815842122
maxi score, test score, baseline:  -0.5357666666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.137]
 [-0.119]
 [-0.136]
 [-0.139]
 [-0.139]
 [-0.138]
 [-0.138]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.137]
 [-0.119]
 [-0.136]
 [-0.139]
 [-0.139]
 [-0.138]
 [-0.138]]
maxi score, test score, baseline:  -0.5357666666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.29999999999999927  reward:  1.0 rdn_beta:  1.333
deleting a thread, now have 2 threads
Frames:  88405 train batches done:  10352 episodes:  2737
printing an ep nov before normalisation:  31.494219319688753
Printing some Q and Qe and total Qs values:  [[0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]] [[41.83]
 [41.83]
 [41.83]
 [41.83]
 [41.83]
 [41.83]
 [41.83]] [[0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]]
printing an ep nov before normalisation:  33.98149810346241
actor:  0 policy actor:  0  step number:  46 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.53306 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.53306 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.53306 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.53306 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.53306 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.53306 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.53306 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
UNIT TEST: sample policy line 217 mcts : [0.204 0.184 0.082 0.082 0.204 0.122 0.122]
actions average: 
K:  1  action  0 :  tensor([0.4411, 0.0146, 0.1102, 0.1066, 0.1479, 0.0993, 0.0804],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0091, 0.9462, 0.0087, 0.0095, 0.0045, 0.0049, 0.0171],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1502, 0.0286, 0.3227, 0.1125, 0.1306, 0.1498, 0.1056],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1334, 0.0658, 0.1318, 0.3095, 0.1407, 0.1247, 0.0940],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2195, 0.0324, 0.1018, 0.0927, 0.3430, 0.0932, 0.1174],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2175, 0.0360, 0.2199, 0.1215, 0.1096, 0.1680, 0.1275],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1863, 0.1698, 0.1390, 0.1230, 0.1399, 0.1143, 0.1276],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.28806049400651
maxi score, test score, baseline:  -0.53306 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.53306 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([0.3128, 0.0813, 0.0909, 0.1183, 0.2116, 0.0830, 0.1020],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0122, 0.9178, 0.0082, 0.0174, 0.0097, 0.0040, 0.0307],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1224, 0.0267, 0.3397, 0.0991, 0.0982, 0.1948, 0.1191],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1738, 0.0668, 0.1361, 0.2556, 0.1428, 0.1134, 0.1115],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2594, 0.0219, 0.1029, 0.1578, 0.2372, 0.1071, 0.1136],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1214, 0.0147, 0.1210, 0.1036, 0.1078, 0.4487, 0.0828],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1501, 0.2864, 0.1001, 0.1118, 0.1402, 0.1162, 0.0954],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.53306 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.53306 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.3435, 0.1122, 0.1321, 0.1224, 0.0876, 0.0945, 0.1079],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0310, 0.8469, 0.0207, 0.0245, 0.0370, 0.0155, 0.0244],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1716, 0.1055, 0.3245, 0.0993, 0.0818, 0.0929, 0.1244],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1477, 0.1895, 0.0884, 0.3159, 0.0806, 0.0969, 0.0810],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2698, 0.0385, 0.0971, 0.1411, 0.2692, 0.0913, 0.0930],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2388, 0.0205, 0.1611, 0.1693, 0.1303, 0.1631, 0.1169],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1697, 0.2765, 0.1221, 0.1385, 0.0911, 0.0891, 0.1129],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.854472982111325
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.177]
 [0.456]
 [0.177]
 [0.177]
 [0.177]
 [0.177]
 [0.177]] [[41.54 ]
 [48.873]
 [41.54 ]
 [41.54 ]
 [41.54 ]
 [41.54 ]
 [41.54 ]] [[1.077]
 [1.584]
 [1.077]
 [1.077]
 [1.077]
 [1.077]
 [1.077]]
printing an ep nov before normalisation:  42.4975453826204
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8355118
printing an ep nov before normalisation:  40.390567167079325
actor:  1 policy actor:  1  step number:  59 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.75143365578511
printing an ep nov before normalisation:  37.38729614488371
printing an ep nov before normalisation:  43.83129986290043
printing an ep nov before normalisation:  37.68535307892424
actor:  1 policy actor:  1  step number:  48 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  0.667
siam score:  -0.83210665
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.294]
 [0.135]
 [0.136]
 [0.215]
 [0.142]
 [0.137]] [[40.9  ]
 [43.445]
 [40.549]
 [41.974]
 [40.774]
 [40.444]
 [40.633]] [[0.688]
 [0.741]
 [0.527]
 [0.555]
 [0.611]
 [0.531]
 [0.531]]
printing an ep nov before normalisation:  32.05917010931457
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.4213, 0.0379, 0.1079, 0.1083, 0.1502, 0.0759, 0.0986],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0173, 0.8588, 0.0386, 0.0174, 0.0074, 0.0214, 0.0390],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1765, 0.1093, 0.1838, 0.1419, 0.1220, 0.1313, 0.1352],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1973, 0.0188, 0.1861, 0.1557, 0.1153, 0.1871, 0.1398],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1679, 0.0835, 0.0844, 0.1150, 0.3756, 0.0872, 0.0864],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1630, 0.0160, 0.1966, 0.1303, 0.1085, 0.2477, 0.1380],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1776, 0.1164, 0.1072, 0.1780, 0.1042, 0.1136, 0.2030],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  34.6735056934327
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]] [[38.071]
 [38.071]
 [38.071]
 [38.071]
 [38.071]
 [38.071]
 [38.071]] [[0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]]
actions average: 
K:  1  action  0 :  tensor([0.3778, 0.1202, 0.1027, 0.1075, 0.1101, 0.0874, 0.0943],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0060, 0.9333, 0.0095, 0.0193, 0.0035, 0.0035, 0.0249],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1543, 0.0154, 0.3607, 0.1337, 0.1390, 0.1014, 0.0955],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1891, 0.0074, 0.1383, 0.2655, 0.1518, 0.1192, 0.1288],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2151, 0.0157, 0.1219, 0.1295, 0.3106, 0.0936, 0.1136],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2030, 0.0375, 0.1718, 0.1549, 0.1305, 0.1348, 0.1675],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1021, 0.1847, 0.1007, 0.1232, 0.0376, 0.0398, 0.4118],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.288231314756594
Printing some Q and Qe and total Qs values:  [[-0.141]
 [-0.139]
 [-0.141]
 [-0.141]
 [-0.141]
 [-0.142]
 [-0.145]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.141]
 [-0.139]
 [-0.141]
 [-0.141]
 [-0.141]
 [-0.142]
 [-0.145]]
printing an ep nov before normalisation:  31.20607852935791
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.34138570325508
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.02235350412394
printing an ep nov before normalisation:  37.152040031184534
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
actor:  1 policy actor:  1  step number:  52 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.09587491987832
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.23178343421835
actor:  1 policy actor:  1  step number:  56 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.771]
 [0.88 ]
 [0.769]
 [0.767]
 [0.766]
 [0.766]
 [0.836]] [[32.46 ]
 [35.901]
 [25.938]
 [25.484]
 [25.715]
 [25.64 ]
 [35.478]] [[0.771]
 [0.88 ]
 [0.769]
 [0.767]
 [0.766]
 [0.766]
 [0.836]]
maxi score, test score, baseline:  -0.5306333333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.19818789641144
actions average: 
K:  3  action  0 :  tensor([0.3467, 0.0587, 0.1015, 0.1189, 0.1616, 0.0907, 0.1219],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0236, 0.8598, 0.0216, 0.0268, 0.0148, 0.0108, 0.0426],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1436, 0.1660, 0.1857, 0.1439, 0.1208, 0.1313, 0.1087],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1494, 0.2671, 0.1116, 0.1289, 0.1321, 0.1141, 0.0969],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1165, 0.2218, 0.0810, 0.1031, 0.3329, 0.0802, 0.0646],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1937, 0.1094, 0.1457, 0.1312, 0.1339, 0.1618, 0.1243],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1283, 0.1455, 0.1419, 0.1314, 0.1258, 0.1319, 0.1954],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  60 total reward:  0.05999999999999939  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5285133333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.775]
 [0.836]
 [0.733]
 [0.732]
 [0.733]
 [0.732]
 [0.733]] [[30.842]
 [33.149]
 [27.269]
 [27.264]
 [27.416]
 [27.205]
 [27.071]] [[0.775]
 [0.836]
 [0.733]
 [0.732]
 [0.733]
 [0.732]
 [0.733]]
printing an ep nov before normalisation:  35.40611490166953
actor:  0 policy actor:  0  step number:  34 total reward:  0.5133333333333333  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  30.1984165713391
printing an ep nov before normalisation:  32.74641780034083
maxi score, test score, baseline:  -0.5254866666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5254866666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.457750658278606
maxi score, test score, baseline:  -0.5254866666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5254866666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5254866666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5254866666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5254866666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.26068449466678
maxi score, test score, baseline:  -0.5254866666666667 0.6986666666666668 0.6986666666666668
actor:  1 policy actor:  1  step number:  43 total reward:  0.52  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5254866666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5254866666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.139]
 [-0.138]
 [-0.128]
 [-0.129]
 [-0.141]
 [-0.13 ]
 [-0.138]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.139]
 [-0.138]
 [-0.128]
 [-0.129]
 [-0.141]
 [-0.13 ]
 [-0.138]]
maxi score, test score, baseline:  -0.5254866666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.969032040179904
maxi score, test score, baseline:  -0.5254866666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.459561347961426
actor:  1 policy actor:  1  step number:  49 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  34.765823210271314
maxi score, test score, baseline:  -0.5254866666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.57795453071035
maxi score, test score, baseline:  -0.5254866666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.17124537172794
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.5254866666666667 0.6986666666666668 0.6986666666666668
actor:  1 policy actor:  1  step number:  52 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.141]
 [-0.063]
 [-0.169]
 [-0.141]
 [-0.172]
 [-0.141]
 [-0.141]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.141]
 [-0.063]
 [-0.169]
 [-0.141]
 [-0.172]
 [-0.141]
 [-0.141]]
maxi score, test score, baseline:  -0.5254866666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.647027240022428
printing an ep nov before normalisation:  53.21325939354189
maxi score, test score, baseline:  -0.5254866666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5254866666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.756532192230225
printing an ep nov before normalisation:  20.780897091630358
printing an ep nov before normalisation:  33.427110907886984
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.5510],
        [-0.2523],
        [-0.0000],
        [-0.2316],
        [ 0.5211],
        [-0.5305],
        [ 0.8309],
        [-0.0657],
        [-0.1032],
        [-0.0000]], dtype=torch.float64)
-0.045026434398 -0.5960412946963919
-0.045026434398 -0.29727740473520936
0.9734999999999999 0.9734999999999999
-0.070771701198 -0.30232984147665665
-0.083839701198 0.4372767340036232
-0.032346567066 -0.5628174266892814
-0.058354513866000005 0.7725096284825501
-0.032346567066 -0.09806596097128514
-0.032346567066 -0.1355145616741484
-0.8756220000000001 -0.8756220000000001
maxi score, test score, baseline:  -0.5254866666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5254866666666667 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  37 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.435]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]] [[46.668]
 [40.738]
 [46.668]
 [46.668]
 [46.668]
 [46.668]
 [46.668]] [[0.688]
 [0.668]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]]
maxi score, test score, baseline:  -0.5225533333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5225533333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5225533333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5225533333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5225533333333334 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.39786150152952
actor:  0 policy actor:  0  step number:  55 total reward:  0.31999999999999984  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  52.869426346567835
maxi score, test score, baseline:  -0.5199133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5199133333333333 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  37 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.51767653736894
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.557473979297562
printing an ep nov before normalisation:  31.70929193556057
printing an ep nov before normalisation:  41.03642534032293
maxi score, test score, baseline:  -0.5169800000000002 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
maxi score, test score, baseline:  -0.5169800000000002 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.5169800000000002 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5169800000000002 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  39.07639265849602
printing an ep nov before normalisation:  26.463794708251953
printing an ep nov before normalisation:  24.83046054840088
printing an ep nov before normalisation:  34.4764749910927
maxi score, test score, baseline:  -0.5169800000000002 0.6986666666666668 0.6986666666666668
maxi score, test score, baseline:  -0.5169800000000002 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5169800000000002 0.6986666666666668 0.6986666666666668
printing an ep nov before normalisation:  39.645015883892384
printing an ep nov before normalisation:  33.865810628276506
maxi score, test score, baseline:  -0.5169800000000002 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.703]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.659]] [[34.858]
 [35.448]
 [34.858]
 [34.858]
 [34.858]
 [34.858]
 [32.799]] [[0.688]
 [0.703]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.659]]
printing an ep nov before normalisation:  45.87490135739022
line 256 mcts: sample exp_bonus 30.239029768731378
printing an ep nov before normalisation:  42.20164000771052
printing an ep nov before normalisation:  48.36145591271392
maxi score, test score, baseline:  -0.5169800000000001 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.46306196594649
printing an ep nov before normalisation:  48.54533014780025
printing an ep nov before normalisation:  32.38530699252963
siam score:  -0.8472709
maxi score, test score, baseline:  -0.5169800000000001 0.6986666666666668 0.6986666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.939624350744484
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  0.0
actor:  0 policy actor:  0  step number:  26 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[-0.09]
 [-0.09]
 [-0.09]
 [-0.09]
 [-0.09]
 [-0.09]
 [-0.09]] [[37.757]
 [37.757]
 [37.757]
 [37.757]
 [37.757]
 [37.757]
 [37.757]] [[0.882]
 [0.882]
 [0.882]
 [0.882]
 [0.882]
 [0.882]
 [0.882]]
actor:  0 policy actor:  0  step number:  26 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
printing an ep nov before normalisation:  41.5542179670673
actor:  1 policy actor:  1  step number:  69 total reward:  0.13333333333333253  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  39.4790526211385
actions average: 
K:  2  action  0 :  tensor([0.2360, 0.0471, 0.1422, 0.1665, 0.1697, 0.1118, 0.1267],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0090, 0.9470, 0.0067, 0.0086, 0.0038, 0.0028, 0.0221],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1436, 0.0152, 0.3367, 0.1320, 0.1394, 0.1176, 0.1156],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1140, 0.0838, 0.1031, 0.3760, 0.0989, 0.0687, 0.1555],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1645, 0.0206, 0.1144, 0.1369, 0.3379, 0.1098, 0.1160],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1424, 0.0094, 0.1468, 0.1731, 0.1535, 0.2807, 0.0941],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1431, 0.0450, 0.1479, 0.1404, 0.1293, 0.1154, 0.2789],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  66 total reward:  0.2333333333333324  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.582]
 [0.433]
 [0.414]
 [0.452]
 [0.433]
 [0.433]] [[38.067]
 [37.332]
 [42.467]
 [40.941]
 [44.264]
 [42.467]
 [42.467]] [[1.12 ]
 [1.225]
 [1.279]
 [1.2  ]
 [1.369]
 [1.279]
 [1.279]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
actor:  1 policy actor:  1  step number:  58 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.308]
 [0.304]
 [0.302]
 [0.307]
 [0.318]
 [0.307]] [[33.85 ]
 [31.926]
 [34.51 ]
 [34.706]
 [34.651]
 [33.876]
 [33.95 ]] [[0.302]
 [0.308]
 [0.304]
 [0.302]
 [0.307]
 [0.318]
 [0.307]]
printing an ep nov before normalisation:  48.25527270557996
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.06516402703567792
actor:  1 policy actor:  1  step number:  55 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  54.313678794557866
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  41.999541460655855
line 256 mcts: sample exp_bonus 60.05128236151128
Printing some Q and Qe and total Qs values:  [[-0.048]
 [-0.034]
 [-0.088]
 [-0.078]
 [-0.08 ]
 [-0.086]
 [-0.086]] [[28.119]
 [32.888]
 [25.784]
 [26.805]
 [27.768]
 [26.688]
 [25.631]] [[1.011]
 [1.361]
 [0.806]
 [0.887]
 [0.954]
 [0.872]
 [0.797]]
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.30666666666666653  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  34.00151014328003
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.1866666666666661  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.16]
 [-0.16]
 [-0.16]
 [-0.16]
 [-0.16]
 [-0.16]
 [-0.16]] [[39.558]
 [39.558]
 [39.558]
 [39.558]
 [39.558]
 [39.558]
 [39.558]] [[1.297]
 [1.297]
 [1.297]
 [1.297]
 [1.297]
 [1.297]
 [1.297]]
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.90477308579012
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.192]
 [-0.172]
 [-0.108]
 [-0.172]
 [-0.172]
 [-0.096]
 [-0.172]] [[40.548]
 [29.419]
 [42.116]
 [29.419]
 [29.419]
 [40.191]
 [29.419]] [[0.911]
 [0.394]
 [1.072]
 [0.394]
 [0.394]
 [0.99 ]
 [0.394]]
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.024810875849596
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  72 total reward:  0.07333333333333225  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
siam score:  -0.83456784
printing an ep nov before normalisation:  29.484074809980868
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.522]
 [0.503]
 [0.416]
 [0.063]
 [0.336]
 [0.438]] [[19.553]
 [35.989]
 [32.14 ]
 [39.463]
 [41.652]
 [40.257]
 [36.809]] [[0.885]
 [1.329]
 [1.224]
 [1.301]
 [0.998]
 [1.239]
 [1.264]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  49 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.722]
 [0.794]
 [0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]] [[29.976]
 [41.604]
 [29.976]
 [29.976]
 [29.976]
 [29.976]
 [29.976]] [[0.722]
 [0.794]
 [0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]]
printing an ep nov before normalisation:  38.67278938989794
printing an ep nov before normalisation:  32.35762923779377
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.684]
 [0.807]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]] [[38.724]
 [51.127]
 [38.724]
 [38.724]
 [38.724]
 [38.724]
 [38.724]] [[0.684]
 [0.807]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]]
Printing some Q and Qe and total Qs values:  [[-0.128]
 [-0.128]
 [-0.128]
 [-0.128]
 [-0.128]
 [-0.128]
 [-0.128]] [[68.745]
 [68.745]
 [68.745]
 [68.745]
 [68.745]
 [68.745]
 [68.745]] [[0.84]
 [0.84]
 [0.84]
 [0.84]
 [0.84]
 [0.84]
 [0.84]]
maxi score, test score, baseline:  -0.4494866666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  60 total reward:  0.059999999999999276  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  1  action  0 :  tensor([0.2988, 0.0079, 0.1377, 0.1526, 0.1653, 0.1284, 0.1093],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0023, 0.9791, 0.0017, 0.0051, 0.0013, 0.0013, 0.0091],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1246, 0.0238, 0.2524, 0.1718, 0.1182, 0.1987, 0.1105],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1516, 0.0197, 0.1157, 0.3270, 0.1808, 0.1169, 0.0883],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1749, 0.0072, 0.1091, 0.1618, 0.3462, 0.1116, 0.0892],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1275, 0.0097, 0.1694, 0.1972, 0.1576, 0.2198, 0.1188],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1540, 0.0841, 0.1469, 0.1999, 0.1513, 0.1440, 0.1197],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.4473666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.031409069045147
maxi score, test score, baseline:  -0.4473666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.965544950212823
maxi score, test score, baseline:  -0.4473666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4473666666666667 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.4473666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 50.613547147418146
printing an ep nov before normalisation:  49.39604020596184
maxi score, test score, baseline:  -0.4473666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.4705, 0.0632, 0.0791, 0.0820, 0.1777, 0.0607, 0.0666],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0022,     0.9727,     0.0073,     0.0046,     0.0008,     0.0027,
            0.0099], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0894, 0.0101, 0.4759, 0.0810, 0.0713, 0.1879, 0.0844],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1320, 0.0094, 0.1498, 0.3063, 0.1101, 0.1046, 0.1877],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2236, 0.0066, 0.1353, 0.1300, 0.3047, 0.1075, 0.0924],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1050, 0.0121, 0.1398, 0.1324, 0.1092, 0.3954, 0.1060],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2206, 0.1837, 0.1265, 0.0972, 0.0922, 0.0787, 0.2011],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.636]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]] [[21.462]
 [27.028]
 [21.462]
 [21.462]
 [21.462]
 [21.462]
 [21.462]] [[1.758]
 [2.12 ]
 [1.758]
 [1.758]
 [1.758]
 [1.758]
 [1.758]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.4473666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.710586997092157
actor:  1 policy actor:  1  step number:  55 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4473666666666667 0.6873333333333335 0.6873333333333335
printing an ep nov before normalisation:  44.405250132952574
maxi score, test score, baseline:  -0.4473666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4473666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.678]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]] [[42.397]
 [38.743]
 [42.397]
 [42.397]
 [42.397]
 [42.397]
 [42.397]] [[1.844]
 [1.732]
 [1.844]
 [1.844]
 [1.844]
 [1.844]
 [1.844]]
printing an ep nov before normalisation:  37.64893507974119
maxi score, test score, baseline:  -0.4473666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.739664197418158
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.503]
 [0.536]
 [0.484]
 [0.472]
 [0.562]
 [0.549]] [[25.372]
 [23.839]
 [24.584]
 [26.112]
 [25.872]
 [23.663]
 [23.531]] [[1.435]
 [1.347]
 [1.437]
 [1.502]
 [1.471]
 [1.393]
 [1.37 ]]
maxi score, test score, baseline:  -0.4473666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.1999999999999994  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.4473666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4473666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.680240631103516
printing an ep nov before normalisation:  59.05125416618613
maxi score, test score, baseline:  -0.4473666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.16658128451539
printing an ep nov before normalisation:  39.86935615539551
maxi score, test score, baseline:  -0.4473666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.626822086540365
maxi score, test score, baseline:  -0.4473666666666667 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.4473666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  46 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.44474 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.264482122905456
Printing some Q and Qe and total Qs values:  [[-0.075]
 [-0.068]
 [-0.078]
 [-0.078]
 [-0.076]
 [-0.078]
 [-0.085]] [[46.165]
 [45.678]
 [46.439]
 [45.235]
 [46.966]
 [45.611]
 [43.304]] [[0.485]
 [0.481]
 [0.488]
 [0.461]
 [0.501]
 [0.469]
 [0.41 ]]
printing an ep nov before normalisation:  33.075475686001035
maxi score, test score, baseline:  -0.44474 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.132393447481697
printing an ep nov before normalisation:  24.878285432988825
maxi score, test score, baseline:  -0.44474 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.923]
 [0.982]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]] [[50.965]
 [49.018]
 [50.965]
 [50.965]
 [50.965]
 [50.965]
 [50.965]] [[0.923]
 [0.982]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]]
maxi score, test score, baseline:  -0.44474 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.4587, 0.0190, 0.0870, 0.1034, 0.1610, 0.0943, 0.0766],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0108, 0.9263, 0.0076, 0.0208, 0.0044, 0.0054, 0.0247],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1701, 0.0594, 0.3091, 0.0738, 0.1283, 0.1304, 0.1289],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2143, 0.0437, 0.1090, 0.2108, 0.2058, 0.1171, 0.0993],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2255, 0.0906, 0.1002, 0.1204, 0.2717, 0.0987, 0.0928],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1931, 0.0227, 0.1736, 0.1242, 0.1674, 0.2143, 0.1047],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1506, 0.0449, 0.1252, 0.1931, 0.1336, 0.1243, 0.2283],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.44474 0.6873333333333335 0.6873333333333335
actor:  0 policy actor:  0  step number:  64 total reward:  0.01999999999999935  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.44270000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  9.86270960013755
maxi score, test score, baseline:  -0.44270000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.04925697287446
printing an ep nov before normalisation:  24.475272531987283
actor:  1 policy actor:  1  step number:  49 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  27.169854640960693
maxi score, test score, baseline:  -0.44270000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.44270000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.2240],
        [-0.0000],
        [ 0.2687],
        [-0.3474],
        [ 0.2390],
        [ 0.6951],
        [-0.2119],
        [ 0.5019],
        [-0.3983],
        [-0.2025]], dtype=torch.float64)
-0.045026434398 -0.26906032808965347
0.99 0.99
-0.058614567066 0.21012128141855685
-0.032346567066 -0.37977694074993124
-0.045026434398 0.19398577154115892
-0.09703970119800001 0.598066415895403
-0.083839701198 -0.29571439148638795
-0.09703970119800001 0.40483742130013245
-0.032346567066 -0.4306430024937424
-0.057834381198 -0.26034935837903006
maxi score, test score, baseline:  -0.44270000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.84360132245474
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.821]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]] [[39.327]
 [44.445]
 [39.327]
 [39.327]
 [39.327]
 [39.327]
 [39.327]] [[1.952]
 [2.294]
 [1.952]
 [1.952]
 [1.952]
 [1.952]
 [1.952]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.44270000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.12 ]
 [-0.117]
 [-0.121]
 [-0.132]
 [-0.131]
 [-0.132]
 [-0.12 ]] [[37.878]
 [36.593]
 [37.81 ]
 [34.349]
 [33.875]
 [33.657]
 [35.056]] [[0.683]
 [0.63 ]
 [0.68 ]
 [0.516]
 [0.496]
 [0.486]
 [0.559]]
maxi score, test score, baseline:  -0.44270000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.72170040293836
maxi score, test score, baseline:  -0.44270000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8546732
actor:  0 policy actor:  0  step number:  50 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.44023333333333337 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.854608
maxi score, test score, baseline:  -0.44023333333333337 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]] [[35.62]
 [35.62]
 [35.62]
 [35.62]
 [35.62]
 [35.62]
 [35.62]] [[1.3]
 [1.3]
 [1.3]
 [1.3]
 [1.3]
 [1.3]
 [1.3]]
Printing some Q and Qe and total Qs values:  [[-0.129]
 [-0.129]
 [-0.129]
 [-0.129]
 [-0.129]
 [-0.129]
 [-0.129]] [[33.686]
 [33.686]
 [33.686]
 [33.686]
 [33.686]
 [33.686]
 [33.686]] [[0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]]
actor:  1 policy actor:  1  step number:  57 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.44023333333333337 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.12]
 [-0.12]
 [-0.12]
 [-0.12]
 [-0.12]
 [-0.12]
 [-0.12]] [[69.618]
 [69.618]
 [69.618]
 [69.618]
 [69.618]
 [69.618]
 [69.618]] [[0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]]
maxi score, test score, baseline:  -0.44023333333333337 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.44023333333333337 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.44023333333333337 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.44023333333333337 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.8583298
printing an ep nov before normalisation:  26.145757349155346
maxi score, test score, baseline:  -0.44023333333333337 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.341]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]] [[28.974]
 [30.697]
 [28.974]
 [28.974]
 [28.974]
 [28.974]
 [28.974]] [[0.912]
 [1.008]
 [0.912]
 [0.912]
 [0.912]
 [0.912]
 [0.912]]
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]] [[28.483]
 [28.483]
 [28.483]
 [28.483]
 [28.483]
 [28.483]
 [28.483]] [[0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]]
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.458]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]] [[24.973]
 [27.748]
 [24.973]
 [24.973]
 [24.973]
 [24.973]
 [24.973]] [[0.523]
 [0.705]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]]
maxi score, test score, baseline:  -0.44023333333333337 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 44.347056925163635
printing an ep nov before normalisation:  27.48521089553833
actor:  1 policy actor:  1  step number:  46 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  31.000615441857335
maxi score, test score, baseline:  -0.44023333333333337 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.06392951428541
maxi score, test score, baseline:  -0.44023333333333337 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.819523583986317
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.44023333333333337 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0014353763901908678
actor:  1 policy actor:  1  step number:  51 total reward:  0.19999999999999962  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  2  action  0 :  tensor([0.3977, 0.0422, 0.1016, 0.1190, 0.1274, 0.1154, 0.0965],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0041, 0.9499, 0.0042, 0.0110, 0.0014, 0.0025, 0.0269],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1370, 0.0189, 0.2392, 0.1527, 0.1387, 0.2085, 0.1049],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1613, 0.0431, 0.1362, 0.2122, 0.1337, 0.1517, 0.1619],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2122, 0.0127, 0.1402, 0.1199, 0.2410, 0.1496, 0.1245],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1605, 0.0305, 0.1458, 0.1362, 0.1123, 0.2546, 0.1600],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1673, 0.0477, 0.1543, 0.1545, 0.1384, 0.1569, 0.1809],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.44023333333333337 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.85912395
maxi score, test score, baseline:  -0.44023333333333337 0.6873333333333335 0.6873333333333335
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4402333333333333 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4402333333333333 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4402333333333333 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.03119945526123
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.622]
 [0.501]
 [0.506]
 [0.51 ]
 [0.509]
 [0.498]] [[27.215]
 [28.208]
 [27.969]
 [27.944]
 [27.921]
 [26.715]
 [27.564]] [[0.505]
 [0.622]
 [0.501]
 [0.506]
 [0.51 ]
 [0.509]
 [0.498]]
maxi score, test score, baseline:  -0.4402333333333333 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.44023333333333337 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.44023333333333337 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.44023333333333337 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.10199214639482
actor:  1 policy actor:  1  step number:  55 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.44023333333333337 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.23187348343526
maxi score, test score, baseline:  -0.44023333333333337 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.44023333333333337 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.44023333333333337 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.58748605606946
siam score:  -0.85436517
maxi score, test score, baseline:  -0.44023333333333337 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  27.98008285612981
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.43780666666666673 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.43780666666666673 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.51574099897666
maxi score, test score, baseline:  -0.43780666666666673 0.6873333333333335 0.6873333333333335
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.725]
 [0.407]
 [0.405]
 [0.381]
 [0.408]
 [0.405]] [[28.866]
 [31.336]
 [31.292]
 [26.527]
 [31.132]
 [31.979]
 [26.527]] [[1.346]
 [1.737]
 [1.418]
 [1.262]
 [1.386]
 [1.441]
 [1.262]]
maxi score, test score, baseline:  -0.43780666666666673 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.779]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.606]
 [0.779]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]]
siam score:  -0.8578045
maxi score, test score, baseline:  -0.43780666666666673 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.592350549192254
maxi score, test score, baseline:  -0.43780666666666673 0.6873333333333335 0.6873333333333335
printing an ep nov before normalisation:  39.07625982790876
maxi score, test score, baseline:  -0.43780666666666673 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.43780666666666673 0.6873333333333335 0.6873333333333335
actor:  1 policy actor:  1  step number:  52 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.806]
 [0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.758]] [[44.997]
 [62.613]
 [44.997]
 [44.997]
 [44.997]
 [44.997]
 [44.997]] [[0.758]
 [0.806]
 [0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.758]]
maxi score, test score, baseline:  -0.43780666666666673 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.43780666666666673 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.38007017583353
printing an ep nov before normalisation:  36.65525909054569
maxi score, test score, baseline:  -0.43780666666666673 0.6873333333333335 0.6873333333333335
printing an ep nov before normalisation:  0.014134497959616965
actor:  0 policy actor:  0  step number:  53 total reward:  0.1999999999999994  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.43540666666666666 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.43540666666666666 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.43540666666666666 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.43540666666666666 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.187]
 [-0.089]
 [-0.224]
 [-0.114]
 [-0.139]
 [-0.114]
 [-0.098]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.187]
 [-0.089]
 [-0.224]
 [-0.114]
 [-0.139]
 [-0.114]
 [-0.098]]
maxi score, test score, baseline:  -0.43540666666666666 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.43540666666666666 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.78438703297492
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.43540666666666666 0.6873333333333335 0.6873333333333335
printing an ep nov before normalisation:  28.456018151408518
Printing some Q and Qe and total Qs values:  [[-0.017]
 [-0.001]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]] [[33.747]
 [34.381]
 [33.747]
 [33.747]
 [33.747]
 [33.747]
 [33.747]] [[0.143]
 [0.164]
 [0.143]
 [0.143]
 [0.143]
 [0.143]
 [0.143]]
maxi score, test score, baseline:  -0.43540666666666666 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  35.77158689640567
maxi score, test score, baseline:  -0.43540666666666666 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.43540666666666666 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.574]
 [0.538]
 [0.567]
 [0.551]
 [0.534]
 [0.56 ]] [[33.47 ]
 [31.085]
 [31.013]
 [29.636]
 [33.586]
 [30.655]
 [33.319]] [[0.564]
 [0.574]
 [0.538]
 [0.567]
 [0.551]
 [0.534]
 [0.56 ]]
maxi score, test score, baseline:  -0.43540666666666666 0.6873333333333335 0.6873333333333335
printing an ep nov before normalisation:  22.58692109732038
siam score:  -0.84970504
printing an ep nov before normalisation:  0.0
actor:  1 policy actor:  1  step number:  50 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.43540666666666666 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.43540666666666666 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.43540666666666666 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  38 total reward:  0.5  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.4324066666666668 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.4324066666666668 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4324066666666668 0.6873333333333335 0.6873333333333335
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.4324066666666668 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  42 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.824]
 [0.605]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.64 ]
 [0.824]
 [0.605]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]]
maxi score, test score, baseline:  -0.4295666666666667 0.6873333333333335 0.6873333333333335
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
actor:  0 policy actor:  0  step number:  44 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  2.0
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  16.29347094252311
maxi score, test score, baseline:  -0.42654000000000003 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.42654000000000003 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.3205, 0.0278, 0.1234, 0.1360, 0.1634, 0.1308, 0.0981],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0160, 0.9122, 0.0146, 0.0144, 0.0076, 0.0102, 0.0249],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1579, 0.0231, 0.3016, 0.1359, 0.1199, 0.1629, 0.0987],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1495, 0.1025, 0.1209, 0.2346, 0.1331, 0.1382, 0.1211],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1445, 0.0019, 0.1020, 0.1087, 0.4676, 0.1003, 0.0749],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1703, 0.0080, 0.1446, 0.1807, 0.1261, 0.2497, 0.1207],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1239, 0.1871, 0.1230, 0.1690, 0.1079, 0.1262, 0.1629],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  35.202534987412534
maxi score, test score, baseline:  -0.42654000000000003 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.42654000000000003 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.92928308909065
maxi score, test score, baseline:  -0.42654000000000003 0.6873333333333335 0.6873333333333335
actor:  0 policy actor:  0  step number:  73 total reward:  0.1599999999999996  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.30380571241605
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.76464696043577
actions average: 
K:  2  action  0 :  tensor([0.2931, 0.1597, 0.1233, 0.1089, 0.1135, 0.1123, 0.0893],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0142, 0.9330, 0.0090, 0.0127, 0.0057, 0.0064, 0.0191],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1120, 0.0207, 0.3135, 0.0978, 0.0819, 0.2849, 0.0892],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1637, 0.1645, 0.1119, 0.2172, 0.1362, 0.1118, 0.0947],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1603, 0.0140, 0.1139, 0.1353, 0.3399, 0.1325, 0.1041],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1197, 0.0310, 0.1417, 0.1607, 0.1468, 0.3137, 0.0864],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1408, 0.1156, 0.1308, 0.1448, 0.1182, 0.1287, 0.2212],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.066]
 [-0.047]
 [-0.066]
 [-0.066]
 [-0.066]
 [-0.066]
 [-0.066]] [[30.176]
 [35.149]
 [30.176]
 [30.176]
 [30.176]
 [30.176]
 [30.176]] [[0.186]
 [0.285]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]]
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.113]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]] [[47.402]
 [47.402]
 [47.402]
 [47.402]
 [47.402]
 [47.402]
 [47.402]] [[0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]]
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.7973944937876
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8423644
printing an ep nov before normalisation:  42.47248342775085
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
printing an ep nov before normalisation:  21.366744430191464
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.80337242552849
siam score:  -0.8501753
actor:  1 policy actor:  1  step number:  61 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.319]
 [0.249]
 [0.17 ]
 [0.167]
 [0.154]
 [0.249]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.186]
 [0.319]
 [0.249]
 [0.17 ]
 [0.167]
 [0.154]
 [0.249]]
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.70533997700435
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.52243925946126
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.092]
 [-0.074]
 [-0.101]
 [-0.102]
 [-0.101]
 [-0.104]
 [-0.104]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.092]
 [-0.074]
 [-0.101]
 [-0.102]
 [-0.101]
 [-0.104]
 [-0.104]]
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
siam score:  -0.85292035
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.386154174804688
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.33333333333333326  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.125]
 [-0.114]
 [-0.128]
 [-0.132]
 [-0.128]
 [-0.128]
 [-0.128]] [[37.86 ]
 [39.518]
 [37.894]
 [38.136]
 [37.894]
 [37.894]
 [37.894]] [[0.292]
 [0.339]
 [0.29 ]
 [0.292]
 [0.29 ]
 [0.29 ]
 [0.29 ]]
printing an ep nov before normalisation:  48.99310105741734
printing an ep nov before normalisation:  34.2460823059082
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.95739193983135
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  44 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  1.0
siam score:  -0.85236067
maxi score, test score, baseline:  -0.42422000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.48570880486736
actor:  0 policy actor:  0  step number:  51 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.4217666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4217666666666667 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.4217666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.097]
 [-0.097]
 [-0.097]
 [-0.097]
 [-0.125]
 [-0.097]
 [-0.097]] [[37.279]
 [37.279]
 [37.279]
 [37.279]
 [39.716]
 [37.279]
 [37.279]] [[0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.542]
 [0.49 ]
 [0.49 ]]
printing an ep nov before normalisation:  33.974143623573525
maxi score, test score, baseline:  -0.4217666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4217666666666667 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.4217666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4217666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 33.58233510410863
printing an ep nov before normalisation:  28.976798057556152
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.688]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]] [[50.46 ]
 [49.494]
 [49.891]
 [49.891]
 [49.891]
 [49.891]
 [49.891]] [[1.216]
 [1.285]
 [1.214]
 [1.214]
 [1.214]
 [1.214]
 [1.214]]
printing an ep nov before normalisation:  39.39025872617439
actor:  1 policy actor:  1  step number:  69 total reward:  0.06666666666666587  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  48 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4217666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4217666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4217666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4217666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4217666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.216486930847168
actor:  0 policy actor:  0  step number:  40 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.045]
 [-0.023]
 [-0.056]
 [-0.052]
 [-0.033]
 [-0.061]
 [-0.054]] [[40.839]
 [39.456]
 [43.469]
 [40.618]
 [42.653]
 [41.728]
 [41.281]] [[1.212]
 [1.149]
 [1.36 ]
 [1.191]
 [1.334]
 [1.249]
 [1.229]]
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
siam score:  -0.8446565
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  50.435529551356396
printing an ep nov before normalisation:  54.677827076159105
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
printing an ep nov before normalisation:  35.210126198565774
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8509269
actor:  1 policy actor:  1  step number:  63 total reward:  0.15999999999999925  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.92218140266507
Printing some Q and Qe and total Qs values:  [[-0.144]
 [-0.106]
 [-0.055]
 [-0.11 ]
 [-0.148]
 [-0.099]
 [-0.106]] [[54.871]
 [62.66 ]
 [58.006]
 [62.43 ]
 [59.775]
 [57.323]
 [62.66 ]] [[1.443]
 [1.87 ]
 [1.688]
 [1.855]
 [1.684]
 [1.609]
 [1.87 ]]
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.697]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]] [[27.9  ]
 [29.697]
 [27.9  ]
 [27.9  ]
 [27.9  ]
 [27.9  ]
 [27.9  ]] [[0.917]
 [0.994]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
siam score:  -0.84987146
printing an ep nov before normalisation:  42.764123944008674
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]] [[43.804]
 [43.804]
 [43.804]
 [43.804]
 [43.804]
 [43.804]
 [43.804]] [[2.249]
 [2.249]
 [2.249]
 [2.249]
 [2.249]
 [2.249]
 [2.249]]
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.336104139064993
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
printing an ep nov before normalisation:  37.42106830872069
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.28666666666666596  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.07438874147338
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]] [[33.839]
 [33.839]
 [33.839]
 [33.839]
 [33.839]
 [33.839]
 [33.839]] [[1.532]
 [1.532]
 [1.532]
 [1.532]
 [1.532]
 [1.532]
 [1.532]]
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
actor:  1 policy actor:  1  step number:  58 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  60 total reward:  0.20666666666666644  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  46.02730955651058
actions average: 
K:  2  action  0 :  tensor([0.4570, 0.0349, 0.0862, 0.0836, 0.2011, 0.0609, 0.0763],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0206, 0.8776, 0.0228, 0.0262, 0.0099, 0.0090, 0.0340],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1694, 0.0609, 0.2891, 0.1341, 0.1147, 0.1200, 0.1117],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1664, 0.0976, 0.1441, 0.2537, 0.0986, 0.1043, 0.1353],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1778, 0.0545, 0.1210, 0.1243, 0.3143, 0.1041, 0.1039],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1742, 0.1242, 0.1642, 0.1601, 0.1331, 0.1191, 0.1250],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1466, 0.1679, 0.1328, 0.1570, 0.1076, 0.0965, 0.1917],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8465434
printing an ep nov before normalisation:  33.575977734339105
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.09333333333333271  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.41898 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  46 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.41646 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.41646 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8398764
siam score:  -0.83722705
actor:  0 policy actor:  0  step number:  43 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.806483530690016
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8388069
printing an ep nov before normalisation:  47.76909705849387
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]] [[38.447]
 [38.447]
 [38.447]
 [38.447]
 [38.447]
 [38.447]
 [38.447]] [[2.139]
 [2.139]
 [2.139]
 [2.139]
 [2.139]
 [2.139]
 [2.139]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.718116029370414
printing an ep nov before normalisation:  11.889964654625658
Printing some Q and Qe and total Qs values:  [[0.295]
 [0.334]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]] [[18.59 ]
 [24.81 ]
 [13.649]
 [13.649]
 [13.649]
 [13.649]
 [13.649]] [[0.295]
 [0.334]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]]
printing an ep nov before normalisation:  7.237666807213827e-05
Printing some Q and Qe and total Qs values:  [[-0.15 ]
 [-0.071]
 [-0.133]
 [-0.1  ]
 [-0.126]
 [-0.135]
 [-0.093]] [[37.124]
 [37.778]
 [50.34 ]
 [35.846]
 [39.697]
 [39.267]
 [36.024]] [[0.444]
 [0.541]
 [0.836]
 [0.458]
 [0.541]
 [0.52 ]
 [0.47 ]]
Printing some Q and Qe and total Qs values:  [[-0.124]
 [-0.089]
 [-0.124]
 [-0.125]
 [-0.145]
 [-0.239]
 [-0.12 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.124]
 [-0.089]
 [-0.124]
 [-0.125]
 [-0.145]
 [-0.239]
 [-0.12 ]]
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.75265748254568
printing an ep nov before normalisation:  2.5988302402981844
actor:  1 policy actor:  1  step number:  41 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.421891704267956
siam score:  -0.8427265
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.89710288998628
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
actor:  1 policy actor:  1  step number:  70 total reward:  0.05999999999999939  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.27721385131385
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.93613578227094
printing an ep nov before normalisation:  31.57710552215576
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.049570083618164
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4137666666666667 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  41 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.4109933333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4109933333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.122940579319874
printing an ep nov before normalisation:  26.907574006694606
line 256 mcts: sample exp_bonus 39.92397469293576
printing an ep nov before normalisation:  38.627495765686035
printing an ep nov before normalisation:  42.76030484011841
maxi score, test score, baseline:  -0.4109933333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.62823679210267
maxi score, test score, baseline:  -0.4109933333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0
actor:  1 policy actor:  1  step number:  57 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.4109933333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4109933333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4109933333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4109933333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.513]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]] [[63.12 ]
 [58.331]
 [63.12 ]
 [63.12 ]
 [63.12 ]
 [63.12 ]
 [63.12 ]] [[1.164]
 [1.117]
 [1.164]
 [1.164]
 [1.164]
 [1.164]
 [1.164]]
printing an ep nov before normalisation:  37.6556965174734
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.52 ]
 [0.127]
 [0.147]
 [0.226]
 [0.231]
 [0.366]] [[33.598]
 [36.409]
 [32.206]
 [33.208]
 [37.289]
 [38.019]
 [34.065]] [[1.549]
 [1.898]
 [1.346]
 [1.404]
 [1.637]
 [1.67 ]
 [1.655]]
actor:  1 policy actor:  1  step number:  41 total reward:  0.46666666666666634  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.55 ]
 [0.531]
 [0.531]
 [0.531]
 [0.531]
 [0.531]] [[32.79 ]
 [42.844]
 [32.79 ]
 [32.79 ]
 [32.79 ]
 [32.79 ]
 [32.79 ]] [[2.028]
 [2.511]
 [2.028]
 [2.028]
 [2.028]
 [2.028]
 [2.028]]
printing an ep nov before normalisation:  40.21768638251821
actor:  1 policy actor:  1  step number:  55 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.4109933333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4109933333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4109933333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.38727272630932
siam score:  -0.8551741
actor:  1 policy actor:  1  step number:  40 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  11.716660261154175
printing an ep nov before normalisation:  17.39197189550154
printing an ep nov before normalisation:  10.83484173976018
printing an ep nov before normalisation:  60.097701728761706
siam score:  -0.85069674
printing an ep nov before normalisation:  42.692718611562675
printing an ep nov before normalisation:  15.017589330673218
printing an ep nov before normalisation:  38.168379633749616
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.513]
 [0.338]
 [0.337]
 [0.386]
 [0.356]
 [0.35 ]] [[35.952]
 [28.987]
 [32.086]
 [31.909]
 [35.952]
 [33.514]
 [33.14 ]] [[2.208]
 [1.752]
 [1.836]
 [1.821]
 [2.208]
 [1.975]
 [1.937]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.4109933333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.655903424495015
UNIT TEST: sample policy line 217 mcts : [0.082 0.347 0.449 0.02  0.02  0.02  0.061]
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.426]
 [0.454]
 [0.318]
 [0.303]
 [0.322]
 [0.32 ]] [[34.862]
 [49.033]
 [46.381]
 [35.531]
 [36.099]
 [35.538]
 [38.102]] [[0.427]
 [0.644]
 [0.652]
 [0.436]
 [0.425]
 [0.44 ]
 [0.457]]
printing an ep nov before normalisation:  50.61192370221964
siam score:  -0.83935
actor:  1 policy actor:  1  step number:  51 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.4109933333333334 0.6873333333333335 0.6873333333333335
printing an ep nov before normalisation:  46.90653619016717
printing an ep nov before normalisation:  48.83608816926825
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  17.36051750185156
maxi score, test score, baseline:  -0.4109933333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.746139835549485
maxi score, test score, baseline:  -0.4109933333333334 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.4109933333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.01700422755868
maxi score, test score, baseline:  -0.4109933333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.68205165863037
line 256 mcts: sample exp_bonus 35.57617646774716
siam score:  -0.8387362
actor:  0 policy actor:  0  step number:  58 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.40860666666666673 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.40860666666666673 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.98790091868596
maxi score, test score, baseline:  -0.40860666666666673 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.385675216805303
maxi score, test score, baseline:  -0.40860666666666673 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.40860666666666673 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.4335, 0.0584, 0.1111, 0.1009, 0.1269, 0.0937, 0.0755],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0104, 0.9322, 0.0110, 0.0166, 0.0064, 0.0062, 0.0172],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1339, 0.0122, 0.3138, 0.1237, 0.1325, 0.1706, 0.1133],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1812, 0.0113, 0.1701, 0.1915, 0.1505, 0.1654, 0.1301],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1649, 0.0037, 0.0893, 0.0961, 0.4610, 0.0978, 0.0871],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1499, 0.0049, 0.2004, 0.1614, 0.1384, 0.2453, 0.0997],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1672, 0.0277, 0.1939, 0.1765, 0.1350, 0.1370, 0.1627],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  35.07184006588921
maxi score, test score, baseline:  -0.40860666666666673 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.40860666666666673 0.6873333333333335 0.6873333333333335
actor:  1 policy actor:  1  step number:  49 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  21.2321210632372
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.531]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.327]
 [0.531]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]]
printing an ep nov before normalisation:  33.06677647186953
maxi score, test score, baseline:  -0.40860666666666673 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.235831954268676
printing an ep nov before normalisation:  50.67423009549165
printing an ep nov before normalisation:  39.46749210357666
printing an ep nov before normalisation:  35.64510294915143
actor:  1 policy actor:  1  step number:  57 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.40860666666666673 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.40860666666666673 0.6873333333333335 0.6873333333333335
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  54 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.4061133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.209]
 [-0.054]
 [-0.145]
 [-0.107]
 [-0.159]
 [-0.126]
 [-0.092]] [[38.657]
 [34.309]
 [36.654]
 [35.702]
 [36.838]
 [39.129]
 [35.075]] [[0.032]
 [0.142]
 [0.075]
 [0.104]
 [0.063]
 [0.12 ]
 [0.112]]
printing an ep nov before normalisation:  31.433961391448975
maxi score, test score, baseline:  -0.4061133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4061133333333334 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.4061133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.09514301420557
printing an ep nov before normalisation:  26.636225229626714
maxi score, test score, baseline:  -0.4061133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  52 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.40351333333333333 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.40351333333333333 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.40351333333333333 0.6873333333333335 0.6873333333333335
printing an ep nov before normalisation:  48.10347140583717
Printing some Q and Qe and total Qs values:  [[-0.076]
 [-0.021]
 [-0.076]
 [-0.076]
 [-0.076]
 [-0.076]
 [-0.076]] [[43.161]
 [47.909]
 [43.161]
 [43.161]
 [43.161]
 [43.161]
 [43.161]] [[1.101]
 [1.35 ]
 [1.101]
 [1.101]
 [1.101]
 [1.101]
 [1.101]]
printing an ep nov before normalisation:  39.01075743449156
maxi score, test score, baseline:  -0.40351333333333333 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.40351333333333333 0.6873333333333335 0.6873333333333335
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.40351333333333333 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.40351333333333333 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.40351333333333333 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.586460404964825
actor:  0 policy actor:  0  step number:  47 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.40095333333333333 0.6873333333333335 0.6873333333333335
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.302]
 [0.313]
 [0.313]] [[47.206]
 [47.206]
 [47.206]
 [47.206]
 [44.339]
 [47.206]
 [47.206]] [[1.19]
 [1.19]
 [1.19]
 [1.19]
 [1.1 ]
 [1.19]
 [1.19]]
Printing some Q and Qe and total Qs values:  [[0.125]
 [0.291]
 [0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.125]] [[38.553]
 [38.902]
 [38.553]
 [38.553]
 [38.553]
 [38.553]
 [38.553]] [[0.97 ]
 [1.149]
 [0.97 ]
 [0.97 ]
 [0.97 ]
 [0.97 ]
 [0.97 ]]
Printing some Q and Qe and total Qs values:  [[-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]] [[37.484]
 [37.484]
 [37.484]
 [37.484]
 [37.484]
 [37.484]
 [37.484]] [[1.227]
 [1.227]
 [1.227]
 [1.227]
 [1.227]
 [1.227]
 [1.227]]
maxi score, test score, baseline:  -0.40095333333333333 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.40095333333333333 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.275943042821194
maxi score, test score, baseline:  -0.40095333333333333 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.227]
 [0.229]
 [0.229]
 [0.072]
 [0.229]
 [0.229]] [[31.92 ]
 [26.326]
 [31.92 ]
 [31.92 ]
 [21.544]
 [31.92 ]
 [31.92 ]] [[0.896]
 [0.749]
 [0.896]
 [0.896]
 [0.471]
 [0.896]
 [0.896]]
maxi score, test score, baseline:  -0.40095333333333333 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.40095333333333333 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.40095333333333333 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.40095333333333333 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.40095333333333333 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.40095333333333333 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.608718777375906
printing an ep nov before normalisation:  49.09489904387986
printing an ep nov before normalisation:  43.36565544833163
actor:  1 policy actor:  1  step number:  53 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.40095333333333333 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.40095333333333333 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.02 ]
 [ 0.207]
 [-0.02 ]
 [-0.02 ]
 [-0.033]
 [-0.02 ]
 [-0.02 ]] [[48.641]
 [49.184]
 [48.641]
 [48.641]
 [39.657]
 [48.641]
 [48.641]] [[0.538]
 [0.772]
 [0.538]
 [0.538]
 [0.4  ]
 [0.538]
 [0.538]]
maxi score, test score, baseline:  -0.40095333333333333 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.40095333333333333 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.40095333333333333 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  73 total reward:  0.013333333333332642  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.693]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.658]
 [0.693]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]]
maxi score, test score, baseline:  -0.40095333333333333 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.40171461853981
maxi score, test score, baseline:  -0.40095333333333333 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.40095333333333333 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  11.631722353370119
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.84372413
maxi score, test score, baseline:  -0.40095333333333333 0.6873333333333335 0.6873333333333335
printing an ep nov before normalisation:  42.36250532598316
siam score:  -0.84316725
actor:  0 policy actor:  0  step number:  54 total reward:  0.16666666666666607  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.39862000000000003 0.6873333333333335 0.6873333333333335
printing an ep nov before normalisation:  42.03791618347168
printing an ep nov before normalisation:  53.341818619709116
actions average: 
K:  3  action  0 :  tensor([0.3605, 0.0411, 0.0980, 0.1056, 0.2015, 0.0989, 0.0945],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0210, 0.8585, 0.0195, 0.0339, 0.0237, 0.0135, 0.0299],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1103, 0.0121, 0.2942, 0.1560, 0.1227, 0.1605, 0.1442],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1715, 0.0463, 0.1499, 0.2033, 0.1498, 0.1452, 0.1341],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2010, 0.0539, 0.1096, 0.0985, 0.3679, 0.0779, 0.0912],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1672, 0.0403, 0.1666, 0.1510, 0.1638, 0.1754, 0.1356],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1533, 0.0750, 0.1217, 0.1973, 0.1580, 0.1246, 0.1702],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  48.978279024351764
maxi score, test score, baseline:  -0.39862000000000003 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.66155129225836
maxi score, test score, baseline:  -0.39862000000000003 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.39862000000000003 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 23.75741481781006
printing an ep nov before normalisation:  45.37754666561051
maxi score, test score, baseline:  -0.39862000000000003 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  55 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.13238347987829
printing an ep nov before normalisation:  30.86252779716182
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.379]
 [0.128]
 [0.369]
 [0.139]
 [0.152]
 [0.225]] [[42.283]
 [43.671]
 [37.006]
 [39.749]
 [39.917]
 [38.811]
 [38.682]] [[1.873]
 [2.007]
 [1.329]
 [1.745]
 [1.527]
 [1.469]
 [1.533]]
printing an ep nov before normalisation:  47.71059128109451
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  40.363666952268744
printing an ep nov before normalisation:  43.26388715999179
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 41.82989274509095
printing an ep nov before normalisation:  25.968267917633057
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.068]
 [-0.069]
 [-0.064]
 [-0.066]
 [-0.066]
 [-0.064]
 [-0.063]] [[17.085]
 [14.626]
 [15.996]
 [16.262]
 [16.539]
 [16.381]
 [16.61 ]] [[0.707]
 [0.594]
 [0.661]
 [0.672]
 [0.684]
 [0.679]
 [0.69 ]]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  0.09999999999999931  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.003]
 [ 0.033]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.056]
 [-0.003]] [[37.754]
 [35.174]
 [37.754]
 [37.754]
 [37.754]
 [34.618]
 [37.754]] [[1.138]
 [1.033]
 [1.138]
 [1.138]
 [1.138]
 [0.914]
 [1.138]]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.317629521544596
siam score:  -0.8410074
printing an ep nov before normalisation:  35.83042085340003
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.26987483241675
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.169]
 [-0.055]
 [-0.119]
 [-0.119]
 [-0.144]
 [-0.141]
 [-0.061]] [[45.117]
 [41.339]
 [47.769]
 [32.141]
 [42.844]
 [32.323]
 [42.202]] [[ 0.065]
 [ 0.149]
 [ 0.137]
 [ 0.01 ]
 [ 0.072]
 [-0.01 ]
 [ 0.15 ]]
printing an ep nov before normalisation:  50.201977630447054
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.42000000000000004  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.154]
 [-0.159]
 [-0.155]
 [-0.157]
 [-0.156]
 [-0.155]
 [-0.158]] [[32.497]
 [33.2  ]
 [28.422]
 [28.237]
 [32.36 ]
 [31.679]
 [29.929]] [[0.112]
 [0.117]
 [0.052]
 [0.048]
 [0.108]
 [0.1  ]
 [0.072]]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
siam score:  -0.84905106
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.1342974268935
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  31.66698294350941
actor:  1 policy actor:  1  step number:  57 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.570793660445126
printing an ep nov before normalisation:  46.58124959350481
printing an ep nov before normalisation:  49.887315277676365
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.778703333825966
printing an ep nov before normalisation:  50.358155097508956
printing an ep nov before normalisation:  44.60952822066422
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.2985911184752865
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.296]
 [0.318]
 [0.318]
 [0.261]
 [0.318]
 [0.249]] [[41.66 ]
 [42.486]
 [41.66 ]
 [41.66 ]
 [40.038]
 [41.66 ]
 [39.265]] [[0.961]
 [0.963]
 [0.961]
 [0.961]
 [0.857]
 [0.961]
 [0.822]]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  36.2841657150031
printing an ep nov before normalisation:  39.68750074320386
siam score:  -0.8437702
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.897074095377901
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.468]
 [0.584]] [[39.954]
 [39.954]
 [39.954]
 [39.954]
 [39.954]
 [42.002]
 [39.954]] [[0.844]
 [0.844]
 [0.844]
 [0.844]
 [0.844]
 [0.75 ]
 [0.844]]
actor:  1 policy actor:  1  step number:  65 total reward:  0.06666666666666587  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
actor:  1 policy actor:  1  step number:  53 total reward:  0.3333333333333328  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
UNIT TEST: sample policy line 217 mcts : [0.061 0.265 0.122 0.041 0.041 0.122 0.347]
printing an ep nov before normalisation:  43.62335016590566
printing an ep nov before normalisation:  32.53054390803046
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.80872314133154
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.06 ]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]
 [-0.063]] [[38.852]
 [33.83 ]
 [33.83 ]
 [33.83 ]
 [33.83 ]
 [33.83 ]
 [33.83 ]] [[1.377]
 [1.091]
 [1.091]
 [1.091]
 [1.091]
 [1.091]
 [1.091]]
actions average: 
K:  0  action  0 :  tensor([0.3529, 0.0090, 0.1157, 0.1238, 0.1566, 0.1272, 0.1148],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0023,     0.9694,     0.0021,     0.0073,     0.0006,     0.0010,
            0.0173], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1073, 0.0111, 0.4604, 0.1078, 0.1099, 0.1092, 0.0944],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1310, 0.0265, 0.1133, 0.3662, 0.1173, 0.1116, 0.1341],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1586, 0.0082, 0.1098, 0.1244, 0.3694, 0.1172, 0.1124],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1388, 0.0049, 0.1498, 0.1460, 0.1287, 0.3424, 0.0894],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1107, 0.1594, 0.1013, 0.1528, 0.1075, 0.1076, 0.2606],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
printing an ep nov before normalisation:  44.7392136207276
printing an ep nov before normalisation:  33.11150090208515
printing an ep nov before normalisation:  27.296423881060782
printing an ep nov before normalisation:  20.659465789794922
printing an ep nov before normalisation:  41.3304385093003
actor:  1 policy actor:  1  step number:  50 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85355455
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.030663858902194
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.26265884970511
printing an ep nov before normalisation:  40.482436841995
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.37730407714844
printing an ep nov before normalisation:  42.18924451281362
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.379533326285745
actor:  1 policy actor:  1  step number:  61 total reward:  0.06666666666666587  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.551]
 [0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]] [[26.978]
 [31.189]
 [26.978]
 [26.978]
 [26.978]
 [26.978]
 [26.978]] [[0.82 ]
 [1.038]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]]
printing an ep nov before normalisation:  32.57328011472492
printing an ep nov before normalisation:  24.110627443297474
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.2688, 0.0023, 0.1333, 0.1061, 0.1831, 0.1362, 0.1702],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0084, 0.9494, 0.0055, 0.0071, 0.0052, 0.0054, 0.0191],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1121, 0.1453, 0.2455, 0.1113, 0.0904, 0.1970, 0.0985],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1378, 0.0656, 0.1339, 0.2733, 0.1089, 0.1325, 0.1479],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1440, 0.0780, 0.1003, 0.1041, 0.3574, 0.1035, 0.1127],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1288, 0.0120, 0.1245, 0.0896, 0.0942, 0.4543, 0.0965],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1136, 0.1696, 0.1172, 0.1381, 0.1167, 0.1371, 0.2077],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]] [[32.461]
 [32.461]
 [32.461]
 [32.461]
 [32.461]
 [32.461]
 [32.461]] [[0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
printing an ep nov before normalisation:  50.41727759060022
printing an ep nov before normalisation:  39.46741219557659
printing an ep nov before normalisation:  44.1241913212972
printing an ep nov before normalisation:  35.029823780059814
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.583733558654785
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.58263714569457
printing an ep nov before normalisation:  30.483283092557713
actor:  1 policy actor:  1  step number:  60 total reward:  0.09999999999999942  reward:  1.0 rdn_beta:  1.333
siam score:  -0.84653777
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.2847, 0.0843, 0.1568, 0.1168, 0.1256, 0.1122, 0.1196],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0202, 0.9251, 0.0119, 0.0084, 0.0054, 0.0045, 0.0246],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1519, 0.0061, 0.2111, 0.1496, 0.1685, 0.2092, 0.1035],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1250, 0.0754, 0.1172, 0.3840, 0.1032, 0.0954, 0.0998],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1820, 0.1882, 0.0980, 0.0838, 0.2674, 0.0883, 0.0925],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2201, 0.0084, 0.1675, 0.1610, 0.1411, 0.1648, 0.1371],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1586, 0.0599, 0.1360, 0.1553, 0.1232, 0.1188, 0.2482],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.745]
 [0.726]
 [0.597]
 [0.593]
 [0.715]
 [0.726]] [[36.196]
 [35.339]
 [37.594]
 [35.541]
 [37.1  ]
 [36.34 ]
 [37.698]] [[2.126]
 [2.118]
 [2.228]
 [1.982]
 [2.067]
 [2.146]
 [2.234]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
printing an ep nov before normalisation:  35.4646395126971
actor:  1 policy actor:  1  step number:  50 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.520148079557988
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.228809246291966
printing an ep nov before normalisation:  42.42256255433471
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  46.077521672186066
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84070647
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  40 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.139]
 [0.193]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.139]
 [0.193]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]]
printing an ep nov before normalisation:  37.850425207765326
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  0.7200000000000002  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.08813364082430933, 0.08813364082430933, 0.08813364082430933, 0.5593317958784533, 0.08813364082430933, 0.08813364082430933]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.08813364082430933, 0.08813364082430933, 0.08813364082430933, 0.5593317958784533, 0.08813364082430933, 0.08813364082430933]
actor:  1 policy actor:  1  step number:  59 total reward:  0.1999999999999993  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.52010092595772
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.08815028243922839, 0.08815028243922839, 0.08815028243922839, 0.559248587803858, 0.08815028243922839, 0.08815028243922839]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.08815028243922839, 0.08815028243922839, 0.08815028243922839, 0.559248587803858, 0.08815028243922839, 0.08815028243922839]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.08815028243922839, 0.08815028243922839, 0.08815028243922839, 0.559248587803858, 0.08815028243922839, 0.08815028243922839]
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.881919557750884
printing an ep nov before normalisation:  40.14851759680436
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.08816691700272748, 0.08816691700272748, 0.08816691700272748, 0.5591654149863627, 0.08816691700272748, 0.08816691700272748]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.08816691700272748, 0.08816691700272748, 0.08816691700272748, 0.5591654149863627, 0.08816691700272748, 0.08816691700272748]
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.08816691700272748, 0.08816691700272748, 0.08816691700272748, 0.5591654149863627, 0.08816691700272748, 0.08816691700272748]
actor:  1 policy actor:  1  step number:  47 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.08816691700272748, 0.08816691700272748, 0.08816691700272748, 0.5591654149863627, 0.08816691700272748, 0.08816691700272748]
printing an ep nov before normalisation:  31.342578541370692
printing an ep nov before normalisation:  30.215083939629324
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.031041145324707
maxi score, test score, baseline:  -0.3961133333333334 0.6873333333333335 0.6873333333333335
probs:  [0.08816691700272748, 0.08816691700272748, 0.08816691700272748, 0.5591654149863627, 0.08816691700272748, 0.08816691700272748]
actor:  1 policy actor:  1  step number:  56 total reward:  0.3133333333333327  reward:  1.0 rdn_beta:  1.333
from probs:  [0.08816691700272748, 0.08816691700272748, 0.08816691700272748, 0.5591654149863627, 0.08816691700272748, 0.08816691700272748]
printing an ep nov before normalisation:  40.67699346973102
Starting evaluation
printing an ep nov before normalisation:  24.23569361368815
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  46.427607123252294
printing an ep nov before normalisation:  38.942338259288825
actor:  0 policy actor:  1  step number:  44 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.39330000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.08818354451928738, 0.08818354451928738, 0.08818354451928738, 0.5590822774035632, 0.08818354451928738, 0.08818354451928738]
printing an ep nov before normalisation:  39.0828275680542
maxi score, test score, baseline:  -0.39330000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.08818354451928738, 0.08818354451928738, 0.08818354451928738, 0.5590822774035632, 0.08818354451928738, 0.08818354451928738]
Printing some Q and Qe and total Qs values:  [[-0.041]
 [-0.037]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.042]
 [-0.041]] [[45.319]
 [43.941]
 [45.319]
 [45.319]
 [44.81 ]
 [45.117]
 [45.319]] [[0.566]
 [0.534]
 [0.566]
 [0.566]
 [0.552]
 [0.56 ]
 [0.566]]
printing an ep nov before normalisation:  35.43317182543261
maxi score, test score, baseline:  -0.39330000000000004 0.6873333333333335 0.6873333333333335
probs:  [0.08818354451928738, 0.08818354451928738, 0.08818354451928738, 0.5590822774035632, 0.08818354451928738, 0.08818354451928738]
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  32.707046620577785
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.37623333333333336 0.6873333333333335 0.6873333333333335
probs:  [0.08818354451928738, 0.08818354451928738, 0.08818354451928738, 0.5590822774035632, 0.08818354451928738, 0.08818354451928738]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  28 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  29 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.36623333333333347 0.6873333333333335 0.6873333333333335
probs:  [0.08818354451928738, 0.08818354451928738, 0.08818354451928738, 0.5590822774035632, 0.08818354451928738, 0.08818354451928738]
maxi score, test score, baseline:  -0.36623333333333347 0.6873333333333335 0.6873333333333335
probs:  [0.08818354451928738, 0.08818354451928738, 0.08818354451928738, 0.5590822774035632, 0.08818354451928738, 0.08818354451928738]
actor:  0 policy actor:  0  step number:  32 total reward:  0.54  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  33 total reward:  0.52  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  33 total reward:  0.52  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  36 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  40 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.3512333333333334 0.6873333333333335 0.6873333333333335
printing an ep nov before normalisation:  42.27333166650687
actor:  0 policy actor:  0  step number:  42 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.34855333333333344 0.6873333333333335 0.6873333333333335
probs:  [0.08818354451928738, 0.08818354451928738, 0.08818354451928738, 0.5590822774035632, 0.08818354451928738, 0.08818354451928738]
Printing some Q and Qe and total Qs values:  [[-0.037]
 [-0.008]
 [-0.048]
 [-0.045]
 [-0.042]
 [-0.043]
 [-0.044]] [[18.767]
 [33.551]
 [18.493]
 [18.405]
 [18.399]
 [18.299]
 [18.286]] [[0.2  ]
 [0.586]
 [0.183]
 [0.184]
 [0.186]
 [0.183]
 [0.182]]
maxi score, test score, baseline:  -0.34855333333333344 0.6873333333333335 0.6873333333333335
probs:  [0.08818354451928738, 0.08818354451928738, 0.08818354451928738, 0.5590822774035632, 0.08818354451928738, 0.08818354451928738]
actor:  0 policy actor:  0  step number:  57 total reward:  0.03999999999999926  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.3464733333333334 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.3464733333333334 0.5936666666666667 0.5936666666666667
probs:  [0.0834670753545889, 0.0834670753545889, 0.0834670753545889, 0.579795083640661, 0.0834670753545889, 0.08633661494098349]
maxi score, test score, baseline:  -0.3464733333333334 0.5936666666666667 0.5936666666666667
probs:  [0.08346744063610732, 0.08346744063610732, 0.08346744063610732, 0.5797828193846783, 0.08346744063610732, 0.08634741807089241]
maxi score, test score, baseline:  -0.3464733333333334 0.5936666666666667 0.5936666666666667
probs:  [0.08346744063610732, 0.08346744063610732, 0.08346744063610732, 0.5797828193846783, 0.08346744063610732, 0.08634741807089241]
printing an ep nov before normalisation:  20.957131385803223
maxi score, test score, baseline:  -0.3464733333333334 0.5936666666666667 0.5936666666666667
probs:  [0.08346744063610732, 0.08346744063610732, 0.08346744063610732, 0.5797828193846783, 0.08346744063610732, 0.08634741807089241]
printing an ep nov before normalisation:  36.567423727020326
maxi score, test score, baseline:  -0.3464733333333334 0.5936666666666667 0.5936666666666667
probs:  [0.08346744063610732, 0.08346744063610732, 0.08346744063610732, 0.5797828193846783, 0.08346744063610732, 0.08634741807089241]
siam score:  -0.85445315
siam score:  -0.85530806
maxi score, test score, baseline:  -0.3464733333333334 0.5936666666666667 0.5936666666666667
actor:  1 policy actor:  1  step number:  51 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  1.333
from probs:  [0.08346744063610732, 0.08346744063610732, 0.08346744063610732, 0.5797828193846783, 0.08346744063610732, 0.08634741807089241]
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.459]
 [0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.219]] [[40.779]
 [44.955]
 [40.779]
 [40.779]
 [40.779]
 [40.779]
 [40.779]] [[1.492]
 [1.973]
 [1.492]
 [1.492]
 [1.492]
 [1.492]
 [1.492]]
siam score:  -0.8548882
maxi score, test score, baseline:  -0.3464733333333334 0.5936666666666667 0.5936666666666667
probs:  [0.08346780589903634, 0.08346780589903634, 0.08346780589903634, 0.5797705557528311, 0.08346780589903634, 0.08635822065102347]
printing an ep nov before normalisation:  42.99944696426687
using another actor
printing an ep nov before normalisation:  12.458676169417462
maxi score, test score, baseline:  -0.3464733333333334 0.5936666666666667 0.5936666666666667
probs:  [0.08346780589903634, 0.08346780589903634, 0.08346780589903634, 0.5797705557528311, 0.08346780589903634, 0.08635822065102347]
actor:  1 policy actor:  1  step number:  52 total reward:  0.23333333333333317  reward:  1.0 rdn_beta:  2.0
siam score:  -0.85541564
printing an ep nov before normalisation:  42.46063336219928
maxi score, test score, baseline:  -0.3464733333333334 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.3464733333333334 0.5936666666666667 0.5936666666666667
probs:  [0.08346780589903634, 0.08346780589903634, 0.08346780589903634, 0.5797705557528311, 0.08346780589903634, 0.08635822065102347]
actor:  0 policy actor:  1  step number:  52 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  27.54962682723999
printing an ep nov before normalisation:  24.09382229694756
printing an ep nov before normalisation:  34.22710590949553
printing an ep nov before normalisation:  31.931835817999804
actor:  1 policy actor:  1  step number:  52 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[ 0.081]
 [ 0.231]
 [ 0.081]
 [ 0.081]
 [-0.002]
 [ 0.081]
 [ 0.081]] [[33.553]
 [38.544]
 [33.553]
 [33.553]
 [32.02 ]
 [33.553]
 [33.553]] [[0.804]
 [1.161]
 [0.804]
 [0.804]
 [0.658]
 [0.804]
 [0.804]]
maxi score, test score, baseline:  -0.3438733333333334 0.5936666666666667 0.5936666666666667
probs:  [0.08346780589903634, 0.08346780589903634, 0.08346780589903634, 0.5797705557528311, 0.08346780589903634, 0.08635822065102347]
maxi score, test score, baseline:  -0.3438733333333334 0.5936666666666667 0.5936666666666667
probs:  [0.08346780589903634, 0.08346780589903634, 0.08346780589903634, 0.5797705557528311, 0.08346780589903634, 0.08635822065102347]
Printing some Q and Qe and total Qs values:  [[0.16 ]
 [0.481]
 [0.16 ]
 [0.16 ]
 [0.387]
 [0.16 ]
 [0.16 ]] [[39.742]
 [42.234]
 [39.742]
 [39.742]
 [42.456]
 [39.742]
 [39.742]] [[1.031]
 [1.46 ]
 [1.031]
 [1.031]
 [1.375]
 [1.031]
 [1.031]]
maxi score, test score, baseline:  -0.3438733333333334 0.5936666666666667 0.5936666666666667
probs:  [0.0834681711433774, 0.0834681711433774, 0.0834681711433774, 0.5797582927450717, 0.0834681711433774, 0.08636902268141865]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.333
from probs:  [0.0834681711433774, 0.0834681711433774, 0.0834681711433774, 0.5797582927450717, 0.0834681711433774, 0.08636902268141865]
printing an ep nov before normalisation:  35.44745954522104
siam score:  -0.85305434
maxi score, test score, baseline:  -0.3438733333333334 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.3438733333333334 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.3438733333333334 0.5936666666666667 0.5936666666666667
probs:  [0.08346853636913187, 0.08346853636913187, 0.08346853636913187, 0.5797460303613523, 0.08346853636913187, 0.08637982416212017]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  0.2933333333333331  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3438733333333334 0.5936666666666667 0.5936666666666667
probs:  [0.08346926676488685, 0.08346926676488685, 0.08346926676488685, 0.5797215074658435, 0.08346926676488685, 0.08640142547460919]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.3438733333333334 0.5936666666666667 0.5936666666666667
actions average: 
K:  4  action  0 :  tensor([0.2490, 0.0125, 0.1342, 0.1280, 0.1779, 0.1372, 0.1612],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0179, 0.8888, 0.0178, 0.0244, 0.0075, 0.0109, 0.0326],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1351, 0.0104, 0.2802, 0.1378, 0.1138, 0.1959, 0.1268],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0939, 0.0537, 0.1325, 0.3663, 0.0562, 0.1319, 0.1655],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1263, 0.0820, 0.1033, 0.1094, 0.3363, 0.1199, 0.1227],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1817, 0.0076, 0.1877, 0.1508, 0.1325, 0.2029, 0.1368],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1147, 0.2087, 0.1272, 0.1292, 0.1138, 0.1508, 0.1555],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.3438733333333334 0.5936666666666667 0.5936666666666667
probs:  [0.08346963193489017, 0.08346963193489017, 0.08346963193489017, 0.5797092469539586, 0.08346963193489017, 0.0864122253064808]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.3438733333333334 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.3438733333333334 0.5936666666666667 0.5936666666666667
probs:  [0.08346963193489017, 0.08346963193489017, 0.08346963193489017, 0.5797092469539586, 0.08346963193489017, 0.0864122253064808]
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.75 ]
 [0.565]
 [0.562]
 [0.472]
 [0.537]
 [0.692]] [[30.836]
 [33.99 ]
 [27.931]
 [29.127]
 [31.112]
 [29.986]
 [35.774]] [[0.328]
 [0.75 ]
 [0.565]
 [0.562]
 [0.472]
 [0.537]
 [0.692]]
printing an ep nov before normalisation:  31.243064496448817
actor:  0 policy actor:  1  step number:  71 total reward:  0.07999999999999885  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  59.59448783358654
printing an ep nov before normalisation:  19.38317775847635
Printing some Q and Qe and total Qs values:  [[0.057]
 [0.218]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.057]
 [0.218]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]]
maxi score, test score, baseline:  -0.3417133333333334 0.5936666666666667 0.5936666666666667
probs:  [0.08346999708631261, 0.08346999708631261, 0.08346999708631261, 0.5796969870659235, 0.08346999708631261, 0.08642302458882602]
maxi score, test score, baseline:  -0.3417133333333334 0.5936666666666667 0.5936666666666667
probs:  [0.08346999708631261, 0.08346999708631261, 0.08346999708631261, 0.5796969870659235, 0.08346999708631261, 0.08642302458882602]
using explorer policy with actor:  1
printing an ep nov before normalisation:  27.21338947703792
printing an ep nov before normalisation:  40.882629378866994
printing an ep nov before normalisation:  25.63868150362923
maxi score, test score, baseline:  -0.3417133333333334 0.5936666666666667 0.5936666666666667
probs:  [0.08347036221915557, 0.08347036221915557, 0.08347036221915557, 0.5796847278016902, 0.08347036221915557, 0.0864338233216874]
actor:  1 policy actor:  1  step number:  53 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  36.15299572895944
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([0.4239, 0.0151, 0.0944, 0.1047, 0.1776, 0.0918, 0.0925],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0054, 0.9633, 0.0068, 0.0055, 0.0024, 0.0032, 0.0134],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1375, 0.0090, 0.3268, 0.1302, 0.1251, 0.1944, 0.0770],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1455, 0.0170, 0.1124, 0.3748, 0.1227, 0.1175, 0.1101],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1612, 0.0039, 0.1113, 0.1144, 0.3992, 0.1142, 0.0959],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2373, 0.0053, 0.1568, 0.1428, 0.1176, 0.2399, 0.1003],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1498, 0.1523, 0.1201, 0.1296, 0.1220, 0.1180, 0.2083],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  28.179402351379395
actor:  1 policy actor:  1  step number:  55 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3417133333333334 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.3417133333333334 0.5936666666666667 0.5936666666666667
probs:  [0.08347218760472803, 0.08347218760472803, 0.08347218760472803, 0.5796234408358883, 0.08347218760472803, 0.08648780874519953]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.56437649964505
maxi score, test score, baseline:  -0.3417133333333334 0.5936666666666667 0.5936666666666667
probs:  [0.08347255262612395, 0.08347255262612395, 0.08347255262612395, 0.5796111853134676, 0.08347255262612395, 0.08649860418203652]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3417133333333334 0.5936666666666667 0.5936666666666667
using explorer policy with actor:  1
siam score:  -0.85637444
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  62 total reward:  0.25999999999999934  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  29.878264665603638
maxi score, test score, baseline:  -0.3417133333333334 0.5936666666666667 0.5936666666666667
probs:  [0.08347255262612395, 0.08347255262612395, 0.08347255262612395, 0.5796111853134676, 0.08347255262612395, 0.08649860418203652]
printing an ep nov before normalisation:  48.77235939742694
maxi score, test score, baseline:  -0.3417133333333334 0.5936666666666667 0.5936666666666667
probs:  [0.08347255262612395, 0.08347255262612395, 0.08347255262612395, 0.5796111853134676, 0.08347255262612395, 0.08649860418203652]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2733333333333331  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3417133333333334 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.3417133333333334 0.5936666666666667 0.5936666666666667
probs:  [0.08347291762895034, 0.08347291762895034, 0.08347291762895034, 0.5795989304145156, 0.08347291762895034, 0.08650939906968293]
printing an ep nov before normalisation:  36.93646154574375
printing an ep nov before normalisation:  27.131296100995286
maxi score, test score, baseline:  -0.3417133333333334 0.5936666666666667 0.5936666666666667
probs:  [0.08347291762895034, 0.08347291762895034, 0.08347291762895034, 0.5795989304145156, 0.08347291762895034, 0.08650939906968293]
printing an ep nov before normalisation:  18.551647663116455
siam score:  -0.85514474
actor:  1 policy actor:  1  step number:  59 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3417133333333335 0.5936666666666667 0.5936666666666667
probs:  [0.08347328261320863, 0.08347328261320863, 0.08347328261320863, 0.579586676138985, 0.08347328261320863, 0.08652019340818057]
maxi score, test score, baseline:  -0.3417133333333335 0.5936666666666667 0.5936666666666667
probs:  [0.08347328261320863, 0.08347328261320863, 0.08347328261320863, 0.579586676138985, 0.08347328261320863, 0.08652019340818057]
maxi score, test score, baseline:  -0.3417133333333335 0.5936666666666667 0.5936666666666667
probs:  [0.08347328261320863, 0.08347328261320863, 0.08347328261320863, 0.579586676138985, 0.08347328261320863, 0.08652019340818057]
maxi score, test score, baseline:  -0.3417133333333335 0.5936666666666667 0.5936666666666667
probs:  [0.08347328261320863, 0.08347328261320863, 0.08347328261320863, 0.579586676138985, 0.08347328261320863, 0.08652019340818057]
maxi score, test score, baseline:  -0.3417133333333335 0.5936666666666667 0.5936666666666667
printing an ep nov before normalisation:  48.82180441750994
actor:  1 policy actor:  1  step number:  54 total reward:  0.16666666666666607  reward:  1.0 rdn_beta:  1.667
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.3794],
        [ 0.8179],
        [-0.2924],
        [-0.0000],
        [-0.2490],
        [-0.1906],
        [-0.0802],
        [ 0.3414],
        [ 0.0441],
        [-0.0000]], dtype=torch.float64)
-0.083839701198 0.29558873962742954
-0.058614567066 0.7593134374575614
-0.032346567066 -0.3247470109204382
-0.9503999999999999 -0.9503999999999999
-0.045546567066 -0.2945511496196559
-0.09703970119800001 -0.28759400644200417
-0.045026434398 -0.12521809919886745
-0.04528388706599999 0.29608759460706124
-0.045546567066 -0.0014343638831567987
-0.9605640000000001 -0.9605640000000001
printing an ep nov before normalisation:  23.631326247040164
printing an ep nov before normalisation:  30.84335431253696
maxi score, test score, baseline:  -0.3417133333333335 0.5936666666666667 0.5936666666666667
probs:  [0.08347328261320863, 0.08347328261320863, 0.08347328261320863, 0.579586676138985, 0.08347328261320863, 0.08652019340818057]
maxi score, test score, baseline:  -0.3417133333333335 0.5936666666666667 0.5936666666666667
probs:  [0.08347328261320863, 0.08347328261320863, 0.08347328261320863, 0.579586676138985, 0.08347328261320863, 0.08652019340818057]
maxi score, test score, baseline:  -0.3417133333333335 0.5936666666666667 0.5936666666666667
probs:  [0.08347328261320863, 0.08347328261320863, 0.08347328261320863, 0.579586676138985, 0.08347328261320863, 0.08652019340818057]
maxi score, test score, baseline:  -0.3417133333333335 0.5936666666666667 0.5936666666666667
actor:  1 policy actor:  1  step number:  50 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.3417133333333335 0.5936666666666667 0.5936666666666667
probs:  [0.08347328261320863, 0.08347328261320863, 0.08347328261320863, 0.579586676138985, 0.08347328261320863, 0.08652019340818057]
actor:  1 policy actor:  1  step number:  76 total reward:  0.019999999999998685  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3417133333333335 0.5936666666666667 0.5936666666666667
probs:  [0.08347328261320863, 0.08347328261320863, 0.08347328261320863, 0.579586676138985, 0.08347328261320863, 0.08652019340818057]
maxi score, test score, baseline:  -0.3417133333333335 0.5936666666666667 0.5936666666666667
probs:  [0.08347328261320863, 0.08347328261320863, 0.08347328261320863, 0.579586676138985, 0.08347328261320863, 0.08652019340818057]
printing an ep nov before normalisation:  37.989219107174705
maxi score, test score, baseline:  -0.3417133333333335 0.5936666666666667 0.5936666666666667
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.606]
 [0.506]
 [0.506]
 [0.55 ]
 [0.506]
 [0.527]] [[23.325]
 [34.864]
 [20.346]
 [20.158]
 [30.764]
 [20.292]
 [27.054]] [[0.514]
 [0.606]
 [0.506]
 [0.506]
 [0.55 ]
 [0.506]
 [0.527]]
printing an ep nov before normalisation:  55.75535662800912
maxi score, test score, baseline:  -0.3417133333333335 0.5936666666666667 0.5936666666666667
probs:  [0.08347328261320863, 0.08347328261320863, 0.08347328261320863, 0.579586676138985, 0.08347328261320863, 0.08652019340818057]
maxi score, test score, baseline:  -0.3417133333333335 0.5936666666666667 0.5936666666666667
probs:  [0.08347328261320863, 0.08347328261320863, 0.08347328261320863, 0.579586676138985, 0.08347328261320863, 0.08652019340818057]
printing an ep nov before normalisation:  30.64092666139237
actor:  1 policy actor:  1  step number:  54 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3417133333333335 0.5936666666666667 0.5936666666666667
probs:  [0.08347364757890018, 0.08347364757890018, 0.08347364757890018, 0.579574422486828, 0.08347364757890018, 0.08653098719757127]
actor:  0 policy actor:  0  step number:  48 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.3392200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08347401252602643, 0.08347401252602643, 0.08347401252602643, 0.5795621694579971, 0.08347401252602643, 0.08654178043789713]
printing an ep nov before normalisation:  31.829690979485036
printing an ep nov before normalisation:  38.19762961521381
maxi score, test score, baseline:  -0.3392200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08347401252602643, 0.08347401252602643, 0.08347401252602643, 0.5795621694579971, 0.08347401252602643, 0.08654178043789713]
maxi score, test score, baseline:  -0.3392200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08347401252602643, 0.08347401252602643, 0.08347401252602643, 0.5795621694579971, 0.08347401252602643, 0.08654178043789713]
maxi score, test score, baseline:  -0.3392200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08347401252602643, 0.08347401252602643, 0.08347401252602643, 0.5795621694579971, 0.08347401252602643, 0.08654178043789713]
printing an ep nov before normalisation:  23.185717203127787
printing an ep nov before normalisation:  30.37431240081787
actor:  1 policy actor:  1  step number:  45 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  67 total reward:  0.10666666666666647  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08347474236458875, 0.08347474236458875, 0.08347474236458875, 0.5795376652701236, 0.08347474236458875, 0.08656336527152134]
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08347474236458875, 0.08347474236458875, 0.08347474236458875, 0.5795376652701236, 0.08347474236458875, 0.08656336527152134]
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
actor:  1 policy actor:  1  step number:  61 total reward:  0.06666666666666599  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08347474236458875, 0.08347474236458875, 0.08347474236458875, 0.5795376652701236, 0.08347474236458875, 0.08656336527152134]
actor:  1 policy actor:  1  step number:  64 total reward:  0.03333333333333277  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08347474236458875, 0.08347474236458875, 0.08347474236458875, 0.5795376652701236, 0.08347474236458875, 0.08656336527152134]
line 256 mcts: sample exp_bonus 39.09834957830584
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08347474236458875, 0.08347474236458875, 0.08347474236458875, 0.5795376652701236, 0.08347474236458875, 0.08656336527152134]
actor:  1 policy actor:  1  step number:  60 total reward:  0.11333333333333295  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08347510725602764, 0.08347510725602764, 0.08347510725602764, 0.5795254141109859, 0.08347510725602764, 0.08657415686490368]
actor:  1 policy actor:  1  step number:  67 total reward:  0.0666666666666661  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  40.57829354119072
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
printing an ep nov before normalisation:  40.39186323414739
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08347547212890688, 0.08347547212890688, 0.08347547212890688, 0.579513163574984, 0.08347547212890688, 0.08658494790938864]
printing an ep nov before normalisation:  31.52341751530954
printing an ep nov before normalisation:  43.16305995746913
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08347547212890688, 0.08347547212890688, 0.08347547212890688, 0.579513163574984, 0.08347547212890688, 0.08658494790938864]
line 256 mcts: sample exp_bonus 46.80045713770034
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08347547212890688, 0.08347547212890688, 0.08347547212890688, 0.579513163574984, 0.08347547212890688, 0.08658494790938864]
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08347547212890688, 0.08347547212890688, 0.08347547212890688, 0.579513163574984, 0.08347547212890688, 0.08658494790938864]
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]] [[30.418]
 [30.418]
 [30.418]
 [30.418]
 [30.418]
 [30.418]
 [30.418]] [[1.457]
 [1.457]
 [1.457]
 [1.457]
 [1.457]
 [1.457]
 [1.457]]
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08347547212890688, 0.08347547212890688, 0.08347547212890688, 0.579513163574984, 0.08347547212890688, 0.08658494790938864]
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08347547212890688, 0.08347547212890688, 0.08347547212890688, 0.579513163574984, 0.08347547212890688, 0.08658494790938864]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  64 total reward:  0.08666666666666611  reward:  1.0 rdn_beta:  0.333
using another actor
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08347583698322793, 0.08347583698322793, 0.08347583698322793, 0.5795009136620705, 0.08347583698322793, 0.08659573840501777]
printing an ep nov before normalisation:  22.746247827306515
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08347583698322793, 0.08347583698322793, 0.08347583698322793, 0.5795009136620705, 0.08347583698322793, 0.08659573840501777]
printing an ep nov before normalisation:  47.65916064089519
printing an ep nov before normalisation:  26.073203086853027
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08347583698322793, 0.08347583698322793, 0.08347583698322793, 0.5795009136620705, 0.08347583698322793, 0.08659573840501777]
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
actor:  1 policy actor:  1  step number:  55 total reward:  0.23999999999999977  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3370066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08347620181899215, 0.08347620181899215, 0.08347620181899215, 0.5794886643721979, 0.08347620181899215, 0.08660652835183354]
maxi score, test score, baseline:  -0.3370066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08347656663620104, 0.08347656663620104, 0.08347656663620104, 0.5794764157053186, 0.08347656663620104, 0.08661731774987752]
printing an ep nov before normalisation:  30.834520123681965
actions average: 
K:  3  action  0 :  tensor([0.3850, 0.0640, 0.1063, 0.1157, 0.1232, 0.1197, 0.0860],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0069, 0.9672, 0.0044, 0.0045, 0.0016, 0.0024, 0.0131],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2294, 0.1243, 0.1231, 0.1292, 0.1459, 0.1328, 0.1152],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1785, 0.1337, 0.1395, 0.2150, 0.1038, 0.1342, 0.0952],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2192, 0.0266, 0.1332, 0.1333, 0.2523, 0.1405, 0.0948],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1395, 0.0350, 0.1373, 0.1445, 0.1817, 0.2479, 0.1141],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1246, 0.2322, 0.1428, 0.1380, 0.0999, 0.1049, 0.1576],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.3370066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08347693143485592, 0.08347693143485592, 0.08347693143485592, 0.579464167661385, 0.08347693143485592, 0.08662810659919135]
printing an ep nov before normalisation:  40.001357538280956
maxi score, test score, baseline:  -0.3370066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08347693143485592, 0.08347693143485592, 0.08347693143485592, 0.579464167661385, 0.08347693143485592, 0.08662810659919135]
maxi score, test score, baseline:  -0.3370066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08347729621495827, 0.08347729621495827, 0.08347729621495827, 0.5794519202403495, 0.08347729621495827, 0.08663889489981731]
actor:  1 policy actor:  1  step number:  89 total reward:  0.0399999999999987  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  29.30537462234497
printing an ep nov before normalisation:  40.72255123514075
siam score:  -0.8454606
maxi score, test score, baseline:  -0.3370066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08347766097650947, 0.08347766097650947, 0.08347766097650947, 0.5794396734421651, 0.08347766097650947, 0.08664968265179714]
maxi score, test score, baseline:  -0.3370066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08347766097650947, 0.08347766097650947, 0.08347766097650947, 0.5794396734421651, 0.08347766097650947, 0.08664968265179714]
maxi score, test score, baseline:  -0.3370066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08347766097650947, 0.08347766097650947, 0.08347766097650947, 0.5794396734421651, 0.08347766097650947, 0.08664968265179714]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.3370066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08347766097650947, 0.08347766097650947, 0.08347766097650947, 0.5794396734421651, 0.08347766097650947, 0.08664968265179714]
maxi score, test score, baseline:  -0.3370066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08347766097650947, 0.08347766097650947, 0.08347766097650947, 0.5794396734421651, 0.08347766097650947, 0.08664968265179714]
printing an ep nov before normalisation:  40.79824476275186
maxi score, test score, baseline:  -0.3370066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08347766097650947, 0.08347766097650947, 0.08347766097650947, 0.5794396734421651, 0.08347766097650947, 0.08664968265179714]
actor:  1 policy actor:  1  step number:  45 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  1.333
siam score:  -0.84459656
maxi score, test score, baseline:  -0.3370066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08347802571951096, 0.08347802571951096, 0.08347802571951096, 0.5794274272667839, 0.08347802571951096, 0.08666046985517241]
maxi score, test score, baseline:  -0.3370066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08347802571951096, 0.08347802571951096, 0.08347802571951096, 0.5794274272667839, 0.08347802571951096, 0.08666046985517241]
actor:  1 policy actor:  1  step number:  65 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3370066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08347839044396413, 0.08347839044396413, 0.08347839044396413, 0.5794151817141581, 0.08347839044396413, 0.08667125650998529]
maxi score, test score, baseline:  -0.3370066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08347839044396413, 0.08347839044396413, 0.08347839044396413, 0.5794151817141581, 0.08347839044396413, 0.08667125650998529]
actions average: 
K:  3  action  0 :  tensor([0.4197, 0.0395, 0.0966, 0.1193, 0.1166, 0.1072, 0.1011],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0214, 0.8523, 0.0367, 0.0162, 0.0068, 0.0120, 0.0547],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1161, 0.0488, 0.3918, 0.1407, 0.0818, 0.1178, 0.1030],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1529, 0.0648, 0.1091, 0.2836, 0.1139, 0.1292, 0.1466],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1762, 0.0147, 0.1101, 0.1794, 0.2727, 0.1328, 0.1142],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1660, 0.0239, 0.1437, 0.1524, 0.1065, 0.2819, 0.1254],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1968, 0.0604, 0.1630, 0.1527, 0.1249, 0.1664, 0.1357],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  36.64314491511026
printing an ep nov before normalisation:  36.29434680611056
actor:  1 policy actor:  1  step number:  54 total reward:  0.3533333333333326  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.3370066666666668 0.5936666666666667 0.5936666666666667
printing an ep nov before normalisation:  36.095326119158855
maxi score, test score, baseline:  -0.3370066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08347839044396413, 0.08347839044396413, 0.08347839044396413, 0.5794151817141581, 0.08347839044396413, 0.08667125650998529]
actions average: 
K:  1  action  0 :  tensor([0.5220, 0.0164, 0.0783, 0.0899, 0.1307, 0.0846, 0.0781],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0108, 0.9270, 0.0113, 0.0088, 0.0024, 0.0044, 0.0354],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1325, 0.0056, 0.3280, 0.1466, 0.1457, 0.1560, 0.0856],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1545, 0.0130, 0.1341, 0.3000, 0.1231, 0.1487, 0.1265],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1543, 0.0016, 0.0990, 0.0937, 0.4828, 0.0951, 0.0735],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1551, 0.0044, 0.1416, 0.1334, 0.1218, 0.3439, 0.0998],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1608, 0.1758, 0.1141, 0.1425, 0.0975, 0.1116, 0.1977],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  64 total reward:  0.17999999999999916  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  4  action  0 :  tensor([0.4629, 0.0160, 0.0835, 0.0900, 0.1867, 0.0936, 0.0673],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0068, 0.8889, 0.0061, 0.0402, 0.0038, 0.0056, 0.0487],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1991, 0.1635, 0.1380, 0.1326, 0.1206, 0.1162, 0.1300],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1223, 0.0933, 0.1132, 0.3182, 0.1262, 0.1304, 0.0964],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1669, 0.1523, 0.0768, 0.1054, 0.3634, 0.0818, 0.0533],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1914, 0.0073, 0.1502, 0.1615, 0.1444, 0.1613, 0.1840],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1464, 0.0890, 0.1250, 0.1693, 0.1212, 0.1350, 0.2142],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  36.6076634179218
printing an ep nov before normalisation:  37.72702298512487
maxi score, test score, baseline:  -0.3370066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08347839044396413, 0.08347839044396413, 0.08347839044396413, 0.5794151817141581, 0.08347839044396413, 0.08667125650998529]
printing an ep nov before normalisation:  43.12156474679613
printing an ep nov before normalisation:  34.681118659587035
maxi score, test score, baseline:  -0.3370066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08347839044396413, 0.08347839044396413, 0.08347839044396413, 0.5794151817141581, 0.08347839044396413, 0.08667125650998529]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3333333333333326  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.3370066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08347839044396413, 0.08347839044396413, 0.08347839044396413, 0.5794151817141581, 0.08347839044396413, 0.08667125650998529]
printing an ep nov before normalisation:  49.61194129445163
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3370066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08347839044396413, 0.08347839044396413, 0.08347839044396413, 0.5794151817141581, 0.08347839044396413, 0.08667125650998529]
actor:  1 policy actor:  1  step number:  44 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3370066666666668 0.5936666666666667 0.5936666666666667
printing an ep nov before normalisation:  47.299784745276646
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.602]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]] [[34.7 ]
 [43.47]
 [34.7 ]
 [34.7 ]
 [34.7 ]
 [34.7 ]
 [34.7 ]] [[1.393]
 [1.851]
 [1.393]
 [1.393]
 [1.393]
 [1.393]
 [1.393]]
Printing some Q and Qe and total Qs values:  [[0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]] [[45.257]
 [45.257]
 [45.257]
 [45.257]
 [45.257]
 [45.257]
 [45.257]] [[2.63]
 [2.63]
 [2.63]
 [2.63]
 [2.63]
 [2.63]
 [2.63]]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.39628685627211
actor:  1 policy actor:  1  step number:  57 total reward:  0.19999999999999973  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.356]
 [0.351]
 [0.349]
 [0.359]
 [0.359]
 [0.352]] [[36.064]
 [35.11 ]
 [28.16 ]
 [28.56 ]
 [27.438]
 [27.299]
 [28.166]] [[1.154]
 [1.143]
 [0.891]
 [0.903]
 [0.873]
 [0.868]
 [0.892]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.3254],
        [-0.0292],
        [ 0.5549],
        [-0.0786],
        [-0.3023],
        [-0.0494],
        [-0.0000],
        [-0.0000],
        [-0.0000],
        [-0.0369]], dtype=torch.float64)
-0.032346567066 -0.3577036358284313
-0.032346567066 -0.0615273289840719
-0.083839701198 0.47109608390498736
-0.070771701198 -0.14936629468298468
-0.032346567066 -0.3346132274223224
-0.09703970119800001 -0.14640725951305475
-0.9768 -0.9768
0.957165 0.957165
0.9249834164999999 0.9249834164999999
-0.032346567066 -0.06926612636825805
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08347875514987041, 0.08347875514987041, 0.08347875514987041, 0.5794029367842406, 0.08347875514987041, 0.08668204261627761]
printing an ep nov before normalisation:  42.77738925316078
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.436]
 [0.178]
 [0.178]
 [0.122]
 [0.178]
 [0.178]] [[35.989]
 [45.519]
 [35.989]
 [35.989]
 [39.95 ]
 [35.989]
 [35.989]] [[0.851]
 [1.45 ]
 [0.851]
 [0.851]
 [0.937]
 [0.851]
 [0.851]]
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.46 ]
 [0.34 ]
 [0.307]
 [0.357]
 [0.34 ]
 [0.34 ]] [[49.201]
 [48.975]
 [43.352]
 [50.693]
 [50.539]
 [43.352]
 [43.352]] [[1.631]
 [1.652]
 [1.322]
 [1.563]
 [1.608]
 [1.322]
 [1.322]]
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.388]
 [0.343]
 [0.313]
 [0.343]
 [0.343]
 [0.33 ]] [[16.904]
 [19.644]
 [17.578]
 [16.625]
 [17.578]
 [17.578]
 [17.362]] [[1.226]
 [1.43 ]
 [1.275]
 [1.195]
 [1.275]
 [1.275]
 [1.251]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08347911983723125, 0.08347911983723125, 0.08347911983723125, 0.579390692476984, 0.08347911983723125, 0.08669282817409082]
printing an ep nov before normalisation:  30.82345485687256
actor:  1 policy actor:  1  step number:  41 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08347948450604802, 0.08347948450604802, 0.08347948450604802, 0.5793784487923408, 0.08347948450604802, 0.08670361318346718]
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
actor:  1 policy actor:  1  step number:  55 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  0.667
siam score:  -0.857404
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08347948450604802, 0.08347948450604802, 0.08347948450604802, 0.5793784487923408, 0.08347948450604802, 0.08670361318346718]
printing an ep nov before normalisation:  29.147736032009732
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08347948450604802, 0.08347948450604802, 0.08347948450604802, 0.5793784487923408, 0.08347948450604802, 0.08670361318346718]
siam score:  -0.85451204
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08347984915632213, 0.08347984915632213, 0.08347984915632213, 0.5793662057302631, 0.08347984915632213, 0.0867143976444484]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
Printing some Q and Qe and total Qs values:  [[-0.124]
 [-0.132]
 [-0.124]
 [-0.125]
 [-0.125]
 [-0.125]
 [-0.125]] [[37.49 ]
 [33.231]
 [36.506]
 [37.68 ]
 [37.45 ]
 [37.221]
 [37.087]] [[1.08 ]
 [0.816]
 [1.021]
 [1.091]
 [1.077]
 [1.064]
 [1.056]]
line 256 mcts: sample exp_bonus 36.30187973034653
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.083480213788055, 0.083480213788055, 0.083480213788055, 0.5793539632907037, 0.083480213788055, 0.08672518155707641]
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.083480213788055, 0.083480213788055, 0.083480213788055, 0.5793539632907037, 0.083480213788055, 0.08672518155707641]
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.083480213788055, 0.083480213788055, 0.083480213788055, 0.5793539632907037, 0.083480213788055, 0.08672518155707641]
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
printing an ep nov before normalisation:  36.13692283630371
actor:  1 policy actor:  1  step number:  58 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
siam score:  -0.8573097
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08348094299590274, 0.08348094299590274, 0.08348094299590274, 0.5793294802789497, 0.08348094299590274, 0.08674674773743934]
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08348094299590274, 0.08348094299590274, 0.08348094299590274, 0.5793294802789497, 0.08348094299590274, 0.08674674773743934]
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08348094299590274, 0.08348094299590274, 0.08348094299590274, 0.5793294802789497, 0.08348094299590274, 0.08674674773743934]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08348094299590274, 0.08348094299590274, 0.08348094299590274, 0.5793294802789497, 0.08348094299590274, 0.08674674773743934]
printing an ep nov before normalisation:  48.78192957290974
line 256 mcts: sample exp_bonus 38.07864778031003
printing an ep nov before normalisation:  29.850109175163414
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.0834813075720204, 0.0834813075720204, 0.0834813075720204, 0.5793172397066602, 0.0834813075720204, 0.08675753000525824]
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.0834813075720204, 0.0834813075720204, 0.0834813075720204, 0.5793172397066602, 0.0834813075720204, 0.08675753000525824]
actor:  1 policy actor:  1  step number:  59 total reward:  0.2533333333333331  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8545361
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.0834813075720204, 0.0834813075720204, 0.0834813075720204, 0.5793172397066602, 0.0834813075720204, 0.08675753000525824]
printing an ep nov before normalisation:  39.53912973403931
printing an ep nov before normalisation:  30.391610490201558
maxi score, test score, baseline:  -0.33700666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.0834813075720204, 0.0834813075720204, 0.0834813075720204, 0.5793172397066602, 0.0834813075720204, 0.08675753000525824]
actor:  0 policy actor:  0  step number:  48 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3342200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.0834813075720204, 0.0834813075720204, 0.0834813075720204, 0.5793172397066602, 0.0834813075720204, 0.08675753000525824]
printing an ep nov before normalisation:  29.535878310952974
Printing some Q and Qe and total Qs values:  [[-0.073]
 [-0.067]
 [-0.066]
 [-0.087]
 [-0.074]
 [-0.066]
 [-0.066]] [[34.687]
 [33.843]
 [34.265]
 [34.707]
 [34.063]
 [34.265]
 [34.265]] [[1.16 ]
 [1.106]
 [1.137]
 [1.148]
 [1.114]
 [1.137]
 [1.137]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  60.67749899920747
maxi score, test score, baseline:  -0.3342200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08348203666865044, 0.08348203666865044, 0.08348203666865044, 0.579292760429019, 0.08348203666865044, 0.08677909289637928]
printing an ep nov before normalisation:  35.502160534777474
printing an ep nov before normalisation:  58.29416890951663
printing an ep nov before normalisation:  48.17371922429103
Printing some Q and Qe and total Qs values:  [[0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]] [[54.542]
 [54.542]
 [54.542]
 [54.542]
 [54.542]
 [54.542]
 [54.542]] [[1.832]
 [1.832]
 [1.832]
 [1.832]
 [1.832]
 [1.832]
 [1.832]]
actor:  1 policy actor:  1  step number:  71 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3342200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08348240118916561, 0.08348240118916561, 0.08348240118916561, 0.5792805217235724, 0.08348240118916561, 0.08678987351976539]
printing an ep nov before normalisation:  45.53926019480934
maxi score, test score, baseline:  -0.3342200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08348240118916561, 0.08348240118916561, 0.08348240118916561, 0.5792805217235724, 0.08348240118916561, 0.08678987351976539]
printing an ep nov before normalisation:  0.0
actor:  1 policy actor:  1  step number:  48 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.3342200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08348240118916561, 0.08348240118916561, 0.08348240118916561, 0.5792805217235724, 0.08348240118916561, 0.08678987351976539]
printing an ep nov before normalisation:  36.37457203161575
printing an ep nov before normalisation:  38.92124360240695
maxi score, test score, baseline:  -0.3342200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08348276569114944, 0.08348276569114944, 0.08348276569114944, 0.5792682836403117, 0.08348276569114944, 0.08680065359509054]
maxi score, test score, baseline:  -0.3342200000000001 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.3342200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08348276569114944, 0.08348276569114944, 0.08348276569114944, 0.5792682836403117, 0.08348276569114944, 0.08680065359509054]
printing an ep nov before normalisation:  46.132163146243506
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.665]
 [0.48 ]
 [0.512]
 [0.523]
 [0.53 ]
 [0.545]] [[35.724]
 [36.063]
 [38.354]
 [37.933]
 [37.712]
 [38.771]
 [36.892]] [[1.683]
 [1.821]
 [1.785]
 [1.79 ]
 [1.787]
 [1.863]
 [1.755]]
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.545]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]] [[25.613]
 [26.021]
 [28.176]
 [28.176]
 [28.176]
 [28.176]
 [28.176]] [[1.6  ]
 [1.679]
 [1.854]
 [1.854]
 [1.854]
 [1.854]
 [1.854]]
printing an ep nov before normalisation:  53.45778407856112
printing an ep nov before normalisation:  44.004366295234156
maxi score, test score, baseline:  -0.3342200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08348276569114944, 0.08348276569114944, 0.08348276569114944, 0.5792682836403117, 0.08348276569114944, 0.08680065359509054]
printing an ep nov before normalisation:  42.55979008458936
actor:  1 policy actor:  1  step number:  62 total reward:  0.12666666666666626  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3342200000000001 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.3342200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08348349463952877, 0.08348349463952877, 0.08348349463952877, 0.5792438093401584, 0.08348349463952877, 0.08682221210172648]
actor:  1 policy actor:  1  step number:  63 total reward:  0.2133333333333325  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3342200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08348385908592709, 0.08348385908592709, 0.08348385908592709, 0.5792315731231712, 0.08348385908592709, 0.08683299053312055]
maxi score, test score, baseline:  -0.3342200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08348385908592709, 0.08348385908592709, 0.08348385908592709, 0.5792315731231712, 0.08348385908592709, 0.08683299053312055]
maxi score, test score, baseline:  -0.3342200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08348385908592709, 0.08348385908592709, 0.08348385908592709, 0.5792315731231712, 0.08348385908592709, 0.08683299053312055]
actor:  1 policy actor:  1  step number:  53 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  44.290295538515515
printing an ep nov before normalisation:  40.900348951512015
printing an ep nov before normalisation:  45.54500934421821
siam score:  -0.86262965
maxi score, test score, baseline:  -0.3342200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.0834842235137997, 0.0834842235137997, 0.0834842235137997, 0.57921933752818, 0.0834842235137997, 0.08684376841662103]
maxi score, test score, baseline:  -0.3342200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.0834842235137997, 0.0834842235137997, 0.0834842235137997, 0.57921933752818, 0.0834842235137997, 0.08684376841662103]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.3342200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.0834842235137997, 0.0834842235137997, 0.0834842235137997, 0.57921933752818, 0.0834842235137997, 0.08684376841662103]
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.555]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]] [[11.077]
 [16.067]
 [11.077]
 [11.077]
 [11.077]
 [11.077]
 [11.077]] [[1.639]
 [2.193]
 [1.639]
 [1.639]
 [1.639]
 [1.639]
 [1.639]]
maxi score, test score, baseline:  -0.3342200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.0834842235137997, 0.0834842235137997, 0.0834842235137997, 0.57921933752818, 0.0834842235137997, 0.08684376841662103]
maxi score, test score, baseline:  -0.3342200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.0834842235137997, 0.0834842235137997, 0.0834842235137997, 0.57921933752818, 0.0834842235137997, 0.08684376841662103]
actor:  1 policy actor:  1  step number:  74 total reward:  0.3399999999999992  reward:  1.0 rdn_beta:  1.667
from probs:  [0.0834842235137997, 0.0834842235137997, 0.0834842235137997, 0.57921933752818, 0.0834842235137997, 0.08684376841662103]
actor:  0 policy actor:  1  step number:  36 total reward:  0.48666666666666647  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.792]
 [0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]] [[33.943]
 [43.86 ]
 [38.08 ]
 [38.08 ]
 [38.08 ]
 [38.08 ]
 [38.08 ]] [[0.719]
 [0.792]
 [0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]]
maxi score, test score, baseline:  -0.33124666666666674 0.5936666666666667 0.5936666666666667
printing an ep nov before normalisation:  37.63945304954367
actor:  0 policy actor:  0  step number:  45 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.3284466666666668 0.5936666666666667 0.5936666666666667
probs:  [0.0834842235137997, 0.0834842235137997, 0.0834842235137997, 0.57921933752818, 0.0834842235137997, 0.08684376841662103]
maxi score, test score, baseline:  -0.3284466666666668 0.5936666666666667 0.5936666666666667
probs:  [0.0834842235137997, 0.0834842235137997, 0.0834842235137997, 0.57921933752818, 0.0834842235137997, 0.08684376841662103]
printing an ep nov before normalisation:  28.220918750814217
actor:  0 policy actor:  0  step number:  50 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3260066666666668 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.3260066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.0834842235137997, 0.0834842235137997, 0.0834842235137997, 0.57921933752818, 0.0834842235137997, 0.08684376841662103]
Printing some Q and Qe and total Qs values:  [[-0.041]
 [-0.028]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]] [[29.89 ]
 [33.488]
 [29.89 ]
 [29.89 ]
 [29.89 ]
 [29.89 ]
 [29.89 ]] [[0.783]
 [0.985]
 [0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]]
printing an ep nov before normalisation:  34.73353344015128
printing an ep nov before normalisation:  43.01166805432348
actor:  1 policy actor:  1  step number:  47 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3260066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08348458792314808, 0.08348458792314808, 0.08348458792314808, 0.579207102555138, 0.08348458792314808, 0.08685454575226964]
maxi score, test score, baseline:  -0.3260066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08348458792314808, 0.08348458792314808, 0.08348458792314808, 0.579207102555138, 0.08348458792314808, 0.08685454575226964]
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.594]
 [0.39 ]
 [0.393]
 [0.393]
 [0.516]
 [0.392]] [[36.664]
 [35.429]
 [34.868]
 [36.387]
 [36.389]
 [34.733]
 [36.682]] [[0.397]
 [0.594]
 [0.39 ]
 [0.393]
 [0.393]
 [0.516]
 [0.392]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3260066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08348495231397356, 0.08348495231397356, 0.08348495231397356, 0.5791948682039971, 0.08348495231397356, 0.08686532254010862]
siam score:  -0.8538327
printing an ep nov before normalisation:  27.548762840862423
printing an ep nov before normalisation:  28.51648942273785
printing an ep nov before normalisation:  41.73512409072938
maxi score, test score, baseline:  -0.3260066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08348495231397356, 0.08348495231397356, 0.08348495231397356, 0.5791948682039971, 0.08348495231397356, 0.08686532254010862]
printing an ep nov before normalisation:  32.329969328803124
maxi score, test score, baseline:  -0.3260066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08348495231397356, 0.08348495231397356, 0.08348495231397356, 0.5791948682039971, 0.08348495231397356, 0.08686532254010862]
maxi score, test score, baseline:  -0.3260066666666668 0.5936666666666667 0.5936666666666667
probs:  [0.08348495231397356, 0.08348495231397356, 0.08348495231397356, 0.5791948682039971, 0.08348495231397356, 0.08686532254010862]
actor:  0 policy actor:  0  step number:  56 total reward:  0.09999999999999942  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  34.28064638452869
maxi score, test score, baseline:  -0.32380666666666674 0.5936666666666667 0.5936666666666667
probs:  [0.08348495231397356, 0.08348495231397356, 0.08348495231397356, 0.5791948682039971, 0.08348495231397356, 0.08686532254010862]
actor:  1 policy actor:  1  step number:  51 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.98038787941403
maxi score, test score, baseline:  -0.32380666666666674 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.32380666666666674 0.5936666666666667 0.5936666666666667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.526]
 [0.422]
 [0.395]
 [0.429]
 [0.422]
 [0.393]] [[52.155]
 [50.69 ]
 [49.343]
 [51.463]
 [51.727]
 [49.343]
 [52.674]] [[1.509]
 [1.612]
 [1.466]
 [1.505]
 [1.547]
 [1.466]
 [1.541]]
maxi score, test score, baseline:  -0.32380666666666674 0.5936666666666667 0.5936666666666667
probs:  [0.08348604537532701, 0.08348604537532701, 0.08348604537532701, 0.5791581688815093, 0.08348604537532701, 0.08689764961718262]
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.433]
 [0.472]
 [0.472]
 [0.472]
 [0.519]
 [0.472]] [[19.563]
 [24.627]
 [19.563]
 [19.563]
 [19.563]
 [26.08 ]
 [19.563]] [[0.774]
 [1.128]
 [0.774]
 [0.774]
 [0.774]
 [1.327]
 [0.774]]
actor:  1 policy actor:  1  step number:  63 total reward:  0.1599999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.32380666666666674 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.32380666666666674 0.5936666666666667 0.5936666666666667
probs:  [0.08348640969207519, 0.08348640969207519, 0.08348640969207519, 0.5791459370175001, 0.08348640969207519, 0.08690842421419905]
maxi score, test score, baseline:  -0.32380666666666674 0.5936666666666667 0.5936666666666667
probs:  [0.08348640969207519, 0.08348640969207519, 0.08348640969207519, 0.5791459370175001, 0.08348640969207519, 0.08690842421419905]
printing an ep nov before normalisation:  40.14670692776754
maxi score, test score, baseline:  -0.32380666666666674 0.5936666666666667 0.5936666666666667
probs:  [0.08348677399030756, 0.08348677399030756, 0.08348677399030756, 0.5791337057751553, 0.08348677399030756, 0.08691919826361429]
maxi score, test score, baseline:  -0.32380666666666674 0.5936666666666667 0.5936666666666667
probs:  [0.08348713827002556, 0.08348713827002556, 0.08348713827002556, 0.5791214751544276, 0.08348713827002556, 0.08692997176547015]
actor:  1 policy actor:  1  step number:  49 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  1.667
from probs:  [0.08348713827002556, 0.08348713827002556, 0.08348713827002556, 0.5791214751544276, 0.08348713827002556, 0.08692997176547015]
maxi score, test score, baseline:  -0.32380666666666674 0.5936666666666667 0.5936666666666667
probs:  [0.08348713827002556, 0.08348713827002556, 0.08348713827002556, 0.5791214751544276, 0.08348713827002556, 0.08692997176547015]
maxi score, test score, baseline:  -0.32380666666666674 0.5936666666666667 0.5936666666666667
probs:  [0.08348713827002556, 0.08348713827002556, 0.08348713827002556, 0.5791214751544276, 0.08348713827002556, 0.08692997176547015]
actor:  0 policy actor:  1  step number:  37 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.076]
 [-0.058]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.076]
 [-0.058]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]]
maxi score, test score, baseline:  -0.32079333333333343 0.5936666666666667 0.5936666666666667
probs:  [0.08348750253123058, 0.08348750253123058, 0.08348750253123058, 0.5791092451552693, 0.08348750253123058, 0.08694074471980844]
maxi score, test score, baseline:  -0.32079333333333343 0.5936666666666667 0.5936666666666667
probs:  [0.08348750253123058, 0.08348750253123058, 0.08348750253123058, 0.5791092451552693, 0.08348750253123058, 0.08694074471980844]
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.538]
 [0.536]
 [0.39 ]
 [0.536]
 [0.536]
 [0.428]] [[37.134]
 [40.982]
 [35.083]
 [34.785]
 [35.083]
 [35.083]
 [34.689]] [[1.549]
 [1.823]
 [1.499]
 [1.338]
 [1.499]
 [1.499]
 [1.37 ]]
maxi score, test score, baseline:  -0.32079333333333343 0.5936666666666667 0.5936666666666667
actor:  1 policy actor:  1  step number:  48 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.32079333333333343 0.5936666666666667 0.5936666666666667
probs:  [0.08348750253123058, 0.08348750253123058, 0.08348750253123058, 0.5791092451552693, 0.08348750253123058, 0.08694074471980844]
maxi score, test score, baseline:  -0.32079333333333343 0.5936666666666667 0.5936666666666667
probs:  [0.08348750253123058, 0.08348750253123058, 0.08348750253123058, 0.5791092451552693, 0.08348750253123058, 0.08694074471980844]
actions average: 
K:  3  action  0 :  tensor([0.3794, 0.0445, 0.1192, 0.1152, 0.1287, 0.1091, 0.1039],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0215, 0.8742, 0.0159, 0.0333, 0.0149, 0.0116, 0.0286],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1231, 0.0086, 0.4687, 0.0935, 0.1016, 0.0996, 0.1050],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1269, 0.3074, 0.0900, 0.1490, 0.1132, 0.0949, 0.1185],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1507, 0.0118, 0.1035, 0.1056, 0.4420, 0.1082, 0.0781],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2084, 0.0198, 0.1647, 0.1479, 0.1697, 0.1486, 0.1409],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1497, 0.1232, 0.1365, 0.1411, 0.1439, 0.1218, 0.1838],
       grad_fn=<DivBackward0>)
siam score:  -0.84248805
printing an ep nov before normalisation:  29.6457062947185
maxi score, test score, baseline:  -0.32079333333333343 0.5936666666666667 0.5936666666666667
probs:  [0.08348750253123058, 0.08348750253123058, 0.08348750253123058, 0.5791092451552693, 0.08348750253123058, 0.08694074471980844]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
actor:  1 policy actor:  1  step number:  69 total reward:  0.05333333333333268  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  34.510548391884306
maxi score, test score, baseline:  -0.32079333333333343 0.5936666666666667 0.5936666666666667
probs:  [0.08348750253123058, 0.08348750253123058, 0.08348750253123058, 0.5791092451552693, 0.08348750253123058, 0.08694074471980844]
actor:  0 policy actor:  1  step number:  52 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.31824666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08348750253123058, 0.08348750253123058, 0.08348750253123058, 0.5791092451552693, 0.08348750253123058, 0.08694074471980844]
maxi score, test score, baseline:  -0.31824666666666673 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.31824666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08348786677392402, 0.08348786677392402, 0.08348786677392402, 0.5790970157776335, 0.08348786677392402, 0.08695151712667044]
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]] [[47.272]
 [47.272]
 [47.272]
 [47.272]
 [47.272]
 [47.272]
 [47.272]] [[1.588]
 [1.588]
 [1.588]
 [1.588]
 [1.588]
 [1.588]
 [1.588]]
siam score:  -0.8486886
maxi score, test score, baseline:  -0.31824666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08348786677392402, 0.08348786677392402, 0.08348786677392402, 0.5790970157776335, 0.08348786677392402, 0.08695151712667044]
siam score:  -0.8486798
maxi score, test score, baseline:  -0.31824666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08348786677392402, 0.08348786677392402, 0.08348786677392402, 0.5790970157776335, 0.08348786677392402, 0.08695151712667044]
maxi score, test score, baseline:  -0.31824666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08348786677392402, 0.08348786677392402, 0.08348786677392402, 0.5790970157776335, 0.08348786677392402, 0.08695151712667044]
line 256 mcts: sample exp_bonus 39.96641765612289
actor:  1 policy actor:  1  step number:  60 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.31824666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08348823099810729, 0.08348823099810729, 0.08348823099810729, 0.5790847870214724, 0.08348823099810729, 0.0869622889860985]
maxi score, test score, baseline:  -0.31824666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08348823099810729, 0.08348823099810729, 0.08348823099810729, 0.5790847870214724, 0.08348823099810729, 0.0869622889860985]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3333333333333327  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.31824666666666673 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.31824666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08348895939094907, 0.08348895939094907, 0.08348895939094907, 0.5790603313733853, 0.08348895939094907, 0.0869838310628183]
actor:  0 policy actor:  1  step number:  45 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.31552666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08348895939094907, 0.08348895939094907, 0.08348895939094907, 0.5790603313733853, 0.08348895939094907, 0.0869838310628183]
maxi score, test score, baseline:  -0.31552666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08348895939094907, 0.08348895939094907, 0.08348895939094907, 0.5790603313733853, 0.08348895939094907, 0.0869838310628183]
printing an ep nov before normalisation:  33.65688580113874
maxi score, test score, baseline:  -0.31552666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08348895939094907, 0.08348895939094907, 0.08348895939094907, 0.5790603313733853, 0.08348895939094907, 0.0869838310628183]
maxi score, test score, baseline:  -0.31552666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08348895939094907, 0.08348895939094907, 0.08348895939094907, 0.5790603313733853, 0.08348895939094907, 0.0869838310628183]
printing an ep nov before normalisation:  41.75550999410714
Printing some Q and Qe and total Qs values:  [[0.101]
 [0.327]
 [0.14 ]
 [0.14 ]
 [0.077]
 [0.14 ]
 [0.14 ]] [[38.839]
 [41.304]
 [31.394]
 [31.394]
 [32.955]
 [31.394]
 [31.394]] [[0.54 ]
 [0.811]
 [0.445]
 [0.445]
 [0.41 ]
 [0.445]
 [0.445]]
maxi score, test score, baseline:  -0.31552666666666673 0.5936666666666667 0.5936666666666667
probs:  [0.08348895939094907, 0.08348895939094907, 0.08348895939094907, 0.5790603313733853, 0.08348895939094907, 0.0869838310628183]
actor:  0 policy actor:  0  step number:  54 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  1.333
UNIT TEST: sample policy line 217 mcts : [0.061 0.327 0.082 0.143 0.061 0.286 0.041]
printing an ep nov before normalisation:  40.02612113952637
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
actor:  1 policy actor:  1  step number:  53 total reward:  0.2133333333333327  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.08348895939094907, 0.08348895939094907, 0.08348895939094907, 0.5790603313733853, 0.08348895939094907, 0.0869838310628183]
printing an ep nov before normalisation:  41.92656049062637
actor:  1 policy actor:  1  step number:  43 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.08348932355961035, 0.08348932355961035, 0.08348932355961035, 0.5790481044813645, 0.08348932355961035, 0.0869946012801941]
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.08348932355961035, 0.08348932355961035, 0.08348932355961035, 0.5790481044813645, 0.08348932355961035, 0.0869946012801941]
actor:  1 policy actor:  1  step number:  42 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  4  action  0 :  tensor([0.3527, 0.0164, 0.0827, 0.1371, 0.1660, 0.1096, 0.1355],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0150, 0.8575, 0.0134, 0.0456, 0.0172, 0.0151, 0.0363],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2002, 0.0156, 0.1950, 0.1424, 0.1669, 0.1591, 0.1208],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1602, 0.0461, 0.1065, 0.3139, 0.1366, 0.1398, 0.0968],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1762, 0.0713, 0.1533, 0.1689, 0.1793, 0.1409, 0.1100],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1620, 0.1338, 0.1232, 0.1360, 0.1276, 0.2294, 0.0880],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1738, 0.0015, 0.1314, 0.1616, 0.1544, 0.1501, 0.2273],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.011]
 [ 0.144]
 [-0.003]
 [-0.006]
 [-0.009]
 [-0.018]
 [-0.   ]] [[36.396]
 [44.099]
 [34.947]
 [36.29 ]
 [36.056]
 [35.302]
 [35.686]] [[0.653]
 [1.08 ]
 [0.611]
 [0.655]
 [0.643]
 [0.608]
 [0.639]]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [ 0.13 ]
 [-0.001]
 [-0.003]
 [-0.006]
 [-0.008]
 [-0.003]] [[31.263]
 [30.844]
 [32.157]
 [33.289]
 [33.642]
 [33.552]
 [32.965]] [[1.151]
 [1.253]
 [1.22 ]
 [1.303]
 [1.327]
 [1.318]
 [1.279]]
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.481]
 [0.354]
 [0.354]
 [0.354]
 [0.354]
 [0.354]] [[38.546]
 [43.428]
 [38.546]
 [38.546]
 [38.546]
 [38.546]
 [38.546]] [[1.074]
 [1.379]
 [1.074]
 [1.074]
 [1.074]
 [1.074]
 [1.074]]
printing an ep nov before normalisation:  18.698136806488037
actor:  1 policy actor:  1  step number:  51 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  41 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.0834900518414208, 0.0834900518414208, 0.0834900518414208, 0.579023652561132, 0.0834900518414208, 0.08701614007318488]
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.0834900518414208, 0.0834900518414208, 0.0834900518414208, 0.579023652561132, 0.0834900518414208, 0.08701614007318488]
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.0834900518414208, 0.0834900518414208, 0.0834900518414208, 0.579023652561132, 0.0834900518414208, 0.08701614007318488]
printing an ep nov before normalisation:  26.76816701889038
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.0834900518414208, 0.0834900518414208, 0.0834900518414208, 0.579023652561132, 0.0834900518414208, 0.08701614007318488]
printing an ep nov before normalisation:  33.02355336043087
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.0834900518414208, 0.0834900518414208, 0.0834900518414208, 0.579023652561132, 0.0834900518414208, 0.08701614007318488]
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.0834900518414208, 0.0834900518414208, 0.0834900518414208, 0.579023652561132, 0.0834900518414208, 0.08701614007318488]
siam score:  -0.86174685
actor:  1 policy actor:  1  step number:  42 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  41 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.141]
 [-0.13 ]
 [-0.141]
 [-0.141]
 [-0.141]
 [-0.141]
 [-0.141]] [[30.571]
 [27.677]
 [30.571]
 [30.571]
 [30.571]
 [30.571]
 [30.571]] [[1.192]
 [1.   ]
 [1.192]
 [1.192]
 [1.192]
 [1.192]
 [1.192]]
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.08349041595457278, 0.08349041595457278, 0.08349041595457278, 0.5790114275328253, 0.08349041595457278, 0.08702690864888353]
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.08349041595457278, 0.08349041595457278, 0.08349041595457278, 0.5790114275328253, 0.08349041595457278, 0.08702690864888353]
printing an ep nov before normalisation:  42.40857381093895
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
printing an ep nov before normalisation:  26.225176195096125
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.08349041595457278, 0.08349041595457278, 0.08349041595457278, 0.5790114275328253, 0.08349041595457278, 0.08702690864888353]
actor:  1 policy actor:  1  step number:  60 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.08349078004922449, 0.08349078004922449, 0.08349078004922449, 0.578999203125662, 0.08349078004922449, 0.08703767667743993]
actions average: 
K:  4  action  0 :  tensor([0.4698, 0.0697, 0.0384, 0.0623, 0.2306, 0.0535, 0.0756],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0319, 0.8805, 0.0193, 0.0164, 0.0142, 0.0123, 0.0254],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1557, 0.0279, 0.3011, 0.1328, 0.1250, 0.1488, 0.1087],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1189, 0.0057, 0.0972, 0.3564, 0.1622, 0.1752, 0.0844],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2551, 0.0064, 0.1525, 0.1421, 0.1701, 0.1622, 0.1116],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1439, 0.0930, 0.1529, 0.1245, 0.1138, 0.2789, 0.0930],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2177, 0.0145, 0.1254, 0.1712, 0.1672, 0.1578, 0.1463],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.08349078004922449, 0.08349078004922449, 0.08349078004922449, 0.578999203125662, 0.08349078004922449, 0.08703767667743993]
actor:  1 policy actor:  1  step number:  53 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.188]
 [-0.057]
 [-0.014]
 [-0.091]
 [-0.19 ]
 [-0.153]
 [-0.141]] [[24.357]
 [21.43 ]
 [30.372]
 [21.748]
 [21.298]
 [21.143]
 [29.022]] [[0.569]
 [0.609]
 [0.93 ]
 [0.585]
 [0.472]
 [0.504]
 [0.761]]
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.08349114412537734, 0.08349114412537734, 0.08349114412537734, 0.578986979339595, 0.08349114412537734, 0.08704844415889575]
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.08349114412537734, 0.08349114412537734, 0.08349114412537734, 0.578986979339595, 0.08349114412537734, 0.08704844415889575]
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.08349114412537734, 0.08349114412537734, 0.08349114412537734, 0.578986979339595, 0.08349114412537734, 0.08704844415889575]
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.08349114412537734, 0.08349114412537734, 0.08349114412537734, 0.578986979339595, 0.08349114412537734, 0.08704844415889575]
printing an ep nov before normalisation:  32.585578499751165
actor:  1 policy actor:  1  step number:  54 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.08349150818303269, 0.08349150818303269, 0.08349150818303269, 0.5789747561745764, 0.08349150818303269, 0.08705921109329282]
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.083491872222192, 0.083491872222192, 0.083491872222192, 0.578962533630559, 0.083491872222192, 0.08706997748067288]
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
printing an ep nov before normalisation:  43.299993252764246
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.083491872222192, 0.083491872222192, 0.083491872222192, 0.578962533630559, 0.083491872222192, 0.08706997748067288]
printing an ep nov before normalisation:  45.395018460729
Printing some Q and Qe and total Qs values:  [[0.748]
 [0.792]
 [0.686]
 [0.683]
 [0.683]
 [0.695]
 [0.684]] [[31.326]
 [31.801]
 [28.348]
 [28.473]
 [28.69 ]
 [28.136]
 [28.692]] [[0.748]
 [0.792]
 [0.686]
 [0.683]
 [0.683]
 [0.695]
 [0.684]]
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.083491872222192, 0.083491872222192, 0.083491872222192, 0.578962533630559, 0.083491872222192, 0.08706997748067288]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  39.97174583579953
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.08349223624285669, 0.08349223624285669, 0.08349223624285669, 0.5789503117074959, 0.08349223624285669, 0.0870807433210773]
printing an ep nov before normalisation:  44.54368513892627
siam score:  -0.85642713
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.08349223624285669, 0.08349223624285669, 0.08349223624285669, 0.5789503117074959, 0.08349223624285669, 0.0870807433210773]
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.08349223624285669, 0.08349223624285669, 0.08349223624285669, 0.5789503117074959, 0.08349223624285669, 0.0870807433210773]
printing an ep nov before normalisation:  36.259120621170254
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.08349223624285669, 0.08349223624285669, 0.08349223624285669, 0.5789503117074959, 0.08349223624285669, 0.0870807433210773]
printing an ep nov before normalisation:  27.916301772080246
actor:  1 policy actor:  1  step number:  53 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.08349223624285669, 0.08349223624285669, 0.08349223624285669, 0.5789503117074959, 0.08349223624285669, 0.0870807433210773]
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.08349223624285669, 0.08349223624285669, 0.08349223624285669, 0.5789503117074959, 0.08349223624285669, 0.0870807433210773]
printing an ep nov before normalisation:  29.372833486729032
printing an ep nov before normalisation:  29.612731017053843
printing an ep nov before normalisation:  35.24217732605058
maxi score, test score, baseline:  -0.31300666666666677 0.5936666666666667 0.5936666666666667
probs:  [0.08349223624285669, 0.08349223624285669, 0.08349223624285669, 0.5789503117074959, 0.08349223624285669, 0.0870807433210773]
printing an ep nov before normalisation:  24.74925231471877
actor:  0 policy actor:  1  step number:  41 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  36 total reward:  0.54  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.31002000000000013 0.5936666666666667 0.5936666666666667
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.537]
 [0.487]
 [0.485]
 [0.486]
 [0.46 ]
 [0.478]] [[34.688]
 [28.986]
 [32.791]
 [28.262]
 [32.311]
 [28.898]
 [28.498]] [[0.481]
 [0.537]
 [0.487]
 [0.485]
 [0.486]
 [0.46 ]
 [0.478]]
maxi score, test score, baseline:  -0.31002000000000013 0.5936666666666667 0.5936666666666667
probs:  [0.08349260024502815, 0.08349260024502815, 0.08349260024502815, 0.5789380904053396, 0.08349260024502815, 0.0870915086145479]
maxi score, test score, baseline:  -0.31002000000000013 0.5936666666666667 0.5936666666666667
probs:  [0.08349296422870774, 0.08349296422870774, 0.08349296422870774, 0.5789258697240424, 0.08349296422870774, 0.08710227336112655]
printing an ep nov before normalisation:  45.176890427724196
siam score:  -0.8539247
Printing some Q and Qe and total Qs values:  [[-0.081]
 [-0.081]
 [-0.081]
 [-0.093]
 [-0.081]
 [-0.081]
 [-0.081]] [[ 0. ]
 [ 0. ]
 [ 0. ]
 [40.9]
 [ 0. ]
 [ 0. ]
 [ 0. ]] [[-0.942]
 [-0.942]
 [-0.942]
 [ 1.334]
 [-0.942]
 [-0.942]
 [-0.942]]
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]]
actor:  0 policy actor:  0  step number:  57 total reward:  0.1999999999999994  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.31043333333333345 0.5936666666666667 0.5936666666666667
probs:  [0.08349296422870774, 0.08349296422870774, 0.08349296422870774, 0.5789258697240424, 0.08349296422870774, 0.08710227336112655]
printing an ep nov before normalisation:  38.38884240355859
Printing some Q and Qe and total Qs values:  [[-0.102]
 [-0.067]
 [-0.102]
 [-0.1  ]
 [-0.101]
 [-0.103]
 [-0.104]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.102]
 [-0.067]
 [-0.102]
 [-0.1  ]
 [-0.101]
 [-0.103]
 [-0.104]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.31043333333333345 0.5936666666666667 0.5936666666666667
probs:  [0.08349296422870774, 0.08349296422870774, 0.08349296422870774, 0.5789258697240424, 0.08349296422870774, 0.08710227336112655]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.31043333333333345 0.5936666666666667 0.5936666666666667
probs:  [0.08349296422870774, 0.08349296422870774, 0.08349296422870774, 0.5789258697240424, 0.08349296422870774, 0.08710227336112655]
maxi score, test score, baseline:  -0.31043333333333345 0.5936666666666667 0.5936666666666667
siam score:  -0.8592091
actor:  1 policy actor:  1  step number:  61 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.31043333333333345 0.5936666666666667 0.5936666666666667
probs:  [0.08349332819389693, 0.08349332819389693, 0.08349332819389693, 0.5789136496635573, 0.08349332819389693, 0.08711303756085485]
printing an ep nov before normalisation:  45.96875821727329
printing an ep nov before normalisation:  37.468948737909756
printing an ep nov before normalisation:  27.83799333738578
actor:  1 policy actor:  1  step number:  45 total reward:  0.56  reward:  1.0 rdn_beta:  1.667
from probs:  [0.08349332819389693, 0.08349332819389693, 0.08349332819389693, 0.5789136496635573, 0.08349332819389693, 0.08711303756085485]
actor:  1 policy actor:  1  step number:  65 total reward:  0.27999999999999925  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.31043333333333345 0.5936666666666667 0.5936666666666667
probs:  [0.08349369214059715, 0.08349369214059715, 0.08349369214059715, 0.5789014302238373, 0.08349369214059715, 0.0871238012137742]
Printing some Q and Qe and total Qs values:  [[ 0.209]
 [ 0.254]
 [ 0.209]
 [ 0.004]
 [ 0.009]
 [-0.011]
 [-0.   ]] [[32.947]
 [33.973]
 [32.947]
 [24.931]
 [25.423]
 [25.673]
 [25.11 ]] [[1.089]
 [1.187]
 [1.089]
 [0.47 ]
 [0.5  ]
 [0.493]
 [0.474]]
printing an ep nov before normalisation:  41.41628268078012
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.451]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]] [[33.929]
 [40.55 ]
 [33.929]
 [33.929]
 [33.929]
 [33.929]
 [33.929]] [[1.136]
 [1.498]
 [1.136]
 [1.136]
 [1.136]
 [1.136]
 [1.136]]
maxi score, test score, baseline:  -0.31043333333333345 0.5936666666666667 0.5936666666666667
probs:  [0.08349369214059715, 0.08349369214059715, 0.08349369214059715, 0.5789014302238373, 0.08349369214059715, 0.0871238012137742]
maxi score, test score, baseline:  -0.31043333333333345 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.31043333333333345 0.5936666666666667 0.5936666666666667
probs:  [0.08349369214059715, 0.08349369214059715, 0.08349369214059715, 0.5789014302238373, 0.08349369214059715, 0.0871238012137742]
Printing some Q and Qe and total Qs values:  [[0.775]
 [0.82 ]
 [0.724]
 [0.783]
 [0.783]
 [0.701]
 [0.764]] [[32.673]
 [35.211]
 [35.64 ]
 [40.427]
 [40.427]
 [38.433]
 [37.381]] [[1.639]
 [1.837]
 [1.767]
 [2.116]
 [2.116]
 [1.913]
 [1.913]]
maxi score, test score, baseline:  -0.31043333333333345 0.5936666666666667 0.5936666666666667
probs:  [0.08349369214059715, 0.08349369214059715, 0.08349369214059715, 0.5789014302238373, 0.08349369214059715, 0.0871238012137742]
actor:  0 policy actor:  0  step number:  53 total reward:  0.1866666666666661  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  55 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3080600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349405606880976, 0.08349405606880976, 0.08349405606880976, 0.5788892114048345, 0.08349405606880976, 0.08713456431992656]
printing an ep nov before normalisation:  44.419035599139534
maxi score, test score, baseline:  -0.3080600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349405606880976, 0.08349405606880976, 0.08349405606880976, 0.5788892114048345, 0.08349405606880976, 0.08713456431992656]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.784165382385254
maxi score, test score, baseline:  -0.3080600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349405606880976, 0.08349405606880976, 0.08349405606880976, 0.5788892114048345, 0.08349405606880976, 0.08713456431992656]
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.695]
 [0.669]
 [0.655]
 [0.657]
 [0.666]
 [0.681]] [[30.349]
 [28.301]
 [30.143]
 [31.827]
 [31.667]
 [30.943]
 [29.919]] [[1.775]
 [1.652]
 [1.758]
 [1.865]
 [1.855]
 [1.812]
 [1.754]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3080600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349478386977777, 0.08349478386977777, 0.08349478386977777, 0.5788647756287922, 0.08349478386977777, 0.08715608889209657]
maxi score, test score, baseline:  -0.3080600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349478386977777, 0.08349478386977777, 0.08349478386977777, 0.5788647756287922, 0.08349478386977777, 0.08715608889209657]
maxi score, test score, baseline:  -0.3080600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349478386977777, 0.08349478386977777, 0.08349478386977777, 0.5788647756287922, 0.08349478386977777, 0.08715608889209657]
maxi score, test score, baseline:  -0.3080600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349478386977777, 0.08349478386977777, 0.08349478386977777, 0.5788647756287922, 0.08349478386977777, 0.08715608889209657]
maxi score, test score, baseline:  -0.3080600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349478386977777, 0.08349478386977777, 0.08349478386977777, 0.5788647756287922, 0.08349478386977777, 0.08715608889209657]
printing an ep nov before normalisation:  33.61431992763015
actor:  0 policy actor:  0  step number:  43 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349478386977777, 0.08349478386977777, 0.08349478386977777, 0.5788647756287922, 0.08349478386977777, 0.08715608889209657]
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349478386977777, 0.08349478386977777, 0.08349478386977777, 0.5788647756287922, 0.08349478386977777, 0.08715608889209657]
actor:  1 policy actor:  1  step number:  60 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349478386977777, 0.08349478386977777, 0.08349478386977777, 0.5788647756287922, 0.08349478386977777, 0.08715608889209657]
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349478386977777, 0.08349478386977777, 0.08349478386977777, 0.5788647756287922, 0.08349478386977777, 0.08715608889209657]
printing an ep nov before normalisation:  39.75256516346337
line 256 mcts: sample exp_bonus 31.566991874679314
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349478386977777, 0.08349478386977777, 0.08349478386977777, 0.5788647756287922, 0.08349478386977777, 0.08715608889209657]
actor:  1 policy actor:  1  step number:  32 total reward:  0.5933333333333335  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  30.373325152883158
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
printing an ep nov before normalisation:  50.97466348820294
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.447]
 [0.337]
 [0.328]
 [0.339]
 [0.342]
 [0.344]] [[38.277]
 [43.536]
 [36.469]
 [37.18 ]
 [34.903]
 [36.503]
 [36.778]] [[0.921]
 [1.177]
 [0.858]
 [0.87 ]
 [0.813]
 [0.863]
 [0.874]]
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349551159681234, 0.08349551159681234, 0.08349551159681234, 0.5788403423350525, 0.08349551159681234, 0.08717761127769805]
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349551159681234, 0.08349551159681234, 0.08349551159681234, 0.5788403423350525, 0.08349551159681234, 0.08717761127769805]
siam score:  -0.85257936
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349551159681234, 0.08349551159681234, 0.08349551159681234, 0.5788403423350525, 0.08349551159681234, 0.08717761127769805]
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349551159681234, 0.08349551159681234, 0.08349551159681234, 0.5788403423350525, 0.08349551159681234, 0.08717761127769805]
printing an ep nov before normalisation:  37.45377352058435
actor:  1 policy actor:  1  step number:  33 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349587543260809, 0.08349587543260809, 0.08349587543260809, 0.5788281266189278, 0.08349587543260809, 0.08718837165063978]
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349587543260809, 0.08349587543260809, 0.08349587543260809, 0.5788281266189278, 0.08349587543260809, 0.08718837165063978]
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349587543260809, 0.08349587543260809, 0.08349587543260809, 0.5788281266189278, 0.08349587543260809, 0.08718837165063978]
printing an ep nov before normalisation:  54.9526847910558
siam score:  -0.855538
actor:  1 policy actor:  1  step number:  43 total reward:  0.44000000000000006  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  58.36649796567346
printing an ep nov before normalisation:  51.93935814851586
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349660304876352, 0.08349660304876352, 0.08349660304876352, 0.5788036970479326, 0.08349660304876352, 0.08720989075701327]
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349660304876352, 0.08349660304876352, 0.08349660304876352, 0.5788036970479326, 0.08349660304876352, 0.08720989075701327]
printing an ep nov before normalisation:  56.73938208085355
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349660304876352, 0.08349660304876352, 0.08349660304876352, 0.5788036970479326, 0.08349660304876352, 0.08720989075701327]
actor:  1 policy actor:  1  step number:  48 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  49 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.3  ]
 [0.45 ]
 [0.385]
 [0.385]
 [0.287]
 [0.385]
 [0.385]] [[50.575]
 [49.288]
 [48.147]
 [48.147]
 [50.742]
 [48.147]
 [48.147]] [[1.446]
 [1.551]
 [1.447]
 [1.447]
 [1.439]
 [1.447]
 [1.447]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 40.97249001602339
printing an ep nov before normalisation:  52.81178251945819
line 256 mcts: sample exp_bonus 33.83526953143823
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349733059101368, 0.08349733059101368, 0.08349733059101368, 0.5787792699582944, 0.08349733059101368, 0.08723140767765086]
printing an ep nov before normalisation:  38.73230457305908
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349733059101368, 0.08349733059101368, 0.08349733059101368, 0.5787792699582944, 0.08349733059101368, 0.08723140767765086]
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
actor:  1 policy actor:  1  step number:  49 total reward:  0.33333333333333326  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349769433442775, 0.08349769433442775, 0.08349769433442775, 0.578767057343866, 0.08349769433442775, 0.08724216531842299]
actor:  1 policy actor:  1  step number:  49 total reward:  0.41333333333333344  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349769433442775, 0.08349769433442775, 0.08349769433442775, 0.578767057343866, 0.08349769433442775, 0.08724216531842299]
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349769433442775, 0.08349769433442775, 0.08349769433442775, 0.578767057343866, 0.08349769433442775, 0.08724216531842299]
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349769433442775, 0.08349769433442775, 0.08349769433442775, 0.578767057343866, 0.08349769433442775, 0.08724216531842299]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349805805936972, 0.08349805805936972, 0.08349805805936972, 0.5787548453496352, 0.08349805805936972, 0.08725292241288606]
actor:  1 policy actor:  1  step number:  46 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349842176584099, 0.08349842176584099, 0.08349842176584099, 0.5787426339755545, 0.08349842176584099, 0.08726367896108164]
printing an ep nov before normalisation:  34.917788236534776
maxi score, test score, baseline:  -0.3052600000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349842176584099, 0.08349842176584099, 0.08349842176584099, 0.5787426339755545, 0.08349842176584099, 0.08726367896108164]
actor:  0 policy actor:  1  step number:  46 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  9.192409858206322
printing an ep nov before normalisation:  0.0098668387380485
siam score:  -0.8564491
maxi score, test score, baseline:  -0.3026066666666667 0.5936666666666667 0.5936666666666667
probs:  [0.08349914912337707, 0.08349914912337707, 0.08349914912337707, 0.578718213087655, 0.08349914912337707, 0.08728519041883662]
Printing some Q and Qe and total Qs values:  [[-0.006]
 [ 0.13 ]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[43.57 ]
 [52.893]
 [43.57 ]
 [43.57 ]
 [43.57 ]
 [43.57 ]
 [43.57 ]] [[0.79 ]
 [1.195]
 [0.79 ]
 [0.79 ]
 [0.79 ]
 [0.79 ]
 [0.79 ]]
printing an ep nov before normalisation:  42.18724660951519
printing an ep nov before normalisation:  36.68083509262886
printing an ep nov before normalisation:  38.74177206105584
printing an ep nov before normalisation:  46.1737822662537
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.3026066666666667 0.5936666666666667 0.5936666666666667
probs:  [0.08349987640704719, 0.08349987640704719, 0.08349987640704719, 0.5786937946797899, 0.08349987640704719, 0.08730669969202147]
maxi score, test score, baseline:  -0.3026066666666667 0.5936666666666667 0.5936666666666667
probs:  [0.08349987640704719, 0.08349987640704719, 0.08349987640704719, 0.5786937946797899, 0.08349987640704719, 0.08730669969202147]
printing an ep nov before normalisation:  47.14851445638885
maxi score, test score, baseline:  -0.3026066666666667 0.5936666666666667 0.5936666666666667
probs:  [0.08349987640704719, 0.08349987640704719, 0.08349987640704719, 0.5786937946797899, 0.08349987640704719, 0.08730669969202147]
printing an ep nov before normalisation:  44.22788575047216
actor:  1 policy actor:  1  step number:  38 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3026066666666667 0.5936666666666667 0.5936666666666667
probs:  [0.08349987640704719, 0.08349987640704719, 0.08349987640704719, 0.5786937946797899, 0.08349987640704719, 0.08730669969202147]
actor:  0 policy actor:  0  step number:  53 total reward:  0.2133333333333327  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349987640704719, 0.08349987640704719, 0.08349987640704719, 0.5786937946797899, 0.08349987640704719, 0.08730669969202147]
printing an ep nov before normalisation:  52.944783670486885
printing an ep nov before normalisation:  41.771923791903674
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[51.083]
 [51.083]
 [51.083]
 [51.083]
 [51.083]
 [51.083]
 [51.083]] [[0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]
 [0.328]]
Printing some Q and Qe and total Qs values:  [[-0.021]
 [ 0.061]
 [-0.02 ]
 [-0.005]
 [-0.021]
 [-0.021]
 [-0.019]] [[29.48 ]
 [40.259]
 [29.631]
 [38.702]
 [30.946]
 [31.814]
 [36.308]] [[0.366]
 [0.797]
 [0.372]
 [0.681]
 [0.414]
 [0.442]
 [0.59 ]]
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349987640704719, 0.08349987640704719, 0.08349987640704719, 0.5786937946797899, 0.08349987640704719, 0.08730669969202147]
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349987640704719, 0.08349987640704719, 0.08349987640704719, 0.5786937946797899, 0.08349987640704719, 0.08730669969202147]
printing an ep nov before normalisation:  40.690591030294605
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08349987640704719, 0.08349987640704719, 0.08349987640704719, 0.5786937946797899, 0.08349987640704719, 0.08730669969202147]
line 256 mcts: sample exp_bonus 28.84114569425583
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
actions average: 
K:  1  action  0 :  tensor([0.5571, 0.0398, 0.0829, 0.0720, 0.0927, 0.0762, 0.0795],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0058, 0.9640, 0.0025, 0.0053, 0.0010, 0.0012, 0.0203],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2186, 0.0038, 0.1598, 0.1500, 0.1649, 0.1751, 0.1278],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1670, 0.0098, 0.1220, 0.3037, 0.1158, 0.1427, 0.1390],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2242, 0.0070, 0.1148, 0.1122, 0.3253, 0.1198, 0.0967],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1082, 0.0042, 0.1202, 0.0795, 0.0554, 0.5743, 0.0582],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2418, 0.0175, 0.1618, 0.1538, 0.1191, 0.1566, 0.1494],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
actor:  1 policy actor:  1  step number:  56 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  1.333
from probs:  [0.08349987640704719, 0.08349987640704719, 0.08349987640704719, 0.5786937946797899, 0.08349987640704719, 0.08730669969202147]
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350024002118604, 0.08350024002118604, 0.08350024002118604, 0.5786815864057521, 0.08350024002118604, 0.08731745350950387]
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.784]
 [0.8  ]
 [0.771]
 [0.771]
 [0.799]
 [0.794]] [[12.937]
 [22.275]
 [16.338]
 [17.36 ]
 [17.36 ]
 [13.342]
 [13.154]] [[0.689]
 [0.784]
 [0.8  ]
 [0.771]
 [0.771]
 [0.799]
 [0.794]]
printing an ep nov before normalisation:  67.01638648674354
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350060361686264, 0.08350060361686264, 0.08350060361686264, 0.578669378751581, 0.08350060361686264, 0.08732820678096846]
printing an ep nov before normalisation:  54.77186732341522
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350060361686264, 0.08350060361686264, 0.08350060361686264, 0.578669378751581, 0.08350060361686264, 0.08732820678096846]
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
printing an ep nov before normalisation:  35.75175402292953
actor:  1 policy actor:  1  step number:  39 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350060361686264, 0.08350060361686264, 0.08350060361686264, 0.578669378751581, 0.08350060361686264, 0.08732820678096846]
actions average: 
K:  3  action  0 :  tensor([0.4942, 0.0672, 0.0856, 0.0693, 0.1122, 0.0822, 0.0893],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0091, 0.9259, 0.0106, 0.0200, 0.0082, 0.0073, 0.0189],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1239, 0.0122, 0.2677, 0.1248, 0.1271, 0.2186, 0.1256],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1154, 0.2693, 0.1051, 0.1421, 0.0956, 0.1062, 0.1662],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1765, 0.0231, 0.1156, 0.1398, 0.3171, 0.1275, 0.1003],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1518, 0.0016, 0.1462, 0.1217, 0.1531, 0.3185, 0.1071],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0987, 0.1810, 0.0923, 0.1986, 0.1004, 0.1020, 0.2270],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350060361686264, 0.08350060361686264, 0.08350060361686264, 0.578669378751581, 0.08350060361686264, 0.08732820678096846]
printing an ep nov before normalisation:  41.103622239055156
printing an ep nov before normalisation:  35.703172253902586
printing an ep nov before normalisation:  39.34147702712543
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350060361686264, 0.08350060361686264, 0.08350060361686264, 0.578669378751581, 0.08350060361686264, 0.08732820678096846]
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.502]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]] [[26.058]
 [28.066]
 [26.058]
 [26.058]
 [26.058]
 [26.058]
 [26.058]] [[1.194]
 [1.427]
 [1.194]
 [1.194]
 [1.194]
 [1.194]
 [1.194]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350096719407837, 0.08350096719407837, 0.08350096719407837, 0.5786571717172297, 0.08350096719407837, 0.08733895950645676]
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350096719407837, 0.08350096719407837, 0.08350096719407837, 0.5786571717172297, 0.08350096719407837, 0.08733895950645676]
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350096719407837, 0.08350096719407837, 0.08350096719407837, 0.5786571717172297, 0.08350096719407837, 0.08733895950645676]
actor:  1 policy actor:  1  step number:  54 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350096719407837, 0.08350096719407837, 0.08350096719407837, 0.5786571717172297, 0.08350096719407837, 0.08733895950645676]
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
printing an ep nov before normalisation:  38.04125789267384
printing an ep nov before normalisation:  28.93861650280734
siam score:  -0.8529631
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350133075283467, 0.08350133075283467, 0.08350133075283467, 0.5786449653026512, 0.08350133075283467, 0.08734971168601036]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350133075283467, 0.08350133075283467, 0.08350133075283467, 0.5786449653026512, 0.08350133075283467, 0.08734971168601036]
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350133075283467, 0.08350133075283467, 0.08350133075283467, 0.5786449653026512, 0.08350133075283467, 0.08734971168601036]
printing an ep nov before normalisation:  32.85812386718181
line 256 mcts: sample exp_bonus 33.917861445279875
printing an ep nov before normalisation:  45.16991328089206
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350133075283467, 0.08350133075283467, 0.08350133075283467, 0.5786449653026512, 0.08350133075283467, 0.08734971168601036]
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350133075283467, 0.08350133075283467, 0.08350133075283467, 0.5786449653026512, 0.08350133075283467, 0.08734971168601036]
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350133075283467, 0.08350133075283467, 0.08350133075283467, 0.5786449653026512, 0.08350133075283467, 0.08734971168601036]
actor:  1 policy actor:  1  step number:  64 total reward:  0.08666666666666634  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  48 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350205781497451, 0.08350205781497451, 0.08350205781497451, 0.5786205543326223, 0.08350205781497451, 0.08737121440747973]
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350205781497451, 0.08350205781497451, 0.08350205781497451, 0.5786205543326223, 0.08350205781497451, 0.08737121440747973]
maxi score, test score, baseline:  -0.3001800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350242131836084, 0.08350242131836084, 0.08350242131836084, 0.5786083497770778, 0.08350242131836084, 0.08738196494947886]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  36 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.29712666666666676 0.5936666666666667 0.5936666666666667
printing an ep nov before normalisation:  44.796776451991484
maxi score, test score, baseline:  -0.29712666666666676 0.5936666666666667 0.5936666666666667
probs:  [0.08350242131836084, 0.08350242131836084, 0.08350242131836084, 0.5786083497770778, 0.08350242131836084, 0.08738196494947886]
using explorer policy with actor:  1
printing an ep nov before normalisation:  22.473111699956362
printing an ep nov before normalisation:  36.17316408888747
printing an ep nov before normalisation:  11.647792268133195
line 256 mcts: sample exp_bonus 34.41523645644528
actor:  1 policy actor:  1  step number:  55 total reward:  0.1599999999999997  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  47.96442546282506
maxi score, test score, baseline:  -0.29712666666666676 0.5936666666666667 0.5936666666666667
probs:  [0.08350242131836084, 0.08350242131836084, 0.08350242131836084, 0.5786083497770778, 0.08350242131836084, 0.08738196494947886]
printing an ep nov before normalisation:  18.563981732268143
maxi score, test score, baseline:  -0.29712666666666676 0.5936666666666667 0.5936666666666667
probs:  [0.08350242131836084, 0.08350242131836084, 0.08350242131836084, 0.5786083497770778, 0.08350242131836084, 0.08738196494947886]
actor:  1 policy actor:  1  step number:  63 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  26.905890506938285
printing an ep nov before normalisation:  23.223422242546786
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  62 total reward:  0.17999999999999883  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  44 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.29712666666666676 0.5936666666666667 0.5936666666666667
probs:  [0.08350314826977352, 0.08350314826977352, 0.08350314826977352, 0.5785839425246928, 0.08350314826977352, 0.08740346439621313]
printing an ep nov before normalisation:  51.07912132131421
maxi score, test score, baseline:  -0.29712666666666676 0.5936666666666667 0.5936666666666667
probs:  [0.08350314826977352, 0.08350314826977352, 0.08350314826977352, 0.5785839425246928, 0.08350314826977352, 0.08740346439621313]
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]] [[51.052]
 [51.052]
 [51.052]
 [51.052]
 [51.052]
 [51.052]
 [51.052]] [[1.646]
 [1.646]
 [1.646]
 [1.646]
 [1.646]
 [1.646]
 [1.646]]
printing an ep nov before normalisation:  37.214389238778686
maxi score, test score, baseline:  -0.29712666666666676 0.5936666666666667 0.5936666666666667
probs:  [0.08350314826977352, 0.08350314826977352, 0.08350314826977352, 0.5785839425246928, 0.08350314826977352, 0.08740346439621313]
maxi score, test score, baseline:  -0.29712666666666676 0.5936666666666667 0.5936666666666667
siam score:  -0.8500368
actor:  1 policy actor:  1  step number:  51 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  57 total reward:  0.11999999999999955  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.29712666666666676 0.5936666666666667 0.5936666666666667
probs:  [0.08350314826977352, 0.08350314826977352, 0.08350314826977352, 0.5785839425246928, 0.08350314826977352, 0.08740346439621313]
printing an ep nov before normalisation:  38.20815563201904
maxi score, test score, baseline:  -0.29712666666666676 0.5936666666666667 0.5936666666666667
probs:  [0.08350314826977352, 0.08350314826977352, 0.08350314826977352, 0.5785839425246928, 0.08350314826977352, 0.08740346439621313]
Printing some Q and Qe and total Qs values:  [[-0.088]
 [-0.101]
 [-0.088]
 [-0.088]
 [-0.088]
 [-0.088]
 [-0.088]] [[42.241]
 [41.809]
 [42.241]
 [42.241]
 [42.241]
 [42.241]
 [42.241]] [[1.245]
 [1.208]
 [1.245]
 [1.245]
 [1.245]
 [1.245]
 [1.245]]
siam score:  -0.8509187
maxi score, test score, baseline:  -0.29712666666666676 0.5936666666666667 0.5936666666666667
probs:  [0.08350351171780261, 0.08350351171780261, 0.08350351171780261, 0.5785717398277581, 0.08350351171780261, 0.0874142133010315]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]] [[14.528]
 [14.528]
 [14.528]
 [14.528]
 [14.528]
 [14.528]
 [14.528]] [[1.774]
 [1.774]
 [1.774]
 [1.774]
 [1.774]
 [1.774]
 [1.774]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.10666666666666624  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.019]
 [ 0.168]
 [-0.019]
 [-0.019]
 [-0.022]
 [-0.019]
 [-0.019]] [[31.514]
 [37.595]
 [31.514]
 [31.514]
 [31.868]
 [31.514]
 [31.514]] [[0.648]
 [1.085]
 [0.648]
 [0.648]
 [0.659]
 [0.648]
 [0.648]]
printing an ep nov before normalisation:  49.22768687474974
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.0120889981575
maxi score, test score, baseline:  -0.29712666666666676 0.5936666666666667 0.5936666666666667
probs:  [0.08350423855851338, 0.08350423855851338, 0.08350423855851338, 0.5785473362921678, 0.08350423855851338, 0.08743570947377875]
printing an ep nov before normalisation:  24.60239677350827
actor:  1 policy actor:  1  step number:  44 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  0.0070733851011084425
actor:  1 policy actor:  1  step number:  59 total reward:  0.1866666666666661  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.105]
 [-0.105]
 [-0.105]
 [-0.105]
 [-0.105]
 [-0.105]
 [-0.105]] [[31.713]
 [31.713]
 [31.713]
 [31.713]
 [31.713]
 [31.713]
 [31.713]] [[0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]]
actions average: 
K:  3  action  0 :  tensor([0.3977, 0.0669, 0.1038, 0.1134, 0.1130, 0.1175, 0.0876],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0287, 0.8764, 0.0135, 0.0212, 0.0119, 0.0136, 0.0346],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1125, 0.0044, 0.5611, 0.0843, 0.0745, 0.1052, 0.0579],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1522, 0.0827, 0.0810, 0.3745, 0.0892, 0.0968, 0.1235],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1699, 0.0213, 0.0848, 0.0893, 0.4591, 0.1060, 0.0695],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1807, 0.1345, 0.0924, 0.1687, 0.1117, 0.2061, 0.1058],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1884, 0.0025, 0.1048, 0.1919, 0.1599, 0.1820, 0.1705],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.221699891213575
printing an ep nov before normalisation:  32.53511749936306
printing an ep nov before normalisation:  37.24287134808548
maxi score, test score, baseline:  -0.29712666666666676 0.5936666666666667 0.5936666666666667
probs:  [0.08350496532543689, 0.08350496532543689, 0.08350496532543689, 0.5785229352339689, 0.08350496532543689, 0.08745720346428361]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.29712666666666676 0.5936666666666667 0.5936666666666667
using explorer policy with actor:  1
printing an ep nov before normalisation:  27.951509952545166
actor:  1 policy actor:  1  step number:  53 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  1.0
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  23.008284109679966
actor:  0 policy actor:  0  step number:  38 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  31.030145573888337
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350569201858445, 0.08350569201858445, 0.08350569201858445, 0.5784985366527844, 0.08350569201858445, 0.08747869527287797]
siam score:  -0.84681964
printing an ep nov before normalisation:  0.9683324211647459
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350569201858445, 0.08350569201858445, 0.08350569201858445, 0.5784985366527844, 0.08350569201858445, 0.08747869527287797]
printing an ep nov before normalisation:  40.134869724220515
actor:  1 policy actor:  1  step number:  42 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350569201858445, 0.08350569201858445, 0.08350569201858445, 0.5784985366527844, 0.08350569201858445, 0.08747869527287797]
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
printing an ep nov before normalisation:  46.74752432780576
actor:  1 policy actor:  1  step number:  47 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350605533749572, 0.08350605533749572, 0.08350605533749572, 0.5784863382909544, 0.08350605533749572, 0.08748944035906263]
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350605533749572, 0.08350605533749572, 0.08350605533749572, 0.5784863382909544, 0.08350605533749572, 0.08748944035906263]
printing an ep nov before normalisation:  36.37993226396657
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350641863796723, 0.08350641863796723, 0.08350641863796723, 0.5784741405482369, 0.08350641863796723, 0.08750018489989407]
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350641863796723, 0.08350641863796723, 0.08350641863796723, 0.5784741405482369, 0.08350641863796723, 0.08750018489989407]
printing an ep nov before normalisation:  33.75830536617899
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350641863796723, 0.08350641863796723, 0.08350641863796723, 0.5784741405482369, 0.08350641863796723, 0.08750018489989407]
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350641863796723, 0.08350641863796723, 0.08350641863796723, 0.5784741405482369, 0.08350641863796723, 0.08750018489989407]
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350641863796723, 0.08350641863796723, 0.08350641863796723, 0.5784741405482369, 0.08350641863796723, 0.08750018489989407]
actor:  1 policy actor:  1  step number:  56 total reward:  0.2733333333333331  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350678192000038, 0.08350678192000038, 0.08350678192000038, 0.5784619434245842, 0.08350678192000038, 0.08751092889541422]
printing an ep nov before normalisation:  47.995049781738636
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350678192000038, 0.08350678192000038, 0.08350678192000038, 0.5784619434245842, 0.08350678192000038, 0.08751092889541422]
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350678192000038, 0.08350678192000038, 0.08350678192000038, 0.5784619434245842, 0.08350678192000038, 0.08751092889541422]
Printing some Q and Qe and total Qs values:  [[-0.115]
 [-0.115]
 [-0.11 ]
 [-0.112]
 [-0.11 ]
 [-0.115]
 [-0.11 ]] [[47.816]
 [49.185]
 [48.976]
 [48.764]
 [48.976]
 [49.428]
 [48.976]] [[1.154]
 [1.209]
 [1.205]
 [1.194]
 [1.205]
 [1.218]
 [1.205]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350714518359655, 0.08350714518359655, 0.08350714518359655, 0.5784497469199494, 0.08350714518359655, 0.08752167234566438]
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350714518359655, 0.08350714518359655, 0.08350714518359655, 0.5784497469199494, 0.08350714518359655, 0.08752167234566438]
printing an ep nov before normalisation:  44.557391429192016
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  49.78945060394162
line 256 mcts: sample exp_bonus 24.921195111602284
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350750842875712, 0.08350750842875712, 0.08350750842875712, 0.5784375510342854, 0.08350750842875712, 0.08753241525068614]
printing an ep nov before normalisation:  34.59340921905155
actor:  1 policy actor:  1  step number:  63 total reward:  0.3333333333333325  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350750842875712, 0.08350750842875712, 0.08350750842875712, 0.5784375510342854, 0.08350750842875712, 0.08753241525068614]
printing an ep nov before normalisation:  41.16423700659004
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.62 ]
 [0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]] [[25.812]
 [24.331]
 [25.812]
 [25.812]
 [25.812]
 [25.812]
 [25.812]] [[1.73 ]
 [1.734]
 [1.73 ]
 [1.73 ]
 [1.73 ]
 [1.73 ]
 [1.73 ]]
UNIT TEST: sample policy line 217 mcts : [0.204 0.449 0.061 0.061 0.041 0.061 0.122]
actor:  1 policy actor:  1  step number:  52 total reward:  0.27333333333333265  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  69 total reward:  0.10666666666666624  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350787165548353, 0.08350787165548353, 0.08350787165548353, 0.578425355767545, 0.08350787165548353, 0.08754315761052076]
printing an ep nov before normalisation:  41.08838076442433
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350787165548353, 0.08350787165548353, 0.08350787165548353, 0.578425355767545, 0.08350787165548353, 0.08754315761052076]
printing an ep nov before normalisation:  44.63852134620996
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350787165548353, 0.08350787165548353, 0.08350787165548353, 0.578425355767545, 0.08350787165548353, 0.08754315761052076]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 42.72633648343182
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350787165548353, 0.08350787165548353, 0.08350787165548353, 0.578425355767545, 0.08350787165548353, 0.08754315761052076]
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350787165548353, 0.08350787165548353, 0.08350787165548353, 0.578425355767545, 0.08350787165548353, 0.08754315761052076]
printing an ep nov before normalisation:  40.64445331233419
printing an ep nov before normalisation:  32.26855775050616
actions average: 
K:  4  action  0 :  tensor([0.3286, 0.0968, 0.1043, 0.1202, 0.1238, 0.1031, 0.1232],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0163, 0.8837, 0.0344, 0.0121, 0.0068, 0.0115, 0.0352],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1761, 0.1301, 0.2169, 0.1023, 0.1261, 0.1330, 0.1154],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2463, 0.0056, 0.1282, 0.1932, 0.1539, 0.1246, 0.1480],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2372, 0.0155, 0.0822, 0.0731, 0.4181, 0.0930, 0.0810],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2046, 0.0206, 0.1710, 0.1226, 0.1373, 0.1995, 0.1445],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1490, 0.2700, 0.0967, 0.1037, 0.1040, 0.0886, 0.1881],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  41.87215691653915
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.552]
 [0.399]
 [0.407]
 [0.409]
 [0.547]
 [0.423]] [[36.436]
 [34.66 ]
 [37.017]
 [36.159]
 [36.077]
 [36.196]
 [36.688]] [[0.563]
 [0.741]
 [0.621]
 [0.617]
 [0.618]
 [0.758]
 [0.64 ]]
printing an ep nov before normalisation:  34.52981877202335
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350787165548353, 0.08350787165548353, 0.08350787165548353, 0.578425355767545, 0.08350787165548353, 0.08754315761052076]
actor:  1 policy actor:  1  step number:  56 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  47.630251877029224
actor:  1 policy actor:  1  step number:  51 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[34.454]
 [34.454]
 [34.454]
 [34.454]
 [34.454]
 [34.454]
 [34.454]] [[0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]]
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350823486377722, 0.08350823486377722, 0.08350823486377722, 0.5784131611196813, 0.08350823486377722, 0.08755389942520998]
printing an ep nov before normalisation:  59.55654069576931
printing an ep nov before normalisation:  38.83092258655283
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350896122507179, 0.08350896122507179, 0.08350896122507179, 0.5783887736803944, 0.08350896122507179, 0.08757538141931837]
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350896122507179, 0.08350896122507179, 0.08350896122507179, 0.5783887736803944, 0.08350896122507179, 0.08757538141931837]
printing an ep nov before normalisation:  42.817956823040475
actor:  1 policy actor:  1  step number:  55 total reward:  0.3199999999999994  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350932437807557, 0.08350932437807557, 0.08350932437807557, 0.5783765808888776, 0.08350932437807557, 0.08758612159882004]
printing an ep nov before normalisation:  31.848487854003906
actor:  1 policy actor:  1  step number:  48 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8520296
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350932437807557, 0.08350932437807557, 0.08350932437807557, 0.5783765808888776, 0.08350932437807557, 0.08758612159882004]
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350932437807557, 0.08350932437807557, 0.08350932437807557, 0.5783765808888776, 0.08350932437807557, 0.08758612159882004]
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  30.192537307739258
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08350968751265218, 0.08350968751265218, 0.08350968751265218, 0.5783643887160486, 0.08350968751265218, 0.08759686123334255]
Printing some Q and Qe and total Qs values:  [[0.299]
 [0.374]
 [0.299]
 [0.299]
 [0.299]
 [0.299]
 [0.299]] [[40.049]
 [40.501]
 [40.049]
 [40.049]
 [40.049]
 [40.049]
 [40.049]] [[1.258]
 [1.356]
 [1.258]
 [1.258]
 [1.258]
 [1.258]
 [1.258]]
printing an ep nov before normalisation:  34.95701789855957
actions average: 
K:  1  action  0 :  tensor([0.3905, 0.0347, 0.1060, 0.1006, 0.1743, 0.1077, 0.0862],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0098, 0.9195, 0.0073, 0.0285, 0.0061, 0.0055, 0.0233],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1552, 0.0061, 0.3193, 0.1202, 0.1123, 0.1837, 0.1032],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1786, 0.0619, 0.1066, 0.2364, 0.1751, 0.1427, 0.0986],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1875, 0.1525, 0.0978, 0.0901, 0.2727, 0.1107, 0.0888],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1189, 0.0321, 0.1599, 0.0980, 0.0956, 0.4044, 0.0912],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1794, 0.2172, 0.0938, 0.1144, 0.1246, 0.1060, 0.1646],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  71 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08351005062880304, 0.08351005062880304, 0.08351005062880304, 0.5783521971618609, 0.08351005062880304, 0.08760760032292694]
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.55 ]
 [0.455]
 [0.476]
 [0.472]
 [0.463]
 [0.455]] [[12.326]
 [24.612]
 [12.26 ]
 [12.396]
 [12.377]
 [12.378]
 [12.26 ]] [[0.463]
 [0.55 ]
 [0.455]
 [0.476]
 [0.472]
 [0.463]
 [0.455]]
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08351005062880304, 0.08351005062880304, 0.08351005062880304, 0.5783521971618609, 0.08351005062880304, 0.08760760032292694]
actor:  1 policy actor:  1  step number:  54 total reward:  0.1799999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08351041372652956, 0.08351041372652956, 0.08351041372652956, 0.5783400062262671, 0.08351041372652956, 0.08761833886761458]
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08351041372652956, 0.08351041372652956, 0.08351041372652956, 0.5783400062262671, 0.08351041372652956, 0.08761833886761458]
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08351041372652956, 0.08351041372652956, 0.08351041372652956, 0.5783400062262671, 0.08351041372652956, 0.08761833886761458]
Printing some Q and Qe and total Qs values:  [[0.772]
 [0.885]
 [0.773]
 [0.816]
 [0.729]
 [0.735]
 [0.756]] [[27.14 ]
 [32.279]
 [25.395]
 [29.727]
 [28.454]
 [28.944]
 [29.109]] [[0.772]
 [0.885]
 [0.773]
 [0.816]
 [0.729]
 [0.735]
 [0.756]]
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08351041372652956, 0.08351041372652956, 0.08351041372652956, 0.5783400062262671, 0.08351041372652956, 0.08761833886761458]
maxi score, test score, baseline:  -0.2941800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08351041372652956, 0.08351041372652956, 0.08351041372652956, 0.5783400062262671, 0.08351041372652956, 0.08761833886761458]
actor:  0 policy actor:  1  step number:  63 total reward:  0.1999999999999993  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2917800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08351041372652956, 0.08351041372652956, 0.08351041372652956, 0.5783400062262671, 0.08351041372652956, 0.08761833886761458]
actor:  1 policy actor:  1  step number:  45 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2917800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08351077680583315, 0.08351077680583315, 0.08351077680583315, 0.5783278159092201, 0.08351077680583315, 0.08762907686744732]
maxi score, test score, baseline:  -0.2917800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08351077680583315, 0.08351077680583315, 0.08351077680583315, 0.5783278159092201, 0.08351077680583315, 0.08762907686744732]
Printing some Q and Qe and total Qs values:  [[0.121]
 [0.175]
 [0.121]
 [0.121]
 [0.079]
 [0.121]
 [0.121]] [[40.85 ]
 [41.368]
 [40.85 ]
 [40.85 ]
 [47.037]
 [40.85 ]
 [40.85 ]] [[1.29 ]
 [1.368]
 [1.29 ]
 [1.29 ]
 [1.54 ]
 [1.29 ]
 [1.29 ]]
maxi score, test score, baseline:  -0.2917800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08351077680583315, 0.08351077680583315, 0.08351077680583315, 0.5783278159092201, 0.08351077680583315, 0.08762907686744732]
printing an ep nov before normalisation:  42.29587959051883
maxi score, test score, baseline:  -0.2917800000000001 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.2917800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08351077680583315, 0.08351077680583315, 0.08351077680583315, 0.5783278159092201, 0.08351077680583315, 0.08762907686744732]
maxi score, test score, baseline:  -0.2917800000000001 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.2917800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08351113986671517, 0.08351113986671517, 0.08351113986671517, 0.578315626210673, 0.08351113986671517, 0.08763981432246643]
actions average: 
K:  1  action  0 :  tensor([0.4462, 0.0084, 0.1387, 0.0867, 0.1207, 0.1199, 0.0795],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0037, 0.9665, 0.0047, 0.0048, 0.0020, 0.0027, 0.0155],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1013, 0.0125, 0.4294, 0.1089, 0.1056, 0.1466, 0.0956],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1226, 0.1100, 0.1186, 0.3148, 0.1221, 0.1154, 0.0964],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1668, 0.0079, 0.0871, 0.1165, 0.4438, 0.1047, 0.0733],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1640, 0.0054, 0.1415, 0.1411, 0.1571, 0.2808, 0.1101],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1225, 0.0807, 0.1438, 0.1640, 0.0941, 0.1328, 0.2621],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  47 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2917800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08351150290917705, 0.08351150290917705, 0.08351150290917705, 0.5783034371305784, 0.08351150290917705, 0.08765055123271338]
maxi score, test score, baseline:  -0.2917800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08351150290917705, 0.08351150290917705, 0.08351150290917705, 0.5783034371305784, 0.08351150290917705, 0.08765055123271338]
maxi score, test score, baseline:  -0.2917800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08351150290917705, 0.08351150290917705, 0.08351150290917705, 0.5783034371305784, 0.08351150290917705, 0.08765055123271338]
maxi score, test score, baseline:  -0.2917800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08351150290917705, 0.08351150290917705, 0.08351150290917705, 0.5783034371305784, 0.08351150290917705, 0.08765055123271338]
maxi score, test score, baseline:  -0.2917800000000001 0.5936666666666667 0.5936666666666667
maxi score, test score, baseline:  -0.2917800000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08351150290917705, 0.08351150290917705, 0.08351150290917705, 0.5783034371305784, 0.08351150290917705, 0.08765055123271338]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
actor:  1 policy actor:  1  step number:  48 total reward:  0.31333333333333324  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  60 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.2894200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08351150290917705, 0.08351150290917705, 0.08351150290917705, 0.5783034371305784, 0.08351150290917705, 0.08765055123271338]
printing an ep nov before normalisation:  45.09069279655542
printing an ep nov before normalisation:  49.336336412241934
maxi score, test score, baseline:  -0.2894200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08351150290917705, 0.08351150290917705, 0.08351150290917705, 0.5783034371305784, 0.08351150290917705, 0.08765055123271338]
maxi score, test score, baseline:  -0.2894200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08351150290917705, 0.08351150290917705, 0.08351150290917705, 0.5783034371305784, 0.08351150290917705, 0.08765055123271338]
maxi score, test score, baseline:  -0.2894200000000001 0.5936666666666667 0.5936666666666667
probs:  [0.08351150290917705, 0.08351150290917705, 0.08351150290917705, 0.5783034371305784, 0.08351150290917705, 0.08765055123271338]
siam score:  -0.84094054
actor:  0 policy actor:  0  step number:  70 total reward:  0.16666666666666596  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2870866666666667 0.5936666666666667 0.5936666666666667
probs:  [0.08351150290917705, 0.08351150290917705, 0.08351150290917705, 0.5783034371305784, 0.08351150290917705, 0.08765055123271338]
actions average: 
K:  0  action  0 :  tensor([0.4084, 0.0079, 0.1194, 0.1108, 0.1389, 0.1058, 0.1088],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0045, 0.9707, 0.0050, 0.0063, 0.0024, 0.0020, 0.0092],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1227, 0.0235, 0.3680, 0.1236, 0.1272, 0.1198, 0.1151],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1788, 0.0092, 0.1405, 0.1980, 0.1556, 0.1300, 0.1878],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1464, 0.0055, 0.1565, 0.1351, 0.3018, 0.1537, 0.1009],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1206, 0.0075, 0.1923, 0.1174, 0.1314, 0.3367, 0.0941],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1351, 0.0092, 0.1207, 0.1525, 0.1249, 0.1124, 0.3453],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  41.328221459043775
actor:  0 policy actor:  0  step number:  38 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.28414000000000006 0.5936666666666667 0.5936666666666667
probs:  [0.08351150290917705, 0.08351150290917705, 0.08351150290917705, 0.5783034371305784, 0.08351150290917705, 0.08765055123271338]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.156]
 [0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.016]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.156]
 [0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.016]]
printing an ep nov before normalisation:  34.50141236126653
maxi score, test score, baseline:  -0.28414000000000006 0.5936666666666667 0.5936666666666667
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.742]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]] [[33.214]
 [40.476]
 [33.214]
 [33.214]
 [33.214]
 [33.214]
 [33.214]] [[0.562]
 [0.742]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]]
printing an ep nov before normalisation:  31.993207931518555
printing an ep nov before normalisation:  46.80664614898666
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.284]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]] [[48.798]
 [51.083]
 [48.798]
 [48.798]
 [48.798]
 [48.798]
 [48.798]] [[1.555]
 [1.617]
 [1.555]
 [1.555]
 [1.555]
 [1.555]
 [1.555]]
printing an ep nov before normalisation:  36.920394604074325
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.837]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]] [[39.61 ]
 [42.833]
 [39.61 ]
 [39.61 ]
 [39.61 ]
 [39.61 ]
 [39.61 ]] [[0.676]
 [0.837]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]]
printing an ep nov before normalisation:  45.47882436731754
Printing some Q and Qe and total Qs values:  [[0.92 ]
 [0.951]
 [0.92 ]
 [0.92 ]
 [0.92 ]
 [0.92 ]
 [0.92 ]] [[55.054]
 [44.544]
 [55.054]
 [55.054]
 [55.054]
 [55.054]
 [55.054]] [[0.92 ]
 [0.951]
 [0.92 ]
 [0.92 ]
 [0.92 ]
 [0.92 ]
 [0.92 ]]
printing an ep nov before normalisation:  53.051707582014366
printing an ep nov before normalisation:  39.41979440802011
printing an ep nov before normalisation:  45.03350971712403
actor:  0 policy actor:  1  step number:  47 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  2.0
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  0.02404106015490015
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
siam score:  -0.8445129
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  33.92791610766792
printing an ep nov before normalisation:  32.34647596670724
printing an ep nov before normalisation:  35.35931471426785
printing an ep nov before normalisation:  43.13459002494421
maxi score, test score, baseline:  -0.21566000000000005 0.6966666666666669 0.6966666666666669
probs:  [0.09576906248480657, 0.09576906248480657, 0.09576906248480657, 0.5211546875759672, 0.09576906248480657, 0.09576906248480657]
maxi score, test score, baseline:  -0.21566000000000005 0.6966666666666669 0.6966666666666669
printing an ep nov before normalisation:  39.28281535586738
actor:  1 policy actor:  1  step number:  44 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  36.503028869628906
maxi score, test score, baseline:  -0.21566000000000005 0.6966666666666669 0.6966666666666669
maxi score, test score, baseline:  -0.21566000000000005 0.6966666666666669 0.6966666666666669
probs:  [0.09576906248480657, 0.09576906248480657, 0.09576906248480657, 0.5211546875759672, 0.09576906248480657, 0.09576906248480657]
printing an ep nov before normalisation:  47.94068043006446
maxi score, test score, baseline:  -0.21566000000000005 0.6966666666666669 0.6966666666666669
printing an ep nov before normalisation:  42.07965014771494
maxi score, test score, baseline:  -0.21566000000000005 0.6966666666666669 0.6966666666666669
probs:  [0.09579564151873311, 0.09579564151873311, 0.09579564151873311, 0.5210217924063345, 0.09579564151873311, 0.09579564151873311]
maxi score, test score, baseline:  -0.21566000000000005 0.6966666666666669 0.6966666666666669
probs:  [0.09579564151873311, 0.09579564151873311, 0.09579564151873311, 0.5210217924063345, 0.09579564151873311, 0.09579564151873311]
maxi score, test score, baseline:  -0.21566000000000005 0.6966666666666669 0.6966666666666669
probs:  [0.09579564151873311, 0.09579564151873311, 0.09579564151873311, 0.5210217924063345, 0.09579564151873311, 0.09579564151873311]
printing an ep nov before normalisation:  42.355172116202944
using explorer policy with actor:  1
siam score:  -0.8364644
Printing some Q and Qe and total Qs values:  [[-0.106]
 [-0.082]
 [-0.089]
 [-0.089]
 [-0.107]
 [-0.107]
 [-0.088]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.106]
 [-0.082]
 [-0.089]
 [-0.089]
 [-0.107]
 [-0.107]
 [-0.088]]
maxi score, test score, baseline:  -0.21566000000000005 0.6966666666666669 0.6966666666666669
maxi score, test score, baseline:  -0.21566000000000005 0.6966666666666669 0.6966666666666669
probs:  [0.09579564151873311, 0.09579564151873311, 0.09579564151873311, 0.5210217924063345, 0.09579564151873311, 0.09579564151873311]
maxi score, test score, baseline:  -0.21566000000000005 0.6966666666666669 0.6966666666666669
probs:  [0.09579564151873311, 0.09579564151873311, 0.09579564151873311, 0.5210217924063345, 0.09579564151873311, 0.09579564151873311]
actor:  1 policy actor:  1  step number:  69 total reward:  0.013333333333332642  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.114]
 [-0.114]
 [-0.114]
 [-0.114]
 [-0.114]
 [-0.114]
 [-0.114]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.114]
 [-0.114]
 [-0.114]
 [-0.114]
 [-0.114]
 [-0.114]
 [-0.114]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.21566000000000005 0.6966666666666669 0.6966666666666669
probs:  [0.09584873984557532, 0.09584873984557532, 0.09584873984557532, 0.5207563007721235, 0.09584873984557532, 0.09584873984557532]
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.48 ]
 [0.219]
 [0.219]
 [0.219]
 [0.219]
 [0.219]] [[38.267]
 [42.539]
 [38.267]
 [38.267]
 [38.267]
 [38.267]
 [38.267]] [[0.864]
 [1.25 ]
 [0.864]
 [0.864]
 [0.864]
 [0.864]
 [0.864]]
printing an ep nov before normalisation:  43.00304857616223
printing an ep nov before normalisation:  57.47418383411787
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.57 ]
 [0.57 ]
 [0.593]
 [0.382]
 [0.471]
 [0.57 ]] [[44.858]
 [42.994]
 [42.994]
 [45.929]
 [47.997]
 [52.573]
 [42.994]] [[0.972]
 [1.198]
 [1.198]
 [1.296]
 [1.139]
 [1.345]
 [1.198]]
maxi score, test score, baseline:  -0.21566000000000005 0.6966666666666669 0.6966666666666669
probs:  [0.09584873984557532, 0.09584873984557532, 0.09584873984557532, 0.5207563007721235, 0.09584873984557532, 0.09584873984557532]
maxi score, test score, baseline:  -0.21566000000000005 0.6966666666666669 0.6966666666666669
probs:  [0.09584873984557532, 0.09584873984557532, 0.09584873984557532, 0.5207563007721235, 0.09584873984557532, 0.09584873984557532]
printing an ep nov before normalisation:  23.743193417134137
printing an ep nov before normalisation:  29.39444591306328
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.21566000000000005 0.6966666666666669 0.6966666666666669
printing an ep nov before normalisation:  21.185374282804617
actor:  1 policy actor:  1  step number:  42 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  60 total reward:  0.28666666666666585  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.21566000000000005 0.6966666666666669 0.6966666666666669
probs:  [0.09587525918323352, 0.09587525918323352, 0.09587525918323352, 0.5206237040838324, 0.09587525918323352, 0.09587525918323352]
Printing some Q and Qe and total Qs values:  [[-0.053]
 [-0.036]
 [-0.058]
 [-0.054]
 [-0.052]
 [-0.058]
 [-0.058]] [[56.32 ]
 [43.009]
 [38.574]
 [45.971]
 [45.708]
 [38.574]
 [38.574]] [[1.286]
 [0.83 ]
 [0.651]
 [0.918]
 [0.91 ]
 [0.651]
 [0.651]]
printing an ep nov before normalisation:  49.070089113663556
printing an ep nov before normalisation:  46.42178125977267
printing an ep nov before normalisation:  48.043511621177935
maxi score, test score, baseline:  -0.21566000000000005 0.6966666666666669 0.6966666666666669
probs:  [0.09587525918323352, 0.09587525918323352, 0.09587525918323352, 0.5206237040838324, 0.09587525918323352, 0.09587525918323352]
Printing some Q and Qe and total Qs values:  [[0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]] [[40.117]
 [40.117]
 [40.117]
 [40.117]
 [40.117]
 [40.117]
 [40.117]] [[2.059]
 [2.059]
 [2.059]
 [2.059]
 [2.059]
 [2.059]
 [2.059]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.69 ]
 [0.763]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]] [[22.167]
 [28.901]
 [22.167]
 [22.167]
 [22.167]
 [22.167]
 [22.167]] [[1.438]
 [1.737]
 [1.438]
 [1.438]
 [1.438]
 [1.438]
 [1.438]]
maxi score, test score, baseline:  -0.21566000000000005 0.6966666666666669 0.6966666666666669
probs:  [0.09587525918323352, 0.09587525918323352, 0.09587525918323352, 0.5206237040838324, 0.09587525918323352, 0.09587525918323352]
actor:  1 policy actor:  1  step number:  46 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  29.82817079741501
maxi score, test score, baseline:  -0.21566000000000005 0.6966666666666669 0.6966666666666669
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.439]
 [0.483]
 [0.548]
 [0.415]
 [0.415]
 [0.545]] [[29.669]
 [32.374]
 [35.067]
 [29.552]
 [28.906]
 [28.691]
 [29.496]] [[1.871]
 [2.03 ]
 [2.207]
 [2.   ]
 [1.835]
 [1.825]
 [1.995]]
printing an ep nov before normalisation:  27.965092361674877
siam score:  -0.83863145
maxi score, test score, baseline:  -0.21566000000000005 0.6966666666666669 0.6966666666666669
printing an ep nov before normalisation:  38.85675491643753
maxi score, test score, baseline:  -0.21566000000000005 0.6966666666666669 0.6966666666666669
probs:  [0.09590175866682266, 0.09590175866682266, 0.09590175866682266, 0.5204912066658867, 0.09590175866682266, 0.09590175866682266]
printing an ep nov before normalisation:  34.60678259531657
printing an ep nov before normalisation:  39.623120955051775
actor:  0 policy actor:  0  step number:  56 total reward:  0.33999999999999986  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.21298000000000006 0.6966666666666669 0.6966666666666669
maxi score, test score, baseline:  -0.21298000000000006 0.6966666666666669 0.6966666666666669
probs:  [0.09590175866682266, 0.09590175866682266, 0.09590175866682266, 0.5204912066658867, 0.09590175866682266, 0.09590175866682266]
maxi score, test score, baseline:  -0.21298000000000006 0.6966666666666669 0.6966666666666669
probs:  [0.09590175866682266, 0.09590175866682266, 0.09590175866682266, 0.5204912066658867, 0.09590175866682266, 0.09590175866682266]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.333
from probs:  [0.09590175866682266, 0.09590175866682266, 0.09590175866682266, 0.5204912066658867, 0.09590175866682266, 0.09590175866682266]
maxi score, test score, baseline:  -0.21298000000000006 0.6966666666666669 0.6966666666666669
probs:  [0.09592823831863, 0.09592823831863, 0.09592823831863, 0.5203588084068501, 0.09592823831863, 0.09592823831863]
actions average: 
K:  0  action  0 :  tensor([0.3515, 0.0148, 0.1257, 0.1184, 0.1338, 0.1392, 0.1166],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0076, 0.9418, 0.0064, 0.0092, 0.0038, 0.0048, 0.0262],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1262, 0.0065, 0.3186, 0.0974, 0.0943, 0.2516, 0.1053],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2173, 0.0033, 0.1377, 0.1659, 0.1554, 0.1780, 0.1425],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1441, 0.0023, 0.0786, 0.0886, 0.5381, 0.0832, 0.0651],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1338, 0.0023, 0.1237, 0.0913, 0.0971, 0.4723, 0.0797],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2093, 0.0016, 0.1585, 0.1548, 0.1716, 0.1818, 0.1223],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.21298000000000006 0.6966666666666669 0.6966666666666669
probs:  [0.09592823831863, 0.09592823831863, 0.09592823831863, 0.5203588084068501, 0.09592823831863, 0.09592823831863]
actor:  1 policy actor:  1  step number:  57 total reward:  0.1733333333333329  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.327]
 [0.505]
 [0.485]
 [0.485]
 [0.504]
 [0.485]] [[ 0.   ]
 [30.75 ]
 [34.094]
 [ 0.   ]
 [ 0.   ]
 [33.536]
 [ 0.   ]] [[0.485]
 [0.327]
 [0.505]
 [0.485]
 [0.485]
 [0.504]
 [0.485]]
maxi score, test score, baseline:  -0.21298000000000006 0.6966666666666669 0.6966666666666669
probs:  [0.09592823831863, 0.09592823831863, 0.09592823831863, 0.5203588084068501, 0.09592823831863, 0.09592823831863]
printing an ep nov before normalisation:  33.90448390503544
maxi score, test score, baseline:  -0.21298000000000006 0.6966666666666669 0.6966666666666669
probs:  [0.09592823831863, 0.09592823831863, 0.09592823831863, 0.5203588084068501, 0.09592823831863, 0.09592823831863]
actor:  0 policy actor:  0  step number:  68 total reward:  0.11333333333333284  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  1  action  0 :  tensor([0.3367, 0.0030, 0.1220, 0.1368, 0.1409, 0.1350, 0.1257],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0034, 0.9486, 0.0039, 0.0118, 0.0011, 0.0011, 0.0302],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1266, 0.0206, 0.4387, 0.0956, 0.0940, 0.1396, 0.0848],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1995, 0.0084, 0.1470, 0.1744, 0.1739, 0.1577, 0.1393],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1643, 0.0092, 0.0793, 0.0786, 0.5057, 0.0912, 0.0718],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1336, 0.0045, 0.1573, 0.1164, 0.1199, 0.3386, 0.1297],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1127, 0.2709, 0.0791, 0.1111, 0.0843, 0.0724, 0.2696],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  51 total reward:  0.3199999999999994  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.21076666666666674 0.6966666666666669 0.6966666666666669
probs:  [0.09592823831863, 0.09592823831863, 0.09592823831863, 0.5203588084068501, 0.09592823831863, 0.09592823831863]
printing an ep nov before normalisation:  33.53613540484837
actor:  1 policy actor:  1  step number:  44 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  36.46117794424258
maxi score, test score, baseline:  -0.21076666666666674 0.6966666666666669 0.6966666666666669
probs:  [0.09595469816091037, 0.09595469816091037, 0.09595469816091037, 0.5202265091954481, 0.09595469816091037, 0.09595469816091037]
printing an ep nov before normalisation:  50.91782580134578
printing an ep nov before normalisation:  28.59922764204353
actor:  0 policy actor:  0  step number:  47 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.2079400000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09595469816091037, 0.09595469816091037, 0.09595469816091037, 0.5202265091954481, 0.09595469816091037, 0.09595469816091037]
maxi score, test score, baseline:  -0.2079400000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09595469816091037, 0.09595469816091037, 0.09595469816091037, 0.5202265091954481, 0.09595469816091037, 0.09595469816091037]
printing an ep nov before normalisation:  38.95269700632922
printing an ep nov before normalisation:  37.83448132230621
actor:  0 policy actor:  1  step number:  33 total reward:  0.56  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.20482000000000009 0.6966666666666669 0.6966666666666669
probs:  [0.09598113821588462, 0.09598113821588462, 0.09598113821588462, 0.5200943089205768, 0.09598113821588462, 0.09598113821588462]
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.010950589324920657
printing an ep nov before normalisation:  38.35229169337977
printing an ep nov before normalisation:  33.539352418014495
maxi score, test score, baseline:  -0.20482000000000009 0.6966666666666669 0.6966666666666669
probs:  [0.09598113821588462, 0.09598113821588462, 0.09598113821588462, 0.5200943089205768, 0.09598113821588462, 0.09598113821588462]
maxi score, test score, baseline:  -0.20482000000000009 0.6966666666666669 0.6966666666666669
probs:  [0.09598113821588462, 0.09598113821588462, 0.09598113821588462, 0.5200943089205768, 0.09598113821588462, 0.09598113821588462]
actor:  0 policy actor:  1  step number:  56 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  30.250883403227927
printing an ep nov before normalisation:  30.073486411238026
maxi score, test score, baseline:  -0.20256666666666676 0.6966666666666669 0.6966666666666669
probs:  [0.09598113821588462, 0.09598113821588462, 0.09598113821588462, 0.5200943089205768, 0.09598113821588462, 0.09598113821588462]
actor:  0 policy actor:  1  step number:  46 total reward:  0.3933333333333333  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  4  action  0 :  tensor([0.2065, 0.0106, 0.1689, 0.1642, 0.1597, 0.1427, 0.1474],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0312, 0.8421, 0.0223, 0.0259, 0.0320, 0.0177, 0.0288],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1054, 0.0969, 0.4029, 0.0883, 0.0704, 0.1546, 0.0815],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1449, 0.0709, 0.1935, 0.1654, 0.1275, 0.1444, 0.1535],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1568, 0.0915, 0.1180, 0.1365, 0.3051, 0.0911, 0.1010],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1630, 0.1489, 0.1304, 0.1582, 0.1311, 0.1324, 0.1360],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1532, 0.0688, 0.1709, 0.1510, 0.0973, 0.0979, 0.2609],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.19978000000000007 0.6966666666666669 0.6966666666666669
probs:  [0.09598113821588462, 0.09598113821588462, 0.09598113821588462, 0.5200943089205768, 0.09598113821588462, 0.09598113821588462]
from probs:  [0.09598113821588462, 0.09598113821588462, 0.09598113821588462, 0.5200943089205768, 0.09598113821588462, 0.09598113821588462]
maxi score, test score, baseline:  -0.19978000000000007 0.6966666666666669 0.6966666666666669
maxi score, test score, baseline:  -0.19978000000000007 0.6966666666666669 0.6966666666666669
probs:  [0.09600755850574044, 0.09600755850574044, 0.09600755850574044, 0.519962207471298, 0.09600755850574044, 0.09600755850574044]
printing an ep nov before normalisation:  53.434061244538796
maxi score, test score, baseline:  -0.19978000000000007 0.6966666666666669 0.6966666666666669
printing an ep nov before normalisation:  25.513739207218787
maxi score, test score, baseline:  -0.19978000000000007 0.6966666666666669 0.6966666666666669
probs:  [0.09600755850574044, 0.09600755850574044, 0.09600755850574044, 0.519962207471298, 0.09600755850574044, 0.09600755850574044]
actions average: 
K:  1  action  0 :  tensor([0.2982, 0.0773, 0.1159, 0.1196, 0.1666, 0.0955, 0.1270],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0131, 0.9337, 0.0082, 0.0083, 0.0061, 0.0044, 0.0262],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1415, 0.0139, 0.3675, 0.1121, 0.1206, 0.1428, 0.1017],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1980, 0.0023, 0.1479, 0.1775, 0.1923, 0.1441, 0.1378],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2111, 0.0084, 0.1199, 0.1418, 0.2964, 0.1013, 0.1212],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1368, 0.0026, 0.1478, 0.1178, 0.1130, 0.3861, 0.0959],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1773, 0.0720, 0.1587, 0.1488, 0.1524, 0.1315, 0.1594],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.19978000000000007 0.6966666666666669 0.6966666666666669
probs:  [0.09600755850574044, 0.09600755850574044, 0.09600755850574044, 0.519962207471298, 0.09600755850574044, 0.09600755850574044]
maxi score, test score, baseline:  -0.19978000000000007 0.6966666666666669 0.6966666666666669
probs:  [0.09600755850574044, 0.09600755850574044, 0.09600755850574044, 0.519962207471298, 0.09600755850574044, 0.09600755850574044]
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.503]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]] [[18.315]
 [22.924]
 [18.315]
 [18.315]
 [18.315]
 [18.315]
 [18.315]] [[1.921]
 [2.379]
 [1.921]
 [1.921]
 [1.921]
 [1.921]
 [1.921]]
printing an ep nov before normalisation:  32.04290815432876
actor:  1 policy actor:  1  step number:  54 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  18.835840925111995
actor:  0 policy actor:  0  step number:  54 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  51.440780079166395
printing an ep nov before normalisation:  31.444003582000732
maxi score, test score, baseline:  -0.19752666666666674 0.6966666666666669 0.6966666666666669
maxi score, test score, baseline:  -0.19752666666666674 0.6966666666666669 0.6966666666666669
probs:  [0.09600755850574044, 0.09600755850574044, 0.09600755850574044, 0.519962207471298, 0.09600755850574044, 0.09600755850574044]
printing an ep nov before normalisation:  34.3213424193289
printing an ep nov before normalisation:  31.117597208181053
actor:  0 policy actor:  1  step number:  39 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.02807477244207
actor:  0 policy actor:  0  step number:  45 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.19880666666666674 0.6966666666666669 0.6966666666666669
probs:  [0.09603395905263258, 0.09603395905263258, 0.09603395905263258, 0.5198302047368372, 0.09603395905263258, 0.09603395905263258]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.19880666666666674 0.6966666666666669 0.6966666666666669
probs:  [0.09603395905263258, 0.09603395905263258, 0.09603395905263258, 0.5198302047368372, 0.09603395905263258, 0.09603395905263258]
actor:  1 policy actor:  1  step number:  37 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.19880666666666674 0.6966666666666669 0.6966666666666669
probs:  [0.09603395905263258, 0.09603395905263258, 0.09603395905263258, 0.5198302047368372, 0.09603395905263258, 0.09603395905263258]
printing an ep nov before normalisation:  35.28396278575864
maxi score, test score, baseline:  -0.19880666666666674 0.6966666666666669 0.6966666666666669
probs:  [0.09603395905263258, 0.09603395905263258, 0.09603395905263258, 0.5198302047368372, 0.09603395905263258, 0.09603395905263258]
printing an ep nov before normalisation:  49.558144936475884
maxi score, test score, baseline:  -0.20216666666666674 0.6966666666666669 0.6966666666666669
probs:  [0.09603395905263258, 0.09603395905263258, 0.09603395905263258, 0.5198302047368372, 0.09603395905263258, 0.09603395905263258]
maxi score, test score, baseline:  -0.20216666666666674 0.6966666666666669 0.6966666666666669
probs:  [0.09603395905263258, 0.09603395905263258, 0.09603395905263258, 0.5198302047368372, 0.09603395905263258, 0.09603395905263258]
maxi score, test score, baseline:  -0.20216666666666674 0.6966666666666669 0.6966666666666669
probs:  [0.09603395905263258, 0.09603395905263258, 0.09603395905263258, 0.5198302047368372, 0.09603395905263258, 0.09603395905263258]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3066666666666664  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.20216666666666674 0.6966666666666669 0.6966666666666669
probs:  [0.09603395905263258, 0.09603395905263258, 0.09603395905263258, 0.5198302047368372, 0.09603395905263258, 0.09603395905263258]
maxi score, test score, baseline:  -0.20216666666666674 0.6966666666666669 0.6966666666666669
probs:  [0.09603395905263258, 0.09603395905263258, 0.09603395905263258, 0.5198302047368372, 0.09603395905263258, 0.09603395905263258]
printing an ep nov before normalisation:  22.50281810760498
actor:  0 policy actor:  1  step number:  48 total reward:  0.24666666666666637  reward:  1.0 rdn_beta:  1.667
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.2353],
        [-0.1485],
        [-0.0927],
        [-0.1152],
        [-0.1121],
        [-0.1632],
        [ 0.3429],
        [ 0.0007],
        [-0.0000],
        [-0.2682]], dtype=torch.float64)
-0.032346567066 -0.2676617610467971
-0.09703970119800001 -0.24554759795139824
-0.09703970119800001 -0.18972419733974422
-0.057834381198 -0.1729947693372259
-0.057834381198 -0.16990581713171604
-0.070771701198 -0.2339849066348866
-0.045026434398 0.29789173707721683
-0.08423175439800001 -0.08348566010852136
-0.4705139999999998 -0.4705139999999998
-0.032346567066 -0.3005830280293032
maxi score, test score, baseline:  -0.20306000000000007 0.6966666666666669 0.6966666666666669
probs:  [0.09603395905263258, 0.09603395905263258, 0.09603395905263258, 0.5198302047368372, 0.09603395905263258, 0.09603395905263258]
maxi score, test score, baseline:  -0.20306000000000007 0.6966666666666669 0.6966666666666669
probs:  [0.09603395905263258, 0.09603395905263258, 0.09603395905263258, 0.5198302047368372, 0.09603395905263258, 0.09603395905263258]
printing an ep nov before normalisation:  39.61803097424951
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  49 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  43.925416298254554
printing an ep nov before normalisation:  33.747332581919466
maxi score, test score, baseline:  -0.20359333333333343 0.6966666666666669 0.6966666666666669
probs:  [0.09603395905263258, 0.09603395905263258, 0.09603395905263258, 0.5198302047368372, 0.09603395905263258, 0.09603395905263258]
printing an ep nov before normalisation:  40.526774401542696
actor:  1 policy actor:  1  step number:  46 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  59.402177566719196
printing an ep nov before normalisation:  29.787517775818795
printing an ep nov before normalisation:  44.84924565391905
printing an ep nov before normalisation:  30.435623526671677
siam score:  -0.83753675
maxi score, test score, baseline:  -0.20359333333333343 0.6966666666666669 0.6966666666666669
siam score:  -0.8384096
printing an ep nov before normalisation:  35.388486150300416
actor:  0 policy actor:  1  step number:  41 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.20407333333333344 0.6966666666666669 0.6966666666666669
probs:  [0.09606033987868266, 0.09606033987868266, 0.09606033987868266, 0.5196983006065866, 0.09606033987868266, 0.09606033987868266]
actor:  1 policy actor:  1  step number:  61 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  44.081577917207326
maxi score, test score, baseline:  -0.20407333333333344 0.6966666666666669 0.6966666666666669
printing an ep nov before normalisation:  43.6379595660167
actor:  0 policy actor:  0  step number:  49 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  1.333
using another actor
printing an ep nov before normalisation:  47.308575635775696
maxi score, test score, baseline:  -0.20491333333333342 0.6966666666666669 0.6966666666666669
actor:  1 policy actor:  1  step number:  48 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.20491333333333342 0.6966666666666669 0.6966666666666669
Printing some Q and Qe and total Qs values:  [[-0.039]
 [ 0.147]
 [-0.054]
 [-0.047]
 [-0.049]
 [-0.047]
 [-0.054]] [[45.411]
 [44.279]
 [37.23 ]
 [41.937]
 [41.687]
 [42.767]
 [37.23 ]] [[0.834]
 [0.982]
 [0.54 ]
 [0.707]
 [0.697]
 [0.736]
 [0.54 ]]
maxi score, test score, baseline:  -0.20491333333333342 0.6966666666666669 0.6966666666666669
maxi score, test score, baseline:  -0.20491333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09611304245657802, 0.09611304245657802, 0.09611304245657802, 0.5194347877171099, 0.09611304245657802, 0.09611304245657802]
printing an ep nov before normalisation:  38.645597464217964
siam score:  -0.8353989
actor:  0 policy actor:  1  step number:  56 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.2057400000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09613936425250147, 0.09613936425250147, 0.09613936425250147, 0.5193031787374928, 0.09613936425250147, 0.09613936425250147]
printing an ep nov before normalisation:  51.8868198962716
printing an ep nov before normalisation:  53.22567420692955
maxi score, test score, baseline:  -0.2057400000000001 0.6966666666666669 0.6966666666666669
printing an ep nov before normalisation:  34.18453283031137
printing an ep nov before normalisation:  31.24796925100374
maxi score, test score, baseline:  -0.2057400000000001 0.6966666666666669 0.6966666666666669
maxi score, test score, baseline:  -0.2057400000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09616566641573936, 0.09616566641573936, 0.09616566641573936, 0.5191716679213032, 0.09616566641573936, 0.09616566641573936]
maxi score, test score, baseline:  -0.2057400000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09616566641573936, 0.09616566641573936, 0.09616566641573936, 0.5191716679213032, 0.09616566641573936, 0.09616566641573936]
printing an ep nov before normalisation:  42.597531539866075
printing an ep nov before normalisation:  38.12063978852579
maxi score, test score, baseline:  -0.2057400000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09616566641573936, 0.09616566641573936, 0.09616566641573936, 0.5191716679213032, 0.09616566641573936, 0.09616566641573936]
printing an ep nov before normalisation:  41.6577613505186
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.2057400000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09616566641573936, 0.09616566641573936, 0.09616566641573936, 0.5191716679213032, 0.09616566641573936, 0.09616566641573936]
printing an ep nov before normalisation:  30.51793336868286
actor:  1 policy actor:  1  step number:  46 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  40 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.20614000000000007 0.6966666666666669 0.6966666666666669
probs:  [0.09619194896824897, 0.09619194896824897, 0.09619194896824897, 0.5190402551587553, 0.09619194896824897, 0.09619194896824897]
maxi score, test score, baseline:  -0.20614000000000007 0.6966666666666669 0.6966666666666669
probs:  [0.09619194896824897, 0.09619194896824897, 0.09619194896824897, 0.5190402551587553, 0.09619194896824897, 0.09619194896824897]
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.55 ]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]] [[56.836]
 [53.324]
 [56.836]
 [56.836]
 [56.836]
 [56.836]
 [56.836]] [[1.417]
 [1.477]
 [1.417]
 [1.417]
 [1.417]
 [1.417]
 [1.417]]
siam score:  -0.8393552
maxi score, test score, baseline:  -0.20614000000000007 0.6966666666666669 0.6966666666666669
probs:  [0.09619194896824897, 0.09619194896824897, 0.09619194896824897, 0.5190402551587553, 0.09619194896824897, 0.09619194896824897]
maxi score, test score, baseline:  -0.20614000000000007 0.6966666666666669 0.6966666666666669
maxi score, test score, baseline:  -0.20614000000000007 0.6966666666666669 0.6966666666666669
probs:  [0.09619194896824897, 0.09619194896824897, 0.09619194896824897, 0.5190402551587553, 0.09619194896824897, 0.09619194896824897]
siam score:  -0.8415949
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.587]
 [0.613]
 [0.42 ]
 [0.535]
 [0.422]
 [0.541]] [[33.812]
 [33.079]
 [32.42 ]
 [34.5  ]
 [33.485]
 [34.407]
 [33.134]] [[1.509]
 [1.502]
 [1.49 ]
 [1.414]
 [1.472]
 [1.411]
 [1.458]]
using explorer policy with actor:  1
main train batch thing paused
add a thread
Adding thread: now have 3 threads
actor:  1 policy actor:  1  step number:  55 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  74 total reward:  0.046666666666665524  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  35.040923032632165
Printing some Q and Qe and total Qs values:  [[-0.076]
 [-0.076]
 [-0.076]
 [-0.076]
 [-0.076]
 [-0.076]
 [-0.076]] [[43.972]
 [43.972]
 [43.972]
 [43.972]
 [43.972]
 [43.972]
 [43.972]] [[0.979]
 [0.979]
 [0.979]
 [0.979]
 [0.979]
 [0.979]
 [0.979]]
printing an ep nov before normalisation:  44.11655928977199
maxi score, test score, baseline:  -0.20948666666666674 0.6966666666666669 0.6966666666666669
probs:  [0.09624445532874695, 0.09624445532874695, 0.09624445532874695, 0.5187777233562653, 0.09624445532874695, 0.09624445532874695]
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.544]
 [0.464]
 [0.379]
 [0.483]
 [0.381]
 [0.376]] [[33.509]
 [33.085]
 [33.056]
 [32.254]
 [33.398]
 [31.968]
 [32.452]] [[0.455]
 [0.544]
 [0.464]
 [0.379]
 [0.483]
 [0.381]
 [0.376]]
maxi score, test score, baseline:  -0.20948666666666674 0.6966666666666669 0.6966666666666669
probs:  [0.09624445532874695, 0.09624445532874695, 0.09624445532874695, 0.5187777233562653, 0.09624445532874695, 0.09624445532874695]
Printing some Q and Qe and total Qs values:  [[0.18 ]
 [0.484]
 [0.134]
 [0.196]
 [0.18 ]
 [0.064]
 [0.192]] [[37.181]
 [30.524]
 [33.815]
 [34.495]
 [37.181]
 [34.803]
 [33.62 ]] [[1.405]
 [1.282]
 [1.144]
 [1.249]
 [1.405]
 [1.137]
 [1.189]]
Printing some Q and Qe and total Qs values:  [[0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]] [[34.343]
 [34.343]
 [34.343]
 [34.343]
 [34.343]
 [34.343]
 [34.343]] [[1.347]
 [1.347]
 [1.347]
 [1.347]
 [1.347]
 [1.347]
 [1.347]]
maxi score, test score, baseline:  -0.20948666666666674 0.6966666666666669 0.6966666666666669
actions average: 
K:  4  action  0 :  tensor([0.3221, 0.1207, 0.1014, 0.1186, 0.1231, 0.1030, 0.1111],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0115, 0.9422, 0.0095, 0.0091, 0.0067, 0.0059, 0.0150],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1961, 0.0039, 0.2872, 0.1475, 0.1293, 0.1228, 0.1131],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1761, 0.0914, 0.1004, 0.2277, 0.1860, 0.1125, 0.1059],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1802, 0.0945, 0.0823, 0.1144, 0.3404, 0.0977, 0.0906],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2163, 0.0209, 0.1443, 0.1678, 0.1295, 0.1581, 0.1631],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1260, 0.1573, 0.0896, 0.2238, 0.0935, 0.0974, 0.2124],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  34.64549455546445
maxi score, test score, baseline:  -0.20948666666666674 0.6966666666666669 0.6966666666666669
probs:  [0.09627067918048603, 0.09627067918048603, 0.09627067918048603, 0.5186466040975699, 0.09627067918048603, 0.09627067918048603]
maxi score, test score, baseline:  -0.20948666666666674 0.6966666666666669 0.6966666666666669
actor:  1 policy actor:  1  step number:  53 total reward:  0.3066666666666661  reward:  1.0 rdn_beta:  1.0
from probs:  [0.09627067918048603, 0.09627067918048603, 0.09627067918048603, 0.5186466040975699, 0.09627067918048603, 0.09627067918048603]
line 256 mcts: sample exp_bonus 0.0183638447477108
actor:  0 policy actor:  0  step number:  48 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.21020666666666676 0.6966666666666669 0.6966666666666669
probs:  [0.09627067918048603, 0.09627067918048603, 0.09627067918048603, 0.5186466040975699, 0.09627067918048603, 0.09627067918048603]
printing an ep nov before normalisation:  41.993934919251146
maxi score, test score, baseline:  -0.21020666666666676 0.6966666666666669 0.6966666666666669
probs:  [0.09627067918048603, 0.09627067918048603, 0.09627067918048603, 0.5186466040975699, 0.09627067918048603, 0.09627067918048603]
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.41279495893097
printing an ep nov before normalisation:  36.04235775779057
actor:  0 policy actor:  0  step number:  51 total reward:  0.29333333333333267  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  26.670924731640074
printing an ep nov before normalisation:  25.51674060684264
printing an ep nov before normalisation:  30.16918420791626
actions average: 
K:  0  action  0 :  tensor([0.3223, 0.0097, 0.1073, 0.1119, 0.2143, 0.1227, 0.1117],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0035, 0.9618, 0.0040, 0.0088, 0.0013, 0.0019, 0.0187],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1608, 0.0348, 0.3653, 0.0956, 0.1027, 0.1481, 0.0928],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1636, 0.1039, 0.1313, 0.1827, 0.1467, 0.1534, 0.1184],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1899, 0.0055, 0.0717, 0.0903, 0.4946, 0.0850, 0.0630],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1104, 0.0051, 0.1635, 0.1111, 0.1163, 0.4030, 0.0907],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0742, 0.2391, 0.1202, 0.1114, 0.0687, 0.0879, 0.2984],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.09541596845752
maxi score, test score, baseline:  -0.21096666666666677 0.6966666666666669 0.6966666666666669
Printing some Q and Qe and total Qs values:  [[0.227]
 [0.227]
 [0.227]
 [0.227]
 [0.225]
 [0.227]
 [0.227]] [[36.766]
 [36.766]
 [36.766]
 [36.766]
 [34.512]
 [36.766]
 [36.766]] [[1.43 ]
 [1.43 ]
 [1.43 ]
 [1.43 ]
 [1.301]
 [1.43 ]
 [1.43 ]]
printing an ep nov before normalisation:  34.7570667381294
siam score:  -0.8398157
printing an ep nov before normalisation:  38.67065658650253
printing an ep nov before normalisation:  26.45204782485962
maxi score, test score, baseline:  -0.21096666666666677 0.6966666666666669 0.6966666666666669
probs:  [0.09627067918048603, 0.09627067918048603, 0.09627067918048603, 0.5186466040975699, 0.09627067918048603, 0.09627067918048603]
Printing some Q and Qe and total Qs values:  [[-0.05 ]
 [-0.054]
 [-0.065]
 [-0.068]
 [-0.068]
 [-0.069]
 [-0.061]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.05 ]
 [-0.054]
 [-0.065]
 [-0.068]
 [-0.068]
 [-0.069]
 [-0.061]]
Printing some Q and Qe and total Qs values:  [[-0.073]
 [-0.048]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.073]
 [-0.048]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]]
printing an ep nov before normalisation:  38.83755357590852
actor:  1 policy actor:  1  step number:  61 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.21096666666666677 0.6966666666666669 0.6966666666666669
probs:  [0.09627067918048603, 0.09627067918048603, 0.09627067918048603, 0.5186466040975699, 0.09627067918048603, 0.09627067918048603]
maxi score, test score, baseline:  -0.21096666666666677 0.6966666666666669 0.6966666666666669
probs:  [0.09627067918048603, 0.09627067918048603, 0.09627067918048603, 0.5186466040975699, 0.09627067918048603, 0.09627067918048603]
Printing some Q and Qe and total Qs values:  [[-0.071]
 [-0.112]
 [-0.139]
 [-0.116]
 [-0.106]
 [-0.139]
 [-0.125]] [[34.881]
 [36.324]
 [34.34 ]
 [33.193]
 [34.798]
 [33.996]
 [34.177]] [[0.194]
 [0.175]
 [0.118]
 [0.124]
 [0.158]
 [0.113]
 [0.13 ]]
actor:  1 policy actor:  1  step number:  66 total reward:  0.20666666666666544  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8349794
actions average: 
K:  4  action  0 :  tensor([0.3760, 0.0795, 0.1095, 0.0914, 0.1321, 0.1107, 0.1008],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0129, 0.9040, 0.0157, 0.0192, 0.0075, 0.0105, 0.0303],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1601, 0.0897, 0.1474, 0.1494, 0.1475, 0.1482, 0.1577],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1875, 0.0599, 0.1311, 0.1543, 0.1162, 0.1447, 0.2063],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1246, 0.0081, 0.0479, 0.0835, 0.6056, 0.0593, 0.0710],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1533, 0.0471, 0.1724, 0.1217, 0.1070, 0.2224, 0.1760],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1368, 0.1651, 0.1086, 0.1006, 0.1052, 0.1005, 0.2831],
       grad_fn=<DivBackward0>)
siam score:  -0.8332454
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.21096666666666677 0.6966666666666669 0.6966666666666669
probs:  [0.09629688350899805, 0.09629688350899805, 0.09629688350899805, 0.5185155824550098, 0.09629688350899805, 0.09629688350899805]
printing an ep nov before normalisation:  28.466857707059702
actions average: 
K:  4  action  0 :  tensor([0.2673, 0.1892, 0.0927, 0.1044, 0.1298, 0.0925, 0.1242],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0188, 0.8957, 0.0185, 0.0131, 0.0121, 0.0156, 0.0261],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1917, 0.0044, 0.1822, 0.1364, 0.1450, 0.1988, 0.1416],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1598, 0.0151, 0.1094, 0.2893, 0.1516, 0.1396, 0.1351],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1351, 0.0035, 0.0495, 0.0459, 0.6709, 0.0525, 0.0425],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1265, 0.0070, 0.1204, 0.0986, 0.0584, 0.5042, 0.0849],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1641, 0.2406, 0.1352, 0.0929, 0.0827, 0.1174, 0.1671],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  60 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.21096666666666677 0.6966666666666669 0.6966666666666669
probs:  [0.096323068336077, 0.096323068336077, 0.096323068336077, 0.5183846583196152, 0.096323068336077, 0.096323068336077]
maxi score, test score, baseline:  -0.21096666666666677 0.6966666666666669 0.6966666666666669
probs:  [0.096323068336077, 0.096323068336077, 0.096323068336077, 0.5183846583196152, 0.096323068336077, 0.096323068336077]
printing an ep nov before normalisation:  0.0037895860846977785
printing an ep nov before normalisation:  49.516238997951575
maxi score, test score, baseline:  -0.21096666666666677 0.6966666666666669 0.6966666666666669
printing an ep nov before normalisation:  36.219265715018615
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.685]
 [0.531]
 [0.63 ]
 [0.506]
 [0.586]
 [0.568]] [[38.837]
 [33.416]
 [37.601]
 [36.761]
 [40.545]
 [39.357]
 [36.364]] [[1.402]
 [1.408]
 [1.4  ]
 [1.47 ]
 [1.478]
 [1.516]
 [1.394]]
maxi score, test score, baseline:  -0.21096666666666677 0.6966666666666669 0.6966666666666669
probs:  [0.09634923368348423, 0.09634923368348423, 0.09634923368348423, 0.5182538315825789, 0.09634923368348423, 0.09634923368348423]
maxi score, test score, baseline:  -0.21096666666666677 0.6966666666666669 0.6966666666666669
probs:  [0.09634923368348423, 0.09634923368348423, 0.09634923368348423, 0.5182538315825789, 0.09634923368348423, 0.09634923368348423]
printing an ep nov before normalisation:  42.98165146243397
Printing some Q and Qe and total Qs values:  [[-0.075]
 [-0.064]
 [-0.075]
 [-0.075]
 [-0.075]
 [-0.077]
 [-0.097]] [[47.196]
 [46.324]
 [47.196]
 [47.196]
 [47.196]
 [35.559]
 [42.837]] [[0.828]
 [0.806]
 [0.828]
 [0.828]
 [0.828]
 [0.394]
 [0.644]]
from probs:  [0.09634923368348423, 0.09634923368348423, 0.09634923368348423, 0.5182538315825789, 0.09634923368348423, 0.09634923368348423]
maxi score, test score, baseline:  -0.21096666666666677 0.6966666666666669 0.6966666666666669
probs:  [0.09637537957294916, 0.09637537957294916, 0.09637537957294916, 0.5181231021352541, 0.09637537957294916, 0.09637537957294916]
maxi score, test score, baseline:  -0.21096666666666677 0.6966666666666669 0.6966666666666669
probs:  [0.09637537957294916, 0.09637537957294916, 0.09637537957294916, 0.5181231021352541, 0.09637537957294916, 0.09637537957294916]
printing an ep nov before normalisation:  35.48367941995395
maxi score, test score, baseline:  -0.21096666666666677 0.6966666666666669 0.6966666666666669
probs:  [0.09637537957294916, 0.09637537957294916, 0.09637537957294916, 0.5181231021352541, 0.09637537957294916, 0.09637537957294916]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.21096666666666677 0.6966666666666669 0.6966666666666669
probs:  [0.0964015060261684, 0.0964015060261684, 0.0964015060261684, 0.5179924698691581, 0.0964015060261684, 0.0964015060261684]
printing an ep nov before normalisation:  36.293291747633965
printing an ep nov before normalisation:  36.50897978150151
maxi score, test score, baseline:  -0.21096666666666677 0.6966666666666669 0.6966666666666669
probs:  [0.0964015060261684, 0.0964015060261684, 0.0964015060261684, 0.5179924698691581, 0.0964015060261684, 0.0964015060261684]
maxi score, test score, baseline:  -0.21096666666666677 0.6966666666666669 0.6966666666666669
printing an ep nov before normalisation:  11.290314165277593
actor:  1 policy actor:  1  step number:  67 total reward:  0.10666666666666591  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.21096666666666677 0.6966666666666669 0.6966666666666669
probs:  [0.09642761306480682, 0.09642761306480682, 0.09642761306480682, 0.517861934675966, 0.09642761306480682, 0.09642761306480682]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.673]
 [0.673]
 [0.677]
 [0.673]
 [0.673]
 [0.673]
 [0.673]] [[14.84 ]
 [14.84 ]
 [17.723]
 [14.84 ]
 [14.84 ]
 [14.84 ]
 [14.84 ]] [[0.673]
 [0.673]
 [0.677]
 [0.673]
 [0.673]
 [0.673]
 [0.673]]
printing an ep nov before normalisation:  19.34664035851388
maxi score, test score, baseline:  -0.21096666666666677 0.6966666666666669 0.6966666666666669
actor:  0 policy actor:  1  step number:  64 total reward:  0.33999999999999897  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.21162000000000009 0.6966666666666669 0.6966666666666669
probs:  [0.09642761306480682, 0.09642761306480682, 0.09642761306480682, 0.517861934675966, 0.09642761306480682, 0.09642761306480682]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.21162000000000009 0.6966666666666669 0.6966666666666669
probs:  [0.09642761306480682, 0.09642761306480682, 0.09642761306480682, 0.517861934675966, 0.09642761306480682, 0.09642761306480682]
actor:  1 policy actor:  1  step number:  61 total reward:  0.25333333333333263  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.21162000000000009 0.6966666666666669 0.6966666666666669
probs:  [0.09645370071049668, 0.09645370071049668, 0.09645370071049668, 0.5177314964475167, 0.09645370071049668, 0.09645370071049668]
maxi score, test score, baseline:  -0.21162000000000009 0.6966666666666669 0.6966666666666669
probs:  [0.09645370071049668, 0.09645370071049668, 0.09645370071049668, 0.5177314964475167, 0.09645370071049668, 0.09645370071049668]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  41 total reward:  0.3599999999999999  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  49.21292806348217
printing an ep nov before normalisation:  45.08565125624389
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.678]
 [0.528]
 [0.505]
 [0.501]
 [0.503]
 [0.528]] [[34.367]
 [37.151]
 [38.624]
 [39.634]
 [42.951]
 [40.519]
 [38.624]] [[0.497]
 [0.678]
 [0.528]
 [0.505]
 [0.501]
 [0.503]
 [0.528]]
Printing some Q and Qe and total Qs values:  [[0.799]
 [0.886]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]] [[33.028]
 [35.741]
 [33.028]
 [33.028]
 [33.028]
 [33.028]
 [33.028]] [[0.799]
 [0.886]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]]
printing an ep nov before normalisation:  32.395726696995396
actor:  0 policy actor:  1  step number:  38 total reward:  0.5  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  45 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  26.864724159240723
Printing some Q and Qe and total Qs values:  [[-0.055]
 [-0.035]
 [-0.094]
 [-0.055]
 [-0.114]
 [-0.099]
 [-0.044]] [[30.426]
 [29.584]
 [28.367]
 [30.426]
 [28.253]
 [25.989]
 [29.347]] [[1.152]
 [1.118]
 [0.981]
 [1.152]
 [0.954]
 [0.826]
 [1.094]]
maxi score, test score, baseline:  -0.21247333333333346 0.6966666666666669 0.6966666666666669
probs:  [0.0965058179094003, 0.0965058179094003, 0.0965058179094003, 0.5174709104529986, 0.0965058179094003, 0.0965058179094003]
maxi score, test score, baseline:  -0.21247333333333346 0.6966666666666669 0.6966666666666669
probs:  [0.0965058179094003, 0.0965058179094003, 0.0965058179094003, 0.5174709104529986, 0.0965058179094003, 0.0965058179094003]
actor:  0 policy actor:  1  step number:  49 total reward:  0.306666666666666  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  24.6790116385921
Printing some Q and Qe and total Qs values:  [[0.276]
 [0.352]
 [0.271]
 [0.276]
 [0.28 ]
 [0.276]
 [0.276]] [[29.015]
 [47.536]
 [34.203]
 [29.015]
 [36.001]
 [29.015]
 [29.015]] [[0.591]
 [1.23 ]
 [0.743]
 [0.591]
 [0.807]
 [0.591]
 [0.591]]
maxi score, test score, baseline:  -0.2131800000000001 0.6966666666666669 0.6966666666666669
maxi score, test score, baseline:  -0.2131800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09653184750571837, 0.09653184750571837, 0.09653184750571837, 0.5173407624714081, 0.09653184750571837, 0.09653184750571837]
printing an ep nov before normalisation:  49.8184011951607
maxi score, test score, baseline:  -0.2131800000000001 0.6966666666666669 0.6966666666666669
printing an ep nov before normalisation:  30.99730243229255
maxi score, test score, baseline:  -0.2131800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09653184750571837, 0.09653184750571837, 0.09653184750571837, 0.5173407624714081, 0.09653184750571837, 0.09653184750571837]
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.513]
 [0.615]
 [0.492]
 [0.493]
 [0.615]
 [0.615]] [[26.036]
 [28.457]
 [31.51 ]
 [25.523]
 [25.619]
 [31.51 ]
 [31.51 ]] [[0.484]
 [0.513]
 [0.615]
 [0.492]
 [0.493]
 [0.615]
 [0.615]]
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.618]
 [0.495]
 [0.489]
 [0.493]
 [0.489]
 [0.483]] [[24.8  ]
 [31.994]
 [23.803]
 [24.231]
 [24.169]
 [24.143]
 [24.265]] [[0.474]
 [0.618]
 [0.495]
 [0.489]
 [0.493]
 [0.489]
 [0.483]]
actor:  1 policy actor:  1  step number:  57 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2131800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09655785779529708, 0.09655785779529708, 0.09655785779529708, 0.5172107110235147, 0.09655785779529708, 0.09655785779529708]
printing an ep nov before normalisation:  43.144060917766105
maxi score, test score, baseline:  -0.2131800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09655785779529708, 0.09655785779529708, 0.09655785779529708, 0.5172107110235147, 0.09655785779529708, 0.09655785779529708]
siam score:  -0.8329068
actor:  1 policy actor:  1  step number:  49 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2131800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09658384879960877, 0.09658384879960877, 0.09658384879960877, 0.5170807560019561, 0.09658384879960877, 0.09658384879960877]
actor:  0 policy actor:  1  step number:  48 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  46.102840597816055
maxi score, test score, baseline:  -0.2135800000000001 0.6966666666666669 0.6966666666666669
printing an ep nov before normalisation:  53.7752967706683
maxi score, test score, baseline:  -0.2135800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09660982054009429, 0.09660982054009429, 0.09660982054009429, 0.5169508972995287, 0.09660982054009429, 0.09660982054009429]
maxi score, test score, baseline:  -0.2135800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09660982054009429, 0.09660982054009429, 0.09660982054009429, 0.5169508972995287, 0.09660982054009429, 0.09660982054009429]
printing an ep nov before normalisation:  43.730590324221495
actor:  1 policy actor:  1  step number:  52 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  52 total reward:  0.2599999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2135800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09668762039252127, 0.09668762039252127, 0.09668762039252127, 0.5165618980373937, 0.09668762039252127, 0.09668762039252127]
printing an ep nov before normalisation:  36.23091883848629
maxi score, test score, baseline:  -0.2135800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09671351529147228, 0.09671351529147228, 0.09671351529147228, 0.5164324235426386, 0.09671351529147228, 0.09671351529147228]
Printing some Q and Qe and total Qs values:  [[-0.043]
 [-0.019]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]] [[31.276]
 [33.203]
 [31.276]
 [31.276]
 [31.276]
 [31.276]
 [31.276]] [[0.636]
 [0.736]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]]
maxi score, test score, baseline:  -0.2135800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09671351529147228, 0.09671351529147228, 0.09671351529147228, 0.5164324235426386, 0.09671351529147228, 0.09671351529147228]
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.616]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]] [[32.695]
 [40.139]
 [32.695]
 [32.695]
 [32.695]
 [32.695]
 [32.695]] [[0.988]
 [1.479]
 [0.988]
 [0.988]
 [0.988]
 [0.988]
 [0.988]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.2135800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09671351529147228, 0.09671351529147228, 0.09671351529147228, 0.5164324235426386, 0.09671351529147228, 0.09671351529147228]
printing an ep nov before normalisation:  31.683962120638007
printing an ep nov before normalisation:  40.71666796884042
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.763]
 [0.576]
 [0.61 ]
 [0.439]
 [0.583]
 [0.62 ]] [[23.198]
 [24.293]
 [23.636]
 [24.94 ]
 [25.631]
 [25.707]
 [25.087]] [[1.329]
 [1.537]
 [1.296]
 [1.439]
 [1.326]
 [1.477]
 [1.461]]
printing an ep nov before normalisation:  23.614297609318413
actor:  1 policy actor:  1  step number:  40 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2135800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09673939103332407, 0.09673939103332407, 0.09673939103332407, 0.5163030448333796, 0.09673939103332407, 0.09673939103332407]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  50 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  10.531541279384069
printing an ep nov before normalisation:  26.184857176579463
using explorer policy with actor:  1
printing an ep nov before normalisation:  24.927206631758043
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2135800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09679108513070324, 0.09679108513070324, 0.09679108513070324, 0.5160445743464839, 0.09679108513070324, 0.09679108513070324]
printing an ep nov before normalisation:  32.098894119262695
maxi score, test score, baseline:  -0.2135800000000001 0.6966666666666669 0.6966666666666669
actor:  1 policy actor:  1  step number:  54 total reward:  0.40666666666666684  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  27.957279682159424
printing an ep nov before normalisation:  28.701818849079032
actor:  1 policy actor:  1  step number:  62 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2135800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09681690352863767, 0.09681690352863767, 0.09681690352863767, 0.5159154823568117, 0.09681690352863767, 0.09681690352863767]
printing an ep nov before normalisation:  48.506562541540184
maxi score, test score, baseline:  -0.2135800000000001 0.6966666666666669 0.6966666666666669
printing an ep nov before normalisation:  42.94948687205467
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.67995738794405
printing an ep nov before normalisation:  40.34367710351654
maxi score, test score, baseline:  -0.2135800000000001 0.6966666666666669 0.6966666666666669
actor:  1 policy actor:  1  step number:  39 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  1.333
from probs:  [0.09681690352863767, 0.09681690352863767, 0.09681690352863767, 0.5159154823568117, 0.09681690352863767, 0.09681690352863767]
maxi score, test score, baseline:  -0.2135800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09684270285428845, 0.09684270285428845, 0.09684270285428845, 0.5157864857285578, 0.09684270285428845, 0.09684270285428845]
printing an ep nov before normalisation:  43.28007660072153
printing an ep nov before normalisation:  26.13275044762345
actor:  1 policy actor:  1  step number:  55 total reward:  0.37333333333333285  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.2135800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09689424437320884, 0.09689424437320884, 0.09689424437320884, 0.5155287781339559, 0.09689424437320884, 0.09689424437320884]
maxi score, test score, baseline:  -0.2135800000000001 0.6966666666666669 0.6966666666666669
maxi score, test score, baseline:  -0.2135800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09689424437320884, 0.09689424437320884, 0.09689424437320884, 0.5155287781339559, 0.09689424437320884, 0.09689424437320884]
maxi score, test score, baseline:  -0.2135800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09689424437320884, 0.09689424437320884, 0.09689424437320884, 0.5155287781339559, 0.09689424437320884, 0.09689424437320884]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2135800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09689424437320884, 0.09689424437320884, 0.09689424437320884, 0.5155287781339559, 0.09689424437320884, 0.09689424437320884]
actor:  0 policy actor:  0  step number:  52 total reward:  0.1933333333333327  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.564]
 [0.435]
 [0.484]
 [0.626]
 [0.46 ]
 [0.471]] [[32.262]
 [42.337]
 [41.593]
 [38.849]
 [39.393]
 [40.957]
 [38.071]] [[1.045]
 [1.481]
 [1.324]
 [1.271]
 [1.434]
 [1.325]
 [1.23 ]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3533333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.21119333333333343 0.6966666666666669 0.6966666666666669
probs:  [0.09691998660863617, 0.09691998660863617, 0.09691998660863617, 0.5154000669568193, 0.09691998660863617, 0.09691998660863617]
printing an ep nov before normalisation:  38.21948655523627
printing an ep nov before normalisation:  46.86049151637837
siam score:  -0.834556
maxi score, test score, baseline:  -0.21119333333333343 0.6966666666666669 0.6966666666666669
probs:  [0.09691998660863617, 0.09691998660863617, 0.09691998660863617, 0.5154000669568193, 0.09691998660863617, 0.09691998660863617]
maxi score, test score, baseline:  -0.21119333333333343 0.6966666666666669 0.6966666666666669
probs:  [0.09691998660863617, 0.09691998660863617, 0.09691998660863617, 0.5154000669568193, 0.09691998660863617, 0.09691998660863617]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.21119333333333343 0.6966666666666669 0.6966666666666669
probs:  [0.09694570985609464, 0.09694570985609464, 0.09694570985609464, 0.5152714507195267, 0.09694570985609464, 0.09694570985609464]
actor:  0 policy actor:  0  step number:  37 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.20828666666666676 0.6966666666666669 0.6966666666666669
probs:  [0.09694570985609464, 0.09694570985609464, 0.09694570985609464, 0.5152714507195267, 0.09694570985609464, 0.09694570985609464]
printing an ep nov before normalisation:  34.23751839357892
maxi score, test score, baseline:  -0.2082866666666668 0.6966666666666669 0.6966666666666669
printing an ep nov before normalisation:  43.60125194808419
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2082866666666668 0.6966666666666669 0.6966666666666669
line 256 mcts: sample exp_bonus 43.937107419339874
printing an ep nov before normalisation:  0.03871645173063598
actor:  1 policy actor:  1  step number:  48 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  35.897343158721924
maxi score, test score, baseline:  -0.2082866666666668 0.6966666666666669 0.6966666666666669
probs:  [0.09697141413658555, 0.09697141413658555, 0.09697141413658555, 0.5151429293170723, 0.09697141413658555, 0.09697141413658555]
actor:  0 policy actor:  0  step number:  39 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  1.333
from probs:  [0.09697141413658555, 0.09697141413658555, 0.09697141413658555, 0.5151429293170723, 0.09697141413658555, 0.09697141413658555]
maxi score, test score, baseline:  -0.20535333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09697141413658555, 0.09697141413658555, 0.09697141413658555, 0.5151429293170723, 0.09697141413658555, 0.09697141413658555]
maxi score, test score, baseline:  -0.20535333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09697141413658555, 0.09697141413658555, 0.09697141413658555, 0.5151429293170723, 0.09697141413658555, 0.09697141413658555]
printing an ep nov before normalisation:  49.08665896144732
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.664]
 [0.509]
 [0.531]
 [0.607]
 [0.607]
 [0.585]] [[34.823]
 [35.252]
 [38.833]
 [37.224]
 [34.823]
 [34.823]
 [35.454]] [[1.634]
 [1.703]
 [1.654]
 [1.629]
 [1.634]
 [1.634]
 [1.63 ]]
Printing some Q and Qe and total Qs values:  [[0.876]
 [0.949]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]] [[32.165]
 [34.763]
 [26.662]
 [26.662]
 [26.662]
 [26.662]
 [26.662]] [[0.876]
 [0.949]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.20535333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09697141413658555, 0.09697141413658555, 0.09697141413658555, 0.5151429293170723, 0.09697141413658555, 0.09697141413658555]
actor:  0 policy actor:  0  step number:  54 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  25.90281023687123
actor:  1 policy actor:  1  step number:  48 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.075]
 [-0.07 ]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.075]
 [-0.07 ]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]]
printing an ep nov before normalisation:  40.1788514568311
printing an ep nov before normalisation:  47.462722027771136
printing an ep nov before normalisation:  37.708890746558865
printing an ep nov before normalisation:  36.275988567088575
actions average: 
K:  3  action  0 :  tensor([0.4550, 0.0709, 0.1293, 0.0787, 0.0682, 0.0931, 0.1048],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0103, 0.9428, 0.0083, 0.0092, 0.0060, 0.0063, 0.0172],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1033, 0.0064, 0.4521, 0.0835, 0.0630, 0.1886, 0.1031],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1942, 0.0255, 0.1576, 0.1802, 0.1263, 0.1601, 0.1561],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1930, 0.1071, 0.1077, 0.1364, 0.2563, 0.1047, 0.0949],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1498, 0.1335, 0.1946, 0.1131, 0.0850, 0.2500, 0.0740],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1612, 0.0473, 0.1690, 0.1560, 0.0771, 0.1733, 0.2162],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.4496473685462
printing an ep nov before normalisation:  40.04979445872237
maxi score, test score, baseline:  -0.20590000000000008 0.6966666666666669 0.6966666666666669
probs:  [0.09702276588051373, 0.09702276588051373, 0.09702276588051373, 0.5148861705974314, 0.09702276588051373, 0.09702276588051373]
actions average: 
K:  0  action  0 :  tensor([0.4313, 0.0035, 0.1105, 0.1029, 0.1336, 0.1166, 0.1016],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0043, 0.9567, 0.0039, 0.0066, 0.0024, 0.0019, 0.0242],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1096, 0.0119, 0.4819, 0.0892, 0.0830, 0.1418, 0.0827],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1634, 0.0304, 0.1134, 0.3409, 0.1107, 0.1422, 0.0990],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1639, 0.0005, 0.0892, 0.1002, 0.4713, 0.0905, 0.0844],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1678, 0.0026, 0.1236, 0.1039, 0.0977, 0.4004, 0.1039],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1523, 0.0821, 0.1503, 0.1294, 0.1197, 0.1343, 0.2319],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  46 total reward:  0.5800000000000003  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  30.549175416037926
actor:  1 policy actor:  1  step number:  50 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.20590000000000008 0.6966666666666669 0.6966666666666669
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]] [[36.419]
 [36.419]
 [36.419]
 [36.419]
 [36.419]
 [36.419]
 [36.419]] [[1.536]
 [1.536]
 [1.536]
 [1.536]
 [1.536]
 [1.536]
 [1.536]]
printing an ep nov before normalisation:  0.004820357482913096
maxi score, test score, baseline:  -0.20590000000000008 0.6966666666666669 0.6966666666666669
probs:  [0.09707404200781133, 0.09707404200781133, 0.09707404200781133, 0.5146297899609433, 0.09707404200781133, 0.09707404200781133]
actor:  1 policy actor:  1  step number:  58 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.836778682308285
Printing some Q and Qe and total Qs values:  [[0.143]
 [0.358]
 [0.165]
 [0.165]
 [0.175]
 [0.165]
 [0.165]] [[32.72 ]
 [37.109]
 [35.333]
 [35.333]
 [32.002]
 [35.333]
 [35.333]] [[0.682]
 [1.049]
 [0.795]
 [0.795]
 [0.689]
 [0.795]
 [0.795]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.211]
 [0.41 ]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]] [[36.434]
 [38.047]
 [36.434]
 [36.434]
 [36.434]
 [36.434]
 [36.434]] [[1.065]
 [1.333]
 [1.065]
 [1.065]
 [1.065]
 [1.065]
 [1.065]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.20590000000000008 0.6966666666666669 0.6966666666666669
maxi score, test score, baseline:  -0.20590000000000008 0.6966666666666669 0.6966666666666669
probs:  [0.09712524268537623, 0.09712524268537623, 0.09712524268537623, 0.5143737865731189, 0.09712524268537623, 0.09712524268537623]
printing an ep nov before normalisation:  47.01989054737565
maxi score, test score, baseline:  -0.20590000000000008 0.6966666666666669 0.6966666666666669
probs:  [0.09712524268537623, 0.09712524268537623, 0.09712524268537623, 0.5143737865731189, 0.09712524268537623, 0.09712524268537623]
maxi score, test score, baseline:  -0.20590000000000008 0.6966666666666669 0.6966666666666669
probs:  [0.09712524268537623, 0.09712524268537623, 0.09712524268537623, 0.5143737865731189, 0.09712524268537623, 0.09712524268537623]
actor:  1 policy actor:  1  step number:  43 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  1.333
from probs:  [0.09712524268537623, 0.09712524268537623, 0.09712524268537623, 0.5143737865731189, 0.09712524268537623, 0.09712524268537623]
printing an ep nov before normalisation:  28.823905371151742
printing an ep nov before normalisation:  42.30048449714569
maxi score, test score, baseline:  -0.20590000000000008 0.6966666666666669 0.6966666666666669
probs:  [0.0971508147825305, 0.0971508147825305, 0.0971508147825305, 0.5142459260873475, 0.0971508147825305, 0.0971508147825305]
printing an ep nov before normalisation:  30.845928791762685
maxi score, test score, baseline:  -0.20590000000000008 0.6966666666666669 0.6966666666666669
probs:  [0.0971508147825305, 0.0971508147825305, 0.0971508147825305, 0.5142459260873475, 0.0971508147825305, 0.0971508147825305]
actor:  1 policy actor:  1  step number:  48 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  43 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.20590000000000008 0.6966666666666669 0.6966666666666669
probs:  [0.09720190259735723, 0.09720190259735723, 0.09720190259735723, 0.5139904870132138, 0.09720190259735723, 0.09720190259735723]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  45 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.53654517717531
maxi score, test score, baseline:  -0.2031800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09720190259735723, 0.09720190259735723, 0.09720190259735723, 0.5139904870132138, 0.09720190259735723, 0.09720190259735723]
maxi score, test score, baseline:  -0.2031800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09720190259735723, 0.09720190259735723, 0.09720190259735723, 0.5139904870132138, 0.09720190259735723, 0.09720190259735723]
printing an ep nov before normalisation:  27.056821081584516
maxi score, test score, baseline:  -0.2031800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09720190259735723, 0.09720190259735723, 0.09720190259735723, 0.5139904870132138, 0.09720190259735723, 0.09720190259735723]
maxi score, test score, baseline:  -0.2031800000000001 0.6966666666666669 0.6966666666666669
maxi score, test score, baseline:  -0.2031800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09720190259735723, 0.09720190259735723, 0.09720190259735723, 0.5139904870132138, 0.09720190259735723, 0.09720190259735723]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.738]
 [0.656]
 [0.678]
 [0.669]
 [0.671]
 [0.66 ]] [[30.813]
 [28.383]
 [31.263]
 [31.294]
 [32.6  ]
 [33.06 ]
 [31.065]] [[0.555]
 [0.738]
 [0.656]
 [0.678]
 [0.669]
 [0.671]
 [0.66 ]]
maxi score, test score, baseline:  -0.2031800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09720190259735723, 0.09720190259735723, 0.09720190259735723, 0.5139904870132138, 0.09720190259735723, 0.09720190259735723]
actor:  0 policy actor:  0  step number:  38 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.0
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  39.618966492491694
printing an ep nov before normalisation:  35.81993081264677
siam score:  -0.8379032
printing an ep nov before normalisation:  33.17706286907196
actor:  1 policy actor:  1  step number:  72 total reward:  0.04666666666666597  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  0.0678170725933569
maxi score, test score, baseline:  -0.2002600000000001 0.6966666666666669 0.6966666666666669
probs:  [0.0972529153775529, 0.0972529153775529, 0.0972529153775529, 0.5137354231122356, 0.0972529153775529, 0.0972529153775529]
printing an ep nov before normalisation:  38.50785493850708
maxi score, test score, baseline:  -0.2002600000000001 0.6966666666666669 0.6966666666666669
probs:  [0.0972529153775529, 0.0972529153775529, 0.0972529153775529, 0.5137354231122356, 0.0972529153775529, 0.0972529153775529]
actor:  0 policy actor:  0  step number:  44 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8358307
maxi score, test score, baseline:  -0.19739333333333345 0.6966666666666669 0.6966666666666669
printing an ep nov before normalisation:  40.106860575573506
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.628]
 [0.576]
 [0.576]
 [0.49 ]
 [0.495]
 [0.576]] [[33.521]
 [36.428]
 [33.521]
 [33.521]
 [35.749]
 [36.731]
 [33.521]] [[1.203]
 [1.362]
 [1.203]
 [1.203]
 [1.199]
 [1.24 ]
 [1.203]]
maxi score, test score, baseline:  -0.19739333333333345 0.6966666666666669 0.6966666666666669
probs:  [0.0972529153775529, 0.0972529153775529, 0.0972529153775529, 0.5137354231122356, 0.0972529153775529, 0.0972529153775529]
printing an ep nov before normalisation:  25.688735496077754
maxi score, test score, baseline:  -0.19739333333333345 0.6966666666666669 0.6966666666666669
probs:  [0.0972529153775529, 0.0972529153775529, 0.0972529153775529, 0.5137354231122356, 0.0972529153775529, 0.0972529153775529]
printing an ep nov before normalisation:  23.434759343380804
actor:  1 policy actor:  1  step number:  64 total reward:  0.03333333333333266  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.059]
 [0.292]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.059]
 [0.292]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]]
maxi score, test score, baseline:  -0.19739333333333345 0.6966666666666669 0.6966666666666669
probs:  [0.09727839368130439, 0.09727839368130439, 0.09727839368130439, 0.5136080315934781, 0.09727839368130439, 0.09727839368130439]
actor:  1 policy actor:  1  step number:  57 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  52.29932805402728
printing an ep nov before normalisation:  51.64453624791282
printing an ep nov before normalisation:  31.808578968048096
actor:  1 policy actor:  1  step number:  43 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  50.57180188147245
maxi score, test score, baseline:  -0.19739333333333345 0.6966666666666669 0.6966666666666669
probs:  [0.09730385328830568, 0.09730385328830568, 0.09730385328830568, 0.5134807335584717, 0.09730385328830568, 0.09730385328830568]
actor:  1 policy actor:  1  step number:  40 total reward:  0.48666666666666647  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.19739333333333345 0.6966666666666669 0.6966666666666669
probs:  [0.09732929421912971, 0.09732929421912971, 0.09732929421912971, 0.5133535289043515, 0.09732929421912971, 0.09732929421912971]
maxi score, test score, baseline:  -0.19739333333333345 0.6966666666666669 0.6966666666666669
actor:  1 policy actor:  1  step number:  44 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  0
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 40.22281454505264
actor:  0 policy actor:  0  step number:  43 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.19446000000000013 0.6966666666666669 0.6966666666666669
probs:  [0.09735471649431923, 0.09735471649431923, 0.09735471649431923, 0.5132264175284038, 0.09735471649431923, 0.09735471649431923]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.827]
 [0.916]
 [0.693]
 [0.827]
 [0.827]
 [0.827]
 [0.827]] [[39.033]
 [36.287]
 [38.637]
 [39.033]
 [39.033]
 [39.033]
 [39.033]] [[0.827]
 [0.916]
 [0.693]
 [0.827]
 [0.827]
 [0.827]
 [0.827]]
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.336]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]] [[41.574]
 [42.279]
 [41.574]
 [41.574]
 [41.574]
 [41.574]
 [41.574]] [[1.437]
 [1.544]
 [1.437]
 [1.437]
 [1.437]
 [1.437]
 [1.437]]
actor:  0 policy actor:  0  step number:  45 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  37.27323147066368
UNIT TEST: sample policy line 217 mcts : [0.02  0.204 0.    0.184 0.184 0.388 0.02 ]
actor:  0 policy actor:  1  step number:  62 total reward:  0.15333333333333266  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  38.94728703547882
maxi score, test score, baseline:  -0.1893800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09735471649431923, 0.09735471649431923, 0.09735471649431923, 0.5132264175284038, 0.09735471649431923, 0.09735471649431923]
printing an ep nov before normalisation:  35.55456759628638
actor:  1 policy actor:  1  step number:  60 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.333
from probs:  [0.09738012013438631, 0.09738012013438631, 0.09738012013438631, 0.5130993993280686, 0.09738012013438631, 0.09738012013438631]
printing an ep nov before normalisation:  35.42763812914662
maxi score, test score, baseline:  -0.1893800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09738012013438631, 0.09738012013438631, 0.09738012013438631, 0.5130993993280686, 0.09738012013438631, 0.09738012013438631]
maxi score, test score, baseline:  -0.1893800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09738012013438631, 0.09738012013438631, 0.09738012013438631, 0.5130993993280686, 0.09738012013438631, 0.09738012013438631]
maxi score, test score, baseline:  -0.1893800000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09738012013438631, 0.09738012013438631, 0.09738012013438631, 0.5130993993280686, 0.09738012013438631, 0.09738012013438631]
printing an ep nov before normalisation:  27.74686555323146
actor:  0 policy actor:  0  step number:  39 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
actor:  1 policy actor:  1  step number:  65 total reward:  0.3466666666666661  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  58 total reward:  0.09999999999999942  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.424]
 [0.424]
 [0.429]
 [0.426]
 [0.428]
 [0.421]
 [0.41 ]] [[35.523]
 [36.225]
 [33.872]
 [33.394]
 [33.283]
 [33.126]
 [33.523]] [[0.424]
 [0.424]
 [0.429]
 [0.426]
 [0.428]
 [0.421]
 [0.41 ]]
actions average: 
K:  0  action  0 :  tensor([0.4301, 0.0252, 0.1011, 0.1012, 0.1206, 0.1122, 0.1097],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0033, 0.9794, 0.0030, 0.0039, 0.0016, 0.0016, 0.0071],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1329, 0.0098, 0.3813, 0.1148, 0.1034, 0.1476, 0.1102],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2080, 0.0213, 0.1519, 0.1552, 0.1538, 0.1611, 0.1487],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.1096,     0.0005,     0.0435,     0.0381,     0.7249,     0.0420,
            0.0414], grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1323, 0.0021, 0.1819, 0.0827, 0.0787, 0.4372, 0.0851],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1690, 0.1442, 0.0878, 0.0907, 0.0753, 0.0704, 0.3626],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09743087159105428, 0.09743087159105428, 0.09743087159105428, 0.5128456420447286, 0.09743087159105428, 0.09743087159105428]
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09743087159105428, 0.09743087159105428, 0.09743087159105428, 0.5128456420447286, 0.09743087159105428, 0.09743087159105428]
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09743087159105428, 0.09743087159105428, 0.09743087159105428, 0.5128456420447286, 0.09743087159105428, 0.09743087159105428]
printing an ep nov before normalisation:  45.15742038210115
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.239]
 [0.239]
 [0.239]
 [0.394]
 [0.239]
 [0.239]] [[31.99 ]
 [32.109]
 [32.109]
 [32.109]
 [36.631]
 [32.109]
 [32.109]] [[1.377]
 [1.288]
 [1.288]
 [1.288]
 [1.732]
 [1.288]
 [1.288]]
printing an ep nov before normalisation:  30.180696431667478
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.612]
 [0.621]
 [0.525]
 [0.479]
 [0.533]
 [0.608]] [[23.077]
 [26.143]
 [23.966]
 [23.724]
 [25.078]
 [25.664]
 [26.678]] [[0.779]
 [1.342]
 [1.21 ]
 [1.099]
 [1.139]
 [1.231]
 [1.371]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  72 total reward:  0.08666666666666589  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  53 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09743087159105428, 0.09743087159105428, 0.09743087159105428, 0.5128456420447286, 0.09743087159105428, 0.09743087159105428]
Printing some Q and Qe and total Qs values:  [[ 0.47 ]
 [ 0.665]
 [ 0.036]
 [ 0.598]
 [ 0.496]
 [-0.003]
 [ 0.601]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.47 ]
 [ 0.665]
 [ 0.036]
 [ 0.598]
 [ 0.496]
 [-0.003]
 [ 0.601]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09745621944852988, 0.09745621944852988, 0.09745621944852988, 0.5127189027573507, 0.09745621944852988, 0.09745621944852988]
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09745621944852988, 0.09745621944852988, 0.09745621944852988, 0.5127189027573507, 0.09745621944852988, 0.09745621944852988]
printing an ep nov before normalisation:  26.779258251190186
printing an ep nov before normalisation:  47.236323250969484
Printing some Q and Qe and total Qs values:  [[-0.046]
 [-0.049]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.048]
 [-0.048]] [[34.364]
 [35.902]
 [39.567]
 [39.567]
 [39.567]
 [33.631]
 [34.013]] [[1.796]
 [1.951]
 [2.325]
 [2.325]
 [2.325]
 [1.719]
 [1.758]]
printing an ep nov before normalisation:  39.23974939211323
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09745621944852988, 0.09745621944852988, 0.09745621944852988, 0.5127189027573507, 0.09745621944852988, 0.09745621944852988]
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09745621944852988, 0.09745621944852988, 0.09745621944852988, 0.5127189027573507, 0.09745621944852988, 0.09745621944852988]
printing an ep nov before normalisation:  52.6417302710055
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09745621944852988, 0.09745621944852988, 0.09745621944852988, 0.5127189027573507, 0.09745621944852988, 0.09745621944852988]
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09745621944852988, 0.09745621944852988, 0.09745621944852988, 0.5127189027573507, 0.09745621944852988, 0.09745621944852988]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  72 total reward:  0.03333333333333244  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09745621944852988, 0.09745621944852988, 0.09745621944852988, 0.5127189027573507, 0.09745621944852988, 0.09745621944852988]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  63 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  1.333
siam score:  -0.84557784
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
printing an ep nov before normalisation:  36.99281823837442
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
probs:  [0.0975068595237277, 0.0975068595237277, 0.0975068595237277, 0.5124657023813616, 0.0975068595237277, 0.0975068595237277]
printing an ep nov before normalisation:  30.707199924566154
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
probs:  [0.0975068595237277, 0.0975068595237277, 0.0975068595237277, 0.5124657023813616, 0.0975068595237277, 0.0975068595237277]
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
probs:  [0.0975068595237277, 0.0975068595237277, 0.0975068595237277, 0.5124657023813616, 0.0975068595237277, 0.0975068595237277]
printing an ep nov before normalisation:  36.416374979174044
printing an ep nov before normalisation:  31.207423210144043
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09753215178214528, 0.09753215178214528, 0.09753215178214528, 0.5123392410892735, 0.09753215178214528, 0.09753215178214528]
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.55 ]
 [0.342]
 [0.481]
 [0.351]
 [0.517]
 [0.349]] [[25.096]
 [31.168]
 [24.901]
 [29.032]
 [25.362]
 [31.352]
 [24.916]] [[0.978]
 [1.496]
 [0.974]
 [1.32 ]
 [1.007]
 [1.472]
 [0.982]]
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09753215178214528, 0.09753215178214528, 0.09753215178214528, 0.5123392410892735, 0.09753215178214528, 0.09753215178214528]
actor:  1 policy actor:  1  step number:  65 total reward:  0.02666666666666584  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.376]
 [0.623]
 [0.376]
 [0.376]
 [0.51 ]
 [0.376]] [[28.276]
 [28.276]
 [28.295]
 [28.276]
 [28.276]
 [24.964]
 [28.276]] [[1.325]
 [1.325]
 [1.572]
 [1.325]
 [1.325]
 [1.282]
 [1.325]]
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]] [[34.791]
 [34.791]
 [34.791]
 [34.791]
 [34.791]
 [34.791]
 [34.791]] [[1.936]
 [1.936]
 [1.936]
 [1.936]
 [1.936]
 [1.936]
 [1.936]]
Printing some Q and Qe and total Qs values:  [[-0.043]
 [-0.024]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]] [[34.48 ]
 [34.832]
 [34.48 ]
 [34.48 ]
 [34.48 ]
 [34.48 ]
 [34.48 ]] [[1.008]
 [1.047]
 [1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]]
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09753215178214528, 0.09753215178214528, 0.09753215178214528, 0.5123392410892735, 0.09753215178214528, 0.09753215178214528]
printing an ep nov before normalisation:  46.740437901833445
Printing some Q and Qe and total Qs values:  [[-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]] [[39.822]
 [39.822]
 [39.822]
 [39.822]
 [39.822]
 [39.822]
 [39.822]] [[0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]]
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09755742554819008, 0.09755742554819008, 0.09755742554819008, 0.5122128722590497, 0.09755742554819008, 0.09755742554819008]
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
printing an ep nov before normalisation:  39.65473809365948
siam score:  -0.83917564
printing an ep nov before normalisation:  31.616370220462997
actor:  1 policy actor:  1  step number:  63 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  45.982475655088834
actions average: 
K:  4  action  0 :  tensor([0.3935, 0.0100, 0.1107, 0.1138, 0.1326, 0.1295, 0.1098],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0343, 0.8694, 0.0184, 0.0212, 0.0111, 0.0121, 0.0335],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1505, 0.0830, 0.3266, 0.1030, 0.0881, 0.1055, 0.1434],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1171, 0.2341, 0.0877, 0.2748, 0.0909, 0.1061, 0.0892],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2258, 0.0292, 0.1083, 0.1468, 0.2978, 0.1044, 0.0877],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1076, 0.2762, 0.1851, 0.0784, 0.0639, 0.1905, 0.0984],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1361, 0.0321, 0.1315, 0.1598, 0.1264, 0.1367, 0.2774],
       grad_fn=<DivBackward0>)
siam score:  -0.8385149
siam score:  -0.8386999
actor:  1 policy actor:  1  step number:  52 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.667
from probs:  [0.09755742554819008, 0.09755742554819008, 0.09755742554819008, 0.5122128722590497, 0.09755742554819008, 0.09755742554819008]
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1887000000000001 0.6966666666666669 0.6966666666666669
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.76 ]
 [0.585]
 [0.656]
 [0.651]
 [0.651]
 [0.667]] [[29.801]
 [39.822]
 [33.05 ]
 [31.308]
 [29.801]
 [29.801]
 [34.337]] [[0.651]
 [0.76 ]
 [0.585]
 [0.656]
 [0.651]
 [0.651]
 [0.667]]
printing an ep nov before normalisation:  40.92993370751652
printing an ep nov before normalisation:  35.33545342960984
actor:  0 policy actor:  0  step number:  48 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.43800574166451
maxi score, test score, baseline:  -0.18596666666666678 0.6966666666666669 0.6966666666666669
probs:  [0.09760791768422486, 0.09760791768422486, 0.09760791768422486, 0.5119604115788757, 0.09760791768422486, 0.09760791768422486]
Printing some Q and Qe and total Qs values:  [[-0.016]
 [ 0.024]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[42.26 ]
 [50.812]
 [42.26 ]
 [42.26 ]
 [42.26 ]
 [42.26 ]
 [42.26 ]] [[1.16 ]
 [1.665]
 [1.16 ]
 [1.16 ]
 [1.16 ]
 [1.16 ]
 [1.16 ]]
maxi score, test score, baseline:  -0.18596666666666678 0.6966666666666669 0.6966666666666669
probs:  [0.09760791768422486, 0.09760791768422486, 0.09760791768422486, 0.5119604115788757, 0.09760791768422486, 0.09760791768422486]
maxi score, test score, baseline:  -0.18596666666666678 0.6966666666666669 0.6966666666666669
probs:  [0.09760791768422486, 0.09760791768422486, 0.09760791768422486, 0.5119604115788757, 0.09760791768422486, 0.09760791768422486]
printing an ep nov before normalisation:  0.011120492806924176
maxi score, test score, baseline:  -0.18596666666666678 0.6966666666666669 0.6966666666666669
actor:  0 policy actor:  0  step number:  45 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  30.368328561660572
maxi score, test score, baseline:  -0.18324666666666675 0.6966666666666669 0.6966666666666669
probs:  [0.09760791768422486, 0.09760791768422486, 0.09760791768422486, 0.5119604115788757, 0.09760791768422486, 0.09760791768422486]
maxi score, test score, baseline:  -0.18324666666666675 0.6966666666666669 0.6966666666666669
probs:  [0.09763313609467333, 0.09763313609467333, 0.09763313609467333, 0.5118343195266335, 0.09763313609467333, 0.09763313609467333]
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.543]
 [0.488]
 [0.469]
 [0.469]
 [0.458]
 [0.497]] [[41.269]
 [38.323]
 [39.918]
 [40.144]
 [40.39 ]
 [42.011]
 [39.736]] [[1.664]
 [1.56 ]
 [1.581]
 [1.573]
 [1.585]
 [1.651]
 [1.582]]
Printing some Q and Qe and total Qs values:  [[-0.041]
 [-0.066]
 [-0.078]
 [-0.07 ]
 [-0.048]
 [-0.06 ]
 [-0.067]] [[54.435]
 [56.717]
 [49.131]
 [54.095]
 [55.591]
 [53.911]
 [55.624]] [[1.015]
 [1.069]
 [0.794]
 [0.974]
 [1.048]
 [0.978]
 [1.029]]
maxi score, test score, baseline:  -0.18324666666666675 0.6966666666666669 0.6966666666666669
probs:  [0.09765833609366527, 0.09765833609366527, 0.09765833609366527, 0.5117083195316736, 0.09765833609366527, 0.09765833609366527]
line 256 mcts: sample exp_bonus 44.352393928961725
actor:  1 policy actor:  1  step number:  47 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  46.18804351473111
maxi score, test score, baseline:  -0.18324666666666675 0.6966666666666669 0.6966666666666669
probs:  [0.09768351770135639, 0.09768351770135639, 0.09768351770135639, 0.5115824114932181, 0.09768351770135639, 0.09768351770135639]
printing an ep nov before normalisation:  33.92828354167621
maxi score, test score, baseline:  -0.18324666666666675 0.6966666666666669 0.6966666666666669
probs:  [0.09770868093787242, 0.09770868093787242, 0.09770868093787242, 0.5114565953106379, 0.09770868093787242, 0.09770868093787242]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.18324666666666675 0.6966666666666669 0.6966666666666669
probs:  [0.09770868093787242, 0.09770868093787242, 0.09770868093787242, 0.5114565953106379, 0.09770868093787242, 0.09770868093787242]
maxi score, test score, baseline:  -0.18324666666666675 0.6966666666666669 0.6966666666666669
probs:  [0.09770868093787242, 0.09770868093787242, 0.09770868093787242, 0.5114565953106379, 0.09770868093787242, 0.09770868093787242]
maxi score, test score, baseline:  -0.18324666666666675 0.6966666666666669 0.6966666666666669
probs:  [0.09770868093787242, 0.09770868093787242, 0.09770868093787242, 0.5114565953106379, 0.09770868093787242, 0.09770868093787242]
siam score:  -0.835843
maxi score, test score, baseline:  -0.18324666666666675 0.6966666666666669 0.6966666666666669
printing an ep nov before normalisation:  42.85097046989819
printing an ep nov before normalisation:  33.274710814039096
printing an ep nov before normalisation:  46.7278488234589
actions average: 
K:  3  action  0 :  tensor([0.4519, 0.0203, 0.1119, 0.0938, 0.1155, 0.0996, 0.1069],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0248, 0.8593, 0.0298, 0.0175, 0.0108, 0.0164, 0.0413],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1425, 0.0180, 0.3808, 0.1283, 0.0918, 0.1262, 0.1124],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1483, 0.0734, 0.1350, 0.2338, 0.1076, 0.1607, 0.1412],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1632, 0.1958, 0.0548, 0.0778, 0.3587, 0.0666, 0.0831],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1644, 0.0098, 0.1566, 0.1280, 0.1135, 0.3008, 0.1269],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1399, 0.1147, 0.1212, 0.1401, 0.1012, 0.1260, 0.2569],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.18324666666666675 0.6966666666666669 0.6966666666666669
probs:  [0.09775895237773687, 0.09775895237773687, 0.09775895237773687, 0.5112052381113158, 0.09775895237773687, 0.09775895237773687]
maxi score, test score, baseline:  -0.18324666666666675 0.6966666666666669 0.6966666666666669
probs:  [0.09775895237773687, 0.09775895237773687, 0.09775895237773687, 0.5112052381113158, 0.09775895237773687, 0.09775895237773687]
printing an ep nov before normalisation:  43.17000231953724
actions average: 
K:  3  action  0 :  tensor([0.4778, 0.1215, 0.0713, 0.0948, 0.0715, 0.0616, 0.1015],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0118, 0.8924, 0.0184, 0.0208, 0.0124, 0.0173, 0.0270],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1464, 0.0731, 0.3090, 0.1223, 0.1251, 0.1258, 0.0982],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1080, 0.1736, 0.0847, 0.3067, 0.0881, 0.1100, 0.1290],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1819, 0.0095, 0.0678, 0.0984, 0.4853, 0.0822, 0.0748],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1429, 0.0497, 0.1972, 0.1312, 0.1241, 0.1935, 0.1614],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1008, 0.2637, 0.1023, 0.1280, 0.1132, 0.0998, 0.1922],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.94952925280373
maxi score, test score, baseline:  -0.18324666666666675 0.6966666666666669 0.6966666666666669
probs:  [0.09775895237773687, 0.09775895237773687, 0.09775895237773687, 0.5112052381113158, 0.09775895237773687, 0.09775895237773687]
printing an ep nov before normalisation:  20.560710366035426
printing an ep nov before normalisation:  16.88557588550943
maxi score, test score, baseline:  -0.18324666666666675 0.6966666666666669 0.6966666666666669
printing an ep nov before normalisation:  38.155813217163086
actor:  0 policy actor:  1  step number:  50 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.1827400000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09778406062119045, 0.09778406062119045, 0.09778406062119045, 0.5110796968940478, 0.09778406062119045, 0.09778406062119045]
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.716]
 [0.619]
 [0.619]
 [0.619]
 [0.619]
 [0.619]] [[56.77 ]
 [59.937]
 [56.77 ]
 [56.77 ]
 [56.77 ]
 [56.77 ]
 [56.77 ]] [[1.183]
 [1.321]
 [1.183]
 [1.183]
 [1.183]
 [1.183]
 [1.183]]
maxi score, test score, baseline:  -0.1827400000000001 0.6966666666666669 0.6966666666666669
maxi score, test score, baseline:  -0.1827400000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09778406062119045, 0.09778406062119045, 0.09778406062119045, 0.5110796968940478, 0.09778406062119045, 0.09778406062119045]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.1827400000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09780915057368009, 0.09780915057368009, 0.09780915057368009, 0.5109542471315996, 0.09780915057368009, 0.09780915057368009]
maxi score, test score, baseline:  -0.1827400000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09780915057368009, 0.09780915057368009, 0.09780915057368009, 0.5109542471315996, 0.09780915057368009, 0.09780915057368009]
maxi score, test score, baseline:  -0.1827400000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09780915057368009, 0.09780915057368009, 0.09780915057368009, 0.5109542471315996, 0.09780915057368009, 0.09780915057368009]
maxi score, test score, baseline:  -0.1827400000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09780915057368009, 0.09780915057368009, 0.09780915057368009, 0.5109542471315996, 0.09780915057368009, 0.09780915057368009]
maxi score, test score, baseline:  -0.1827400000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09780915057368009, 0.09780915057368009, 0.09780915057368009, 0.5109542471315996, 0.09780915057368009, 0.09780915057368009]
actor:  1 policy actor:  1  step number:  52 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  68 total reward:  0.1799999999999996  reward:  1.0 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  -0.1827400000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09780915057368009, 0.09780915057368009, 0.09780915057368009, 0.5109542471315996, 0.09780915057368009, 0.09780915057368009]
printing an ep nov before normalisation:  48.21454275258531
Printing some Q and Qe and total Qs values:  [[0.77]
 [0.77]
 [0.77]
 [0.77]
 [0.77]
 [0.77]
 [0.77]] [[17.406]
 [17.406]
 [17.406]
 [17.406]
 [17.406]
 [17.406]
 [17.406]] [[0.77]
 [0.77]
 [0.77]
 [0.77]
 [0.77]
 [0.77]
 [0.77]]
using another actor
from probs:  [0.09780915057368009, 0.09780915057368009, 0.09780915057368009, 0.5109542471315996, 0.09780915057368009, 0.09780915057368009]
maxi score, test score, baseline:  -0.1827400000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09780915057368009, 0.09780915057368009, 0.09780915057368009, 0.5109542471315996, 0.09780915057368009, 0.09780915057368009]
maxi score, test score, baseline:  -0.1827400000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09780915057368009, 0.09780915057368009, 0.09780915057368009, 0.5109542471315996, 0.09780915057368009, 0.09780915057368009]
actions average: 
K:  4  action  0 :  tensor([0.4363, 0.0099, 0.0986, 0.1381, 0.1053, 0.1122, 0.0996],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0202, 0.8951, 0.0135, 0.0207, 0.0125, 0.0122, 0.0259],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1774, 0.0131, 0.2768, 0.1538, 0.1253, 0.1366, 0.1170],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2124, 0.1275, 0.1163, 0.1731, 0.1193, 0.1171, 0.1342],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2662, 0.0075, 0.0604, 0.0996, 0.4253, 0.0717, 0.0692],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1054, 0.0224, 0.1174, 0.1763, 0.1014, 0.3590, 0.1181],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1683, 0.2162, 0.0948, 0.1337, 0.0881, 0.1035, 0.1953],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.414267454914956
printing an ep nov before normalisation:  46.877213970194
printing an ep nov before normalisation:  36.688593093859254
maxi score, test score, baseline:  -0.18274000000000012 0.6966666666666669 0.6966666666666669
probs:  [0.09780915057368009, 0.09780915057368009, 0.09780915057368009, 0.5109542471315996, 0.09780915057368009, 0.09780915057368009]
siam score:  -0.84096587
maxi score, test score, baseline:  -0.18274000000000012 0.6966666666666669 0.6966666666666669
probs:  [0.09780915057368009, 0.09780915057368009, 0.09780915057368009, 0.5109542471315996, 0.09780915057368009, 0.09780915057368009]
maxi score, test score, baseline:  -0.18274000000000012 0.6966666666666669 0.6966666666666669
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  62 total reward:  0.23333333333333273  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  44.99056302713417
line 256 mcts: sample exp_bonus 38.371263535755006
Printing some Q and Qe and total Qs values:  [[-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]] [[47.923]
 [47.923]
 [47.923]
 [47.923]
 [47.923]
 [47.923]
 [47.923]] [[1.173]
 [1.173]
 [1.173]
 [1.173]
 [1.173]
 [1.173]
 [1.173]]
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09783422225518551, 0.09783422225518551, 0.09783422225518551, 0.5108288887240726, 0.09783422225518551, 0.09783422225518551]
printing an ep nov before normalisation:  31.203854084014893
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09785927568565728, 0.09785927568565728, 0.09785927568565728, 0.5107036215717137, 0.09785927568565728, 0.09785927568565728]
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09785927568565728, 0.09785927568565728, 0.09785927568565728, 0.5107036215717137, 0.09785927568565728, 0.09785927568565728]
printing an ep nov before normalisation:  30.45738400611571
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]] [[48.524]
 [48.524]
 [48.524]
 [48.524]
 [48.524]
 [48.524]
 [48.524]] [[1.769]
 [1.769]
 [1.769]
 [1.769]
 [1.769]
 [1.769]
 [1.769]]
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09785927568565728, 0.09785927568565728, 0.09785927568565728, 0.5107036215717137, 0.09785927568565728, 0.09785927568565728]
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09785927568565728, 0.09785927568565728, 0.09785927568565728, 0.5107036215717137, 0.09785927568565728, 0.09785927568565728]
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09785927568565728, 0.09785927568565728, 0.09785927568565728, 0.5107036215717137, 0.09785927568565728, 0.09785927568565728]
actor:  1 policy actor:  1  step number:  53 total reward:  0.1866666666666661  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  61 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.46669853381295
printing an ep nov before normalisation:  38.589930117301485
printing an ep nov before normalisation:  50.04064291635584
actor:  1 policy actor:  1  step number:  55 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  43.528916388898196
printing an ep nov before normalisation:  58.48624831108687
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09788431088501709, 0.09788431088501709, 0.09788431088501709, 0.5105784455749145, 0.09788431088501709, 0.09788431088501709]
Printing some Q and Qe and total Qs values:  [[-0.066]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]] [[37.903]
 [37.736]
 [37.736]
 [37.736]
 [37.736]
 [37.736]
 [37.736]] [[1.814]
 [1.801]
 [1.801]
 [1.801]
 [1.801]
 [1.801]
 [1.801]]
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09788431088501709, 0.09788431088501709, 0.09788431088501709, 0.5105784455749145, 0.09788431088501709, 0.09788431088501709]
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.325159063772276
printing an ep nov before normalisation:  42.3645063330822
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.745]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]] [[40.629]
 [41.624]
 [40.629]
 [40.629]
 [40.629]
 [40.629]
 [40.629]] [[1.791]
 [1.953]
 [1.791]
 [1.791]
 [1.791]
 [1.791]
 [1.791]]
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09788431088501709, 0.09788431088501709, 0.09788431088501709, 0.5105784455749145, 0.09788431088501709, 0.09788431088501709]
using explorer policy with actor:  1
siam score:  -0.83727014
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09788431088501709, 0.09788431088501709, 0.09788431088501709, 0.5105784455749145, 0.09788431088501709, 0.09788431088501709]
actions average: 
K:  2  action  0 :  tensor([0.3727, 0.0109, 0.0924, 0.1051, 0.2258, 0.0906, 0.1026],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0027, 0.9468, 0.0032, 0.0037, 0.0011, 0.0011, 0.0413],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1285, 0.0177, 0.2810, 0.1042, 0.1038, 0.2582, 0.1066],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1159, 0.1053, 0.0764, 0.2966, 0.1551, 0.1285, 0.1222],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2851, 0.0554, 0.1174, 0.1390, 0.1367, 0.1264, 0.1401],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1322, 0.0151, 0.1307, 0.1056, 0.1343, 0.3663, 0.1157],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1100, 0.1586, 0.0831, 0.1337, 0.0841, 0.0760, 0.3545],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  47 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09788431088501709, 0.09788431088501709, 0.09788431088501709, 0.5105784455749145, 0.09788431088501709, 0.09788431088501709]
actor:  1 policy actor:  1  step number:  44 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8379366
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.0979093278731574, 0.0979093278731574, 0.0979093278731574, 0.510453360634213, 0.0979093278731574, 0.0979093278731574]
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.0979093278731574, 0.0979093278731574, 0.0979093278731574, 0.510453360634213, 0.0979093278731574, 0.0979093278731574]
printing an ep nov before normalisation:  28.089113235473633
Printing some Q and Qe and total Qs values:  [[-0.068]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.071]] [[31.933]
 [24.98 ]
 [24.98 ]
 [24.98 ]
 [24.98 ]
 [24.98 ]
 [24.98 ]] [[0.869]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]]
printing an ep nov before normalisation:  0.0005731715620527211
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09793432666994209, 0.09793432666994209, 0.09793432666994209, 0.5103283666502896, 0.09793432666994209, 0.09793432666994209]
printing an ep nov before normalisation:  36.127918411837754
siam score:  -0.8402839
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09795930729520581, 0.09795930729520581, 0.09795930729520581, 0.5102034635239711, 0.09795930729520581, 0.09795930729520581]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3133333333333327  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  52.63026014014044
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.647]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]] [[31.405]
 [33.14 ]
 [31.405]
 [31.405]
 [31.405]
 [31.405]
 [31.405]] [[1.25 ]
 [1.452]
 [1.25 ]
 [1.25 ]
 [1.25 ]
 [1.25 ]
 [1.25 ]]
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09798426976875498, 0.09798426976875498, 0.09798426976875498, 0.5100786511562252, 0.09798426976875498, 0.09798426976875498]
printing an ep nov before normalisation:  22.845897674560547
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09798426976875498, 0.09798426976875498, 0.09798426976875498, 0.5100786511562252, 0.09798426976875498, 0.09798426976875498]
line 256 mcts: sample exp_bonus 41.236209839454325
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09798426976875498, 0.09798426976875498, 0.09798426976875498, 0.5100786511562252, 0.09798426976875498, 0.09798426976875498]
siam score:  -0.84131235
printing an ep nov before normalisation:  40.65906524658203
actor:  1 policy actor:  1  step number:  62 total reward:  0.19333333333333258  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09800921411036634, 0.09800921411036634, 0.09800921411036634, 0.5099539294481683, 0.09800921411036634, 0.09800921411036634]
actor:  1 policy actor:  1  step number:  62 total reward:  0.28666666666666574  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09800921411036634, 0.09800921411036634, 0.09800921411036634, 0.5099539294481683, 0.09800921411036634, 0.09800921411036634]
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09800921411036634, 0.09800921411036634, 0.09800921411036634, 0.5099539294481683, 0.09800921411036634, 0.09800921411036634]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3266666666666661  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09803414033978847, 0.09803414033978847, 0.09803414033978847, 0.5098292983010577, 0.09803414033978847, 0.09803414033978847]
printing an ep nov before normalisation:  42.138906960374236
printing an ep nov before normalisation:  30.879979133605957
printing an ep nov before normalisation:  37.03811011888816
actor:  1 policy actor:  1  step number:  41 total reward:  0.4933333333333332  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.18027333333333342 0.6966666666666669 0.6966666666666669
printing an ep nov before normalisation:  38.153061405556045
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.881]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]] [[21.829]
 [28.112]
 [21.829]
 [21.829]
 [21.829]
 [21.829]
 [21.829]] [[0.815]
 [0.881]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]]
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  67 total reward:  0.026666666666666172  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  50 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  27.226343212703622
siam score:  -0.844841
Printing some Q and Qe and total Qs values:  [[-0.027]
 [-0.015]
 [-0.027]
 [-0.027]
 [-0.042]
 [-0.038]
 [-0.027]] [[33.775]
 [37.277]
 [33.775]
 [33.775]
 [39.079]
 [43.64 ]
 [33.775]] [[0.799]
 [0.977]
 [0.799]
 [0.799]
 [1.036]
 [1.258]
 [0.799]]
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.455]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]
 [0.37 ]] [[30.075]
 [31.73 ]
 [30.075]
 [30.075]
 [30.075]
 [30.075]
 [30.075]] [[1.266]
 [1.43 ]
 [1.266]
 [1.266]
 [1.266]
 [1.266]
 [1.266]]
maxi score, test score, baseline:  -0.1782200000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09805904847674157, 0.09805904847674157, 0.09805904847674157, 0.5097047576162923, 0.09805904847674157, 0.09805904847674157]
actions average: 
K:  0  action  0 :  tensor([0.1920, 0.0764, 0.1231, 0.1245, 0.2081, 0.1245, 0.1514],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0025, 0.9664, 0.0025, 0.0050, 0.0029, 0.0021, 0.0187],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1683, 0.0034, 0.2705, 0.1149, 0.1560, 0.1578, 0.1291],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1532, 0.0377, 0.1237, 0.2046, 0.1705, 0.1480, 0.1621],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1554, 0.0038, 0.1058, 0.1321, 0.3830, 0.1104, 0.1094],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1682, 0.0045, 0.1475, 0.1297, 0.1511, 0.2562, 0.1429],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1786, 0.0064, 0.1404, 0.1549, 0.1555, 0.1378, 0.2264],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.1782200000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09805904847674157, 0.09805904847674157, 0.09805904847674157, 0.5097047576162923, 0.09805904847674157, 0.09805904847674157]
maxi score, test score, baseline:  -0.1782200000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09805904847674157, 0.09805904847674157, 0.09805904847674157, 0.5097047576162923, 0.09805904847674157, 0.09805904847674157]
Printing some Q and Qe and total Qs values:  [[-0.077]
 [-0.076]
 [-0.081]
 [-0.081]
 [-0.081]
 [-0.081]
 [-0.081]] [[17.041]
 [25.015]
 [19.338]
 [19.338]
 [19.338]
 [19.338]
 [19.338]] [[0.516]
 [0.797]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]]
maxi score, test score, baseline:  -0.1782200000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09808393854091653, 0.09808393854091653, 0.09808393854091653, 0.5095803072954174, 0.09808393854091653, 0.09808393854091653]
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.629324535315135
maxi score, test score, baseline:  -0.1782200000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09808393854091653, 0.09808393854091653, 0.09808393854091653, 0.5095803072954174, 0.09808393854091653, 0.09808393854091653]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1782200000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09808393854091653, 0.09808393854091653, 0.09808393854091653, 0.5095803072954174, 0.09808393854091653, 0.09808393854091653]
printing an ep nov before normalisation:  29.26328991047394
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.0003795392774463835
actor:  1 policy actor:  1  step number:  64 total reward:  0.08666666666666634  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.633]
 [0.565]
 [0.546]
 [0.543]
 [0.611]
 [0.533]] [[32.282]
 [33.606]
 [32.231]
 [33.407]
 [34.083]
 [33.765]
 [31.823]] [[0.922]
 [1.075]
 [0.969]
 [0.983]
 [0.998]
 [1.057]
 [0.926]]
Printing some Q and Qe and total Qs values:  [[-0.056]
 [-0.051]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]] [[12.69 ]
 [25.891]
 [12.69 ]
 [12.69 ]
 [12.69 ]
 [12.69 ]
 [12.69 ]] [[0.024]
 [0.182]
 [0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.024]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.17999999999999938  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.1782200000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09810881055197622, 0.09810881055197622, 0.09810881055197622, 0.5094559472401189, 0.09810881055197622, 0.09810881055197622]
printing an ep nov before normalisation:  47.03936648636402
printing an ep nov before normalisation:  40.42424984551108
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.31999999999999984  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.138]
 [-0.038]
 [-0.032]
 [-0.054]
 [-0.066]
 [-0.031]
 [-0.052]] [[44.255]
 [47.601]
 [41.969]
 [41.797]
 [44.017]
 [42.571]
 [46.382]] [[0.846]
 [1.086]
 [0.858]
 [0.829]
 [0.909]
 [0.884]
 [1.02 ]]
maxi score, test score, baseline:  -0.1782200000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09810881055197622, 0.09810881055197622, 0.09810881055197622, 0.5094559472401189, 0.09810881055197622, 0.09810881055197622]
actions average: 
K:  3  action  0 :  tensor([0.3028, 0.0948, 0.1073, 0.1161, 0.1568, 0.1325, 0.0897],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0141, 0.8994, 0.0170, 0.0185, 0.0093, 0.0199, 0.0218],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1701, 0.0089, 0.3215, 0.1015, 0.1282, 0.1784, 0.0915],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1402, 0.0714, 0.1212, 0.2962, 0.1152, 0.1344, 0.1213],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1976, 0.0329, 0.1196, 0.1341, 0.2612, 0.1396, 0.1150],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1193, 0.0163, 0.1594, 0.1320, 0.0956, 0.3763, 0.1011],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1262, 0.0952, 0.1211, 0.2051, 0.0775, 0.1057, 0.2692],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.1782200000000001 0.6966666666666669 0.6966666666666669
actor:  1 policy actor:  1  step number:  53 total reward:  0.3066666666666661  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.764]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]] [[37.512]
 [32.527]
 [37.512]
 [37.512]
 [37.512]
 [37.512]
 [37.512]] [[0.651]
 [0.949]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]]
printing an ep nov before normalisation:  44.89447976955962
line 256 mcts: sample exp_bonus 18.92240434885025
actor:  1 policy actor:  1  step number:  84 total reward:  0.12666666666666604  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.1782200000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09815850049325749, 0.09815850049325749, 0.09815850049325749, 0.5092074975337126, 0.09815850049325749, 0.09815850049325749]
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.557]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]] [[30.388]
 [36.46 ]
 [30.388]
 [30.388]
 [30.388]
 [30.388]
 [30.388]] [[1.424]
 [1.697]
 [1.424]
 [1.424]
 [1.424]
 [1.424]
 [1.424]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.1782200000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09818331846266212, 0.09818331846266212, 0.09818331846266212, 0.5090834076866894, 0.09818331846266212, 0.09818331846266212]
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.393]
 [0.37 ]
 [0.333]
 [0.322]
 [0.356]
 [0.359]] [[47.509]
 [46.203]
 [47.181]
 [48.514]
 [48.949]
 [47.963]
 [48.015]] [[1.526]
 [1.536]
 [1.55 ]
 [1.562]
 [1.567]
 [1.565]
 [1.569]]
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.472]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]] [[35.1  ]
 [43.715]
 [35.1  ]
 [35.1  ]
 [35.1  ]
 [35.1  ]
 [35.1  ]] [[1.015]
 [1.331]
 [1.015]
 [1.015]
 [1.015]
 [1.015]
 [1.015]]
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.46 ]
 [0.35 ]
 [0.35 ]
 [0.355]
 [0.355]
 [0.351]] [[33.322]
 [44.485]
 [32.461]
 [31.882]
 [32.275]
 [31.914]
 [31.78 ]] [[0.786]
 [1.196]
 [0.769]
 [0.754]
 [0.769]
 [0.76 ]
 [0.752]]
maxi score, test score, baseline:  -0.1782200000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09818331846266212, 0.09818331846266212, 0.09818331846266212, 0.5090834076866894, 0.09818331846266212, 0.09818331846266212]
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.55328538314519
printing an ep nov before normalisation:  48.884545881614486
printing an ep nov before normalisation:  48.46106167172681
maxi score, test score, baseline:  -0.1782200000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09818331846266212, 0.09818331846266212, 0.09818331846266212, 0.5090834076866894, 0.09818331846266212, 0.09818331846266212]
maxi score, test score, baseline:  -0.1782200000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09818331846266212, 0.09818331846266212, 0.09818331846266212, 0.5090834076866894, 0.09818331846266212, 0.09818331846266212]
actor:  1 policy actor:  1  step number:  33 total reward:  0.56  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  48.07748125698788
printing an ep nov before normalisation:  13.75989681593154
maxi score, test score, baseline:  -0.1782200000000001 0.6966666666666669 0.6966666666666669
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.83566225628356
actor:  1 policy actor:  1  step number:  66 total reward:  0.059999999999999276  reward:  1.0 rdn_beta:  1.0
using another actor
maxi score, test score, baseline:  -0.1782200000000001 0.6966666666666669 0.6966666666666669
printing an ep nov before normalisation:  0.6948068045986133
maxi score, test score, baseline:  -0.1782200000000001 0.6966666666666669 0.6966666666666669
printing an ep nov before normalisation:  40.011388443566524
Printing some Q and Qe and total Qs values:  [[0.787]
 [0.82 ]
 [0.787]
 [0.787]
 [0.705]
 [0.787]
 [0.787]] [[38.002]
 [45.554]
 [38.002]
 [38.002]
 [41.144]
 [38.002]
 [38.002]] [[0.787]
 [0.82 ]
 [0.787]
 [0.787]
 [0.705]
 [0.787]
 [0.787]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  26.968213117206222
maxi score, test score, baseline:  -0.1782200000000001 0.6966666666666669 0.6966666666666669
probs:  [0.09825766460043431, 0.09825766460043431, 0.09825766460043431, 0.5087116769978284, 0.09825766460043431, 0.09825766460043431]
maxi score, test score, baseline:  -0.18035333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09825766460043431, 0.09825766460043431, 0.09825766460043431, 0.5087116769978284, 0.09825766460043431, 0.09825766460043431]
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.313939464851224
siam score:  -0.8507814
maxi score, test score, baseline:  -0.18035333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09830713907843415, 0.09830713907843415, 0.09830713907843415, 0.5084643046078292, 0.09830713907843415, 0.09830713907843415]
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.63 ]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]] [[47.101]
 [50.985]
 [47.101]
 [47.101]
 [47.101]
 [47.101]
 [47.101]] [[2.403]
 [2.63 ]
 [2.403]
 [2.403]
 [2.403]
 [2.403]
 [2.403]]
maxi score, test score, baseline:  -0.18035333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09830713907843415, 0.09830713907843415, 0.09830713907843415, 0.5084643046078292, 0.09830713907843415, 0.09830713907843415]
actor:  1 policy actor:  1  step number:  44 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  2.0
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.645]
 [0.684]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.645]
 [0.684]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]]
Printing some Q and Qe and total Qs values:  [[0.852]
 [0.866]
 [0.852]
 [0.852]
 [0.852]
 [0.852]
 [0.852]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.852]
 [0.866]
 [0.852]
 [0.852]
 [0.852]
 [0.852]
 [0.852]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[-0.004]
 [-0.008]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]
 [-0.024]] [[32.693]
 [22.809]
 [25.091]
 [25.091]
 [25.091]
 [25.091]
 [25.091]] [[1.295]
 [0.724]
 [0.839]
 [0.839]
 [0.839]
 [0.839]
 [0.839]]
Printing some Q and Qe and total Qs values:  [[0.711]
 [0.808]
 [0.711]
 [0.711]
 [0.711]
 [0.711]
 [0.711]] [[50.42 ]
 [40.428]
 [50.42 ]
 [50.42 ]
 [50.42 ]
 [50.42 ]
 [50.42 ]] [[0.711]
 [0.808]
 [0.711]
 [0.711]
 [0.711]
 [0.711]
 [0.711]]
printing an ep nov before normalisation:  32.07827976771764
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  75 total reward:  0.13333333333333208  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  43.75534608907788
printing an ep nov before normalisation:  42.048381702400064
printing an ep nov before normalisation:  28.68882179260254
printing an ep nov before normalisation:  34.17462026067899
actions average: 
K:  1  action  0 :  tensor([0.4399, 0.0039, 0.1077, 0.1059, 0.1471, 0.1014, 0.0941],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0107, 0.9588, 0.0076, 0.0038, 0.0031, 0.0034, 0.0126],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1446, 0.0665, 0.2828, 0.1023, 0.1165, 0.1799, 0.1074],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1686, 0.0509, 0.1384, 0.2429, 0.1294, 0.1309, 0.1388],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1308, 0.0236, 0.0854, 0.0816, 0.4906, 0.0913, 0.0966],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1929, 0.0143, 0.1593, 0.1563, 0.1762, 0.1550, 0.1461],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1506, 0.1222, 0.1440, 0.1427, 0.1156, 0.1490, 0.1759],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.582]
 [0.351]
 [0.538]
 [0.561]
 [0.171]
 [0.425]] [[34.67 ]
 [36.499]
 [29.885]
 [31.667]
 [34.677]
 [32.103]
 [30.131]] [[0.571]
 [0.582]
 [0.351]
 [0.538]
 [0.561]
 [0.171]
 [0.425]]
printing an ep nov before normalisation:  37.02367977543681
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.18035333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09830713907843415, 0.09830713907843415, 0.09830713907843415, 0.5084643046078292, 0.09830713907843415, 0.09830713907843415]
printing an ep nov before normalisation:  47.918498083056335
from probs:  [0.09830713907843415, 0.09830713907843415, 0.09830713907843415, 0.5084643046078292, 0.09830713907843415, 0.09830713907843415]
printing an ep nov before normalisation:  55.15889782169112
line 256 mcts: sample exp_bonus 35.407280579787674
printing an ep nov before normalisation:  45.63816691845302
printing an ep nov before normalisation:  36.940609182612526
printing an ep nov before normalisation:  32.76308858541613
maxi score, test score, baseline:  -0.18035333333333342 0.6966666666666669 0.6966666666666669
probs:  [0.09830713907843415, 0.09830713907843415, 0.09830713907843415, 0.5084643046078292, 0.09830713907843415, 0.09830713907843415]
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  37.803368740814314
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.11484666666666674 0.6950000000000002 0.6950000000000002
probs:  [0.09668946244071404, 0.09668946244071404, 0.09668946244071404, 0.5165526877964299, 0.09668946244071404, 0.09668946244071404]
printing an ep nov before normalisation:  26.63187598474205
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.11484666666666674 0.6950000000000002 0.6950000000000002
probs:  [0.09668946244071404, 0.09668946244071404, 0.09668946244071404, 0.5165526877964299, 0.09668946244071404, 0.09668946244071404]
maxi score, test score, baseline:  -0.11484666666666674 0.6950000000000002 0.6950000000000002
actor:  1 policy actor:  1  step number:  47 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  32.28399712936139
printing an ep nov before normalisation:  34.082563051132325
maxi score, test score, baseline:  -0.11484666666666674 0.6950000000000002 0.6950000000000002
probs:  [0.09671201966341052, 0.09671201966341052, 0.09671201966341052, 0.5164399016829474, 0.09671201966341052, 0.09671201966341052]
printing an ep nov before normalisation:  49.733606031494375
printing an ep nov before normalisation:  48.14910142013955
actor:  1 policy actor:  1  step number:  61 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  32.45720980924787
maxi score, test score, baseline:  -0.11484666666666674 0.6950000000000002 0.6950000000000002
probs:  [0.09673456234810622, 0.09673456234810622, 0.09673456234810622, 0.5163271882594689, 0.09673456234810622, 0.09673456234810622]
actor:  1 policy actor:  1  step number:  48 total reward:  0.42000000000000015  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  67 total reward:  0.13333333333333275  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.044]
 [-0.057]
 [-0.042]
 [-0.047]
 [-0.056]
 [-0.051]
 [-0.046]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.044]
 [-0.057]
 [-0.042]
 [-0.047]
 [-0.056]
 [-0.051]
 [-0.046]]
printing an ep nov before normalisation:  53.48584970479038
printing an ep nov before normalisation:  38.85378108533288
maxi score, test score, baseline:  -0.11484666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.0967570905088509, 0.0967570905088509, 0.0967570905088509, 0.5162145474557456, 0.0967570905088509, 0.0967570905088509]
maxi score, test score, baseline:  -0.11484666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.0967570905088509, 0.0967570905088509, 0.0967570905088509, 0.5162145474557456, 0.0967570905088509, 0.0967570905088509]
maxi score, test score, baseline:  -0.11484666666666676 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.11484666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.11484666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actions average: 
K:  1  action  0 :  tensor([0.4124, 0.0120, 0.1093, 0.1199, 0.1330, 0.1230, 0.0903],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0087, 0.9583, 0.0045, 0.0049, 0.0065, 0.0029, 0.0142],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1514, 0.0941, 0.2406, 0.1260, 0.1203, 0.1570, 0.1106],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1402, 0.1377, 0.0940, 0.3059, 0.1238, 0.0932, 0.1052],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1897, 0.0061, 0.1067, 0.1235, 0.3673, 0.1297, 0.0769],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1241, 0.0426, 0.0941, 0.0975, 0.0962, 0.4601, 0.0854],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1471, 0.1320, 0.0897, 0.2016, 0.1033, 0.1145, 0.2119],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.11484666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  30.253736395654585
actor:  1 policy actor:  1  step number:  47 total reward:  0.44000000000000017  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  54 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  48 total reward:  0.4333333333333329  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  3  action  0 :  tensor([0.2212, 0.0436, 0.1435, 0.1385, 0.1599, 0.1604, 0.1328],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0094, 0.9174, 0.0076, 0.0111, 0.0057, 0.0095, 0.0393],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1709, 0.0601, 0.2682, 0.1234, 0.1099, 0.1275, 0.1400],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1730, 0.0364, 0.1221, 0.2293, 0.1409, 0.1246, 0.1737],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1993, 0.0120, 0.0872, 0.1160, 0.4039, 0.0850, 0.0966],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1225, 0.0074, 0.0919, 0.0735, 0.0843, 0.5626, 0.0577],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1666, 0.1324, 0.1355, 0.1435, 0.1253, 0.1489, 0.1477],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.11484666666666676 0.6950000000000002 0.6950000000000002
Printing some Q and Qe and total Qs values:  [[0.132]
 [0.27 ]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]] [[37.676]
 [48.874]
 [37.676]
 [37.676]
 [37.676]
 [37.676]
 [37.676]] [[1.076]
 [1.761]
 [1.076]
 [1.076]
 [1.076]
 [1.076]
 [1.076]]
maxi score, test score, baseline:  -0.11484666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.11484666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  44.91138683043243
maxi score, test score, baseline:  -0.11484666666666676 0.6950000000000002 0.6950000000000002
printing an ep nov before normalisation:  43.43372482570867
maxi score, test score, baseline:  -0.11484666666666676 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.11484666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  41.14559113075012
actor:  1 policy actor:  1  step number:  50 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  28.30107512421909
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.5295],
        [-0.0126],
        [ 0.2812],
        [ 0.3057],
        [ 0.5780],
        [ 0.0281],
        [ 0.4080],
        [-0.1350],
        [ 0.7181],
        [ 0.2246]], dtype=torch.float64)
-0.071551887066 0.4579475443658948
-0.070771701198 -0.08332892519927804
-0.032346567066 0.24882853666624774
-0.04528388706599999 0.2603907445525978
-0.071031754398 0.5070014096875308
-0.09703970119800001 -0.06891569771743426
-0.070771701198 0.33725155642007887
-0.032346567066 -0.167377158878037
-0.09703970119800001 0.6210862465617645
-0.058614567066 0.16597179661190226
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.358]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]] [[36.721]
 [38.401]
 [36.721]
 [36.721]
 [36.721]
 [36.721]
 [36.721]] [[0.911]
 [0.994]
 [0.911]
 [0.911]
 [0.911]
 [0.911]
 [0.911]]
printing an ep nov before normalisation:  49.57654255578892
siam score:  -0.84368014
maxi score, test score, baseline:  -0.11484666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.11484666666666676 0.6950000000000002 0.6950000000000002
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.562]
 [0.562]
 [0.562]
 [0.595]
 [0.562]
 [0.562]] [[38.6  ]
 [37.436]
 [37.436]
 [37.436]
 [37.723]
 [37.436]
 [37.436]] [[0.619]
 [0.562]
 [0.562]
 [0.562]
 [0.595]
 [0.562]
 [0.562]]
maxi score, test score, baseline:  -0.11484666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  68 total reward:  0.0599999999999995  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.728]
 [0.632]
 [0.636]
 [0.636]
 [0.636]
 [0.632]] [[36.025]
 [33.815]
 [37.223]
 [37.2  ]
 [37.071]
 [36.977]
 [37.292]] [[0.658]
 [0.728]
 [0.632]
 [0.636]
 [0.636]
 [0.636]
 [0.632]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.11484666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.11484666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  45.652961078280015
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.39 ]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]] [[35.067]
 [38.633]
 [35.067]
 [35.067]
 [35.067]
 [35.067]
 [35.067]] [[1.233]
 [1.504]
 [1.233]
 [1.233]
 [1.233]
 [1.233]
 [1.233]]
maxi score, test score, baseline:  -0.11484666666666674 0.6950000000000002 0.6950000000000002
printing an ep nov before normalisation:  36.22644297438196
maxi score, test score, baseline:  -0.11484666666666674 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.11484666666666674 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  40.5908953616145
printing an ep nov before normalisation:  32.93636519415486
maxi score, test score, baseline:  -0.11484666666666674 0.6950000000000002 0.6950000000000002
printing an ep nov before normalisation:  29.5080210205739
Printing some Q and Qe and total Qs values:  [[0.649]
 [0.742]
 [0.687]
 [0.623]
 [0.687]
 [0.635]
 [0.635]] [[27.498]
 [21.95 ]
 [32.242]
 [29.831]
 [32.242]
 [27.952]
 [28.468]] [[1.634]
 [1.527]
 [1.841]
 [1.691]
 [1.841]
 [1.635]
 [1.653]]
maxi score, test score, baseline:  -0.11484666666666674 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  29.15136538401772
siam score:  -0.84709275
actor:  1 policy actor:  1  step number:  50 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
actor:  1 policy actor:  1  step number:  46 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  43 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.11484666666666674 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.11484666666666674 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.11484666666666674 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.11484666666666674 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.11484666666666674 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.11484666666666674 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  31.13146675052713
actor:  1 policy actor:  1  step number:  50 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  50.658794859259494
maxi score, test score, baseline:  -0.11484666666666674 0.6950000000000002 0.6950000000000002
printing an ep nov before normalisation:  31.286580562591553
maxi score, test score, baseline:  -0.11484666666666674 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  34.72491183589974
siam score:  -0.84707975
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.903]
 [0.912]
 [0.903]
 [0.903]
 [0.903]
 [0.903]
 [0.903]] [[40.156]
 [41.473]
 [40.156]
 [40.156]
 [40.156]
 [40.156]
 [40.156]] [[0.903]
 [0.912]
 [0.903]
 [0.903]
 [0.903]
 [0.903]
 [0.903]]
maxi score, test score, baseline:  -0.11484666666666674 0.6950000000000002 0.6950000000000002
actor:  0 policy actor:  0  step number:  46 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  59 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8487108
printing an ep nov before normalisation:  32.35127325554917
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.10984666666666675 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  35.687293953318246
maxi score, test score, baseline:  -0.10984666666666675 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.10984666666666675 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.10984666666666675 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  31.273958102674797
actor:  1 policy actor:  1  step number:  35 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  43 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  0  action  0 :  tensor([0.4102, 0.0594, 0.0875, 0.1167, 0.0971, 0.0905, 0.1385],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0009,     0.9873,     0.0018,     0.0015,     0.0002,     0.0003,
            0.0079], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1192, 0.0024, 0.4093, 0.0748, 0.0781, 0.2376, 0.0787],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1428, 0.0439, 0.1048, 0.3147, 0.1275, 0.1325, 0.1338],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1887, 0.0037, 0.0987, 0.1057, 0.3897, 0.1095, 0.1039],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1122, 0.0143, 0.2653, 0.0855, 0.0899, 0.3381, 0.0946],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1653, 0.1091, 0.1354, 0.1181, 0.1310, 0.1367, 0.2043],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  36.35749831429017
maxi score, test score, baseline:  -0.10938000000000007 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
siam score:  -0.850119
maxi score, test score, baseline:  -0.10938000000000007 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.10938000000000007 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.10938000000000007 0.6950000000000002 0.6950000000000002
printing an ep nov before normalisation:  16.625258922576904
actor:  1 policy actor:  1  step number:  62 total reward:  0.24666666666666592  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.021]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]] [[24.146]
 [24.146]
 [24.146]
 [24.146]
 [24.146]
 [24.146]
 [24.146]] [[0.93]
 [0.93]
 [0.93]
 [0.93]
 [0.93]
 [0.93]
 [0.93]]
maxi score, test score, baseline:  -0.10938000000000007 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Printing some Q and Qe and total Qs values:  [[-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.049]
 [-0.08 ]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [19.307]] [[-0.826]
 [-0.826]
 [-0.826]
 [-0.826]
 [-0.826]
 [-0.826]
 [ 0.895]]
Printing some Q and Qe and total Qs values:  [[0.156]
 [0.43 ]
 [0.214]
 [0.181]
 [0.255]
 [0.214]
 [0.139]] [[47.152]
 [49.225]
 [53.749]
 [50.266]
 [48.894]
 [53.749]
 [48.509]] [[1.164]
 [1.509]
 [1.448]
 [1.295]
 [1.322]
 [1.448]
 [1.193]]
maxi score, test score, baseline:  -0.10938000000000007 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Printing some Q and Qe and total Qs values:  [[-0.042]
 [-0.039]
 [-0.041]
 [-0.042]
 [-0.042]
 [-0.044]
 [-0.044]] [[19.208]
 [27.309]
 [19.113]
 [34.94 ]
 [34.94 ]
 [19.23 ]
 [19.25 ]] [[0.519]
 [1.294]
 [0.511]
 [2.017]
 [2.017]
 [0.519]
 [0.521]]
printing an ep nov before normalisation:  46.537961791702074
Printing some Q and Qe and total Qs values:  [[-0.04]
 [-0.04]
 [-0.04]
 [-0.04]
 [-0.04]
 [-0.04]
 [-0.04]] [[51.988]
 [51.988]
 [51.988]
 [51.988]
 [51.988]
 [51.988]
 [51.988]] [[1.293]
 [1.293]
 [1.293]
 [1.293]
 [1.293]
 [1.293]
 [1.293]]
printing an ep nov before normalisation:  25.522340340618438
maxi score, test score, baseline:  -0.10938000000000007 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Printing some Q and Qe and total Qs values:  [[-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]
 [-0.052]] [[59.138]
 [59.138]
 [59.138]
 [59.138]
 [59.138]
 [59.138]
 [59.138]] [[1.281]
 [1.281]
 [1.281]
 [1.281]
 [1.281]
 [1.281]
 [1.281]]
printing an ep nov before normalisation:  51.44794018294265
maxi score, test score, baseline:  -0.10938000000000007 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  36.08290087413688
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  38.2820276755587
maxi score, test score, baseline:  -0.10938000000000007 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.243]
 [0.243]
 [0.243]
 [0.258]
 [0.243]
 [0.243]] [[53.328]
 [54.232]
 [54.232]
 [54.232]
 [53.396]
 [54.232]
 [54.232]] [[1.619]
 [1.415]
 [1.415]
 [1.415]
 [1.406]
 [1.415]
 [1.415]]
printing an ep nov before normalisation:  31.263837814331055
printing an ep nov before normalisation:  53.24713277844849
siam score:  -0.8450905
line 256 mcts: sample exp_bonus 0.0
siam score:  -0.8462638
actor:  1 policy actor:  1  step number:  32 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.10938000000000007 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.10938000000000007 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  44 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  56.00531873125974
Printing some Q and Qe and total Qs values:  [[ 0.134]
 [ 0.249]
 [-0.055]
 [ 0.089]
 [ 0.122]
 [-0.054]
 [ 0.314]] [[31.283]
 [38.654]
 [32.271]
 [28.267]
 [30.034]
 [30.747]
 [40.129]] [[0.564]
 [0.86 ]
 [0.4  ]
 [0.446]
 [0.522]
 [0.363]
 [0.961]]
printing an ep nov before normalisation:  45.720060538296806
actor:  1 policy actor:  1  step number:  41 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  1.333
from probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  30.61670466052182
printing an ep nov before normalisation:  46.134876564042855
maxi score, test score, baseline:  -0.11218000000000007 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  54.587054197670966
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.335]
 [0.262]
 [0.283]
 [0.262]
 [0.262]
 [0.29 ]] [[45.043]
 [41.164]
 [37.161]
 [45.966]
 [37.161]
 [37.161]
 [44.376]] [[0.581]
 [0.588]
 [0.472]
 [0.589]
 [0.472]
 [0.472]
 [0.579]]
printing an ep nov before normalisation:  38.630520863950736
actions average: 
K:  1  action  0 :  tensor([0.2503, 0.0083, 0.1778, 0.1257, 0.1488, 0.1587, 0.1304],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0103, 0.9441, 0.0086, 0.0060, 0.0041, 0.0035, 0.0233],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1360, 0.0347, 0.3036, 0.1189, 0.1157, 0.1580, 0.1332],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1289, 0.0099, 0.1567, 0.2118, 0.1469, 0.2042, 0.1415],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1516, 0.0395, 0.1196, 0.1027, 0.3755, 0.1222, 0.0889],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0716, 0.0751, 0.1068, 0.0789, 0.0848, 0.5217, 0.0612],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1008, 0.1657, 0.0919, 0.1417, 0.0819, 0.0871, 0.3309],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[ 0.095]
 [ 0.154]
 [-0.075]
 [ 0.059]
 [ 0.242]
 [ 0.242]
 [ 0.242]] [[37.689]
 [40.621]
 [41.342]
 [40.374]
 [36.443]
 [36.443]
 [36.443]] [[0.602]
 [0.742]
 [0.533]
 [0.64 ]
 [0.714]
 [0.714]
 [0.714]]
maxi score, test score, baseline:  -0.1121800000000001 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  31.33366503776879
actor:  1 policy actor:  1  step number:  62 total reward:  0.2333333333333325  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  47.33991657564551
printing an ep nov before normalisation:  45.03999324325715
maxi score, test score, baseline:  -0.1121800000000001 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  33.203573840919276
actor:  1 policy actor:  1  step number:  43 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.1121800000000001 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  43.947313535176725
maxi score, test score, baseline:  -0.1121800000000001 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.1121800000000001 0.6950000000000002 0.6950000000000002
printing an ep nov before normalisation:  38.30261086724209
maxi score, test score, baseline:  -0.1121800000000001 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.1121800000000001 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.1121800000000001 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.1121800000000001 0.6950000000000002 0.6950000000000002
printing an ep nov before normalisation:  36.81608658720842
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.575]
 [0.573]
 [0.573]] [[28.629]
 [28.629]
 [28.629]
 [28.629]
 [32.473]
 [28.629]
 [28.629]] [[0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.575]
 [0.573]
 [0.573]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  1.333
from probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Printing some Q and Qe and total Qs values:  [[0.723]
 [0.81 ]
 [0.685]
 [0.77 ]
 [0.73 ]
 [0.736]
 [0.748]] [[35.559]
 [33.832]
 [33.466]
 [34.026]
 [35.889]
 [36.098]
 [36.595]] [[0.723]
 [0.81 ]
 [0.685]
 [0.77 ]
 [0.73 ]
 [0.736]
 [0.748]]
actions average: 
K:  2  action  0 :  tensor([0.3156, 0.0728, 0.1276, 0.1481, 0.1471, 0.1090, 0.0798],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0058, 0.9450, 0.0108, 0.0069, 0.0030, 0.0059, 0.0227],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1826, 0.0822, 0.2066, 0.1362, 0.1300, 0.1429, 0.1194],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1649, 0.0149, 0.1194, 0.2777, 0.1395, 0.1066, 0.1770],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1539, 0.0156, 0.1203, 0.1241, 0.3974, 0.0996, 0.0891],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1262, 0.0027, 0.1854, 0.1055, 0.1177, 0.3892, 0.0733],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0969, 0.1918, 0.0717, 0.1371, 0.0957, 0.0623, 0.3445],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  1  step number:  53 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  1.333
from probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.10946000000000009 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  40.446720123291016
printing an ep nov before normalisation:  57.40328082662564
actor:  0 policy actor:  0  step number:  32 total reward:  0.5533333333333333  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  57 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.10635333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.10635333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.10635333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  50.05509101988431
maxi score, test score, baseline:  -0.10635333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.7  ]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]] [[38.188]
 [50.836]
 [38.188]
 [38.188]
 [38.188]
 [38.188]
 [38.188]] [[0.689]
 [0.7  ]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]]
printing an ep nov before normalisation:  44.40023471148639
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.10635333333333342 0.6950000000000002 0.6950000000000002
actions average: 
K:  2  action  0 :  tensor([0.3790, 0.0077, 0.1134, 0.1084, 0.1333, 0.1059, 0.1524],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0086, 0.9348, 0.0087, 0.0093, 0.0053, 0.0082, 0.0251],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1760, 0.0128, 0.2252, 0.1531, 0.1536, 0.1517, 0.1275],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1227, 0.0048, 0.1077, 0.3720, 0.1172, 0.1227, 0.1529],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1920, 0.0090, 0.0888, 0.1090, 0.3908, 0.0955, 0.1149],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1464, 0.0014, 0.1219, 0.1968, 0.1178, 0.2966, 0.1191],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1110, 0.1985, 0.1092, 0.1006, 0.0928, 0.1025, 0.2854],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.10635333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.10635333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  0 policy actor:  1  step number:  43 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.10336666666666673 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.10336666666666673 0.6950000000000002 0.6950000000000002
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.215]
 [0.419]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]] [[51.258]
 [49.221]
 [49.274]
 [49.274]
 [49.274]
 [49.274]
 [49.274]] [[1.287]
 [1.42 ]
 [1.217]
 [1.217]
 [1.217]
 [1.217]
 [1.217]]
maxi score, test score, baseline:  -0.10676666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.10676666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  49.34363539232004
actor:  1 policy actor:  1  step number:  48 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  51.453369535867374
maxi score, test score, baseline:  -0.10676666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  13.531667414702136
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.2927198674353
maxi score, test score, baseline:  -0.10676666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  55 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.10676666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  33 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  60.15101942191863
siam score:  -0.8366493
printing an ep nov before normalisation:  40.27478133419828
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.579]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]] [[33.749]
 [42.342]
 [33.749]
 [33.749]
 [33.749]
 [33.749]
 [33.749]] [[0.534]
 [0.834]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]]
actions average: 
K:  4  action  0 :  tensor([0.3632, 0.0645, 0.1007, 0.1248, 0.1112, 0.1318, 0.1038],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0174, 0.8727, 0.0239, 0.0203, 0.0120, 0.0256, 0.0280],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1887, 0.0274, 0.1360, 0.1743, 0.1718, 0.1679, 0.1339],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1829, 0.0145, 0.1510, 0.1715, 0.1377, 0.1851, 0.1574],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1205, 0.0445, 0.1437, 0.1174, 0.3171, 0.1710, 0.0857],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1335, 0.1308, 0.1269, 0.1362, 0.1193, 0.2401, 0.1132],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1582, 0.0799, 0.1546, 0.1496, 0.1158, 0.1406, 0.2014],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.10676666666666676 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.10676666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.203]
 [0.35 ]
 [0.237]
 [0.073]
 [0.237]
 [0.078]
 [0.237]] [[44.385]
 [45.503]
 [41.407]
 [41.338]
 [41.407]
 [38.471]
 [41.407]] [[1.155]
 [1.349]
 [1.064]
 [0.897]
 [1.064]
 [0.781]
 [1.064]]
maxi score, test score, baseline:  -0.10676666666666676 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.10676666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  52.5078466625555
maxi score, test score, baseline:  -0.10676666666666676 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.10676666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.723289998746665
maxi score, test score, baseline:  -0.10676666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  34.14649997963979
actor:  1 policy actor:  1  step number:  66 total reward:  0.086666666666666  reward:  1.0 rdn_beta:  1.333
siam score:  -0.83503264
maxi score, test score, baseline:  -0.10676666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.09910253377761
maxi score, test score, baseline:  -0.10676666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3399999999999994  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.10676666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  51.85722827911377
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.24 ]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]] [[39.023]
 [46.384]
 [39.023]
 [39.023]
 [39.023]
 [39.023]
 [39.023]] [[0.831]
 [1.096]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]]
printing an ep nov before normalisation:  26.589620113372803
actor:  1 policy actor:  1  step number:  51 total reward:  0.3599999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.10676666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.10676666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  51.82061852778479
actor:  1 policy actor:  1  step number:  64 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  1.0
using another actor
from probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.10676666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  0 policy actor:  0  step number:  40 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.11080666666666678 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  40.23909541166809
maxi score, test score, baseline:  -0.11080666666666678 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.11080666666666678 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.265162679888423
maxi score, test score, baseline:  -0.11080666666666678 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.214]
 [0.089]
 [0.089]
 [0.082]
 [0.087]
 [0.084]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.114]
 [0.214]
 [0.089]
 [0.089]
 [0.082]
 [0.087]
 [0.084]]
actor:  0 policy actor:  0  step number:  43 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.11148666666666678 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.11148666666666678 0.6950000000000002 0.6950000000000002
actor:  0 policy actor:  0  step number:  36 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.11179333333333341 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  67 total reward:  0.1866666666666661  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.087]
 [-0.041]
 [-0.167]
 [-0.069]
 [-0.077]
 [-0.159]
 [-0.042]] [[26.381]
 [31.272]
 [24.464]
 [25.228]
 [25.92 ]
 [25.483]
 [43.929]] [[0.394]
 [0.666]
 [0.224]
 [0.358]
 [0.382]
 [0.279]
 [1.253]]
maxi score, test score, baseline:  -0.11179333333333341 0.6950000000000002 0.6950000000000002
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  38.17543518830038
maxi score, test score, baseline:  -0.11179333333333341 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  31.54313010289301
maxi score, test score, baseline:  -0.11179333333333341 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
actor:  1 policy actor:  1  step number:  46 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  35.11887020325124
Printing some Q and Qe and total Qs values:  [[0.76 ]
 [0.625]
 [0.625]
 [0.625]
 [0.645]
 [0.625]
 [0.625]] [[39.975]
 [42.473]
 [42.473]
 [42.473]
 [37.07 ]
 [42.473]
 [42.473]] [[0.76 ]
 [0.625]
 [0.625]
 [0.625]
 [0.645]
 [0.625]
 [0.625]]
printing an ep nov before normalisation:  19.340285905218998
maxi score, test score, baseline:  -0.11179333333333341 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.11179333333333341 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  31.010401248931885
Printing some Q and Qe and total Qs values:  [[0.802]
 [0.922]
 [0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.802]] [[38.308]
 [38.211]
 [38.308]
 [38.308]
 [38.308]
 [38.308]
 [38.308]] [[0.802]
 [0.922]
 [0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.802]]
maxi score, test score, baseline:  -0.11179333333333341 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  74 total reward:  0.16666666666666585  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  36 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2.0
from probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.1122333333333334 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.1122333333333334 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.1122333333333334 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.1122333333333334 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  54.00643358624068
actor:  1 policy actor:  1  step number:  58 total reward:  0.23333333333333317  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.1122333333333334 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.1122333333333334 0.6950000000000002 0.6950000000000002
printing an ep nov before normalisation:  50.990600408721576
Printing some Q and Qe and total Qs values:  [[0.085]
 [0.107]
 [0.085]
 [0.085]
 [0.085]
 [0.085]
 [0.085]] [[22.948]
 [39.285]
 [22.948]
 [22.948]
 [22.948]
 [22.948]
 [22.948]] [[0.505]
 [1.112]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]]
actor:  0 policy actor:  0  step number:  50 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  44.815599593777044
maxi score, test score, baseline:  -0.11320666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  49.33412954296819
printing an ep nov before normalisation:  33.20955991744995
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.39 ]
 [0.361]
 [0.345]
 [0.346]
 [0.34 ]
 [0.325]] [[25.75 ]
 [28.832]
 [26.   ]
 [25.961]
 [25.867]
 [25.94 ]
 [26.115]] [[1.34 ]
 [1.646]
 [1.371]
 [1.352]
 [1.345]
 [1.345]
 [1.345]]
maxi score, test score, baseline:  -0.11320666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  33 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  1.667
siam score:  -0.83833647
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.518789291381836
maxi score, test score, baseline:  -0.11320666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  54 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  55.950297652602714
actor:  1 policy actor:  1  step number:  49 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.746]
 [0.767]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]] [[33.922]
 [35.501]
 [34.36 ]
 [34.36 ]
 [34.36 ]
 [34.36 ]
 [34.36 ]] [[1.624]
 [1.721]
 [1.603]
 [1.603]
 [1.603]
 [1.603]
 [1.603]]
actor:  1 policy actor:  1  step number:  30 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.11320666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.487]
 [0.546]
 [0.465]
 [0.401]
 [0.4  ]
 [0.433]] [[45.884]
 [49.251]
 [45.884]
 [48.748]
 [50.377]
 [47.81 ]
 [50.014]] [[1.745]
 [1.832]
 [1.745]
 [1.788]
 [1.795]
 [1.682]
 [1.811]]
printing an ep nov before normalisation:  48.25001385215189
maxi score, test score, baseline:  -0.11320666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.11320666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  56 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  47 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  14.738707043359902
printing an ep nov before normalisation:  25.374155435687
maxi score, test score, baseline:  -0.11320666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.11320666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  52 total reward:  0.23333333333333317  reward:  1.0 rdn_beta:  1.667
from probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.371]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]] [[30.746]
 [38.098]
 [30.746]
 [30.746]
 [30.746]
 [30.746]
 [30.746]] [[1.289]
 [1.846]
 [1.289]
 [1.289]
 [1.289]
 [1.289]
 [1.289]]
actions average: 
K:  3  action  0 :  tensor([0.5554, 0.0172, 0.0870, 0.0993, 0.0952, 0.0712, 0.0746],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0111, 0.9393, 0.0104, 0.0097, 0.0050, 0.0063, 0.0182],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1917, 0.0637, 0.2144, 0.1559, 0.1469, 0.0966, 0.1309],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1622, 0.1490, 0.1052, 0.2848, 0.0662, 0.0733, 0.1592],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1991, 0.0159, 0.1048, 0.1029, 0.4055, 0.0925, 0.0793],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1508, 0.0347, 0.1701, 0.0964, 0.1075, 0.3555, 0.0850],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2075, 0.0051, 0.1370, 0.1693, 0.1660, 0.1643, 0.1507],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.11659333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.11659333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  45.785166853597104
maxi score, test score, baseline:  -0.11659333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.661]
 [0.666]
 [0.664]
 [0.655]
 [0.655]
 [0.655]] [[18.173]
 [22.518]
 [19.208]
 [19.133]
 [19.164]
 [19.459]
 [18.173]] [[1.284]
 [1.584]
 [1.365]
 [1.357]
 [1.35 ]
 [1.371]
 [1.284]]
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]] [[51.169]
 [51.169]
 [51.169]
 [51.169]
 [51.169]
 [51.169]
 [51.169]] [[0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]]
printing an ep nov before normalisation:  27.169269228496677
maxi score, test score, baseline:  -0.11659333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  62 total reward:  0.15333333333333254  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.11659333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  63 total reward:  0.026666666666666283  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.11659333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  58.128724021516334
line 256 mcts: sample exp_bonus 39.833990773972346
printing an ep nov before normalisation:  48.900320138114964
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.21 ]
 [0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]] [[38.596]
 [47.528]
 [38.596]
 [38.596]
 [38.596]
 [38.596]
 [38.596]] [[0.846]
 [1.316]
 [0.846]
 [0.846]
 [0.846]
 [0.846]
 [0.846]]
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.464]
 [0.31 ]
 [0.305]
 [0.294]
 [0.294]
 [0.319]] [[26.935]
 [28.566]
 [29.718]
 [29.692]
 [26.935]
 [26.935]
 [27.666]] [[0.437]
 [0.625]
 [0.482]
 [0.478]
 [0.437]
 [0.437]
 [0.47 ]]
maxi score, test score, baseline:  -0.11659333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.11659333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.11659333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  51 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  0.6363540844368742
actor:  1 policy actor:  1  step number:  42 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.11659333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Printing some Q and Qe and total Qs values:  [[-0.056]
 [-0.002]
 [-0.056]
 [-0.056]
 [-0.067]
 [-0.056]
 [-0.056]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.056]
 [-0.002]
 [-0.056]
 [-0.056]
 [-0.067]
 [-0.056]
 [-0.056]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.279999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.11659333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actions average: 
K:  2  action  0 :  tensor([0.5530, 0.0154, 0.0849, 0.0794, 0.1086, 0.0834, 0.0753],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0032, 0.9588, 0.0039, 0.0080, 0.0018, 0.0018, 0.0225],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2405, 0.0083, 0.1508, 0.1124, 0.1268, 0.2659, 0.0954],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1764, 0.0156, 0.1124, 0.2790, 0.1522, 0.1199, 0.1445],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1857, 0.0147, 0.0942, 0.1031, 0.4212, 0.0990, 0.0822],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1340, 0.0088, 0.1893, 0.1064, 0.1047, 0.3377, 0.1192],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2631, 0.0154, 0.1480, 0.1279, 0.1411, 0.1195, 0.1850],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.1199800000000001 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.729]
 [0.581]
 [0.581]
 [0.581]
 [0.581]
 [0.581]] [[43.911]
 [46.863]
 [43.911]
 [43.911]
 [43.911]
 [43.911]
 [43.911]] [[1.807]
 [2.11 ]
 [1.807]
 [1.807]
 [1.807]
 [1.807]
 [1.807]]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.1199800000000001 0.6950000000000002 0.6950000000000002
from probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
line 256 mcts: sample exp_bonus 0.0
from probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  39.790455584668635
maxi score, test score, baseline:  -0.1267000000000001 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.1267000000000001 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.1267000000000001 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  36.03119842893677
maxi score, test score, baseline:  -0.1267000000000001 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  32.51765584036661
maxi score, test score, baseline:  -0.1267000000000001 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  33.15528392791748
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [ 0.1896],
        [ 0.2115],
        [-0.1554],
        [-0.1585],
        [-0.1788],
        [ 0.6785],
        [-0.0408],
        [-0.0087],
        [ 0.1010]], dtype=torch.float64)
-0.858 -0.858
-0.071551887066 0.11806867294970862
-0.058226434398 0.1532945930627222
-0.070771701198 -0.22616943468779188
-0.032346567066 -0.19088293464583772
-0.032346567066 -0.21114225954742627
-0.09703970119800001 0.5814988063393163
-0.032346567066 -0.07316884678954724
-0.045026434398 -0.05373314793715112
-0.09703970119800001 0.003950904496087926
printing an ep nov before normalisation:  31.577605764574606
maxi score, test score, baseline:  -0.1267000000000001 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  18.291923999786377
printing an ep nov before normalisation:  46.04520344665877
actor:  0 policy actor:  0  step number:  50 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1309666666666668 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.1309666666666668 0.6950000000000002 0.6950000000000002
using explorer policy with actor:  1
printing an ep nov before normalisation:  24.497114109364507
printing an ep nov before normalisation:  44.13764552552274
maxi score, test score, baseline:  -0.13432666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]
 [0.765]] [[40.816]
 [40.816]
 [40.816]
 [40.816]
 [40.816]
 [40.816]
 [40.816]] [[0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]]
maxi score, test score, baseline:  -0.13432666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  61 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  49.10085212573871
printing an ep nov before normalisation:  68.3337709800666
maxi score, test score, baseline:  -0.13432666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  60 total reward:  0.21999999999999975  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.13432666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  47.12974253396801
printing an ep nov before normalisation:  49.5879028459587
printing an ep nov before normalisation:  46.946877647485266
printing an ep nov before normalisation:  51.40596719583005
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.13432666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
siam score:  -0.8406598
actor:  1 policy actor:  1  step number:  58 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  60 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.198]
 [0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.344]] [[44.824]
 [48.341]
 [44.824]
 [44.824]
 [44.824]
 [44.824]
 [44.824]] [[1.799]
 [1.833]
 [1.799]
 [1.799]
 [1.799]
 [1.799]
 [1.799]]
actions average: 
K:  0  action  0 :  tensor([0.4036, 0.0016, 0.1009, 0.1081, 0.1743, 0.1113, 0.1002],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0034, 0.9751, 0.0029, 0.0038, 0.0020, 0.0023, 0.0105],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1635, 0.0100, 0.2983, 0.1374, 0.1264, 0.1456, 0.1188],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1942, 0.0068, 0.1139, 0.3479, 0.1076, 0.1246, 0.1050],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0829, 0.0029, 0.0690, 0.0814, 0.6255, 0.0763, 0.0619],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1620, 0.0124, 0.1429, 0.1436, 0.1318, 0.2929, 0.1144],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1758, 0.0881, 0.1266, 0.1483, 0.1254, 0.1350, 0.2008],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  28.498023748397827
Printing some Q and Qe and total Qs values:  [[0.04 ]
 [0.184]
 [0.123]
 [0.03 ]
 [0.036]
 [0.031]
 [0.037]] [[36.06 ]
 [43.045]
 [40.723]
 [33.416]
 [34.081]
 [34.158]
 [34.731]] [[0.894]
 [1.375]
 [1.202]
 [0.757]
 [0.796]
 [0.794]
 [0.828]]
maxi score, test score, baseline:  -0.13432666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.13432666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  49 total reward:  0.2933333333333331  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.13432666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  10.664055347442627
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.447]
 [0.447]
 [0.447]
 [0.447]
 [0.447]
 [0.447]] [[29.131]
 [13.286]
 [13.286]
 [13.286]
 [13.286]
 [13.286]
 [13.286]] [[1.452]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  27.283611297607422
maxi score, test score, baseline:  -0.13432666666666676 0.6950000000000002 0.6950000000000002
actor:  1 policy actor:  1  step number:  52 total reward:  0.24666666666666637  reward:  1.0 rdn_beta:  1.667
using another actor
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.419]
 [0.419]
 [0.419]
 [0.551]
 [0.419]
 [0.419]] [[43.682]
 [31.171]
 [31.171]
 [31.171]
 [40.11 ]
 [31.171]
 [31.171]] [[1.566]
 [0.986]
 [0.986]
 [0.986]
 [1.436]
 [0.986]
 [0.986]]
printing an ep nov before normalisation:  35.151382287343345
line 256 mcts: sample exp_bonus 34.359421320893674
maxi score, test score, baseline:  -0.13432666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.13432666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.13432666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  48.50831690425656
actor:  1 policy actor:  1  step number:  36 total reward:  0.5  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  40 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.13818000000000014 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  0 policy actor:  1  step number:  49 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.13892666666666673 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.611]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]] [[49.376]
 [49.518]
 [49.376]
 [49.376]
 [49.376]
 [49.376]
 [49.376]] [[1.649]
 [1.736]
 [1.649]
 [1.649]
 [1.649]
 [1.649]
 [1.649]]
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.536]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]] [[45.662]
 [48.644]
 [45.662]
 [45.662]
 [45.662]
 [45.662]
 [45.662]] [[1.401]
 [1.629]
 [1.401]
 [1.401]
 [1.401]
 [1.401]
 [1.401]]
printing an ep nov before normalisation:  31.82511329650879
maxi score, test score, baseline:  -0.13892666666666673 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
siam score:  -0.840471
maxi score, test score, baseline:  -0.13892666666666673 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.13892666666666673 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.82456594197233
siam score:  -0.84340507
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.368]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.213]] [[49.648]
 [46.68 ]
 [49.648]
 [49.648]
 [49.648]
 [49.648]
 [47.755]] [[1.693]
 [1.672]
 [1.693]
 [1.693]
 [1.693]
 [1.693]
 [1.571]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.6014601472911
maxi score, test score, baseline:  -0.13892666666666673 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.13892666666666673 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  32.83352602711735
maxi score, test score, baseline:  -0.13892666666666673 0.6950000000000002 0.6950000000000002
printing an ep nov before normalisation:  50.735396647625706
Printing some Q and Qe and total Qs values:  [[0.183]
 [0.164]
 [0.179]
 [0.186]
 [0.174]
 [0.191]
 [0.176]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.183]
 [0.164]
 [0.179]
 [0.186]
 [0.174]
 [0.191]
 [0.176]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
actor:  1 policy actor:  1  step number:  45 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  1.667
from probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  27.835282983379514
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.645]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]
 [0.53 ]] [[53.246]
 [55.211]
 [53.246]
 [53.246]
 [53.246]
 [53.246]
 [53.246]] [[1.444]
 [1.606]
 [1.444]
 [1.444]
 [1.444]
 [1.444]
 [1.444]]
printing an ep nov before normalisation:  42.367496490478516
printing an ep nov before normalisation:  14.206252779634365
printing an ep nov before normalisation:  43.72148244711988
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  37 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.038]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.038]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.038]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.038]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 29.532929165550392
maxi score, test score, baseline:  -0.1423000000000001 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.1423000000000001 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  24.657816886901855
maxi score, test score, baseline:  -0.1423000000000001 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  38.70608658331476
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.684]
 [0.58 ]
 [0.586]
 [0.583]
 [0.58 ]
 [0.578]] [[46.707]
 [41.95 ]
 [44.497]
 [43.859]
 [43.828]
 [43.963]
 [43.088]] [[0.583]
 [0.684]
 [0.58 ]
 [0.586]
 [0.583]
 [0.58 ]
 [0.578]]
printing an ep nov before normalisation:  31.37673556804563
maxi score, test score, baseline:  -0.1423000000000001 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.716]
 [0.826]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]] [[39.863]
 [40.78 ]
 [39.863]
 [39.863]
 [39.863]
 [39.863]
 [39.863]] [[0.716]
 [0.826]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]]
printing an ep nov before normalisation:  44.144026662873245
maxi score, test score, baseline:  -0.1423000000000001 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  43.53373234967999
printing an ep nov before normalisation:  48.972738724262165
printing an ep nov before normalisation:  32.04214890468511
actor:  0 policy actor:  1  step number:  46 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.425]
 [0.281]
 [0.278]
 [0.278]
 [0.285]
 [0.279]] [[35.874]
 [36.06 ]
 [29.124]
 [30.199]
 [30.819]
 [31.321]
 [28.665]] [[0.855]
 [0.886]
 [0.593]
 [0.614]
 [0.627]
 [0.645]
 [0.582]]
actor:  0 policy actor:  0  step number:  35 total reward:  0.52  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.1419666666666668 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.1419666666666668 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  42.953724186529065
line 256 mcts: sample exp_bonus 53.52446950421792
actor:  1 policy actor:  1  step number:  56 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.1419666666666668 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  40.911377914675946
printing an ep nov before normalisation:  17.252059571387246
maxi score, test score, baseline:  -0.1419666666666668 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.1419666666666668 0.6950000000000002 0.6950000000000002
Printing some Q and Qe and total Qs values:  [[-0.014]
 [ 0.05 ]
 [ 0.047]
 [-0.003]
 [-0.007]
 [-0.008]
 [ 0.047]] [[53.35 ]
 [53.361]
 [42.945]
 [53.322]
 [55.399]
 [55.188]
 [42.945]] [[0.247]
 [0.311]
 [0.196]
 [0.258]
 [0.276]
 [0.273]
 [0.196]]
maxi score, test score, baseline:  -0.1419666666666668 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  47.55117978206846
maxi score, test score, baseline:  -0.1419666666666668 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Printing some Q and Qe and total Qs values:  [[0.078]
 [0.253]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]] [[39.048]
 [43.774]
 [37.904]
 [37.904]
 [37.904]
 [37.904]
 [37.904]] [[0.301]
 [0.526]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]
 [0.39 ]]
actor:  1 policy actor:  1  step number:  77 total reward:  0.026666666666665617  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.14196666666666674 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
UNIT TEST: sample policy line 217 mcts : [0.02  0.694 0.02  0.061 0.143 0.02  0.041]
maxi score, test score, baseline:  -0.14196666666666674 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.14196666666666674 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  57 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.14196666666666674 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  44.38893475083883
printing an ep nov before normalisation:  41.22728897773898
maxi score, test score, baseline:  -0.14196666666666674 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.14196666666666674 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  31.500367842796777
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.707]
 [0.571]
 [0.615]
 [0.615]
 [0.615]
 [0.615]] [[41.487]
 [40.456]
 [42.795]
 [38.839]
 [38.839]
 [38.839]
 [38.839]] [[1.321]
 [1.335]
 [1.261]
 [1.201]
 [1.201]
 [1.201]
 [1.201]]
printing an ep nov before normalisation:  28.323485851287842
maxi score, test score, baseline:  -0.14196666666666674 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.14196666666666674 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.14196666666666674 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.14196666666666674 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  70 total reward:  0.25999999999999923  reward:  1.0 rdn_beta:  1.0
using another actor
actor:  1 policy actor:  1  step number:  61 total reward:  0.11999999999999944  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  27.075451612472534
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.14196666666666677 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.14196666666666677 0.6950000000000002 0.6950000000000002
actions average: 
K:  3  action  0 :  tensor([0.3264, 0.1840, 0.0811, 0.0995, 0.1054, 0.0925, 0.1112],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0121, 0.9313, 0.0105, 0.0080, 0.0076, 0.0084, 0.0223],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1535, 0.0269, 0.2561, 0.1165, 0.1192, 0.1974, 0.1304],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1619, 0.0066, 0.1260, 0.1822, 0.1846, 0.1844, 0.1542],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1587, 0.0672, 0.0870, 0.1058, 0.3902, 0.0966, 0.0944],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0892, 0.0040, 0.1188, 0.0999, 0.0986, 0.4719, 0.1176],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1782, 0.0180, 0.1300, 0.1777, 0.1363, 0.1466, 0.2131],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  33.600352385495576
maxi score, test score, baseline:  -0.14196666666666677 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.14196666666666677 0.6950000000000002 0.6950000000000002
Printing some Q and Qe and total Qs values:  [[0.748]
 [0.793]
 [0.775]
 [0.671]
 [0.748]
 [0.722]
 [0.784]] [[32.127]
 [33.28 ]
 [34.439]
 [33.672]
 [33.826]
 [37.807]
 [32.89 ]] [[1.319]
 [1.41 ]
 [1.438]
 [1.304]
 [1.387]
 [1.519]
 [1.386]]
siam score:  -0.8349077
printing an ep nov before normalisation:  38.385820168390325
actor:  1 policy actor:  1  step number:  54 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  53 total reward:  0.27999999999999936  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.1394066666666668 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  32.59468718709351
printing an ep nov before normalisation:  43.46688349182444
printing an ep nov before normalisation:  46.6427992863756
maxi score, test score, baseline:  -0.1394066666666668 0.6950000000000002 0.6950000000000002
printing an ep nov before normalisation:  41.46962797117724
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.83830243
actor:  1 policy actor:  1  step number:  49 total reward:  0.3733333333333333  reward:  1.0 rdn_beta:  1.333
from probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
using explorer policy with actor:  1
printing an ep nov before normalisation:  62.674411126920276
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1394066666666668 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.1394066666666668 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Printing some Q and Qe and total Qs values:  [[-0.054]
 [-0.035]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.054]] [[58.27 ]
 [57.854]
 [58.27 ]
 [58.27 ]
 [58.27 ]
 [58.27 ]
 [58.27 ]] [[1.279]
 [1.285]
 [1.279]
 [1.279]
 [1.279]
 [1.279]
 [1.279]]
actor:  1 policy actor:  1  step number:  65 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  38.382649105216764
actor:  0 policy actor:  0  step number:  42 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  46.37192167549404
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.362]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]] [[49.247]
 [45.389]
 [49.247]
 [49.247]
 [49.247]
 [49.247]
 [49.247]] [[1.595]
 [1.471]
 [1.595]
 [1.595]
 [1.595]
 [1.595]
 [1.595]]
maxi score, test score, baseline:  -0.13667333333333342 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.13667333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.13667333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.13667333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.13667333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  44.2492519381599
printing an ep nov before normalisation:  48.38200249786409
printing an ep nov before normalisation:  36.80656775906404
printing an ep nov before normalisation:  34.85902159515533
maxi score, test score, baseline:  -0.13667333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  55 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.13667333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.13667333333333342 0.6950000000000002 0.6950000000000002
Printing some Q and Qe and total Qs values:  [[-0.032]
 [-0.066]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]
 [-0.032]] [[36.512]
 [53.697]
 [36.512]
 [36.512]
 [36.512]
 [36.512]
 [36.512]] [[0.463]
 [0.953]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]]
maxi score, test score, baseline:  -0.13667333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  54 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.13667333333333342 0.6950000000000002 0.6950000000000002
printing an ep nov before normalisation:  35.64398631929959
actor:  1 policy actor:  1  step number:  55 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.11631946398126
from probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.382]
 [0.377]] [[27.416]
 [26.547]
 [26.547]
 [26.547]
 [26.861]
 [27.137]
 [26.547]] [[0.59 ]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.382]
 [0.377]]
maxi score, test score, baseline:  -0.13667333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  62 total reward:  0.3266666666666661  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.06 ]
 [-0.042]
 [-0.06 ]
 [-0.059]
 [-0.059]
 [-0.06 ]
 [-0.053]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.06 ]
 [-0.042]
 [-0.06 ]
 [-0.059]
 [-0.059]
 [-0.06 ]
 [-0.053]]
Printing some Q and Qe and total Qs values:  [[-0.029]
 [ 0.077]
 [-0.023]
 [-0.029]
 [-0.018]
 [-0.029]
 [-0.012]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.029]
 [ 0.077]
 [-0.023]
 [-0.029]
 [-0.018]
 [-0.029]
 [-0.012]]
maxi score, test score, baseline:  -0.13667333333333342 0.6950000000000002 0.6950000000000002
printing an ep nov before normalisation:  37.75449621177024
line 256 mcts: sample exp_bonus 42.649905970720226
Printing some Q and Qe and total Qs values:  [[-0.016]
 [ 0.089]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[28.535]
 [34.031]
 [28.535]
 [28.535]
 [28.535]
 [28.535]
 [28.535]] [[0.761]
 [1.159]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]]
printing an ep nov before normalisation:  25.56728782941242
maxi score, test score, baseline:  -0.13667333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  45.716394119915996
maxi score, test score, baseline:  -0.13667333333333342 0.6950000000000002 0.6950000000000002
printing an ep nov before normalisation:  39.743750211708075
maxi score, test score, baseline:  -0.13667333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  31.31007257323226
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.642]
 [0.508]
 [0.5  ]
 [0.5  ]
 [0.439]
 [0.583]] [[30.65 ]
 [27.843]
 [28.15 ]
 [30.65 ]
 [30.65 ]
 [29.605]
 [28.806]] [[1.919]
 [1.83 ]
 [1.722]
 [1.919]
 [1.919]
 [1.772]
 [1.851]]
actor:  1 policy actor:  1  step number:  39 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.056]
 [-0.006]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]] [[ 0.   ]
 [25.763]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.988]
 [ 0.959]
 [-0.988]
 [-0.988]
 [-0.988]
 [-0.988]
 [-0.988]]
maxi score, test score, baseline:  -0.13667333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  63 total reward:  0.27999999999999925  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.13667333333333342 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.378]
 [0.392]
 [0.403]] [[39.739]
 [39.739]
 [39.739]
 [39.739]
 [43.919]
 [43.334]
 [39.739]] [[1.106]
 [1.106]
 [1.106]
 [1.106]
 [1.227]
 [1.221]
 [1.106]]
actor:  0 policy actor:  0  step number:  48 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  42 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  39.057871237311375
printing an ep nov before normalisation:  35.25918055467896
actions average: 
K:  4  action  0 :  tensor([0.1437, 0.0136, 0.1393, 0.1775, 0.1787, 0.2142, 0.1331],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0176, 0.8951, 0.0123, 0.0259, 0.0082, 0.0155, 0.0253],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1486, 0.0879, 0.2552, 0.1083, 0.1200, 0.1274, 0.1527],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1236, 0.0338, 0.1066, 0.3600, 0.0987, 0.1442, 0.1330],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1392, 0.0266, 0.0961, 0.1177, 0.3784, 0.1323, 0.1097],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1474, 0.2399, 0.1270, 0.1109, 0.1211, 0.1219, 0.1317],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1351, 0.1390, 0.1703, 0.1193, 0.1220, 0.1555, 0.1588],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  35.412631425989616
maxi score, test score, baseline:  -0.13404666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  28.741057310128063
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.439]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]] [[36.235]
 [36.311]
 [36.235]
 [36.235]
 [36.235]
 [36.235]
 [36.235]] [[1.119]
 [1.141]
 [1.119]
 [1.119]
 [1.119]
 [1.119]
 [1.119]]
actor:  0 policy actor:  0  step number:  38 total reward:  0.5  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.13104666666666678 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.745]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]] [[38.057]
 [36.845]
 [31.204]
 [31.204]
 [31.204]
 [31.204]
 [31.204]] [[1.859]
 [1.856]
 [1.483]
 [1.483]
 [1.483]
 [1.483]
 [1.483]]
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.462]
 [0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]] [[38.993]
 [31.445]
 [38.993]
 [38.993]
 [38.993]
 [38.993]
 [38.993]] [[1.447]
 [1.21 ]
 [1.447]
 [1.447]
 [1.447]
 [1.447]
 [1.447]]
maxi score, test score, baseline:  -0.13104666666666678 0.6950000000000002 0.6950000000000002
printing an ep nov before normalisation:  23.41081142425537
printing an ep nov before normalisation:  12.799418317278732
printing an ep nov before normalisation:  13.194150699455633
actor:  1 policy actor:  1  step number:  42 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.13104666666666678 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.13104666666666678 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Printing some Q and Qe and total Qs values:  [[0.7  ]
 [0.843]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]] [[47.933]
 [50.441]
 [47.933]
 [47.933]
 [47.933]
 [47.933]
 [47.933]] [[0.7  ]
 [0.843]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]
 [0.7  ]]
printing an ep nov before normalisation:  25.579357147216797
maxi score, test score, baseline:  -0.13104666666666678 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.13104666666666678 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  57 total reward:  0.11999999999999944  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  55.10487674427426
maxi score, test score, baseline:  -0.13104666666666678 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  0 policy actor:  1  step number:  42 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.14 ]
 [0.127]
 [0.17 ]
 [0.17 ]
 [0.141]
 [0.17 ]
 [0.17 ]] [[39.581]
 [43.754]
 [40.492]
 [40.492]
 [45.616]
 [40.492]
 [40.492]] [[0.963]
 [1.1  ]
 [1.026]
 [1.026]
 [1.181]
 [1.026]
 [1.026]]
maxi score, test score, baseline:  -0.13064666666666677 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  46.5256139606357
maxi score, test score, baseline:  -0.13064666666666677 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  46.97397408663992
siam score:  -0.82879484
printing an ep nov before normalisation:  32.734818081792554
maxi score, test score, baseline:  -0.13064666666666677 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  46 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  70 total reward:  0.0066666666666662655  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.59035262543133
printing an ep nov before normalisation:  31.679017813075646
printing an ep nov before normalisation:  42.47884304692363
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.326]
 [0.235]
 [0.285]
 [0.241]
 [0.239]
 [0.223]] [[36.656]
 [40.589]
 [30.892]
 [36.656]
 [32.399]
 [36.578]
 [30.627]] [[0.854]
 [1.003]
 [0.644]
 [0.854]
 [0.691]
 [0.805]
 [0.624]]
actions average: 
K:  4  action  0 :  tensor([0.4042, 0.0988, 0.0995, 0.0930, 0.1026, 0.1204, 0.0816],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0094, 0.9511, 0.0047, 0.0094, 0.0033, 0.0031, 0.0189],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1964, 0.0028, 0.3107, 0.1626, 0.1173, 0.1349, 0.0753],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1723, 0.1969, 0.1025, 0.2164, 0.1006, 0.0973, 0.1140],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2236, 0.0140, 0.1105, 0.1275, 0.2884, 0.1296, 0.1063],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1209, 0.0366, 0.2139, 0.0955, 0.0658, 0.3890, 0.0783],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1478, 0.1787, 0.0745, 0.2531, 0.1030, 0.0948, 0.1482],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  53.11420721055156
actor:  1 policy actor:  1  step number:  57 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  23.085696697235107
printing an ep nov before normalisation:  54.85364718634595
maxi score, test score, baseline:  -0.13064666666666677 0.6950000000000002 0.6950000000000002
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.684]
 [0.681]
 [0.672]
 [0.671]
 [0.674]
 [0.671]] [[41.338]
 [40.09 ]
 [38.039]
 [38.707]
 [41.338]
 [38.96 ]
 [41.338]] [[1.671]
 [1.636]
 [1.556]
 [1.571]
 [1.671]
 [1.584]
 [1.671]]
maxi score, test score, baseline:  -0.13064666666666677 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
siam score:  -0.8317636
Printing some Q and Qe and total Qs values:  [[0.159]
 [0.347]
 [0.116]
 [0.123]
 [0.133]
 [0.112]
 [0.153]] [[27.854]
 [41.089]
 [30.029]
 [28.044]
 [29.139]
 [27.837]
 [30.31 ]] [[0.602]
 [1.231]
 [0.632]
 [0.573]
 [0.619]
 [0.555]
 [0.678]]
printing an ep nov before normalisation:  19.206566344760674
actor:  1 policy actor:  1  step number:  53 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  35.63360432618304
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.732]
 [0.605]
 [0.605]
 [0.601]
 [0.605]
 [0.605]] [[35.233]
 [46.116]
 [34.674]
 [34.674]
 [30.954]
 [34.674]
 [34.674]] [[0.595]
 [0.732]
 [0.605]
 [0.605]
 [0.601]
 [0.605]
 [0.605]]
printing an ep nov before normalisation:  30.213757511540535
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.61 ]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]] [[32.563]
 [36.696]
 [32.563]
 [32.563]
 [32.563]
 [32.563]
 [32.563]] [[0.603]
 [0.61 ]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.015]
 [-0.009]
 [-0.012]
 [-0.013]
 [-0.015]
 [-0.014]
 [-0.02 ]] [[38.907]
 [32.233]
 [35.147]
 [35.175]
 [36.826]
 [36.181]
 [34.802]] [[1.312]
 [0.887]
 [1.072]
 [1.073]
 [1.178]
 [1.137]
 [1.042]]
maxi score, test score, baseline:  -0.13064666666666677 0.6950000000000002 0.6950000000000002
printing an ep nov before normalisation:  44.565169839950194
actor:  1 policy actor:  1  step number:  51 total reward:  0.37333333333333285  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  39 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.203]
 [0.235]
 [0.203]
 [0.203]
 [0.203]
 [0.203]
 [0.203]] [[37.676]
 [47.537]
 [37.676]
 [37.676]
 [37.676]
 [37.676]
 [37.676]] [[0.67 ]
 [0.902]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]]
printing an ep nov before normalisation:  28.47801923751831
maxi score, test score, baseline:  -0.12982000000000007 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.242923736572266
printing an ep nov before normalisation:  34.00814358399597
actor:  1 policy actor:  1  step number:  42 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  0.667
from probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actions average: 
K:  0  action  0 :  tensor([0.2907, 0.0033, 0.1448, 0.1323, 0.1570, 0.1258, 0.1461],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0038, 0.9635, 0.0068, 0.0075, 0.0019, 0.0069, 0.0096],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1470, 0.0173, 0.3408, 0.1040, 0.1281, 0.1439, 0.1189],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1363, 0.0019, 0.1307, 0.3579, 0.1330, 0.0996, 0.1406],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1791, 0.0025, 0.0967, 0.0793, 0.4848, 0.0615, 0.0960],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0987, 0.0023, 0.1602, 0.0788, 0.0799, 0.4929, 0.0872],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2098, 0.1128, 0.1044, 0.1111, 0.1194, 0.0908, 0.2518],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  24.386123486820228
actor:  1 policy actor:  1  step number:  58 total reward:  0.086666666666666  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  41.06483920029308
printing an ep nov before normalisation:  39.08832970354199
printing an ep nov before normalisation:  30.0128369557499
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.12982000000000007 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Printing some Q and Qe and total Qs values:  [[0.752]
 [0.912]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]] [[37.854]
 [42.699]
 [37.854]
 [37.854]
 [37.854]
 [37.854]
 [37.854]] [[0.752]
 [0.912]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]]
actor:  0 policy actor:  1  step number:  48 total reward:  0.39333333333333276  reward:  1.0 rdn_beta:  1.333
siam score:  -0.82883364
maxi score, test score, baseline:  -0.12703333333333344 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.12703333333333344 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  0 policy actor:  0  step number:  60 total reward:  0.13999999999999946  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.12688666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  16.30740698594707
printing an ep nov before normalisation:  15.178321445358982
printing an ep nov before normalisation:  40.902211086330034
maxi score, test score, baseline:  -0.12688666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  41.43547538762445
actor:  1 policy actor:  1  step number:  49 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.42585293996895
maxi score, test score, baseline:  -0.12688666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.12688666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  46.75079164816168
printing an ep nov before normalisation:  45.34527134205203
maxi score, test score, baseline:  -0.12688666666666676 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.12688666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.12688666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  34.76012861194787
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.762]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]] [[37.78 ]
 [39.001]
 [37.78 ]
 [37.78 ]
 [37.78 ]
 [37.78 ]
 [37.78 ]] [[1.758]
 [1.847]
 [1.758]
 [1.758]
 [1.758]
 [1.758]
 [1.758]]
maxi score, test score, baseline:  -0.12688666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  36 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  39.23445406400528
printing an ep nov before normalisation:  64.55065111585758
actor:  1 policy actor:  1  step number:  46 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 27.527798116365204
maxi score, test score, baseline:  -0.12688666666666676 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  46.61068603092792
printing an ep nov before normalisation:  39.28962568654645
printing an ep nov before normalisation:  29.596889507228166
actor:  0 policy actor:  0  step number:  42 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  52 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.12399333333333344 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.12399333333333344 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  67 total reward:  0.013333333333332864  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.12399333333333344 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  39 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  41 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  1.333
from probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.12399333333333344 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.391]
 [0.251]
 [0.263]
 [0.263]
 [0.283]
 [0.301]] [[44.146]
 [44.536]
 [37.729]
 [44.146]
 [44.146]
 [41.517]
 [50.604]] [[1.524]
 [1.672]
 [1.185]
 [1.524]
 [1.524]
 [1.41 ]
 [1.892]]
maxi score, test score, baseline:  -0.12399333333333344 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  45.94568064475746
maxi score, test score, baseline:  -0.12399333333333344 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.12399333333333344 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  28.20033885492918
printing an ep nov before normalisation:  45.862331832794894
maxi score, test score, baseline:  -0.12399333333333344 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  0 policy actor:  0  step number:  35 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.12092666666666675 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  50 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  61 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  44.855369368288976
siam score:  -0.83343446
printing an ep nov before normalisation:  51.90698903954822
printing an ep nov before normalisation:  34.33031537749582
maxi score, test score, baseline:  -0.12092666666666675 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.12092666666666675 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  22.276175230906894
maxi score, test score, baseline:  -0.12092666666666675 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.12092666666666675 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.12092666666666675 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  25.607942326780933
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.621]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]] [[39.225]
 [43.852]
 [39.225]
 [39.225]
 [39.225]
 [39.225]
 [39.225]] [[1.433]
 [1.944]
 [1.433]
 [1.433]
 [1.433]
 [1.433]
 [1.433]]
actor:  0 policy actor:  0  step number:  43 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  51.39576158989673
Printing some Q and Qe and total Qs values:  [[-0.046]
 [-0.032]
 [-0.045]
 [-0.049]
 [-0.046]
 [-0.048]
 [-0.046]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.046]
 [-0.032]
 [-0.045]
 [-0.049]
 [-0.046]
 [-0.048]
 [-0.046]]
maxi score, test score, baseline:  -0.11815333333333344 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  34.72872525336391
printing an ep nov before normalisation:  20.97124685721254
printing an ep nov before normalisation:  16.325105050339914
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.11815333333333344 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  59 total reward:  0.1866666666666661  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  56.665717718531575
Printing some Q and Qe and total Qs values:  [[-0.05 ]
 [-0.039]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]
 [-0.05 ]] [[30.33 ]
 [34.395]
 [30.33 ]
 [30.33 ]
 [30.33 ]
 [30.33 ]
 [30.33 ]] [[0.686]
 [0.858]
 [0.686]
 [0.686]
 [0.686]
 [0.686]
 [0.686]]
maxi score, test score, baseline:  -0.11815333333333344 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
line 256 mcts: sample exp_bonus 27.005475526039323
maxi score, test score, baseline:  -0.11815333333333344 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.11815333333333344 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  37.498178619422504
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 32.66978966351513
printing an ep nov before normalisation:  47.97500152055123
printing an ep nov before normalisation:  28.231664725960734
maxi score, test score, baseline:  -0.11815333333333344 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actions average: 
K:  0  action  0 :  tensor([0.3718, 0.0212, 0.0981, 0.1248, 0.1410, 0.1292, 0.1140],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0036, 0.9680, 0.0028, 0.0034, 0.0021, 0.0022, 0.0179],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1725, 0.0150, 0.2319, 0.1261, 0.1398, 0.1917, 0.1230],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1614, 0.0519, 0.1108, 0.2269, 0.1727, 0.1402, 0.1360],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1256, 0.0029, 0.0567, 0.0837, 0.5928, 0.0807, 0.0574],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1158, 0.0058, 0.1644, 0.0869, 0.1103, 0.4376, 0.0792],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1818, 0.0388, 0.0896, 0.0918, 0.0925, 0.0877, 0.4177],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  69 total reward:  0.11999999999999911  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.403]
 [0.377]
 [0.254]
 [0.377]
 [0.377]
 [0.248]] [[27.345]
 [32.806]
 [27.345]
 [27.936]
 [27.345]
 [27.345]
 [27.635]] [[1.052]
 [1.352]
 [1.052]
 [0.958]
 [1.052]
 [1.052]
 [0.937]]
printing an ep nov before normalisation:  36.7595257509451
actor:  1 policy actor:  1  step number:  70 total reward:  0.05999999999999939  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.11815333333333344 0.6950000000000002 0.6950000000000002
maxi score, test score, baseline:  -0.11815333333333344 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
siam score:  -0.8284335
maxi score, test score, baseline:  -0.11815333333333344 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  1 policy actor:  1  step number:  55 total reward:  0.2933333333333332  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.11815333333333344 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  21.216084957122803
printing an ep nov before normalisation:  60.042525619626645
maxi score, test score, baseline:  -0.11815333333333344 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actions average: 
K:  2  action  0 :  tensor([0.2669, 0.0217, 0.1240, 0.1507, 0.1847, 0.1283, 0.1237],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0165, 0.8801, 0.0173, 0.0203, 0.0152, 0.0119, 0.0386],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0985, 0.0322, 0.4251, 0.0821, 0.0822, 0.1948, 0.0850],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1428, 0.0417, 0.1543, 0.1947, 0.1574, 0.1625, 0.1466],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1249, 0.1858, 0.0846, 0.1100, 0.3004, 0.0886, 0.1057],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1443, 0.0167, 0.1905, 0.1607, 0.1455, 0.2407, 0.1016],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1080, 0.3315, 0.0959, 0.1599, 0.1029, 0.0705, 0.1312],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  47.68620227328537
actor:  1 policy actor:  1  step number:  48 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  41 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.11815333333333344 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.11815333333333344 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  0.52  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  28.02009594652301
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  38.572263710978895
maxi score, test score, baseline:  -0.11815333333333344 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
actor:  0 policy actor:  0  step number:  40 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.11528666666666679 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  19.70406128410187
printing an ep nov before normalisation:  31.908687285777095
printing an ep nov before normalisation:  35.61053903365937
printing an ep nov before normalisation:  39.89651110457548
actor:  1 policy actor:  1  step number:  67 total reward:  0.1466666666666656  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.11528666666666679 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]] [[30.619]
 [30.619]
 [30.619]
 [30.619]
 [30.619]
 [30.619]
 [30.619]] [[0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]]
actor:  0 policy actor:  0  step number:  58 total reward:  0.11333333333333284  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.1130600000000001 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  51.660381813804726
maxi score, test score, baseline:  -0.1130600000000001 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
maxi score, test score, baseline:  -0.1130600000000001 0.6950000000000002 0.6950000000000002
actions average: 
K:  2  action  0 :  tensor([0.4720, 0.0057, 0.0899, 0.1026, 0.1460, 0.0907, 0.0931],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0144, 0.9128, 0.0127, 0.0126, 0.0067, 0.0077, 0.0331],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1341, 0.0068, 0.3506, 0.1127, 0.0846, 0.2006, 0.1106],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1774, 0.0077, 0.1577, 0.1574, 0.1929, 0.1641, 0.1428],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1238, 0.0061, 0.0917, 0.0926, 0.5139, 0.0860, 0.0860],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0829, 0.0054, 0.1263, 0.0836, 0.0823, 0.4767, 0.1428],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0942, 0.1222, 0.1036, 0.1524, 0.0881, 0.0884, 0.3513],
       grad_fn=<DivBackward0>)
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.49 ]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.391]
 [0.49 ]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]]
maxi score, test score, baseline:  -0.1130600000000001 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  34.42160742623466
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
Printing some Q and Qe and total Qs values:  [[0.718]
 [0.814]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]] [[37.898]
 [38.785]
 [38.474]
 [38.474]
 [38.474]
 [38.474]
 [38.474]] [[0.718]
 [0.814]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]]
siam score:  -0.84073263
Printing some Q and Qe and total Qs values:  [[0.768]
 [0.693]
 [0.669]
 [0.669]
 [0.734]
 [0.669]
 [0.669]] [[35.108]
 [31.15 ]
 [26.247]
 [26.247]
 [34.24 ]
 [26.247]
 [26.247]] [[0.768]
 [0.693]
 [0.669]
 [0.669]
 [0.734]
 [0.669]
 [0.669]]
printing an ep nov before normalisation:  42.29058745408091
printing an ep nov before normalisation:  37.488063949509524
maxi score, test score, baseline:  -0.1130600000000001 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  41.195969581604004
printing an ep nov before normalisation:  37.15611934661865
Printing some Q and Qe and total Qs values:  [[0.771]
 [0.847]
 [0.781]
 [0.775]
 [0.78 ]
 [0.775]
 [0.775]] [[41.314]
 [48.494]
 [37.859]
 [37.156]
 [38.859]
 [37.156]
 [37.156]] [[0.771]
 [0.847]
 [0.781]
 [0.775]
 [0.78 ]
 [0.775]
 [0.775]]
maxi score, test score, baseline:  -0.1130600000000001 0.6950000000000002 0.6950000000000002
printing an ep nov before normalisation:  34.908789271276646
printing an ep nov before normalisation:  28.774893283843994
maxi score, test score, baseline:  -0.1130600000000001 0.6950000000000002 0.6950000000000002
printing an ep nov before normalisation:  18.031645861079063
printing an ep nov before normalisation:  17.77154915268973
line 256 mcts: sample exp_bonus 25.800743103027344
printing an ep nov before normalisation:  32.41863059406954
maxi score, test score, baseline:  -0.1130600000000001 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
printing an ep nov before normalisation:  26.26873753405727
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  42.23483107582532
actor:  0 policy actor:  1  step number:  47 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  55.63101130028435
Printing some Q and Qe and total Qs values:  [[-0.047]
 [-0.05 ]
 [-0.043]
 [-0.044]
 [-0.044]
 [-0.044]
 [-0.032]] [[17.44 ]
 [25.508]
 [17.607]
 [17.525]
 [17.582]
 [17.57 ]
 [31.986]] [[0.65 ]
 [1.283]
 [0.667]
 [0.66 ]
 [0.664]
 [0.664]
 [1.812]]
printing an ep nov before normalisation:  63.57860507423195
printing an ep nov before normalisation:  23.603442274790915
printing an ep nov before normalisation:  25.004746428813554
printing an ep nov before normalisation:  14.347692728042603
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.455]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]] [[28.434]
 [27.485]
 [28.434]
 [28.434]
 [28.434]
 [28.434]
 [28.434]] [[1.09 ]
 [1.204]
 [1.09 ]
 [1.09 ]
 [1.09 ]
 [1.09 ]
 [1.09 ]]
printing an ep nov before normalisation:  13.453723129834177
printing an ep nov before normalisation:  47.69123910147919
maxi score, test score, baseline:  -0.08788666666666677 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Printing some Q and Qe and total Qs values:  [[0.75 ]
 [0.798]
 [0.766]
 [0.763]
 [0.764]
 [0.765]
 [0.759]] [[20.494]
 [38.519]
 [10.471]
 [10.422]
 [10.391]
 [10.435]
 [22.982]] [[0.75 ]
 [0.798]
 [0.766]
 [0.763]
 [0.764]
 [0.765]
 [0.759]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.86 ]
 [0.985]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]] [[31.272]
 [38.875]
 [31.272]
 [31.272]
 [31.272]
 [31.272]
 [31.272]] [[0.86 ]
 [0.985]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]
 [0.86 ]]
actor:  0 policy actor:  0  step number:  54 total reward:  0.09999999999999931  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  55 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  55 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  55 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  30.745508810607983
maxi score, test score, baseline:  -0.08734000000000011 0.6950000000000002 0.6950000000000002
probs:  [0.09677960415967674, 0.09677960415967674, 0.09677960415967674, 0.5161019792016165, 0.09677960415967674, 0.09677960415967674]
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.67 ]
 [0.543]
 [0.539]
 [0.582]
 [0.606]
 [0.576]] [[32.921]
 [35.626]
 [34.358]
 [37.977]
 [38.831]
 [37.646]
 [38.3  ]] [[1.114]
 [1.286]
 [1.122]
 [1.224]
 [1.292]
 [1.282]
 [1.271]]
actor:  0 policy actor:  0  step number:  58 total reward:  0.01999999999999924  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.08530000000000008 -0.053333333333333455 -0.053333333333333455
probs:  [0.05614541259375315, 0.09097886998727886, 0.13704297960441678, 0.3922838790328255, 0.14888177802670052, 0.17466708075502513]
actor:  1 policy actor:  1  step number:  66 total reward:  0.23333333333333273  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.66759602315409
maxi score, test score, baseline:  -0.08530000000000008 -0.053333333333333455 -0.053333333333333455
probs:  [0.056094553161240866, 0.09089638154674182, 0.13782594539115944, 0.39192780366575, 0.14874671336548156, 0.17450860286962627]
printing an ep nov before normalisation:  35.51469802856445
maxi score, test score, baseline:  -0.08530000000000008 -0.053333333333333455 -0.053333333333333455
probs:  [0.056094553161240866, 0.09089638154674182, 0.13782594539115944, 0.39192780366575, 0.14874671336548156, 0.17450860286962627]
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.764]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]] [[32.649]
 [33.422]
 [32.649]
 [32.649]
 [32.649]
 [32.649]
 [32.649]] [[2.142]
 [2.336]
 [2.142]
 [2.142]
 [2.142]
 [2.142]
 [2.142]]
actor:  1 policy actor:  1  step number:  73 total reward:  0.05333333333333268  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.08530000000000008 -0.053333333333333455 -0.053333333333333455
printing an ep nov before normalisation:  23.56143156119021
printing an ep nov before normalisation:  41.316075282436515
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.551]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]] [[27.476]
 [37.148]
 [27.476]
 [27.476]
 [27.476]
 [27.476]
 [27.476]] [[0.507]
 [0.551]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]]
Printing some Q and Qe and total Qs values:  [[0.788]
 [0.872]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]] [[41.836]
 [45.292]
 [41.836]
 [41.836]
 [41.836]
 [41.836]
 [41.836]] [[0.788]
 [0.872]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]]
printing an ep nov before normalisation:  27.64583110809326
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.677]
 [0.533]
 [0.678]
 [0.533]
 [0.533]
 [0.533]] [[31.144]
 [34.591]
 [22.38 ]
 [33.989]
 [22.38 ]
 [22.38 ]
 [22.38 ]] [[1.365]
 [1.62 ]
 [0.926]
 [1.594]
 [0.926]
 [0.926]
 [0.926]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.405]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]] [[53.534]
 [55.433]
 [53.534]
 [53.534]
 [53.534]
 [53.534]
 [53.534]] [[0.82 ]
 [0.942]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]]
actions average: 
K:  2  action  0 :  tensor([0.4251, 0.0533, 0.1046, 0.0995, 0.1075, 0.0966, 0.1133],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0127, 0.8927, 0.0126, 0.0215, 0.0046, 0.0075, 0.0483],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1683, 0.0114, 0.1674, 0.1293, 0.1056, 0.3069, 0.1111],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1057, 0.0197, 0.0690, 0.5443, 0.0823, 0.1000, 0.0790],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1948, 0.0096, 0.1463, 0.1445, 0.1908, 0.1899, 0.1239],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1758, 0.0696, 0.1263, 0.0868, 0.0757, 0.3786, 0.0871],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2321, 0.0991, 0.1322, 0.1203, 0.1366, 0.1401, 0.1397],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  0.06653240043505093
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 34.242205434536714
actor:  1 policy actor:  1  step number:  54 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
Printing some Q and Qe and total Qs values:  [[-0.007]
 [ 0.011]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]] [[12.415]
 [36.362]
 [12.415]
 [12.415]
 [12.415]
 [12.415]
 [12.415]] [[0.161]
 [0.815]
 [0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.161]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  56.83052380344554
maxi score, test score, baseline:  -0.09042000000000008 -0.053333333333333455 -0.053333333333333455
probs:  [0.056047789328843266, 0.09297087746522123, 0.13706519290972852, 0.39160116812807677, 0.14839811314081292, 0.1739168590273174]
siam score:  -0.84011096
maxi score, test score, baseline:  -0.09042000000000008 -0.053333333333333455 -0.053333333333333455
probs:  [0.056047789328843266, 0.09297087746522123, 0.13706519290972852, 0.39160116812807677, 0.14839811314081292, 0.1739168590273174]
printing an ep nov before normalisation:  52.05605299810143
maxi score, test score, baseline:  -0.09042000000000008 -0.053333333333333455 -0.053333333333333455
maxi score, test score, baseline:  -0.09042000000000008 -0.053333333333333455 -0.053333333333333455
probs:  [0.056047789328843266, 0.09297087746522123, 0.13706519290972852, 0.39160116812807677, 0.14839811314081292, 0.1739168590273174]
line 256 mcts: sample exp_bonus 53.38561719096531
printing an ep nov before normalisation:  29.974673865443897
maxi score, test score, baseline:  -0.09042000000000008 -0.053333333333333455 -0.053333333333333455
probs:  [0.056047789328843266, 0.09297087746522123, 0.13706519290972852, 0.39160116812807677, 0.14839811314081292, 0.1739168590273174]
printing an ep nov before normalisation:  0.0011371685843641899
maxi score, test score, baseline:  -0.09042000000000008 -0.053333333333333455 -0.053333333333333455
probs:  [0.056047789328843266, 0.09297087746522123, 0.13706519290972852, 0.39160116812807677, 0.14839811314081292, 0.1739168590273174]
actor:  1 policy actor:  1  step number:  39 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  41.68133412855788
actor:  1 policy actor:  1  step number:  53 total reward:  0.23999999999999977  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  41 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.384743815913744
maxi score, test score, baseline:  -0.09042000000000008 -0.053333333333333455 -0.053333333333333455
probs:  [0.05589951095748965, 0.09247952735638182, 0.14005804141693076, 0.3905643882719016, 0.14739175957459683, 0.17360677242269948]
printing an ep nov before normalisation:  36.27265296405759
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.306]
 [0.306]
 [0.306]
 [0.342]
 [0.306]
 [0.306]] [[47.872]
 [54.685]
 [54.685]
 [54.685]
 [48.457]
 [54.685]
 [54.685]] [[1.526]
 [1.639]
 [1.639]
 [1.639]
 [1.45 ]
 [1.639]
 [1.639]]
maxi score, test score, baseline:  -0.09042000000000008 -0.053333333333333455 -0.053333333333333455
probs:  [0.05589951095748965, 0.09247952735638182, 0.14005804141693076, 0.3905643882719016, 0.14739175957459683, 0.17360677242269948]
printing an ep nov before normalisation:  44.50070075534068
maxi score, test score, baseline:  -0.09042000000000008 -0.053333333333333455 -0.053333333333333455
printing an ep nov before normalisation:  35.602396200306586
printing an ep nov before normalisation:  27.842360792283763
actor:  1 policy actor:  1  step number:  47 total reward:  0.4399999999999996  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  48 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  13.995695797474475
actor:  1 policy actor:  1  step number:  53 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.212]
 [0.045]
 [0.045]
 [0.045]
 [0.045]
 [0.045]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.045]
 [0.212]
 [0.045]
 [0.045]
 [0.045]
 [0.045]
 [0.045]]
line 256 mcts: sample exp_bonus 43.80146087690802
maxi score, test score, baseline:  -0.09042000000000008 -0.053333333333333455 -0.053333333333333455
probs:  [0.056339610765668846, 0.09233351383770187, 0.13914968813167025, 0.3936504120718423, 0.14636589978248396, 0.17216087541063277]
actions average: 
K:  1  action  0 :  tensor([0.5908, 0.0057, 0.0631, 0.0692, 0.1445, 0.0608, 0.0660],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0043, 0.9559, 0.0053, 0.0038, 0.0021, 0.0023, 0.0264],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1332, 0.0993, 0.3516, 0.0968, 0.0820, 0.1281, 0.1088],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1781, 0.1110, 0.1261, 0.2258, 0.1138, 0.1336, 0.1116],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2046, 0.0019, 0.1106, 0.0917, 0.3550, 0.0992, 0.1370],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0907, 0.0025, 0.1408, 0.0786, 0.0811, 0.5406, 0.0657],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1550, 0.1677, 0.1308, 0.1247, 0.0987, 0.1140, 0.2090],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  38 total reward:  0.5666666666666668  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  75.2293401776341
maxi score, test score, baseline:  -0.09042000000000008 -0.053333333333333455 -0.053333333333333455
probs:  [0.05655556917713504, 0.09205771459966644, 0.1387625627828685, 0.3951645943879614, 0.145926220684239, 0.17153333836812956]
actions average: 
K:  4  action  0 :  tensor([0.3359, 0.1313, 0.1100, 0.0931, 0.1199, 0.0963, 0.1135],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0235, 0.9430, 0.0087, 0.0060, 0.0058, 0.0027, 0.0104],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1771, 0.0154, 0.2543, 0.1305, 0.1366, 0.1634, 0.1227],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2455, 0.1224, 0.1327, 0.1297, 0.0956, 0.1317, 0.1425],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0914, 0.2743, 0.0529, 0.0895, 0.3446, 0.0612, 0.0863],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1656, 0.0938, 0.1235, 0.1590, 0.1018, 0.2521, 0.1041],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1493, 0.2154, 0.1238, 0.1192, 0.0880, 0.1206, 0.1839],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.18617932754396
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  39 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  2  action  0 :  tensor([0.4130, 0.0401, 0.0999, 0.0949, 0.1352, 0.1009, 0.1160],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0075, 0.9373, 0.0096, 0.0064, 0.0030, 0.0034, 0.0328],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1372, 0.0621, 0.3596, 0.0863, 0.0783, 0.1529, 0.1236],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1985, 0.0134, 0.1350, 0.2548, 0.1197, 0.1325, 0.1461],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1489, 0.0102, 0.0985, 0.0846, 0.4389, 0.0983, 0.1205],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1588, 0.0215, 0.1916, 0.1311, 0.1213, 0.2346, 0.1411],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1639, 0.2108, 0.1003, 0.0884, 0.1375, 0.0983, 0.2008],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.08748666666666675 -0.053333333333333455 -0.053333333333333455
probs:  [0.05655556917713504, 0.09205771459966644, 0.1387625627828685, 0.3951645943879614, 0.145926220684239, 0.17153333836812956]
printing an ep nov before normalisation:  18.4535251376095
actor:  1 policy actor:  1  step number:  60 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  38.76656926225461
actor:  1 policy actor:  1  step number:  37 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  50 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  50.44921539614851
printing an ep nov before normalisation:  5.018890192332037e-06
printing an ep nov before normalisation:  37.98075146500292
actor:  1 policy actor:  1  step number:  69 total reward:  0.026666666666666172  reward:  1.0 rdn_beta:  2.0
using another actor
maxi score, test score, baseline:  -0.08499333333333342 -0.053333333333333455 -0.053333333333333455
probs:  [0.056692625342486126, 0.09193535677981335, 0.13893812357504956, 0.3961260765380164, 0.14541024598084054, 0.17089757178379425]
using another actor
Printing some Q and Qe and total Qs values:  [[0.712]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.68 ]
 [0.6  ]
 [0.6  ]] [[43.819]
 [31.565]
 [31.565]
 [31.565]
 [41.554]
 [31.565]
 [31.565]] [[0.712]
 [0.6  ]
 [0.6  ]
 [0.6  ]
 [0.68 ]
 [0.6  ]
 [0.6  ]]
maxi score, test score, baseline:  -0.08499333333333342 -0.053333333333333455 -0.053333333333333455
probs:  [0.05677873242116143, 0.09207511804725708, 0.1391494427274647, 0.39672894340578896, 0.144110216804705, 0.1711575465936228]
maxi score, test score, baseline:  -0.08499333333333342 -0.053333333333333455 -0.053333333333333455
probs:  [0.05677873242116143, 0.09207511804725708, 0.1391494427274647, 0.39672894340578896, 0.144110216804705, 0.1711575465936228]
line 256 mcts: sample exp_bonus 32.52393129548673
maxi score, test score, baseline:  -0.08499333333333342 -0.053333333333333455 -0.053333333333333455
probs:  [0.05677873242116143, 0.09207511804725708, 0.1391494427274647, 0.39672894340578896, 0.144110216804705, 0.1711575465936228]
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.709]
 [0.652]
 [0.648]
 [0.642]
 [0.642]
 [0.651]] [[22.441]
 [25.583]
 [23.026]
 [22.941]
 [22.88 ]
 [22.941]
 [22.726]] [[0.661]
 [0.709]
 [0.652]
 [0.648]
 [0.642]
 [0.642]
 [0.651]]
actor:  0 policy actor:  1  step number:  46 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.08478000000000009 -0.053333333333333455 -0.053333333333333455
probs:  [0.05677873242116143, 0.09207511804725708, 0.1391494427274647, 0.39672894340578896, 0.144110216804705, 0.1711575465936228]
maxi score, test score, baseline:  -0.08478000000000009 -0.053333333333333455 -0.053333333333333455
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.428]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]] [[34.673]
 [42.365]
 [34.673]
 [34.673]
 [34.673]
 [34.673]
 [34.673]] [[1.228]
 [1.648]
 [1.228]
 [1.228]
 [1.228]
 [1.228]
 [1.228]]
maxi score, test score, baseline:  -0.08478000000000009 -0.053333333333333455 -0.053333333333333455
probs:  [0.056791599485298425, 0.09186903362160094, 0.13918102035301338, 0.3968190303855476, 0.14414292127337153, 0.1711963948811681]
maxi score, test score, baseline:  -0.08478000000000009 -0.053333333333333455 -0.053333333333333455
probs:  [0.056791599485298425, 0.09186903362160094, 0.13918102035301338, 0.3968190303855476, 0.14414292127337153, 0.1711963948811681]
printing an ep nov before normalisation:  26.36539077585856
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.606]
 [0.579]
 [0.579]
 [0.551]
 [0.579]
 [0.52 ]] [[38.527]
 [36.239]
 [44.929]
 [44.929]
 [37.19 ]
 [44.929]
 [41.224]] [[0.564]
 [0.606]
 [0.579]
 [0.579]
 [0.551]
 [0.579]
 [0.52 ]]
maxi score, test score, baseline:  -0.08478000000000009 -0.053333333333333455 -0.053333333333333455
probs:  [0.056791599485298425, 0.09186903362160094, 0.13918102035301338, 0.3968190303855476, 0.14414292127337153, 0.1711963948811681]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  34.80045043715788
printing an ep nov before normalisation:  23.84102386673652
actor:  0 policy actor:  0  step number:  64 total reward:  0.03333333333333255  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]] [[26.259]
 [26.259]
 [26.259]
 [26.259]
 [26.259]
 [26.259]
 [26.259]] [[1.033]
 [1.033]
 [1.033]
 [1.033]
 [1.033]
 [1.033]
 [1.033]]
maxi score, test score, baseline:  -0.08271333333333342 -0.053333333333333455 -0.053333333333333455
probs:  [0.056659422231494574, 0.09176463748143782, 0.13861662360549357, 0.39589426541050304, 0.14618645700824712, 0.1708785942628239]
siam score:  -0.8421574
maxi score, test score, baseline:  -0.08271333333333342 -0.053333333333333455 -0.053333333333333455
probs:  [0.056659422231494574, 0.09176463748143782, 0.13861662360549357, 0.39589426541050304, 0.14618645700824712, 0.1708785942628239]
actor:  1 policy actor:  1  step number:  50 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  36.40683095447282
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]
 [0.232]] [[37.005]
 [37.005]
 [37.005]
 [37.005]
 [37.005]
 [37.005]
 [37.005]] [[0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]]
siam score:  -0.8420704
printing an ep nov before normalisation:  27.14271729593564
maxi score, test score, baseline:  -0.08271333333333342 -0.053333333333333455 -0.053333333333333455
probs:  [0.05670986539607576, 0.09174877582604703, 0.1385122704686187, 0.39624797545561047, 0.14606780638202735, 0.17071330647162072]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.08271333333333342 -0.053333333333333455 -0.053333333333333455
probs:  [0.05670986539607576, 0.09174877582604703, 0.1385122704686187, 0.39624797545561047, 0.14606780638202735, 0.17071330647162072]
maxi score, test score, baseline:  -0.08271333333333342 -0.053333333333333455 -0.053333333333333455
printing an ep nov before normalisation:  32.67238028838019
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.08271333333333342 -0.053333333333333455 -0.053333333333333455
probs:  [0.05672258406257714, 0.09154474687764648, 0.1385433786600912, 0.3963370237183878, 0.14610061308977482, 0.17075165359152264]
using explorer policy with actor:  0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.08271333333333342 -0.053333333333333455 -0.053333333333333455
probs:  [0.05672258406257714, 0.09154474687764648, 0.1385433786600912, 0.3963370237183878, 0.14610061308977482, 0.17075165359152264]
maxi score, test score, baseline:  -0.08271333333333342 -0.053333333333333455 -0.053333333333333455
actor:  0 policy actor:  1  step number:  74 total reward:  0.0733333333333328  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  46.70046358611815
printing an ep nov before normalisation:  46.91755119747625
maxi score, test score, baseline:  -0.08056666666666677 -0.053333333333333455 -0.053333333333333455
probs:  [0.05672258406257714, 0.09154474687764648, 0.1385433786600912, 0.3963370237183878, 0.14610061308977482, 0.17075165359152264]
maxi score, test score, baseline:  -0.08056666666666677 -0.053333333333333455 -0.053333333333333455
probs:  [0.05672258406257714, 0.09154474687764648, 0.1385433786600912, 0.3963370237183878, 0.14610061308977482, 0.17075165359152264]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  60 total reward:  0.15333333333333288  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.08056666666666677 -0.053333333333333455 -0.053333333333333455
probs:  [0.056498516401758736, 0.09118279957059391, 0.1415053942526375, 0.39476824007756456, 0.14552264980538984, 0.17052239989205537]
printing an ep nov before normalisation:  52.231156213645754
maxi score, test score, baseline:  -0.08056666666666677 -0.053333333333333455 -0.053333333333333455
probs:  [0.056498516401758736, 0.09118279957059391, 0.1415053942526375, 0.39476824007756456, 0.14552264980538984, 0.17052239989205537]
printing an ep nov before normalisation:  32.640297596466404
maxi score, test score, baseline:  -0.08056666666666677 -0.053333333333333455 -0.053333333333333455
probs:  [0.056498516401758736, 0.09118279957059391, 0.1415053942526375, 0.39476824007756456, 0.14552264980538984, 0.17052239989205537]
printing an ep nov before normalisation:  48.10807845750537
maxi score, test score, baseline:  -0.08056666666666677 -0.053333333333333455 -0.053333333333333455
probs:  [0.056498516401758736, 0.09118279957059391, 0.1415053942526375, 0.39476824007756456, 0.14552264980538984, 0.17052239989205537]
actor:  1 policy actor:  1  step number:  45 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  17.184370756149292
printing an ep nov before normalisation:  35.13573408126831
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  36 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.08056666666666677 -0.053333333333333455 -0.053333333333333455
actor:  1 policy actor:  1  step number:  47 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.08056666666666677 -0.053333333333333455 -0.053333333333333455
printing an ep nov before normalisation:  41.606075092628096
maxi score, test score, baseline:  -0.08056666666666677 -0.053333333333333455 -0.053333333333333455
probs:  [0.05633953487247685, 0.0906817012513815, 0.14050792688725533, 0.3936565056983372, 0.1495756149954305, 0.16923871629511866]
actor:  0 policy actor:  1  step number:  39 total reward:  0.48  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.847]
 [0.892]
 [0.844]
 [0.844]
 [0.844]
 [0.844]
 [0.844]] [[28.588]
 [40.036]
 [27.053]
 [27.053]
 [27.053]
 [27.053]
 [27.053]] [[0.847]
 [0.892]
 [0.844]
 [0.844]
 [0.844]
 [0.844]
 [0.844]]
maxi score, test score, baseline:  -0.07760666666666675 -0.053333333333333455 -0.053333333333333455
probs:  [0.05633953487247685, 0.0906817012513815, 0.14050792688725533, 0.3936565056983372, 0.1495756149954305, 0.16923871629511866]
printing an ep nov before normalisation:  32.64356476812275
maxi score, test score, baseline:  -0.07760666666666675 -0.053333333333333455 -0.053333333333333455
actor:  0 policy actor:  0  step number:  34 total reward:  0.5533333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.07450000000000011 -0.053333333333333455 -0.053333333333333455
printing an ep nov before normalisation:  49.634750261862386
maxi score, test score, baseline:  -0.07450000000000011 -0.053333333333333455 -0.053333333333333455
probs:  [0.05633953487247685, 0.0906817012513815, 0.14050792688725533, 0.3936565056983372, 0.1495756149954305, 0.16923871629511866]
printing an ep nov before normalisation:  41.67780719683428
maxi score, test score, baseline:  -0.07450000000000011 -0.053333333333333455 -0.053333333333333455
probs:  [0.05633953487247685, 0.0906817012513815, 0.14050792688725533, 0.3936565056983372, 0.1495756149954305, 0.16923871629511866]
printing an ep nov before normalisation:  35.99006652832031
actor:  0 policy actor:  1  step number:  48 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.07171333333333345 -0.053333333333333455 -0.053333333333333455
probs:  [0.05633953487247685, 0.0906817012513815, 0.14050792688725533, 0.3936565056983372, 0.1495756149954305, 0.16923871629511866]
line 256 mcts: sample exp_bonus 48.68779704553445
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]] [[31.548]
 [31.548]
 [31.548]
 [31.548]
 [31.548]
 [31.548]
 [31.548]] [[2.216]
 [2.216]
 [2.216]
 [2.216]
 [2.216]
 [2.216]
 [2.216]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  29.82540271997744
printing an ep nov before normalisation:  38.97058246763844
printing an ep nov before normalisation:  35.60724646643805
maxi score, test score, baseline:  -0.07171333333333345 -0.053333333333333455 -0.053333333333333455
probs:  [0.05644715210770259, 0.0904563305032425, 0.1403373350973603, 0.39441097313956847, 0.14937505099615514, 0.168973158155971]
maxi score, test score, baseline:  -0.07171333333333345 -0.053333333333333455 -0.053333333333333455
probs:  [0.05644715210770259, 0.0904563305032425, 0.1403373350973603, 0.39441097313956847, 0.14937505099615514, 0.168973158155971]
maxi score, test score, baseline:  -0.07171333333333345 -0.053333333333333455 -0.053333333333333455
probs:  [0.05644715210770259, 0.0904563305032425, 0.1403373350973603, 0.39441097313956847, 0.14937505099615514, 0.168973158155971]
maxi score, test score, baseline:  -0.07171333333333345 -0.053333333333333455 -0.053333333333333455
probs:  [0.05644715210770259, 0.0904563305032425, 0.1403373350973603, 0.39441097313956847, 0.1493750509961551, 0.168973158155971]
siam score:  -0.8353084
maxi score, test score, baseline:  -0.07171333333333345 -0.053333333333333455 -0.053333333333333455
probs:  [0.05644715210770259, 0.0904563305032425, 0.1403373350973603, 0.39441097313956847, 0.1493750509961551, 0.168973158155971]
maxi score, test score, baseline:  -0.07171333333333345 -0.053333333333333455 -0.053333333333333455
probs:  [0.05644715210770259, 0.0904563305032425, 0.1403373350973603, 0.39441097313956847, 0.1493750509961551, 0.168973158155971]
printing an ep nov before normalisation:  47.04775807459486
actor:  1 policy actor:  1  step number:  63 total reward:  0.27999999999999936  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  29.204902032183206
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.07171333333333345 -0.053333333333333455 -0.053333333333333455
probs:  [0.056382648827342854, 0.0903528723186563, 0.14017674204952968, 0.39395935823518785, 0.15034861359914464, 0.16877976497013875]
maxi score, test score, baseline:  -0.07171333333333345 -0.053333333333333455 -0.053333333333333455
actor:  1 policy actor:  1  step number:  85 total reward:  0.026666666666665395  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.07171333333333345 -0.053333333333333455 -0.053333333333333455
probs:  [0.056379471862312895, 0.09036442605247752, 0.14018276573675176, 0.39393721410443167, 0.1503535082908878, 0.16878261395313818]
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.724]
 [0.656]
 [0.53 ]
 [0.523]
 [0.551]
 [0.656]] [[44.625]
 [43.244]
 [44.625]
 [49.948]
 [50.887]
 [50.419]
 [44.625]] [[2.164]
 [2.169]
 [2.164]
 [2.282]
 [2.317]
 [2.324]
 [2.164]]
siam score:  -0.83104354
maxi score, test score, baseline:  -0.07171333333333342 -0.053333333333333455 -0.053333333333333455
probs:  [0.056379471862312895, 0.09036442605247752, 0.14018276573675176, 0.39393721410443167, 0.1503535082908878, 0.16878261395313818]
maxi score, test score, baseline:  -0.07171333333333342 -0.053333333333333455 -0.053333333333333455
probs:  [0.056379471862312895, 0.09036442605247752, 0.14018276573675176, 0.39393721410443167, 0.1503535082908878, 0.16878261395313818]
actor:  1 policy actor:  1  step number:  58 total reward:  0.3799999999999991  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  30.531230512300734
siam score:  -0.83776474
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.082]
 [0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[29.402]
 [33.438]
 [29.402]
 [29.402]
 [29.402]
 [29.402]
 [29.402]] [[0.266]
 [0.415]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]]
maxi score, test score, baseline:  -0.07434000000000011 -0.053333333333333455 -0.053333333333333455
probs:  [0.05627081472769195, 0.09019011639767001, 0.13991221644797325, 0.39317645909253623, 0.15006331104621945, 0.17038708228790922]
maxi score, test score, baseline:  -0.07434000000000011 -0.053333333333333455 -0.053333333333333455
probs:  [0.05627081472769195, 0.09019011639767001, 0.13991221644797325, 0.39317645909253623, 0.15006331104621945, 0.17038708228790922]
maxi score, test score, baseline:  -0.07434000000000011 -0.053333333333333455 -0.053333333333333455
Printing some Q and Qe and total Qs values:  [[0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]
 [0.32]] [[50.508]
 [50.508]
 [50.508]
 [50.508]
 [50.508]
 [50.508]
 [50.508]] [[1.768]
 [1.768]
 [1.768]
 [1.768]
 [1.768]
 [1.768]
 [1.768]]
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.231]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]] [[31.346]
 [39.396]
 [31.346]
 [31.346]
 [31.346]
 [31.346]
 [31.346]] [[0.402]
 [0.452]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]]
maxi score, test score, baseline:  -0.07434000000000011 -0.053333333333333455 -0.053333333333333455
probs:  [0.05627081472769195, 0.09019011639767001, 0.13991221644797325, 0.39317645909253623, 0.15006331104621945, 0.17038708228790922]
actor:  1 policy actor:  1  step number:  53 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  1.333
using another actor
from probs:  [0.05627081472769195, 0.09019011639767001, 0.13991221644797325, 0.39317645909253623, 0.15006331104621945, 0.17038708228790922]
printing an ep nov before normalisation:  39.06610649797266
maxi score, test score, baseline:  -0.07434000000000011 -0.053333333333333455 -0.053333333333333455
probs:  [0.056365690867415275, 0.090166224026451, 0.1397142220318078, 0.3938417104642031, 0.14982977254953017, 0.17008238006059267]
printing an ep nov before normalisation:  32.81416754571527
maxi score, test score, baseline:  -0.07434000000000011 -0.053333333333333455 -0.053333333333333455
probs:  [0.0563454678992967, 0.09023994485551771, 0.13975291551068877, 0.3937007516284099, 0.14986131496389657, 0.17009960514219044]
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.07434000000000011 -0.053333333333333455 -0.053333333333333455
probs:  [0.0563454678992967, 0.09023994485551771, 0.13975291551068877, 0.3937007516284099, 0.14986131496389657, 0.17009960514219044]
maxi score, test score, baseline:  -0.07434000000000011 -0.053333333333333455 -0.053333333333333455
probs:  [0.0563454678992967, 0.09023994485551771, 0.13975291551068877, 0.3937007516284099, 0.14986131496389657, 0.17009960514219044]
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.65 ]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]] [[30.793]
 [31.312]
 [30.793]
 [30.793]
 [30.793]
 [30.793]
 [30.793]] [[0.653]
 [0.65 ]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]]
printing an ep nov before normalisation:  25.232795766938327
maxi score, test score, baseline:  -0.07434000000000011 -0.053333333333333455 -0.053333333333333455
maxi score, test score, baseline:  -0.07434000000000011 -0.053333333333333455 -0.053333333333333455
probs:  [0.0563454678992967, 0.09023994485551771, 0.13975291551068877, 0.3937007516284099, 0.14986131496389657, 0.17009960514219044]
using another actor
maxi score, test score, baseline:  -0.07434000000000011 -0.053333333333333455 -0.053333333333333455
maxi score, test score, baseline:  -0.07434000000000011 -0.053333333333333455 -0.053333333333333455
probs:  [0.0563454678992967, 0.09023994485551771, 0.13975291551068877, 0.3937007516284099, 0.1498613149638965, 0.17009960514219044]
printing an ep nov before normalisation:  21.65139675140381
actor:  0 policy actor:  1  step number:  59 total reward:  0.3066666666666664  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  54 total reward:  0.23333333333333306  reward:  1.0 rdn_beta:  2.0
siam score:  -0.83958924
printing an ep nov before normalisation:  3.206169107926371
actor:  0 policy actor:  1  step number:  46 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  23.54590727132245
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.06907333333333346 -0.053333333333333455 -0.053333333333333455
probs:  [0.05631676809465377, 0.09019393983679218, 0.13935101803338817, 0.39349981121316924, 0.14978486957062392, 0.17085359325137278]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.06907333333333346 -0.053333333333333455 -0.053333333333333455
probs:  [0.05631676809465377, 0.09019393983679218, 0.13935101803338817, 0.39349981121316924, 0.14978486957062392, 0.17085359325137278]
maxi score, test score, baseline:  -0.06907333333333346 -0.053333333333333455 -0.053333333333333455
probs:  [0.05631676809465377, 0.09019393983679218, 0.13935101803338817, 0.39349981121316924, 0.14978486957062392, 0.17085359325137278]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.06907333333333346 -0.053333333333333455 -0.053333333333333455
probs:  [0.05631676809465377, 0.09019393983679218, 0.13935101803338817, 0.39349981121316924, 0.14978486957062392, 0.17085359325137278]
actor:  1 policy actor:  1  step number:  48 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.06907333333333346 -0.053333333333333455 -0.053333333333333455
probs:  [0.05622790704680474, 0.09005149795535339, 0.13913082830314258, 0.39287765450946854, 0.15112853378685145, 0.17058357839837926]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  29.875651927815596
printing an ep nov before normalisation:  33.90797979207926
printing an ep nov before normalisation:  43.04041402460807
maxi score, test score, baseline:  -0.06907333333333346 -0.053333333333333455 -0.053333333333333455
probs:  [0.05607418365078187, 0.08980508346575766, 0.13874991547438298, 0.3918013669384087, 0.15345297887761666, 0.17011647159305215]
printing an ep nov before normalisation:  39.12010099634112
actor:  1 policy actor:  1  step number:  76 total reward:  0.11333333333333218  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.06907333333333346 -0.053333333333333455 -0.053333333333333455
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.466]
 [0.501]
 [0.5  ]
 [0.473]
 [0.502]
 [0.48 ]] [[28.54 ]
 [27.313]
 [27.81 ]
 [26.467]
 [26.131]
 [23.779]
 [25.301]] [[1.275]
 [1.237]
 [1.286]
 [1.247]
 [1.211]
 [1.174]
 [1.194]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.4389],
        [-0.5774],
        [-0.3959],
        [ 0.1978],
        [ 0.2693],
        [ 0.2125],
        [-0.0000],
        [-0.3655],
        [ 0.4262],
        [ 0.1433]], dtype=torch.float64)
-0.057834381198 0.38101764976682634
-0.032346567066 -0.6097568278938926
-0.032346567066 -0.42821210599552034
-0.071159833866 0.1266322804075594
-0.045414567066 0.2239031623432189
-0.08423175439800001 0.12828396291341682
-0.9768 -0.9768
-0.058483887065999995 -0.42400020419925494
-0.070771701198 0.35544073064192605
-0.09703970119800001 0.04628651314705795
actor:  1 policy actor:  1  step number:  58 total reward:  0.20666666666666655  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.06907333333333346 -0.053333333333333455 -0.053333333333333455
probs:  [0.05610555556980565, 0.08935814000746807, 0.13919566285672894, 0.3920193420617306, 0.1533165135130838, 0.17000478599118304]
maxi score, test score, baseline:  -0.06907333333333346 -0.053333333333333455 -0.053333333333333455
probs:  [0.05610555556980565, 0.08935814000746807, 0.13919566285672894, 0.3920193420617306, 0.1533165135130838, 0.17000478599118304]
printing an ep nov before normalisation:  46.875019885173344
printing an ep nov before normalisation:  32.72577530485516
Printing some Q and Qe and total Qs values:  [[0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]] [[40.499]
 [40.499]
 [40.499]
 [40.499]
 [40.499]
 [40.499]
 [40.499]] [[0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]]
maxi score, test score, baseline:  -0.06907333333333346 -0.053333333333333455 -0.053333333333333455
probs:  [0.05610555556980565, 0.08935814000746807, 0.13919566285672894, 0.3920193420617306, 0.1533165135130838, 0.17000478599118304]
maxi score, test score, baseline:  -0.06907333333333346 -0.053333333333333455 -0.053333333333333455
Printing some Q and Qe and total Qs values:  [[-0.003]
 [ 0.35 ]
 [ 0.246]
 [ 0.269]
 [ 0.213]
 [ 0.246]
 [ 0.275]] [[31.076]
 [33.449]
 [16.843]
 [32.579]
 [40.542]
 [16.843]
 [30.209]] [[-0.003]
 [ 0.35 ]
 [ 0.246]
 [ 0.269]
 [ 0.213]
 [ 0.246]
 [ 0.275]]
actor:  0 policy actor:  1  step number:  37 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  39 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.06616666666666676 -0.053333333333333455 -0.053333333333333455
probs:  [0.05610555556980565, 0.08935814000746807, 0.13919566285672894, 0.3920193420617306, 0.1533165135130838, 0.17000478599118304]
Printing some Q and Qe and total Qs values:  [[0.104]
 [0.663]
 [0.454]
 [0.369]
 [0.47 ]
 [0.47 ]
 [0.378]] [[28.516]
 [25.049]
 [26.514]
 [30.055]
 [30.673]
 [19.571]
 [27.844]] [[0.104]
 [0.663]
 [0.454]
 [0.369]
 [0.47 ]
 [0.47 ]
 [0.378]]
printing an ep nov before normalisation:  34.429609889511624
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.06616666666666676 -0.053333333333333455 -0.053333333333333455
maxi score, test score, baseline:  -0.06616666666666676 -0.053333333333333455 -0.053333333333333455
probs:  [0.05610555556980565, 0.08935814000746807, 0.13919566285672894, 0.3920193420617306, 0.1533165135130838, 0.17000478599118304]
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.559]
 [0.643]
 [0.643]
 [0.564]
 [0.586]
 [0.643]] [[41.907]
 [40.079]
 [39.688]
 [39.688]
 [43.181]
 [41.868]
 [39.688]] [[1.767]
 [1.682]
 [1.748]
 [1.748]
 [1.83 ]
 [1.792]
 [1.748]]
actor:  0 policy actor:  0  step number:  35 total reward:  0.56  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.06304666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.05610555556980565, 0.08935814000746807, 0.13919566285672894, 0.3920193420617306, 0.1533165135130838, 0.17000478599118304]
printing an ep nov before normalisation:  39.38768990487909
printing an ep nov before normalisation:  43.06989598464354
maxi score, test score, baseline:  -0.06304666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.05610555556980565, 0.08935814000746807, 0.13919566285672894, 0.3920193420617306, 0.1533165135130838, 0.17000478599118304]
printing an ep nov before normalisation:  63.2334785942806
actor:  1 policy actor:  1  step number:  53 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.06304666666666678 -0.053333333333333455 -0.053333333333333455
maxi score, test score, baseline:  -0.06304666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.05618289372598101, 0.08934114250664683, 0.13903727918876985, 0.39256161614481977, 0.15311806980929454, 0.1697589986244881]
maxi score, test score, baseline:  -0.06304666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.05618289372598101, 0.08934114250664683, 0.13903727918876985, 0.39256161614481977, 0.15311806980929454, 0.1697589986244881]
printing an ep nov before normalisation:  43.51974662337751
printing an ep nov before normalisation:  32.43489095725153
maxi score, test score, baseline:  -0.06304666666666678 -0.053333333333333455 -0.053333333333333455
maxi score, test score, baseline:  -0.06304666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.05618289372598101, 0.08934114250664683, 0.13903727918876985, 0.39256161614481977, 0.15311806980929454, 0.1697589986244881]
printing an ep nov before normalisation:  59.52846367465459
printing an ep nov before normalisation:  31.91156931271899
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.06304666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.05618289372598101, 0.08934114250664683, 0.13903727918876985, 0.39256161614481977, 0.1531180698092946, 0.16975899862448812]
printing an ep nov before normalisation:  28.605201630987654
printing an ep nov before normalisation:  24.360132436102646
actor:  1 policy actor:  1  step number:  53 total reward:  0.306666666666666  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  48 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.06304666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.05640622748791499, 0.08929205786344326, 0.13857990572470624, 0.3941275716442115, 0.15254501271702453, 0.1690492245626993]
printing an ep nov before normalisation:  40.60599381344599
printing an ep nov before normalisation:  39.29653913930758
maxi score, test score, baseline:  -0.06304666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.05640622748791499, 0.08929205786344326, 0.13857990572470624, 0.3941275716442115, 0.15254501271702453, 0.1690492245626993]
maxi score, test score, baseline:  -0.06304666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.05640622748791499, 0.08929205786344326, 0.13857990572470624, 0.3941275716442115, 0.15254501271702453, 0.1690492245626993]
printing an ep nov before normalisation:  39.9453915290058
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.696]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]] [[45.233]
 [42.331]
 [45.233]
 [45.233]
 [45.233]
 [45.233]
 [45.233]] [[1.868]
 [1.756]
 [1.868]
 [1.868]
 [1.868]
 [1.868]
 [1.868]]
maxi score, test score, baseline:  -0.06304666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.05640622748791499, 0.08929205786344326, 0.13857990572470624, 0.3941275716442115, 0.15254501271702453, 0.1690492245626993]
printing an ep nov before normalisation:  40.44592871307436
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.632]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]] [[29.991]
 [30.608]
 [29.991]
 [29.991]
 [29.991]
 [29.991]
 [29.991]] [[1.563]
 [1.673]
 [1.563]
 [1.563]
 [1.563]
 [1.563]
 [1.563]]
printing an ep nov before normalisation:  32.49359320816467
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.1405277326000487
actor:  1 policy actor:  1  step number:  40 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  49 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  46 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.06304666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.05682720205964422, 0.08901176942644376, 0.13777084059068523, 0.3970792100532148, 0.15152690696970514, 0.16778407090030673]
maxi score, test score, baseline:  -0.06304666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.05682720205964422, 0.08901176942644376, 0.13777084059068523, 0.3970792100532148, 0.15152690696970514, 0.16778407090030673]
printing an ep nov before normalisation:  20.839028358459473
maxi score, test score, baseline:  -0.06304666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.05682720205964422, 0.08901176942644376, 0.13777084059068523, 0.3970792100532148, 0.15152690696970514, 0.16778407090030673]
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.647]
 [0.647]
 [0.647]
 [0.728]
 [0.647]
 [0.647]] [[31.93 ]
 [31.93 ]
 [31.93 ]
 [31.93 ]
 [34.889]
 [31.93 ]
 [31.93 ]] [[1.85 ]
 [1.85 ]
 [1.85 ]
 [1.85 ]
 [2.102]
 [1.85 ]
 [1.85 ]]
maxi score, test score, baseline:  -0.06304666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.05682720205964422, 0.08901176942644376, 0.13777084059068523, 0.3970792100532148, 0.15152690696970514, 0.16778407090030673]
actor:  0 policy actor:  0  step number:  37 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.060140000000000096 -0.053333333333333455 -0.053333333333333455
actor:  1 policy actor:  1  step number:  43 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  39.705170777776345
maxi score, test score, baseline:  -0.060140000000000096 -0.053333333333333455 -0.053333333333333455
probs:  [0.056729985626767336, 0.08865195863740694, 0.13753482542726678, 0.39639854385664075, 0.1531880961339563, 0.16749659031796185]
printing an ep nov before normalisation:  43.29484925500534
maxi score, test score, baseline:  -0.060140000000000096 -0.053333333333333455 -0.053333333333333455
probs:  [0.056729985626767336, 0.08865195863740694, 0.13753482542726678, 0.39639854385664075, 0.1531880961339563, 0.16749659031796185]
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]] [[37.472]
 [37.472]
 [37.472]
 [37.472]
 [37.472]
 [37.472]
 [37.472]] [[1.484]
 [1.484]
 [1.484]
 [1.484]
 [1.484]
 [1.484]
 [1.484]]
maxi score, test score, baseline:  -0.060140000000000096 -0.053333333333333455 -0.053333333333333455
probs:  [0.056729985626767336, 0.08865195863740694, 0.13753482542726678, 0.39639854385664075, 0.1531880961339563, 0.16749659031796185]
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.351]
 [0.342]
 [0.342]
 [0.315]
 [0.342]
 [0.342]] [[39.084]
 [42.24 ]
 [39.084]
 [39.084]
 [37.787]
 [39.084]
 [39.084]] [[1.078]
 [1.197]
 [1.078]
 [1.078]
 [1.006]
 [1.078]
 [1.078]]
Printing some Q and Qe and total Qs values:  [[ 0.252]
 [ 0.31 ]
 [-0.023]
 [ 0.259]
 [ 0.229]
 [ 0.133]
 [ 0.263]] [[34.574]
 [38.964]
 [38.523]
 [38.144]
 [40.06 ]
 [35.547]
 [32.701]] [[1.662]
 [2.005]
 [1.643]
 [1.901]
 [1.995]
 [1.606]
 [1.552]]
maxi score, test score, baseline:  -0.060140000000000096 -0.053333333333333455 -0.053333333333333455
probs:  [0.056729985626767336, 0.08865195863740694, 0.13753482542726678, 0.39639854385664075, 0.1531880961339563, 0.16749659031796185]
maxi score, test score, baseline:  -0.060140000000000096 -0.053333333333333455 -0.053333333333333455
probs:  [0.056729985626767336, 0.08865195863740694, 0.13753482542726678, 0.39639854385664075, 0.1531880961339563, 0.16749659031796185]
printing an ep nov before normalisation:  38.679440270032195
printing an ep nov before normalisation:  28.31078976898887
actor:  1 policy actor:  1  step number:  38 total reward:  0.6733333333333336  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  15.034946812721515
printing an ep nov before normalisation:  34.11497591793158
actions average: 
K:  2  action  0 :  tensor([0.4544, 0.0391, 0.1095, 0.0859, 0.1215, 0.1066, 0.0831],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0282, 0.9036, 0.0136, 0.0111, 0.0096, 0.0110, 0.0229],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1232, 0.0960, 0.4498, 0.0512, 0.0437, 0.1295, 0.1065],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1462, 0.1305, 0.1015, 0.2669, 0.0984, 0.1184, 0.1381],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1512, 0.0287, 0.1000, 0.1005, 0.3810, 0.1083, 0.1304],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1601, 0.0203, 0.1924, 0.1286, 0.1224, 0.2383, 0.1378],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1311, 0.2302, 0.0988, 0.0976, 0.1064, 0.1168, 0.2190],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.060140000000000096 -0.053333333333333455 -0.053333333333333455
probs:  [0.05633187124680438, 0.08985578649581943, 0.13656831156057128, 0.39361112414999555, 0.15211147362701924, 0.1715214329197901]
Printing some Q and Qe and total Qs values:  [[-0.022]
 [ 0.081]
 [-0.023]
 [-0.015]
 [-0.017]
 [-0.018]
 [-0.018]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.022]
 [ 0.081]
 [-0.023]
 [-0.015]
 [-0.017]
 [-0.018]
 [-0.018]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
actor:  1 policy actor:  1  step number:  50 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.060140000000000096 -0.053333333333333455 -0.053333333333333455
probs:  [0.05611968215230188, 0.08951702123057823, 0.13605317392043553, 0.3921254705456055, 0.15531032929243066, 0.1708743228586483]
maxi score, test score, baseline:  -0.060140000000000096 -0.053333333333333455 -0.053333333333333455
probs:  [0.05611968215230188, 0.08951702123057823, 0.13605317392043553, 0.3921254705456055, 0.15531032929243066, 0.1708743228586483]
maxi score, test score, baseline:  -0.060140000000000096 -0.053333333333333455 -0.053333333333333455
maxi score, test score, baseline:  -0.060140000000000096 -0.053333333333333455 -0.053333333333333455
probs:  [0.05611968215230188, 0.08951702123057823, 0.13605317392043553, 0.3921254705456055, 0.15531032929243066, 0.1708743228586483]
maxi score, test score, baseline:  -0.060140000000000096 -0.053333333333333455 -0.053333333333333455
probs:  [0.05611968215230188, 0.08951702123057823, 0.13605317392043553, 0.3921254705456055, 0.15531032929243066, 0.1708743228586483]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.060140000000000096 -0.053333333333333455 -0.053333333333333455
probs:  [0.05611968215230188, 0.08951702123057823, 0.13605317392043553, 0.3921254705456055, 0.15531032929243066, 0.1708743228586483]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 39.309795108591814
maxi score, test score, baseline:  -0.060140000000000096 -0.053333333333333455 -0.053333333333333455
probs:  [0.05611968215230188, 0.08951702123057823, 0.13605317392043553, 0.3921254705456055, 0.15531032929243066, 0.1708743228586483]
actor:  1 policy actor:  1  step number:  60 total reward:  0.1799999999999996  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  26.908068656921387
maxi score, test score, baseline:  -0.060140000000000096 -0.053333333333333455 -0.053333333333333455
probs:  [0.05623830389723668, 0.08907813100688013, 0.13580892815513546, 0.3929522856782599, 0.15514662947605443, 0.17077572178643333]
actor:  1 policy actor:  1  step number:  44 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  22.94934604411743
maxi score, test score, baseline:  -0.060140000000000096 -0.053333333333333455 -0.053333333333333455
siam score:  -0.8427701
printing an ep nov before normalisation:  26.09461814774341
maxi score, test score, baseline:  -0.060140000000000096 -0.053333333333333455 -0.053333333333333455
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]] [[22.735]
 [22.735]
 [22.735]
 [22.735]
 [22.735]
 [22.735]
 [22.735]] [[2.445]
 [2.445]
 [2.445]
 [2.445]
 [2.445]
 [2.445]
 [2.445]]
printing an ep nov before normalisation:  36.151536918932536
actor:  1 policy actor:  1  step number:  46 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  25.63463236452407
printing an ep nov before normalisation:  51.13541085923932
siam score:  -0.8444116
maxi score, test score, baseline:  -0.0601400000000001 -0.053333333333333455 -0.053333333333333455
probs:  [0.05607717903839844, 0.08882269538729543, 0.1354192891180167, 0.39182417181336765, 0.15643497548484833, 0.1714216891580733]
printing an ep nov before normalisation:  31.847948684806735
printing an ep nov before normalisation:  29.745189569739683
actor:  1 policy actor:  1  step number:  37 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.0601400000000001 -0.053333333333333455 -0.053333333333333455
probs:  [0.055936289017652435, 0.08859933860670895, 0.13507858283334448, 0.39083773193702237, 0.1560413430058834, 0.1735067145993885]
maxi score, test score, baseline:  -0.0601400000000001 -0.053333333333333455 -0.053333333333333455
probs:  [0.05595392938927589, 0.08862730436747257, 0.13480537588451713, 0.39096124079812394, 0.15609062842138155, 0.17356152113922893]
maxi score, test score, baseline:  -0.0601400000000001 -0.053333333333333455 -0.053333333333333455
probs:  [0.055953929389275896, 0.08862730436747258, 0.1348053758845171, 0.390961240798124, 0.15609062842138152, 0.1735615211392289]
printing an ep nov before normalisation:  34.20059073915137
maxi score, test score, baseline:  -0.0601400000000001 -0.053333333333333455 -0.053333333333333455
maxi score, test score, baseline:  -0.0601400000000001 -0.053333333333333455 -0.053333333333333455
probs:  [0.05593457306140994, 0.08869995294849806, 0.1348463220973258, 0.39082632978511955, 0.1561169617910951, 0.17357586031655156]
printing an ep nov before normalisation:  31.2937331199646
maxi score, test score, baseline:  -0.0601400000000001 -0.053333333333333455 -0.053333333333333455
probs:  [0.05593457306140994, 0.08869995294849806, 0.1348463220973258, 0.39082632978511955, 0.1561169617910951, 0.17357586031655156]
actor:  0 policy actor:  0  step number:  41 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.125]
 [0.225]
 [0.123]
 [0.094]
 [0.123]
 [0.123]
 [0.123]] [[33.564]
 [37.788]
 [29.126]
 [30.355]
 [29.126]
 [29.126]
 [29.126]] [[0.549]
 [0.741]
 [0.45 ]
 [0.448]
 [0.45 ]
 [0.45 ]
 [0.45 ]]
printing an ep nov before normalisation:  23.167975844864493
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  15.199675559997559
actor:  1 policy actor:  1  step number:  57 total reward:  0.1599999999999996  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  40 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  44.320294262567934
maxi score, test score, baseline:  -0.05720666666666677 -0.053333333333333455 -0.053333333333333455
maxi score, test score, baseline:  -0.05720666666666677 -0.053333333333333455 -0.053333333333333455
probs:  [0.05604130716066846, 0.08844824605524612, 0.1345893583436166, 0.39157482961712037, 0.15576196338370782, 0.17358429543964063]
maxi score, test score, baseline:  -0.05720666666666677 -0.053333333333333455 -0.053333333333333455
probs:  [0.05604130716066846, 0.08844824605524612, 0.1345893583436166, 0.39157482961712037, 0.15576196338370782, 0.17358429543964063]
maxi score, test score, baseline:  -0.05720666666666677 -0.053333333333333455 -0.053333333333333455
maxi score, test score, baseline:  -0.05720666666666677 -0.053333333333333455 -0.053333333333333455
probs:  [0.05604130716066846, 0.08844824605524612, 0.1345893583436166, 0.39157482961712037, 0.15576196338370782, 0.17358429543964063]
maxi score, test score, baseline:  -0.05720666666666677 -0.053333333333333455 -0.053333333333333455
probs:  [0.056052847450194225, 0.08826022522346402, 0.13461711216159317, 0.39165562919736824, 0.15579408755370486, 0.17362009841367546]
printing an ep nov before normalisation:  29.27885254715939
maxi score, test score, baseline:  -0.05720666666666677 -0.053333333333333455 -0.053333333333333455
actor:  1 policy actor:  1  step number:  55 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  47.48037246706513
maxi score, test score, baseline:  -0.057206666666666774 -0.053333333333333455 -0.053333333333333455
probs:  [0.056137884694844, 0.088244989855999, 0.13445755190759073, 0.39225186894073427, 0.155568596112935, 0.17333910848789702]
maxi score, test score, baseline:  -0.057206666666666774 -0.053333333333333455 -0.053333333333333455
probs:  [0.056137884694844, 0.088244989855999, 0.13445755190759073, 0.39225186894073427, 0.155568596112935, 0.17333910848789702]
maxi score, test score, baseline:  -0.057206666666666774 -0.053333333333333455 -0.053333333333333455
probs:  [0.056137884694844, 0.088244989855999, 0.13445755190759073, 0.39225186894073427, 0.155568596112935, 0.17333910848789702]
actions average: 
K:  3  action  0 :  tensor([0.4334, 0.1357, 0.0859, 0.0683, 0.1079, 0.0630, 0.1057],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0091, 0.9396, 0.0068, 0.0118, 0.0062, 0.0079, 0.0185],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1625, 0.0258, 0.2669, 0.1172, 0.1467, 0.1417, 0.1393],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1195, 0.0724, 0.0959, 0.2439, 0.1240, 0.1417, 0.2025],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1336, 0.0185, 0.0806, 0.0936, 0.5034, 0.0819, 0.0884],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1557, 0.0168, 0.1044, 0.1092, 0.0987, 0.4074, 0.1078],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1508, 0.1983, 0.0809, 0.1338, 0.0950, 0.1075, 0.2338],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.92203209926173
maxi score, test score, baseline:  -0.057206666666666774 -0.053333333333333455 -0.053333333333333455
probs:  [0.056137884694844, 0.088244989855999, 0.13445755190759073, 0.39225186894073427, 0.155568596112935, 0.17333910848789702]
maxi score, test score, baseline:  -0.057206666666666774 -0.053333333333333455 -0.053333333333333455
probs:  [0.056137884694844, 0.088244989855999, 0.13445755190759073, 0.39225186894073427, 0.155568596112935, 0.17333910848789702]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  39 total reward:  0.52  reward:  1.0 rdn_beta:  1.333
using another actor
printing an ep nov before normalisation:  49.03846428148831
printing an ep nov before normalisation:  41.64022634762759
maxi score, test score, baseline:  -0.057206666666666774 -0.053333333333333455 -0.053333333333333455
probs:  [0.05631093819223619, 0.0882139853986611, 0.13413284171603126, 0.39346523578133064, 0.15510971385942426, 0.1727672850523165]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3333333333333327  reward:  1.0 rdn_beta:  1.333
using another actor
actor:  1 policy actor:  1  step number:  62 total reward:  0.19333333333333258  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.057206666666666774 -0.053333333333333455 -0.053333333333333455
probs:  [0.05639187274629568, 0.0882692892329332, 0.13415125461221797, 0.3940323120613103, 0.15440061208243921, 0.17275465926480357]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.057206666666666774 -0.053333333333333455 -0.053333333333333455
probs:  [0.05639187274629568, 0.0882692892329332, 0.13415125461221797, 0.3940323120613103, 0.15440061208243921, 0.17275465926480357]
printing an ep nov before normalisation:  32.812626513802456
maxi score, test score, baseline:  -0.057206666666666774 -0.053333333333333455 -0.053333333333333455
probs:  [0.05639187274629568, 0.0882692892329332, 0.13415125461221797, 0.3940323120613103, 0.15440061208243921, 0.17275465926480357]
maxi score, test score, baseline:  -0.057206666666666774 -0.053333333333333455 -0.053333333333333455
probs:  [0.05639187274629568, 0.0882692892329332, 0.13415125461221797, 0.3940323120613103, 0.15440061208243921, 0.17275465926480357]
actor:  0 policy actor:  1  step number:  58 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.054793333333333444 -0.053333333333333455 -0.053333333333333455
probs:  [0.05639187274629568, 0.0882692892329332, 0.13415125461221797, 0.3940323120613103, 0.15440061208243921, 0.17275465926480357]
actor:  0 policy actor:  1  step number:  42 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  23.371540310871026
Printing some Q and Qe and total Qs values:  [[0.737]
 [0.829]
 [0.731]
 [0.725]
 [0.722]
 [0.731]
 [0.733]] [[31.492]
 [31.487]
 [33.353]
 [33.247]
 [35.886]
 [36.621]
 [34.411]] [[0.737]
 [0.829]
 [0.731]
 [0.725]
 [0.722]
 [0.731]
 [0.733]]
printing an ep nov before normalisation:  48.314735055825786
maxi score, test score, baseline:  -0.055300000000000106 -0.053333333333333455 -0.053333333333333455
printing an ep nov before normalisation:  36.466082250180065
maxi score, test score, baseline:  -0.055300000000000106 -0.053333333333333455 -0.053333333333333455
probs:  [0.05637263208938394, 0.08834063971688039, 0.13419177315417147, 0.39389819471697823, 0.15442752338220572, 0.17276923694038027]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  53 total reward:  0.3466666666666659  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  46.459846522977166
Printing some Q and Qe and total Qs values:  [[1.024]
 [1.024]
 [1.024]
 [1.024]
 [1.024]
 [1.024]
 [1.024]] [[35.869]
 [35.869]
 [35.869]
 [35.869]
 [35.869]
 [35.869]
 [35.869]] [[1.024]
 [1.024]
 [1.024]
 [1.024]
 [1.024]
 [1.024]
 [1.024]]
printing an ep nov before normalisation:  33.49929938040818
printing an ep nov before normalisation:  37.56955636367181
maxi score, test score, baseline:  -0.05870000000000011 -0.053333333333333455 -0.053333333333333455
maxi score, test score, baseline:  -0.05870000000000011 -0.053333333333333455 -0.053333333333333455
probs:  [0.05635113874313016, 0.08819289933546329, 0.13386295894035338, 0.3937483522079562, 0.15555657693495728, 0.17228807383813977]
maxi score, test score, baseline:  -0.0621000000000001 -0.053333333333333455 -0.053333333333333455
maxi score, test score, baseline:  -0.0621000000000001 -0.053333333333333455 -0.053333333333333455
actor:  1 policy actor:  1  step number:  35 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.667
using another actor
from probs:  [0.05635113874313016, 0.08819289933546329, 0.13386295894035338, 0.3937483522079562, 0.15555657693495728, 0.17228807383813977]
actor:  1 policy actor:  1  step number:  60 total reward:  0.153333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.0655000000000001 -0.053333333333333455 -0.053333333333333455
probs:  [0.05614270409878587, 0.08786640754866594, 0.13336713993570382, 0.39228897697011733, 0.15827600191691335, 0.17205876952981367]
actor:  1 policy actor:  1  step number:  56 total reward:  0.17999999999999938  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.0655000000000001 -0.053333333333333455 -0.053333333333333455
probs:  [0.05617107134948315, 0.0878617188220368, 0.1333150396204341, 0.39248787086015596, 0.15819794666299097, 0.171966352684899]
actor:  0 policy actor:  0  step number:  48 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.06622000000000011 -0.053333333333333455 -0.053333333333333455
probs:  [0.05617107134948315, 0.0878617188220368, 0.1333150396204341, 0.39248787086015596, 0.15819794666299097, 0.171966352684899]
maxi score, test score, baseline:  -0.06622000000000011 -0.053333333333333455 -0.053333333333333455
probs:  [0.05617107134948315, 0.0878617188220368, 0.1333150396204341, 0.39248787086015596, 0.15819794666299097, 0.171966352684899]
maxi score, test score, baseline:  -0.06622000000000011 -0.053333333333333455 -0.053333333333333455
probs:  [0.05617107134948315, 0.0878617188220368, 0.1333150396204341, 0.39248787086015596, 0.15819794666299097, 0.171966352684899]
maxi score, test score, baseline:  -0.06622000000000011 -0.053333333333333455 -0.053333333333333455
probs:  [0.05617107134948315, 0.0878617188220368, 0.1333150396204341, 0.39248787086015596, 0.15819794666299097, 0.171966352684899]
actor:  1 policy actor:  1  step number:  64 total reward:  0.09999999999999953  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  42 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.06622000000000011 -0.053333333333333455 -0.053333333333333455
probs:  [0.056294960312739895, 0.08781952573490313, 0.13303463806117033, 0.3933566276689746, 0.15801085809914356, 0.17148339012306849]
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.649]
 [0.546]
 [0.526]
 [0.53 ]
 [0.547]
 [0.564]] [[12.103]
 [ 9.436]
 [12.78 ]
 [11.993]
 [11.965]
 [11.921]
 [11.913]] [[1.558]
 [1.487]
 [1.681]
 [1.591]
 [1.593]
 [1.606]
 [1.623]]
printing an ep nov before normalisation:  36.72016260316132
printing an ep nov before normalisation:  46.798432208673866
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.492]
 [0.407]
 [0.407]
 [0.407]
 [0.418]
 [0.407]] [[44.942]
 [46.675]
 [44.942]
 [44.942]
 [44.942]
 [49.616]
 [44.942]] [[1.847]
 [2.019]
 [1.847]
 [1.847]
 [1.847]
 [2.092]
 [1.847]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  1.333
using another actor
line 256 mcts: sample exp_bonus 57.93150847124788
using explorer policy with actor:  0
printing an ep nov before normalisation:  56.116830719701085
maxi score, test score, baseline:  -0.06622000000000011 -0.053333333333333455 -0.053333333333333455
probs:  [0.056423844097140144, 0.08782778613601266, 0.13256618638658624, 0.394260115076065, 0.15775054331534333, 0.17117152498885246]
actor:  1 policy actor:  1  step number:  42 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.698]
 [0.723]
 [0.698]
 [0.698]
 [0.683]
 [0.716]
 [0.698]] [[28.554]
 [38.689]
 [28.554]
 [28.554]
 [34.493]
 [37.075]
 [33.407]] [[0.698]
 [0.723]
 [0.698]
 [0.698]
 [0.683]
 [0.716]
 [0.698]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  35.83474760410231
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.26 ]
 [0.125]
 [0.099]
 [0.116]
 [0.125]
 [0.125]] [[34.229]
 [39.458]
 [36.634]
 [37.508]
 [33.598]
 [36.634]
 [36.634]] [[1.144]
 [1.559]
 [1.279]
 [1.299]
 [1.114]
 [1.279]
 [1.279]]
printing an ep nov before normalisation:  47.92398762378346
siam score:  -0.833819
maxi score, test score, baseline:  -0.06622000000000011 -0.053333333333333455 -0.053333333333333455
probs:  [0.05622667056790142, 0.08752061157328306, 0.13210230314511182, 0.39287957613007746, 0.15886468119422287, 0.17240615738940343]
Printing some Q and Qe and total Qs values:  [[0.939]
 [0.946]
 [0.869]
 [0.922]
 [0.862]
 [0.865]
 [0.966]] [[25.526]
 [24.704]
 [26.911]
 [29.269]
 [31.699]
 [28.778]
 [26.9  ]] [[0.939]
 [0.946]
 [0.869]
 [0.922]
 [0.862]
 [0.865]
 [0.966]]
maxi score, test score, baseline:  -0.06622000000000011 -0.053333333333333455 -0.053333333333333455
maxi score, test score, baseline:  -0.06622000000000011 -0.053333333333333455 -0.053333333333333455
probs:  [0.05622667056790142, 0.08752061157328306, 0.13210230314511182, 0.39287957613007746, 0.15886468119422287, 0.17240615738940343]
actor:  0 policy actor:  1  step number:  51 total reward:  0.4266666666666662  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.593505082156927
actor:  1 policy actor:  1  step number:  55 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]] [[28.981]
 [28.981]
 [28.981]
 [28.981]
 [28.981]
 [28.981]
 [28.981]] [[2.344]
 [2.344]
 [2.344]
 [2.344]
 [2.344]
 [2.344]
 [2.344]]
printing an ep nov before normalisation:  26.058055298050782
maxi score, test score, baseline:  -0.06676666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.056130005134545274, 0.08737001751363982, 0.1335967971387729, 0.39220275913545255, 0.15859114040608263, 0.17210928067150674]
actor:  1 policy actor:  1  step number:  53 total reward:  0.1733333333333329  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  54.32458734354049
Printing some Q and Qe and total Qs values:  [[0.154]
 [0.338]
 [0.154]
 [0.154]
 [0.159]
 [0.154]
 [0.154]] [[40.112]
 [46.181]
 [40.112]
 [40.112]
 [39.858]
 [40.112]
 [40.112]] [[1.287]
 [1.76 ]
 [1.287]
 [1.287]
 [1.28 ]
 [1.287]
 [1.287]]
maxi score, test score, baseline:  -0.06676666666666678 -0.053333333333333455 -0.053333333333333455
line 256 mcts: sample exp_bonus 48.7826145857645
printing an ep nov before normalisation:  41.04656014040581
siam score:  -0.8324084
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.06676666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.05610286799350046, 0.08732774084766771, 0.13353211806546653, 0.3920127545225628, 0.1585143485820403, 0.1725101699887622]
actor:  1 policy actor:  1  step number:  67 total reward:  0.026666666666665506  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  47 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.06676666666666678 -0.053333333333333455 -0.053333333333333455
maxi score, test score, baseline:  -0.06676666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.056086713232476194, 0.08731931590183173, 0.13350783291864762, 0.3918997428287961, 0.1584814879880267, 0.1727049071302218]
printing an ep nov before normalisation:  40.52911297888354
maxi score, test score, baseline:  -0.06676666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.056086713232476194, 0.08731931590183173, 0.13350783291864762, 0.3918997428287961, 0.1584814879880267, 0.1727049071302218]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.06676666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.05601242528788524, 0.08720356126762996, 0.13465736694118174, 0.3913796049237341, 0.15827125324490893, 0.17247578833465996]
printing an ep nov before normalisation:  40.33758528286847
printing an ep nov before normalisation:  68.52520296637685
Printing some Q and Qe and total Qs values:  [[-0.047]
 [-0.047]
 [-0.047]
 [-0.047]
 [-0.047]
 [-0.047]
 [-0.047]] [[39.682]
 [39.682]
 [39.682]
 [39.682]
 [39.682]
 [39.682]
 [39.682]] [[0.956]
 [0.956]
 [0.956]
 [0.956]
 [0.956]
 [0.956]
 [0.956]]
printing an ep nov before normalisation:  44.596417297033156
actor:  1 policy actor:  1  step number:  47 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.515]
 [0.518]] [[31.161]
 [31.161]
 [31.161]
 [31.161]
 [31.161]
 [49.909]
 [31.161]] [[1.035]
 [1.035]
 [1.035]
 [1.035]
 [1.035]
 [2.213]
 [1.035]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.06676666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.05581532572013034, 0.08689644295264057, 0.13644674029708156, 0.3899995834838468, 0.15771346183980628, 0.17312844570649435]
maxi score, test score, baseline:  -0.06676666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.05581532572013034, 0.08689644295264057, 0.13644674029708156, 0.3899995834838468, 0.15771346183980628, 0.17312844570649435]
printing an ep nov before normalisation:  23.557918071746826
maxi score, test score, baseline:  -0.06676666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.05581532572013034, 0.08689644295264057, 0.13644674029708156, 0.3899995834838468, 0.15771346183980628, 0.17312844570649435]
actor:  1 policy actor:  1  step number:  56 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.06676666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.05576889739950161, 0.08682409886908068, 0.13633308065367417, 0.38967450880907495, 0.15758206977997052, 0.17381734448869812]
printing an ep nov before normalisation:  50.18133848401549
maxi score, test score, baseline:  -0.06676666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.05576889739950161, 0.08682409886908068, 0.13633308065367417, 0.38967450880907495, 0.15758206977997052, 0.17381734448869812]
actor:  1 policy actor:  1  step number:  48 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.333
from probs:  [0.05576889739950161, 0.08682409886908068, 0.13633308065367417, 0.38967450880907495, 0.15758206977997052, 0.17381734448869812]
printing an ep nov before normalisation:  42.25505273775757
actor:  1 policy actor:  1  step number:  39 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.06676666666666678 -0.053333333333333455 -0.053333333333333455
probs:  [0.05562839835529558, 0.08656734838942225, 0.13589099921727135, 0.38869099793159373, 0.1570604452921319, 0.1761618108142852]
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]] [[43.002]
 [43.002]
 [43.002]
 [43.002]
 [43.002]
 [43.002]
 [43.002]] [[0.931]
 [0.931]
 [0.931]
 [0.931]
 [0.931]
 [0.931]
 [0.931]]
printing an ep nov before normalisation:  30.723502630763928
maxi score, test score, baseline:  -0.0701666666666668 -0.053333333333333455 -0.053333333333333455
probs:  [0.05562839835529558, 0.08656734838942225, 0.13589099921727135, 0.38869099793159373, 0.1570604452921319, 0.1761618108142852]
maxi score, test score, baseline:  -0.0701666666666668 -0.053333333333333455 -0.053333333333333455
probs:  [0.05562839835529558, 0.08656734838942225, 0.13589099921727135, 0.38869099793159373, 0.1570604452921319, 0.1761618108142852]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  0.43333333333333346  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  49 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0701666666666668 -0.053333333333333455 -0.053333333333333455
Printing some Q and Qe and total Qs values:  [[-0.013]
 [ 0.022]
 [-0.034]
 [-0.012]
 [-0.016]
 [-0.016]
 [-0.007]] [[26.187]
 [40.758]
 [22.363]
 [17.456]
 [19.978]
 [19.227]
 [18.744]] [[0.517]
 [1.09 ]
 [0.355]
 [0.197]
 [0.285]
 [0.258]
 [0.25 ]]
printing an ep nov before normalisation:  29.019624711999455
actor:  1 policy actor:  1  step number:  40 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  1.333
from probs:  [0.05544760101805181, 0.08748700552324211, 0.13544871490563243, 0.3874251182457582, 0.15654919318796834, 0.17764236711934694]
printing an ep nov before normalisation:  29.263978004455566
maxi score, test score, baseline:  -0.0735266666666668 -0.053333333333333455 -0.053333333333333455
probs:  [0.05555089409623611, 0.08747176131453974, 0.13525602511101995, 0.38814931023179783, 0.15627843722807508, 0.17729357201833143]
printing an ep nov before normalisation:  7.909574151199195e-06
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]] [[26.485]
 [26.485]
 [26.485]
 [26.485]
 [26.485]
 [26.485]
 [26.485]] [[0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.02021072681517
maxi score, test score, baseline:  -0.0735266666666668 -0.053333333333333455 -0.053333333333333455
probs:  [0.055481870604877366, 0.087362979929928, 0.13508772774752822, 0.38766603069174194, 0.15608395615186796, 0.1783174348740567]
maxi score, test score, baseline:  -0.0735266666666668 -0.053333333333333455 -0.053333333333333455
printing an ep nov before normalisation:  32.522234399223024
Printing some Q and Qe and total Qs values:  [[0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]] [[38.378]
 [38.378]
 [38.378]
 [38.378]
 [38.378]
 [38.378]
 [38.378]] [[0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]]
maxi score, test score, baseline:  -0.0735266666666668 -0.053333333333333455 -0.053333333333333455
probs:  [0.055481870604877366, 0.087362979929928, 0.13508772774752822, 0.38766603069174194, 0.15608395615186796, 0.1783174348740567]
maxi score, test score, baseline:  -0.0735266666666668 -0.053333333333333455 -0.053333333333333455
probs:  [0.055481870604877366, 0.087362979929928, 0.13508772774752822, 0.38766603069174194, 0.15608395615186796, 0.1783174348740567]
Printing some Q and Qe and total Qs values:  [[0.228]
 [0.333]
 [0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]] [[34.634]
 [38.11 ]
 [34.634]
 [34.634]
 [34.634]
 [34.634]
 [34.634]] [[0.973]
 [1.224]
 [0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]]
printing an ep nov before normalisation:  0.1298098446517315
actor:  0 policy actor:  0  step number:  43 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  4  action  0 :  tensor([0.4780, 0.0026, 0.0890, 0.0959, 0.1485, 0.1020, 0.0841],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0217, 0.8996, 0.0176, 0.0142, 0.0126, 0.0121, 0.0222],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1735, 0.0304, 0.1856, 0.1141, 0.1236, 0.2491, 0.1236],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1090, 0.1965, 0.0889, 0.3226, 0.0830, 0.0569, 0.1430],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2078, 0.1618, 0.1266, 0.0993, 0.1793, 0.1150, 0.1102],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1241, 0.0923, 0.1221, 0.1088, 0.1336, 0.2905, 0.1286],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0867, 0.2895, 0.0465, 0.0722, 0.0612, 0.0566, 0.3872],
       grad_fn=<DivBackward0>)
siam score:  -0.8366821
actor:  1 policy actor:  1  step number:  40 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0741666666666668 -0.053333333333333455 -0.053333333333333455
actor:  1 policy actor:  1  step number:  49 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
Printing some Q and Qe and total Qs values:  [[0.677]
 [0.803]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]] [[42.005]
 [44.877]
 [42.005]
 [42.005]
 [42.005]
 [42.005]
 [42.005]] [[0.677]
 [0.803]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]]
maxi score, test score, baseline:  -0.0741666666666668 -0.053333333333333455 -0.053333333333333455
probs:  [0.05544938502539231, 0.08991367586292316, 0.1344787856637186, 0.38743973119036706, 0.15532297233564815, 0.1773954499219507]
printing an ep nov before normalisation:  20.745214344116388
actor:  0 policy actor:  0  step number:  36 total reward:  0.48666666666666647  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 26.826272294356308
printing an ep nov before normalisation:  28.22779153165121
actor:  0 policy actor:  0  step number:  37 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  35.91213977958228
printing an ep nov before normalisation:  40.44223050410244
printing an ep nov before normalisation:  40.29646851962379
maxi score, test score, baseline:  -0.07476666666666679 -0.053333333333333455 -0.053333333333333455
probs:  [0.055349653410773934, 0.08993706443805023, 0.13466137858169916, 0.386740516291315, 0.15558002900018414, 0.17773135827797762]
maxi score, test score, baseline:  -0.07476666666666679 -0.053333333333333455 -0.053333333333333455
probs:  [0.055349653410773934, 0.08993706443805023, 0.13466137858169916, 0.386740516291315, 0.15558002900018414, 0.17773135827797762]
using another actor
printing an ep nov before normalisation:  31.143573380388382
line 256 mcts: sample exp_bonus 29.417424405019315
maxi score, test score, baseline:  -0.07476666666666679 -0.053333333333333455 -0.053333333333333455
probs:  [0.05533211174844322, 0.09000351140414892, 0.13469939033436257, 0.3866182589773512, 0.15560474091134488, 0.1777419866243493]
printing an ep nov before normalisation:  20.521577604405365
actor:  1 policy actor:  1  step number:  54 total reward:  0.32666666666666666  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  68 total reward:  0.21999999999999975  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.07476666666666679 -0.053333333333333455 -0.053333333333333455
probs:  [0.0553681273373526, 0.0899902232755319, 0.13462254339325883, 0.3868707857303845, 0.15554438837772416, 0.17760393188574794]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.07476666666666679 -0.053333333333333455 -0.053333333333333455
probs:  [0.0553681273373526, 0.0899902232755319, 0.13462254339325883, 0.3868707857303845, 0.15554438837772416, 0.17760393188574794]
maxi score, test score, baseline:  -0.07476666666666679 -0.053333333333333455 -0.053333333333333455
maxi score, test score, baseline:  -0.07476666666666679 -0.053333333333333455 -0.053333333333333455
probs:  [0.0553681273373526, 0.0899902232755319, 0.13462254339325883, 0.3868707857303845, 0.15554438837772416, 0.17760393188574794]
maxi score, test score, baseline:  -0.07476666666666679 -0.053333333333333455 -0.053333333333333455
probs:  [0.0553681273373526, 0.0899902232755319, 0.13462254339325883, 0.3868707857303845, 0.15554438837772416, 0.17760393188574794]
maxi score, test score, baseline:  -0.07476666666666679 -0.053333333333333455 -0.053333333333333455
probs:  [0.0553681273373526, 0.0899902232755319, 0.13462254339325883, 0.3868707857303845, 0.15554438837772416, 0.17760393188574794]
UNIT TEST: sample policy line 217 mcts : [0.02  0.163 0.265 0.163 0.122 0.02  0.245]
actor:  0 policy actor:  1  step number:  58 total reward:  0.09999999999999953  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  41 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.07595333333333346 -0.053333333333333455 -0.053333333333333455
maxi score, test score, baseline:  -0.07595333333333346 -0.053333333333333455 -0.053333333333333455
probs:  [0.05527566446944312, 0.0898398032419617, 0.13439740914320958, 0.38622338954694874, 0.15695688653030757, 0.17730684706812946]
actor:  1 policy actor:  1  step number:  53 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  27.695157527923584
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.558]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]] [[26.201]
 [44.568]
 [26.201]
 [26.201]
 [26.201]
 [26.201]
 [26.201]] [[0.724]
 [1.3  ]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]]
Printing some Q and Qe and total Qs values:  [[-0.018]
 [-0.006]
 [-0.01 ]
 [-0.012]
 [-0.01 ]
 [-0.012]
 [-0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.018]
 [-0.006]
 [-0.01 ]
 [-0.012]
 [-0.01 ]
 [-0.012]
 [-0.01 ]]
siam score:  -0.82113194
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.758]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]] [[34.123]
 [35.677]
 [34.123]
 [34.123]
 [34.123]
 [34.123]
 [34.123]] [[1.57 ]
 [1.674]
 [1.57 ]
 [1.57 ]
 [1.57 ]
 [1.57 ]
 [1.57 ]]
printing an ep nov before normalisation:  37.715914782063216
actor:  1 policy actor:  1  step number:  52 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  24.87356185913086
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.729]
 [0.617]
 [0.617]
 [0.617]
 [0.617]
 [0.617]] [[46.186]
 [38.365]
 [37.253]
 [37.253]
 [37.253]
 [37.253]
 [37.253]] [[1.351]
 [1.33 ]
 [1.19 ]
 [1.19 ]
 [1.19 ]
 [1.19 ]
 [1.19 ]]
Printing some Q and Qe and total Qs values:  [[0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]] [[32.66]
 [32.66]
 [32.66]
 [32.66]
 [32.66]
 [32.66]
 [32.66]] [[0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]]
Printing some Q and Qe and total Qs values:  [[-0.092]
 [-0.093]
 [-0.058]
 [-0.058]
 [-0.097]
 [-0.058]
 [-0.058]] [[26.806]
 [18.612]
 [27.257]
 [27.257]
 [20.778]
 [27.257]
 [27.257]] [[0.868]
 [0.574]
 [0.918]
 [0.918]
 [0.647]
 [0.918]
 [0.918]]
UNIT TEST: sample policy line 217 mcts : [0.204 0.204 0.02  0.082 0.449 0.02  0.02 ]
maxi score, test score, baseline:  -0.07595333333333346 -0.053333333333333455 -0.053333333333333455
probs:  [0.05504485463780958, 0.09074349637335812, 0.136738597551814, 0.38460733104176253, 0.15630046936763092, 0.17656525102762474]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]] [[44.684]
 [44.684]
 [44.684]
 [44.684]
 [44.684]
 [44.684]
 [44.684]] [[1.018]
 [1.018]
 [1.018]
 [1.018]
 [1.018]
 [1.018]
 [1.018]]
actor:  1 policy actor:  1  step number:  76 total reward:  0.1799999999999985  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.07595333333333346 -0.053333333333333455 -0.053333333333333455
probs:  [0.0550730548807085, 0.09073615119138001, 0.13668545467742613, 0.3848050382365943, 0.15622784858004304, 0.17647245243384801]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
actor:  1 policy actor:  1  step number:  53 total reward:  0.2133333333333327  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.159]
 [0.242]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]] [[42.924]
 [35.574]
 [42.924]
 [42.924]
 [42.924]
 [42.924]
 [42.924]] [[0.701]
 [0.632]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]]
maxi score, test score, baseline:  -0.07595333333333346 -0.053333333333333455 -0.053333333333333455
probs:  [0.055084801818442136, 0.09073309152286409, 0.13666331777461083, 0.38488739405585104, 0.15619759806507333, 0.1764337967631585]
Printing some Q and Qe and total Qs values:  [[0.091]
 [0.091]
 [0.091]
 [0.091]
 [0.091]
 [0.091]
 [0.091]] [[54.066]
 [54.066]
 [54.066]
 [54.066]
 [54.066]
 [54.066]
 [54.066]] [[0.711]
 [0.711]
 [0.711]
 [0.711]
 [0.711]
 [0.711]
 [0.711]]
printing an ep nov before normalisation:  52.2040353338939
maxi score, test score, baseline:  -0.07595333333333346 -0.053333333333333455 -0.053333333333333455
probs:  [0.055084801818442136, 0.09073309152286409, 0.13666331777461083, 0.38488739405585104, 0.15619759806507333, 0.1764337967631585]
actor:  1 policy actor:  1  step number:  48 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.07595333333333346 -0.053333333333333455 -0.053333333333333455
probs:  [0.054904249620687684, 0.09371844483727013, 0.13621472630177453, 0.3836232226391478, 0.15568482358685956, 0.17585453301426027]
maxi score, test score, baseline:  -0.07595333333333346 -0.053333333333333455 -0.053333333333333455
probs:  [0.054904249620687684, 0.09371844483727013, 0.13621472630177453, 0.3836232226391478, 0.15568482358685956, 0.17585453301426027]
maxi score, test score, baseline:  -0.07595333333333346 -0.053333333333333455 -0.053333333333333455
probs:  [0.054904249620687684, 0.09371844483727013, 0.13621472630177453, 0.3836232226391478, 0.15568482358685956, 0.17585453301426027]
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.375]] [[27.08]
 [27.08]
 [27.08]
 [27.08]
 [27.08]
 [27.08]
 [27.08]] [[18.437]
 [18.437]
 [18.437]
 [18.437]
 [18.437]
 [18.437]
 [18.437]]
maxi score, test score, baseline:  -0.07595333333333346 -0.053333333333333455 -0.053333333333333455
probs:  [0.054904249620687684, 0.09371844483727013, 0.13621472630177453, 0.3836232226391478, 0.15568482358685956, 0.17585453301426027]
printing an ep nov before normalisation:  22.243266371828696
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  63 total reward:  0.05333333333333279  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  59 total reward:  0.03999999999999948  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  58.281579706270776
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.07595333333333346 -0.053333333333333455 -0.053333333333333455
probs:  [0.05491225903862953, 0.09365906016801878, 0.13621501535805736, 0.383679343157486, 0.15568326397371235, 0.17585105830409614]
actor:  1 policy actor:  1  step number:  49 total reward:  0.2666666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.07595333333333346 -0.053333333333333455 -0.053333333333333455
probs:  [0.05496524580150859, 0.09363989401342164, 0.13611660301452466, 0.38405082149323055, 0.1555485985487684, 0.17567883712854618]
maxi score, test score, baseline:  -0.07931333333333347 -0.053333333333333455 -0.053333333333333455
probs:  [0.05496524580150859, 0.09363989401342164, 0.13611660301452466, 0.38405082149323055, 0.1555485985487684, 0.17567883712854618]
Printing some Q and Qe and total Qs values:  [[0.062]
 [0.192]
 [0.054]
 [0.054]
 [0.054]
 [0.054]
 [0.054]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.062]
 [0.192]
 [0.054]
 [0.054]
 [0.054]
 [0.054]
 [0.054]]
Printing some Q and Qe and total Qs values:  [[0.189]
 [0.237]
 [0.189]
 [0.169]
 [0.189]
 [0.189]
 [0.213]] [[32.864]
 [42.411]
 [32.864]
 [32.403]
 [32.864]
 [32.864]
 [32.537]] [[0.913]
 [1.37 ]
 [0.913]
 [0.874]
 [0.913]
 [0.913]
 [0.923]]
printing an ep nov before normalisation:  31.123251776545906
printing an ep nov before normalisation:  29.599838256835938
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]] [[31.218]
 [31.218]
 [31.218]
 [31.218]
 [31.218]
 [31.218]
 [31.218]] [[1.441]
 [1.441]
 [1.441]
 [1.441]
 [1.441]
 [1.441]
 [1.441]]
maxi score, test score, baseline:  -0.07931333333333347 -0.053333333333333455 -0.053333333333333455
probs:  [0.05496524580150859, 0.09363989401342164, 0.13611660301452466, 0.38405082149323055, 0.1555485985487684, 0.17567883712854618]
maxi score, test score, baseline:  -0.07931333333333347 -0.053333333333333455 -0.053333333333333455
probs:  [0.05496524580150859, 0.09363989401342164, 0.13611660301452466, 0.38405082149323055, 0.1555485985487684, 0.17567883712854618]
actor:  1 policy actor:  1  step number:  55 total reward:  0.2666666666666666  reward:  1.0 rdn_beta:  1.333
using another actor
maxi score, test score, baseline:  -0.07931333333333347 -0.053333333333333455 -0.053333333333333455
probs:  [0.05501618302850356, 0.09362146920854045, 0.13602199727510497, 0.38440793099159193, 0.15541914200252874, 0.17551327749373036]
printing an ep nov before normalisation:  47.43678608735065
maxi score, test score, baseline:  -0.07931333333333347 -0.053333333333333455 -0.053333333333333455
probs:  [0.05501618302850356, 0.09362146920854045, 0.13602199727510497, 0.38440793099159193, 0.15541914200252874, 0.17551327749373036]
actor:  1 policy actor:  1  step number:  68 total reward:  0.21999999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.07931333333333347 -0.053333333333333455 -0.053333333333333455
probs:  [0.05498370293550709, 0.093566142254326, 0.13653291089367256, 0.38418051470470327, 0.15532724278079288, 0.17540948643099838]
printing an ep nov before normalisation:  33.93816379093752
printing an ep nov before normalisation:  35.65340518951416
printing an ep nov before normalisation:  41.86434274968209
maxi score, test score, baseline:  -0.07931333333333347 -0.053333333333333455 -0.053333333333333455
printing an ep nov before normalisation:  39.22181588343787
actor:  1 policy actor:  1  step number:  49 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  40.97082987216702
maxi score, test score, baseline:  -0.07931333333333347 -0.053333333333333455 -0.053333333333333455
probs:  [0.05487336090954884, 0.09337818441138558, 0.1382685969266073, 0.3834079315155501, 0.15501504089419652, 0.1750568853427117]
printing an ep nov before normalisation:  41.06364727020264
printing an ep nov before normalisation:  35.404896048547236
actor:  0 policy actor:  1  step number:  68 total reward:  0.27333333333333243  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.08015333333333345 -0.053333333333333455 -0.053333333333333455
probs:  [0.05487336090954884, 0.09337818441138558, 0.1382685969266073, 0.3834079315155501, 0.15501504089419652, 0.1750568853427117]
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.692]
 [0.632]
 [0.672]
 [0.566]
 [0.567]
 [0.714]] [[28.656]
 [29.428]
 [30.627]
 [35.02 ]
 [35.654]
 [32.34 ]
 [32.614]] [[1.94 ]
 [2.015]
 [2.058]
 [2.473]
 [2.421]
 [2.139]
 [2.309]]
maxi score, test score, baseline:  -0.08015333333333345 -0.053333333333333455 -0.053333333333333455
printing an ep nov before normalisation:  42.476715067836864
maxi score, test score, baseline:  -0.08015333333333345 -0.053333333333333455 -0.053333333333333455
probs:  [0.05487336090954884, 0.09337818441138558, 0.1382685969266073, 0.3834079315155501, 0.15501504089419652, 0.1750568853427117]
actions average: 
K:  0  action  0 :  tensor([0.3521, 0.0071, 0.1293, 0.1094, 0.1375, 0.1329, 0.1317],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0071, 0.9375, 0.0060, 0.0058, 0.0038, 0.0039, 0.0359],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1309, 0.0035, 0.4854, 0.0793, 0.0955, 0.1074, 0.0980],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1477, 0.0914, 0.1089, 0.2174, 0.1497, 0.1324, 0.1525],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2160, 0.0023, 0.1423, 0.1525, 0.1848, 0.1560, 0.1461],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0995, 0.0025, 0.1755, 0.0573, 0.0700, 0.5129, 0.0822],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1298, 0.0899, 0.0890, 0.1112, 0.1144, 0.1121, 0.3536],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  61 total reward:  0.1599999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]] [[31.58]
 [31.58]
 [31.58]
 [31.58]
 [31.58]
 [31.58]
 [31.58]] [[0.864]
 [0.864]
 [0.864]
 [0.864]
 [0.864]
 [0.864]
 [0.864]]
maxi score, test score, baseline:  -0.08015333333333345 -0.053333333333333455 -0.053333333333333455
probs:  [0.05485080840380311, 0.09333976822049135, 0.1382116862379966, 0.3832500253446136, 0.1549512308020748, 0.17539648099102073]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.02756688113365
printing an ep nov before normalisation:  30.148083118518592
printing an ep nov before normalisation:  25.615295591777354
actor:  1 policy actor:  1  step number:  48 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  14.68022618886535
actor:  1 policy actor:  1  step number:  72 total reward:  0.04666666666666608  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  25.84662437438965
printing an ep nov before normalisation:  29.550277064182747
printing an ep nov before normalisation:  34.46921460039147
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.038]
 [0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.017]
 [0.038]
 [0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]]
Printing some Q and Qe and total Qs values:  [[ 0.089]
 [-0.02 ]
 [-0.029]
 [-0.004]
 [ 0.027]
 [-0.015]
 [-0.013]] [[32.967]
 [39.732]
 [35.132]
 [31.764]
 [32.178]
 [33.006]
 [35.936]] [[0.417]
 [0.455]
 [0.346]
 [0.298]
 [0.338]
 [0.314]
 [0.379]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  37.58061408996582
maxi score, test score, baseline:  -0.08015333333333345 -0.053333333333333455 -0.053333333333333455
probs:  [0.05473424534847238, 0.09437646762163558, 0.13816263037244947, 0.3824333876622102, 0.15491573093115174, 0.17537753806408066]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.08015333333333345 -0.053333333333333455 -0.053333333333333455
probs:  [0.054665384988968335, 0.09425761226666379, 0.13798855386985398, 0.3819512478194857, 0.15598067326856088, 0.17515652778646734]
printing an ep nov before normalisation:  40.91881072920636
maxi score, test score, baseline:  -0.08015333333333345 -0.053333333333333455 -0.053333333333333455
probs:  [0.054665384988968335, 0.09425761226666379, 0.13798855386985398, 0.3819512478194857, 0.15598067326856088, 0.17515652778646734]
actor:  1 policy actor:  1  step number:  63 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.08015333333333345 -0.053333333333333455 -0.053333333333333455
probs:  [0.05462676352173855, 0.09489861702446853, 0.13789092019514052, 0.3816808317445468, 0.1558702969833912, 0.17503257053071428]
actor:  0 policy actor:  1  step number:  50 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.08080666666666679 -0.053333333333333455 -0.053333333333333455
probs:  [0.05462676352173855, 0.09489861702446853, 0.13789092019514052, 0.3816808317445468, 0.1558702969833912, 0.17503257053071428]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.08080666666666679 -0.053333333333333455 -0.053333333333333455
probs:  [0.05462676352173855, 0.09489861702446853, 0.13789092019514052, 0.3816808317445468, 0.1558702969833912, 0.17503257053071428]
printing an ep nov before normalisation:  30.895387014301885
printing an ep nov before normalisation:  30.31591839260525
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.413]
 [0.361]
 [0.38 ]
 [0.332]
 [0.401]
 [0.437]] [[33.868]
 [36.707]
 [32.718]
 [32.242]
 [31.293]
 [36.272]
 [30.91 ]] [[0.929]
 [1.139]
 [0.926]
 [0.926]
 [0.839]
 [1.11 ]
 [0.929]]
printing an ep nov before normalisation:  45.143741892634985
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.281]
 [0.281]
 [0.281]
 [0.396]
 [0.281]
 [0.281]] [[36.965]
 [33.325]
 [33.325]
 [33.325]
 [40.36 ]
 [33.325]
 [33.325]] [[1.365]
 [1.07 ]
 [1.07 ]
 [1.07 ]
 [1.51 ]
 [1.07 ]
 [1.07 ]]
siam score:  -0.82048774
printing an ep nov before normalisation:  33.63589722427318
printing an ep nov before normalisation:  26.494426727294922
actor:  1 policy actor:  1  step number:  53 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  32.4209217875387
actor:  1 policy actor:  1  step number:  42 total reward:  0.5  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  48.51583369388276
printing an ep nov before normalisation:  49.82051153904878
printing an ep nov before normalisation:  48.104662027724316
actor:  0 policy actor:  0  step number:  47 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.08150000000000013 -0.053333333333333455 -0.053333333333333455
siam score:  -0.8256866
maxi score, test score, baseline:  -0.08150000000000013 -0.053333333333333455 -0.053333333333333455
maxi score, test score, baseline:  -0.08150000000000013 -0.053333333333333455 -0.053333333333333455
probs:  [0.054541873917218466, 0.09471690069614162, 0.13749613710909295, 0.38108752645064387, 0.15770384572593674, 0.1744537161009664]
printing an ep nov before normalisation:  51.51216154013061
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]] [[27.902]
 [27.902]
 [27.902]
 [27.902]
 [27.902]
 [27.902]
 [27.902]] [[0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]]
line 256 mcts: sample exp_bonus 48.159062117165554
printing an ep nov before normalisation:  42.57351652831749
maxi score, test score, baseline:  -0.08150000000000013 -0.053333333333333455 -0.053333333333333455
probs:  [0.054541873917218466, 0.09471690069614162, 0.13749613710909295, 0.38108752645064387, 0.15770384572593674, 0.1744537161009664]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3399999999999993  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.08150000000000013 -0.053333333333333455 -0.053333333333333455
probs:  [0.054445457527391855, 0.09631995290452879, 0.13725271925912363, 0.38041244555487574, 0.15742461823148965, 0.17414480652259032]
printing an ep nov before normalisation:  43.58570354877951
printing an ep nov before normalisation:  44.00684620188925
printing an ep nov before normalisation:  42.90345304402042
siam score:  -0.8296002
printing an ep nov before normalisation:  42.23987579345703
maxi score, test score, baseline:  -0.08150000000000013 -0.053333333333333455 -0.053333333333333455
probs:  [0.054445457527391855, 0.09631995290452879, 0.13725271925912363, 0.38041244555487574, 0.15742461823148965, 0.17414480652259032]
actor:  1 policy actor:  1  step number:  50 total reward:  0.4333333333333329  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  57 total reward:  0.14666666666666617  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.08486000000000012 -0.053333333333333455 -0.053333333333333455
probs:  [0.054588509653260675, 0.09625588235046163, 0.1369861840738245, 0.3814153235852721, 0.15705830732980458, 0.1736957930073765]
maxi score, test score, baseline:  -0.08486000000000012 -0.053333333333333455 -0.053333333333333455
probs:  [0.054588509653260675, 0.09625588235046163, 0.1369861840738245, 0.3814153235852721, 0.15705830732980458, 0.1736957930073765]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.08486000000000012 -0.053333333333333455 -0.053333333333333455
probs:  [0.054588509653260675, 0.09625588235046163, 0.1369861840738245, 0.3814153235852721, 0.15705830732980458, 0.1736957930073765]
actor:  1 policy actor:  1  step number:  59 total reward:  0.3466666666666661  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.08486000000000012 -0.053333333333333455 -0.053333333333333455
probs:  [0.05451075519097398, 0.09611863275774211, 0.13679077735583559, 0.38087090653628636, 0.15683424045126615, 0.1748746877078957]
maxi score, test score, baseline:  -0.08486000000000012 -0.053333333333333455 -0.053333333333333455
probs:  [0.05451075519097398, 0.09611863275774211, 0.13679077735583559, 0.38087090653628636, 0.15683424045126615, 0.1748746877078957]
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.19059281632697
maxi score, test score, baseline:  -0.08486000000000012 -0.053333333333333455 -0.053333333333333455
probs:  [0.05451075519097398, 0.09611863275774211, 0.13679077735583559, 0.38087090653628636, 0.15683424045126615, 0.1748746877078957]
printing an ep nov before normalisation:  39.642479750850704
actor:  1 policy actor:  1  step number:  54 total reward:  0.21999999999999975  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.08486000000000012 -0.053333333333333455 -0.053333333333333455
actor:  1 policy actor:  1  step number:  70 total reward:  0.09999999999999909  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 39.39840435735702
maxi score, test score, baseline:  -0.08486000000000012 -0.053333333333333455 -0.053333333333333455
probs:  [0.0544631324894583, 0.09603457074773002, 0.13667109553268006, 0.3805374644435574, 0.1573626193796333, 0.1749311174069409]
maxi score, test score, baseline:  -0.08486000000000012 -0.053333333333333455 -0.053333333333333455
probs:  [0.0544631324894583, 0.09603457074773002, 0.13667109553268006, 0.3805374644435574, 0.1573626193796333, 0.1749311174069409]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.64658124343863
printing an ep nov before normalisation:  0.3183710291477837
actor:  1 policy actor:  1  step number:  76 total reward:  0.01999999999999913  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  46 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.45868456626872
maxi score, test score, baseline:  -0.08486000000000012 -0.053333333333333455 -0.053333333333333455
probs:  [0.05455472699328846, 0.09601005161665786, 0.13650371212192003, 0.3811797930030582, 0.15712249156797567, 0.17462922469709988]
Printing some Q and Qe and total Qs values:  [[-0.009]
 [ 0.055]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]] [[33.074]
 [41.848]
 [33.074]
 [33.074]
 [33.074]
 [33.074]
 [33.074]] [[0.162]
 [0.314]
 [0.162]
 [0.162]
 [0.162]
 [0.162]
 [0.162]]
Printing some Q and Qe and total Qs values:  [[ 0.188]
 [ 0.392]
 [-0.092]
 [ 0.082]
 [ 0.178]
 [-0.09 ]
 [ 0.188]] [[33.563]
 [35.707]
 [34.197]
 [35.815]
 [37.774]
 [35.894]
 [37.573]] [[0.878]
 [1.177]
 [0.626]
 [0.872]
 [1.055]
 [0.703]
 [1.056]]
printing an ep nov before normalisation:  30.07378196816991
maxi score, test score, baseline:  -0.08486000000000012 -0.053333333333333455 -0.053333333333333455
probs:  [0.05456643243040021, 0.09581575708128626, 0.13653304384069506, 0.3812617517252198, 0.1571562581484333, 0.17466675677396543]
printing an ep nov before normalisation:  48.552466327030494
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.607]
 [0.569]
 [0.577]
 [0.599]
 [0.515]
 [0.515]] [[32.325]
 [31.698]
 [31.977]
 [32.31 ]
 [32.234]
 [33.764]
 [34.176]] [[1.729]
 [1.7  ]
 [1.682]
 [1.714]
 [1.73 ]
 [1.755]
 [1.784]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  45 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.99148138555073
Printing some Q and Qe and total Qs values:  [[0.768]
 [0.819]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]] [[42.22]
 [51.2 ]
 [42.22]
 [42.22]
 [42.22]
 [42.22]
 [42.22]] [[0.768]
 [0.819]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]]
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([0.5447, 0.0227, 0.0855, 0.0847, 0.0901, 0.0876, 0.0846],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0049, 0.9567, 0.0037, 0.0101, 0.0051, 0.0036, 0.0159],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1313, 0.0053, 0.3872, 0.0894, 0.0922, 0.1962, 0.0985],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1090, 0.1393, 0.0885, 0.3519, 0.0956, 0.0879, 0.1278],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1415, 0.1389, 0.0552, 0.1480, 0.3238, 0.0738, 0.1189],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1988, 0.0357, 0.1391, 0.1547, 0.1618, 0.1533, 0.1567],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1417, 0.2180, 0.1307, 0.1138, 0.1076, 0.1186, 0.1695],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.08486000000000012 -0.053333333333333455 -0.053333333333333455
probs:  [0.05512478556732073, 0.09428720527418483, 0.1353776702040829, 0.3851595579523969, 0.15618989841715436, 0.17386088258486032]
printing an ep nov before normalisation:  40.85774069110332
maxi score, test score, baseline:  -0.08822000000000013 -0.053333333333333455 -0.053333333333333455
probs:  [0.05500461756633362, 0.09433367177938883, 0.13559897496963974, 0.38431704545867396, 0.15649975836895813, 0.1742459318570058]
maxi score, test score, baseline:  -0.08822000000000013 -0.053333333333333455 -0.053333333333333455
probs:  [0.0549868356858447, 0.0943990122121099, 0.1356373833057119, 0.3841931261588179, 0.15652452565855712, 0.17425911697895843]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  48.36655928071696
maxi score, test score, baseline:  -0.09158000000000012 -0.053333333333333455 -0.053333333333333455
maxi score, test score, baseline:  -0.09158000000000012 -0.053333333333333455 -0.053333333333333455
probs:  [0.054969181396278675, 0.09446388380543475, 0.13567551604916336, 0.3840700960212662, 0.1565491152345932, 0.17427220749326372]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.09158000000000012 -0.053333333333333455 -0.053333333333333455
probs:  [0.054969181396278675, 0.09446388380543475, 0.13567551604916336, 0.3840700960212662, 0.1565491152345932, 0.17427220749326372]
printing an ep nov before normalisation:  43.764137599212994
siam score:  -0.82171893
actor:  1 policy actor:  1  step number:  52 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.09158000000000012 -0.053333333333333455 -0.053333333333333455
probs:  [0.05489805045437033, 0.09434152196217759, 0.1354996961738889, 0.3835720689271526, 0.15634621898614884, 0.1753424434962618]
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.76114348768621
maxi score, test score, baseline:  -0.09158000000000012 -0.053333333333333455 -0.053333333333333455
probs:  [0.05489805045437033, 0.09434152196217759, 0.1354996961738889, 0.3835720689271526, 0.15634621898614884, 0.1753424434962618]
maxi score, test score, baseline:  -0.09158000000000012 -0.053333333333333455 -0.053333333333333455
probs:  [0.05489805045437033, 0.09434152196217759, 0.1354996961738889, 0.3835720689271526, 0.15634621898614884, 0.1753424434962618]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.773]
 [0.719]
 [0.754]
 [0.719]
 [0.668]
 [0.733]] [[35.583]
 [36.368]
 [35.583]
 [37.691]
 [35.583]
 [35.025]
 [33.606]] [[0.719]
 [0.773]
 [0.719]
 [0.754]
 [0.719]
 [0.668]
 [0.733]]
maxi score, test score, baseline:  -0.09158000000000012 -0.053333333333333455 -0.053333333333333455
probs:  [0.05507293650436936, 0.09427410202483025, 0.13517943662434445, 0.384798202945604, 0.15589789674081042, 0.17477742516004138]
actor:  0 policy actor:  0  step number:  46 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  21.905190642335416
Printing some Q and Qe and total Qs values:  [[0.146]
 [0.255]
 [0.146]
 [0.146]
 [0.146]
 [0.146]
 [0.146]] [[31.931]
 [36.989]
 [31.931]
 [31.931]
 [31.931]
 [31.931]
 [31.931]] [[1.246]
 [1.689]
 [1.246]
 [1.246]
 [1.246]
 [1.246]
 [1.246]]
maxi score, test score, baseline:  -0.09207333333333345 -0.053333333333333455 -0.053333333333333455
probs:  [0.05507293650436936, 0.09427410202483025, 0.13517943662434445, 0.384798202945604, 0.15589789674081042, 0.17477742516004138]
actor:  0 policy actor:  0  step number:  49 total reward:  0.3333333333333328  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0894066666666668 -0.053333333333333455 -0.053333333333333455
probs:  [0.05507293650436936, 0.09427410202483025, 0.13517943662434445, 0.384798202945604, 0.15589789674081042, 0.17477742516004138]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.0894066666666668 -0.053333333333333455 -0.053333333333333455
probs:  [0.05507293650436936, 0.09427410202483025, 0.13517943662434445, 0.384798202945604, 0.15589789674081042, 0.17477742516004138]
maxi score, test score, baseline:  -0.0894066666666668 -0.053333333333333455 -0.053333333333333455
actor:  0 policy actor:  0  step number:  62 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.08715333333333347 -0.053333333333333455 -0.053333333333333455
probs:  [0.05507293650436936, 0.09427410202483025, 0.13517943662434445, 0.384798202945604, 0.15589789674081042, 0.17477742516004138]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3333333333333328  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  27.596580811889247
maxi score, test score, baseline:  -0.08715333333333347 -0.053333333333333455 -0.053333333333333455
probs:  [0.05499969954302948, 0.09414860824858114, 0.13499941430358428, 0.3842854282309715, 0.15569025583470628, 0.17587659383912727]
Starting evaluation
maxi score, test score, baseline:  -0.08715333333333347 -0.053333333333333455 -0.053333333333333455
probs:  [0.05499969954302948, 0.09414860824858114, 0.13499941430358428, 0.3842854282309715, 0.15569025583470628, 0.17587659383912727]
using explorer policy with actor:  0
printing an ep nov before normalisation:  30.762368223604543
printing an ep nov before normalisation:  33.39765412466867
printing an ep nov before normalisation:  42.371945994786266
maxi score, test score, baseline:  -0.08715333333333347 -0.053333333333333455 -0.053333333333333455
probs:  [0.05499969954302949, 0.09414860824858115, 0.1349994143035843, 0.38428542823097156, 0.15569025583470628, 0.17587659383912727]
printing an ep nov before normalisation:  30.381473838984263
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.49 ]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]] [[39.504]
 [38.53 ]
 [39.504]
 [39.504]
 [39.504]
 [39.504]
 [39.504]] [[2.419]
 [2.413]
 [2.419]
 [2.419]
 [2.419]
 [2.419]
 [2.419]]
printing an ep nov before normalisation:  40.89774203986036
maxi score, test score, baseline:  -0.08715333333333347 -0.053333333333333455 -0.053333333333333455
printing an ep nov before normalisation:  36.76707805544846
printing an ep nov before normalisation:  23.827393054962158
maxi score, test score, baseline:  -0.08715333333333347 -0.053333333333333455 -0.053333333333333455
probs:  [0.05499969954302949, 0.09414860824858115, 0.1349994143035843, 0.38428542823097156, 0.15569025583470628, 0.17587659383912727]
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.541]
 [0.611]
 [0.611]
 [0.416]
 [0.425]
 [0.565]] [[32.93 ]
 [30.563]
 [36.536]
 [36.536]
 [36.207]
 [34.986]
 [29.263]] [[1.939]
 [1.93 ]
 [2.483]
 [2.483]
 [2.262]
 [2.172]
 [1.848]]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]] [[31.835]
 [31.835]
 [31.835]
 [31.835]
 [31.835]
 [31.835]
 [31.835]] [[0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]]
printing an ep nov before normalisation:  34.144630432128906
printing an ep nov before normalisation:  37.90905191018684
line 256 mcts: sample exp_bonus 32.8953431349906
actor:  1 policy actor:  1  step number:  52 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.808]
 [0.84 ]
 [0.808]
 [0.808]
 [0.808]
 [0.754]
 [0.808]] [[34.978]
 [41.477]
 [34.978]
 [34.978]
 [34.978]
 [30.506]
 [34.978]] [[0.808]
 [0.84 ]
 [0.808]
 [0.808]
 [0.808]
 [0.754]
 [0.808]]
Printing some Q and Qe and total Qs values:  [[0.88 ]
 [0.854]
 [0.88 ]
 [0.805]
 [0.786]
 [0.796]
 [0.845]] [[42.191]
 [42.64 ]
 [42.191]
 [40.399]
 [39.364]
 [38.316]
 [37.075]] [[0.88 ]
 [0.854]
 [0.88 ]
 [0.805]
 [0.786]
 [0.796]
 [0.845]]
printing an ep nov before normalisation:  28.102211952209473
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.915]
 [0.928]
 [0.956]
 [0.956]
 [0.956]
 [0.956]
 [0.956]] [[46.306]
 [34.471]
 [41.733]
 [41.733]
 [41.733]
 [41.733]
 [41.733]] [[0.915]
 [0.928]
 [0.956]
 [0.956]
 [0.956]
 [0.956]
 [0.956]]
printing an ep nov before normalisation:  35.23147180498303
using explorer policy with actor:  0
printing an ep nov before normalisation:  29.058423042297363
actor:  0 policy actor:  1  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  26 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  27.9036118828956
actor:  0 policy actor:  1  step number:  27 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  27 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  27 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  27 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  27 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  28 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  28 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  28 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.496]
 [0.541]
 [0.426]
 [0.541]
 [0.399]
 [0.541]] [[40.02 ]
 [39.064]
 [39.442]
 [37.983]
 [39.442]
 [38.455]
 [39.442]] [[1.501]
 [1.488]
 [1.549]
 [1.374]
 [1.549]
 [1.366]
 [1.549]]
maxi score, test score, baseline:  -0.03730000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8105502
printing an ep nov before normalisation:  20.106771050454718
maxi score, test score, baseline:  -0.03730000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  38 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.03730000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  37.550644874572754
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.03730000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  38.156289075081794
actor:  1 policy actor:  1  step number:  39 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.91453853560626
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.03730000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.03730000000000011 0.6643333333333334 0.6643333333333334
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.03730000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  51.1825063717019
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  48.1646211169345
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]]
printing an ep nov before normalisation:  46.94998146221036
maxi score, test score, baseline:  -0.03730000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.923]
 [0.815]
 [0.815]
 [0.815]
 [0.698]
 [0.815]] [[51.323]
 [51.255]
 [51.323]
 [51.323]
 [51.323]
 [53.522]
 [51.323]] [[1.964]
 [2.07 ]
 [1.964]
 [1.964]
 [1.964]
 [1.932]
 [1.964]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.812]
 [0.437]
 [0.437]
 [0.437]
 [0.716]
 [0.437]
 [0.437]] [[52.643]
 [54.242]
 [54.242]
 [54.242]
 [56.052]
 [54.242]
 [54.242]] [[1.876]
 [1.545]
 [1.545]
 [1.545]
 [1.874]
 [1.545]
 [1.545]]
printing an ep nov before normalisation:  36.83331694985705
maxi score, test score, baseline:  -0.03730000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.03730000000000011 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.03730000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actions average: 
K:  1  action  0 :  tensor([0.6065, 0.0013, 0.0714, 0.0846, 0.0865, 0.0718, 0.0778],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0105, 0.9318, 0.0088, 0.0160, 0.0039, 0.0044, 0.0245],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1806, 0.0059, 0.2716, 0.1496, 0.1380, 0.1425, 0.1117],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1815, 0.0861, 0.1163, 0.2138, 0.1250, 0.1619, 0.1155],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1754, 0.0017, 0.1015, 0.1361, 0.3318, 0.1272, 0.1263],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1105, 0.0195, 0.1337, 0.0866, 0.0865, 0.4890, 0.0741],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2960, 0.0178, 0.0867, 0.1806, 0.1085, 0.1013, 0.2091],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  40 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.03730000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.03730000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  38.4218692779541
maxi score, test score, baseline:  -0.03730000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  31.66728973388672
printing an ep nov before normalisation:  55.87775866782265
maxi score, test score, baseline:  -0.03730000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  62.83909708800597
printing an ep nov before normalisation:  43.55313301086426
Printing some Q and Qe and total Qs values:  [[0.461]
 [0.49 ]
 [0.461]
 [0.41 ]
 [0.461]
 [0.461]
 [0.461]] [[27.952]
 [35.476]
 [27.952]
 [25.594]
 [27.952]
 [27.952]
 [27.952]] [[1.318]
 [1.763]
 [1.318]
 [1.136]
 [1.318]
 [1.318]
 [1.318]]
maxi score, test score, baseline:  -0.03730000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.03730000000000011 0.6643333333333334 0.6643333333333334
printing an ep nov before normalisation:  49.46117541080618
siam score:  -0.8212994
maxi score, test score, baseline:  -0.03730000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.03730000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  44 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8198659
printing an ep nov before normalisation:  13.892353226157885
printing an ep nov before normalisation:  31.80732628569327
actor:  1 policy actor:  1  step number:  66 total reward:  0.03333333333333255  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  52 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.726]
 [0.618]
 [0.618]
 [0.617]
 [0.618]
 [0.618]] [[36.209]
 [38.437]
 [37.728]
 [37.728]
 [41.854]
 [37.728]
 [37.728]] [[0.605]
 [0.726]
 [0.618]
 [0.618]
 [0.617]
 [0.618]
 [0.618]]
maxi score, test score, baseline:  -0.03730000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
siam score:  -0.81755424
maxi score, test score, baseline:  -0.03730000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.03730000000000011 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.03730000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  48.463269782806925
maxi score, test score, baseline:  -0.03730000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[0.914]
 [0.473]
 [0.914]
 [0.914]
 [0.914]
 [0.914]
 [0.473]] [[19.581]
 [ 0.062]
 [19.581]
 [19.581]
 [19.581]
 [19.581]
 [ 0.061]] [[0.914]
 [0.473]
 [0.914]
 [0.914]
 [0.914]
 [0.914]
 [0.473]]
actor:  0 policy actor:  0  step number:  47 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.03460666666666676 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[0.066]
 [0.704]
 [0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.066]] [[67.446]
 [59.885]
 [67.446]
 [67.446]
 [67.446]
 [67.446]
 [67.446]] [[1.335]
 [1.782]
 [1.335]
 [1.335]
 [1.335]
 [1.335]
 [1.335]]
maxi score, test score, baseline:  -0.03460666666666676 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.03460666666666676 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.03460666666666676 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[-0.022]
 [-0.015]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.017]
 [-0.02 ]] [[19.925]
 [34.065]
 [32.161]
 [17.774]
 [17.966]
 [17.639]
 [17.345]] [[0.375]
 [0.932]
 [0.855]
 [0.295]
 [0.303]
 [0.291]
 [0.277]]
Printing some Q and Qe and total Qs values:  [[-0.043]
 [-0.027]
 [-0.044]
 [-0.042]
 [-0.043]
 [-0.042]
 [-0.045]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.043]
 [-0.027]
 [-0.044]
 [-0.042]
 [-0.043]
 [-0.042]
 [-0.045]]
actor:  1 policy actor:  1  step number:  70 total reward:  0.2466666666666656  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  57.73741946042829
maxi score, test score, baseline:  -0.03460666666666676 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.03460666666666676 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.03460666666666676 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.03460666666666676 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.03460666666666676 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.03460666666666676 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  67 total reward:  0.2266666666666659  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.03460666666666676 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.494]
 [0.462]
 [0.504]
 [0.509]
 [0.516]
 [0.449]
 [0.482]] [[13.789]
 [18.019]
 [13.982]
 [14.045]
 [14.066]
 [17.447]
 [14.796]] [[1.198]
 [1.596]
 [1.227]
 [1.239]
 [1.248]
 [1.525]
 [1.289]]
printing an ep nov before normalisation:  41.48752760524
printing an ep nov before normalisation:  38.67374190630731
maxi score, test score, baseline:  -0.03460666666666676 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.03460666666666676 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  35.39678531267432
maxi score, test score, baseline:  -0.03460666666666676 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.03460666666666676 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.03460666666666676 0.6643333333333334 0.6643333333333334
actor:  0 policy actor:  1  step number:  59 total reward:  0.05333333333333279  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.03250000000000009 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.03250000000000009 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  18.627378940582275
printing an ep nov before normalisation:  25.479207404230337
Printing some Q and Qe and total Qs values:  [[0.802]
 [0.722]
 [0.551]
 [0.622]
 [0.537]
 [0.47 ]
 [0.495]] [[21.597]
 [20.253]
 [18.696]
 [19.372]
 [21.543]
 [19.691]
 [18.69 ]] [[2.145]
 [1.98 ]
 [1.711]
 [1.824]
 [1.877]
 [1.692]
 [1.654]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.03250000000000009 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.03250000000000009 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  17.86117434501648
actor:  0 policy actor:  0  step number:  60 total reward:  0.09999999999999931  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.03030000000000011 0.6643333333333334 0.6643333333333334
printing an ep nov before normalisation:  51.61474720273844
printing an ep nov before normalisation:  32.31874813777039
printing an ep nov before normalisation:  19.3598459788115
printing an ep nov before normalisation:  37.29325794307669
maxi score, test score, baseline:  -0.03030000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.03030000000000011 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.03030000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  19.15189504623413
maxi score, test score, baseline:  -0.03030000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.441]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]] [[34.657]
 [36.775]
 [34.657]
 [34.657]
 [34.657]
 [34.657]
 [34.657]] [[1.101]
 [1.267]
 [1.101]
 [1.101]
 [1.101]
 [1.101]
 [1.101]]
maxi score, test score, baseline:  -0.03030000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.03030000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  35.76055271219871
actor:  1 policy actor:  1  step number:  61 total reward:  0.13333333333333286  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  0.16666666666666607  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  52 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  49.47493315387388
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.678]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]] [[34.498]
 [41.363]
 [34.498]
 [34.498]
 [34.498]
 [34.498]
 [34.498]] [[0.666]
 [0.678]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]]
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.337]
 [0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]] [[31.045]
 [43.326]
 [31.045]
 [31.045]
 [31.045]
 [31.045]
 [31.045]] [[0.932]
 [1.673]
 [0.932]
 [0.932]
 [0.932]
 [0.932]
 [0.932]]
actions average: 
K:  2  action  0 :  tensor([0.5259, 0.0441, 0.0676, 0.0792, 0.1115, 0.0861, 0.0856],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0040, 0.9677, 0.0033, 0.0045, 0.0027, 0.0031, 0.0146],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1081, 0.0359, 0.3131, 0.0846, 0.0779, 0.2813, 0.0991],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1462, 0.0213, 0.0643, 0.4836, 0.1020, 0.0747, 0.1080],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1346, 0.0778, 0.0653, 0.0881, 0.4502, 0.1004, 0.0836],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1678, 0.0013, 0.1384, 0.1124, 0.1377, 0.3311, 0.1114],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1455, 0.0374, 0.1058, 0.1764, 0.1130, 0.1247, 0.2973],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  47.44927253904781
printing an ep nov before normalisation:  43.320326985245345
printing an ep nov before normalisation:  31.598803997039795
printing an ep nov before normalisation:  33.24937385171229
Printing some Q and Qe and total Qs values:  [[0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.599]
 [0.599]] [[28.812]
 [28.812]
 [28.812]
 [28.812]
 [28.812]
 [28.812]
 [28.812]] [[1.51]
 [1.51]
 [1.51]
 [1.51]
 [1.51]
 [1.51]
 [1.51]]
Printing some Q and Qe and total Qs values:  [[0.858]
 [0.882]
 [0.858]
 [0.899]
 [0.549]
 [0.827]
 [0.748]] [[31.642]
 [27.399]
 [24.108]
 [26.977]
 [25.446]
 [23.578]
 [25.1  ]] [[0.858]
 [0.882]
 [0.858]
 [0.899]
 [0.549]
 [0.827]
 [0.748]]
maxi score, test score, baseline:  -0.033100000000000115 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  0 policy actor:  1  step number:  56 total reward:  0.42000000000000004  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  63 total reward:  0.13333333333333253  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  60 total reward:  0.08666666666666623  reward:  1.0 rdn_beta:  2.0
from probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.030260000000000113 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  36.39654274816796
printing an ep nov before normalisation:  24.34662111494379
printing an ep nov before normalisation:  34.68620643503953
printing an ep nov before normalisation:  35.23164095244883
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.515]
 [0.462]
 [0.464]
 [0.464]
 [0.465]
 [0.463]] [[24.752]
 [31.636]
 [23.378]
 [23.647]
 [23.799]
 [23.511]
 [23.563]] [[0.938]
 [1.39 ]
 [0.857]
 [0.875]
 [0.883]
 [0.868]
 [0.868]]
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.591]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]] [[27.7  ]
 [29.984]
 [30.348]
 [30.348]
 [30.348]
 [30.348]
 [30.348]] [[1.514]
 [1.557]
 [1.523]
 [1.523]
 [1.523]
 [1.523]
 [1.523]]
printing an ep nov before normalisation:  32.35054503930819
actor:  1 policy actor:  1  step number:  58 total reward:  0.28666666666666596  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.030260000000000113 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.030260000000000113 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  0 policy actor:  0  step number:  55 total reward:  0.14666666666666606  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  60 total reward:  0.13999999999999957  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.030540000000000116 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.030540000000000116 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  34.60581385362683
actions average: 
K:  0  action  0 :  tensor([0.4102, 0.0079, 0.1100, 0.1117, 0.1187, 0.1129, 0.1286],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0023,     0.9517,     0.0039,     0.0027,     0.0006,     0.0021,
            0.0368], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1999, 0.0093, 0.3333, 0.1139, 0.1063, 0.1047, 0.1326],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1962, 0.0026, 0.0943, 0.3719, 0.0926, 0.0940, 0.1485],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1876, 0.0114, 0.1040, 0.1012, 0.3637, 0.1070, 0.1251],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1042, 0.0542, 0.1000, 0.0788, 0.0691, 0.4874, 0.1063],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1961, 0.0912, 0.1179, 0.1220, 0.1099, 0.1280, 0.2349],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  50 total reward:  0.566666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.030540000000000116 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.652]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]] [[31.007]
 [31.02 ]
 [31.007]
 [31.007]
 [31.007]
 [31.007]
 [31.007]] [[1.26]
 [1.21]
 [1.26]
 [1.26]
 [1.26]
 [1.26]
 [1.26]]
maxi score, test score, baseline:  -0.030540000000000116 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  0.06293153174553368
actor:  1 policy actor:  1  step number:  54 total reward:  0.206666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.030540000000000116 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  45.434991501652206
printing an ep nov before normalisation:  31.927007999405564
maxi score, test score, baseline:  -0.030540000000000116 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  43 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  28.114326685205945
maxi score, test score, baseline:  -0.030540000000000116 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
line 256 mcts: sample exp_bonus 38.921525697069676
actor:  1 policy actor:  1  step number:  61 total reward:  0.19999999999999962  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.030540000000000116 0.6643333333333334 0.6643333333333334
actor:  1 policy actor:  1  step number:  72 total reward:  0.03333333333333255  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.030540000000000116 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  35.387306681003274
printing an ep nov before normalisation:  26.104990540337763
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.030540000000000116 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.030540000000000116 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.685]
 [0.685]
 [0.674]
 [0.674]
 [0.674]
 [0.674]] [[17.399]
 [18.086]
 [15.037]
 [17.399]
 [17.399]
 [17.399]
 [17.399]] [[1.278]
 [1.339]
 [1.123]
 [1.278]
 [1.278]
 [1.278]
 [1.278]]
printing an ep nov before normalisation:  37.38092798834814
actor:  1 policy actor:  1  step number:  37 total reward:  0.5733333333333334  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.647]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]] [[45.29 ]
 [44.332]
 [45.29 ]
 [45.29 ]
 [45.29 ]
 [45.29 ]
 [45.29 ]] [[0.538]
 [0.647]
 [0.538]
 [0.538]
 [0.538]
 [0.538]
 [0.538]]
printing an ep nov before normalisation:  28.84332797672611
printing an ep nov before normalisation:  27.472389793767867
siam score:  -0.8178019
actor:  0 policy actor:  1  step number:  56 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.02796666666666678 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
siam score:  -0.813243
actions average: 
K:  3  action  0 :  tensor([0.3090, 0.0289, 0.1488, 0.1212, 0.1641, 0.1149, 0.1131],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0149, 0.9181, 0.0219, 0.0088, 0.0076, 0.0075, 0.0212],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1853, 0.0943, 0.2281, 0.1167, 0.1212, 0.1229, 0.1315],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1281, 0.0711, 0.1331, 0.3278, 0.1137, 0.1396, 0.0866],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1553, 0.0067, 0.0977, 0.1107, 0.4626, 0.0886, 0.0785],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0823, 0.0061, 0.1557, 0.0950, 0.0787, 0.4732, 0.1090],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2486, 0.1520, 0.1038, 0.1147, 0.1074, 0.1224, 0.1510],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.02796666666666678 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  22.86315643953708
printing an ep nov before normalisation:  30.148210525512695
printing an ep nov before normalisation:  18.909850120544434
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]
 [0.379]] [[39.573]
 [39.573]
 [39.573]
 [39.573]
 [39.573]
 [39.573]
 [39.573]] [[1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.455]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  0.3199999999999994  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.09453258561755
printing an ep nov before normalisation:  51.52555711229945
maxi score, test score, baseline:  -0.02796666666666678 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.02796666666666678 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  36.25670608994906
actor:  1 policy actor:  1  step number:  43 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.02796666666666678 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  30.60207240770092
printing an ep nov before normalisation:  0.0
actor:  1 policy actor:  1  step number:  51 total reward:  0.34666666666666657  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.02796666666666678 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  0 policy actor:  1  step number:  42 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.025126666666666773 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actions average: 
K:  1  action  0 :  tensor([0.4100, 0.0381, 0.0892, 0.1083, 0.1297, 0.1063, 0.1185],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0022,     0.9673,     0.0016,     0.0056,     0.0005,     0.0007,
            0.0221], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0941, 0.0866, 0.2841, 0.0998, 0.0828, 0.2495, 0.1033],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1197, 0.0077, 0.0681, 0.4502, 0.1651, 0.0849, 0.1042],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1763, 0.0147, 0.0635, 0.1250, 0.4440, 0.0713, 0.1052],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0930, 0.0220, 0.1784, 0.0956, 0.0801, 0.4439, 0.0870],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1277, 0.1394, 0.0918, 0.1416, 0.1291, 0.1149, 0.2555],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.025126666666666773 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  48 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.025126666666666773 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  20.671464931418555
Printing some Q and Qe and total Qs values:  [[0.095]
 [0.275]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]] [[22.605]
 [27.302]
 [22.605]
 [22.605]
 [22.605]
 [22.605]
 [22.605]] [[0.728]
 [1.204]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]]
printing an ep nov before normalisation:  25.46169027811433
maxi score, test score, baseline:  -0.025126666666666773 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.025126666666666773 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  25.996645234067017
printing an ep nov before normalisation:  35.07668413951767
actor:  1 policy actor:  1  step number:  64 total reward:  0.13999999999999968  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.025126666666666773 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
siam score:  -0.821687
printing an ep nov before normalisation:  37.235537501121854
printing an ep nov before normalisation:  37.312013240750176
siam score:  -0.82339025
maxi score, test score, baseline:  -0.025126666666666773 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.025126666666666773 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.025126666666666773 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.025126666666666773 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  78 total reward:  0.08666666666666578  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]] [[34.297]
 [34.297]
 [34.297]
 [34.297]
 [34.297]
 [34.297]
 [34.297]] [[1.856]
 [1.856]
 [1.856]
 [1.856]
 [1.856]
 [1.856]
 [1.856]]
printing an ep nov before normalisation:  30.264452310942414
maxi score, test score, baseline:  -0.025126666666666773 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  0 policy actor:  1  step number:  50 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  56 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.02242000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.418483365862773
Printing some Q and Qe and total Qs values:  [[0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]] [[60.716]
 [60.716]
 [60.716]
 [60.716]
 [60.716]
 [60.716]
 [60.716]] [[1.319]
 [1.319]
 [1.319]
 [1.319]
 [1.319]
 [1.319]
 [1.319]]
printing an ep nov before normalisation:  34.93758743386375
maxi score, test score, baseline:  -0.02242000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.02242000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.02242000000000011 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.02242000000000011 0.6643333333333334 0.6643333333333334
actor:  1 policy actor:  1  step number:  68 total reward:  0.20666666666666633  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.031]
 [-0.021]
 [-0.03 ]
 [-0.036]
 [-0.038]
 [-0.034]
 [-0.035]] [[47.32 ]
 [44.983]
 [46.253]
 [50.081]
 [50.276]
 [49.231]
 [49.565]] [[1.168]
 [1.091]
 [1.129]
 [1.266]
 [1.27 ]
 [1.236]
 [1.248]]
maxi score, test score, baseline:  -0.02242000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.02242000000000011 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  0 policy actor:  1  step number:  45 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.019753333333333446 0.6643333333333334 0.6643333333333334
printing an ep nov before normalisation:  28.57755444532397
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.365312489490435
actor:  1 policy actor:  1  step number:  49 total reward:  0.33333333333333326  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  0  action  0 :  tensor([0.4288, 0.0062, 0.0948, 0.1186, 0.1340, 0.1094, 0.1082],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0030, 0.9707, 0.0020, 0.0031, 0.0013, 0.0012, 0.0187],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2003, 0.0104, 0.2490, 0.1497, 0.1109, 0.1392, 0.1405],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1831, 0.0195, 0.1151, 0.2613, 0.1120, 0.1469, 0.1620],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2074, 0.0098, 0.1053, 0.1578, 0.2608, 0.1276, 0.1314],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1576, 0.0047, 0.1476, 0.1513, 0.0979, 0.3289, 0.1121],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1375, 0.1153, 0.0856, 0.1463, 0.1039, 0.1166, 0.2948],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.696377960220936
maxi score, test score, baseline:  -0.019753333333333446 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  48 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.019753333333333446 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.019753333333333446 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
line 256 mcts: sample exp_bonus 35.39468021882143
line 256 mcts: sample exp_bonus 46.92280177910318
printing an ep nov before normalisation:  36.571801620126855
Printing some Q and Qe and total Qs values:  [[ 0.075]
 [ 0.075]
 [-0.002]
 [-0.002]
 [ 0.075]
 [ 0.075]
 [ 0.075]] [[35.162]
 [33.047]
 [32.813]
 [32.843]
 [35.162]
 [35.162]
 [35.162]] [[1.331]
 [1.179]
 [1.086]
 [1.087]
 [1.331]
 [1.331]
 [1.331]]
from probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.267]
 [0.112]
 [0.077]
 [0.121]
 [0.075]
 [0.085]] [[35.731]
 [34.08 ]
 [31.584]
 [29.502]
 [30.125]
 [28.813]
 [29.043]] [[1.001]
 [0.979]
 [0.72 ]
 [0.6  ]
 [0.669]
 [0.569]
 [0.589]]
siam score:  -0.82035124
maxi score, test score, baseline:  -0.019753333333333446 0.6643333333333334 0.6643333333333334
actor:  1 policy actor:  1  step number:  49 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.019753333333333446 0.6643333333333334 0.6643333333333334
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.019753333333333446 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[0.925]
 [0.989]
 [0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]] [[30.726]
 [29.798]
 [30.726]
 [30.726]
 [30.726]
 [30.726]
 [30.726]] [[0.925]
 [0.989]
 [0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]]
actor:  0 policy actor:  0  step number:  48 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.020246666666666784 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  34.66872091280005
printing an ep nov before normalisation:  57.61878977072765
printing an ep nov before normalisation:  0.012650125009940894
actor:  0 policy actor:  0  step number:  51 total reward:  0.4266666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.01739333333333344 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]] [[29.262]
 [29.262]
 [29.262]
 [29.262]
 [29.262]
 [29.262]
 [29.262]] [[1.737]
 [1.737]
 [1.737]
 [1.737]
 [1.737]
 [1.737]
 [1.737]]
Printing some Q and Qe and total Qs values:  [[0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]] [[24.469]
 [24.469]
 [24.469]
 [24.469]
 [24.469]
 [24.469]
 [24.469]] [[1.563]
 [1.563]
 [1.563]
 [1.563]
 [1.563]
 [1.563]
 [1.563]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.43533711195155
maxi score, test score, baseline:  -0.01739333333333344 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  45.58872835655996
maxi score, test score, baseline:  -0.01739333333333344 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.01739333333333344 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  30.617530018906024
siam score:  -0.81370836
maxi score, test score, baseline:  -0.01739333333333344 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.01739333333333344 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[0.787]
 [0.84 ]
 [0.856]
 [0.767]
 [0.787]
 [0.761]
 [0.824]] [[24.492]
 [24.562]
 [18.019]
 [23.939]
 [24.492]
 [24.657]
 [24.24 ]] [[0.787]
 [0.84 ]
 [0.856]
 [0.767]
 [0.787]
 [0.761]
 [0.824]]
actor:  0 policy actor:  0  step number:  42 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.014660000000000119 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.014660000000000119 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  26.31043403256605
maxi score, test score, baseline:  -0.014660000000000119 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.014660000000000119 0.6643333333333334 0.6643333333333334
actor:  1 policy actor:  1  step number:  65 total reward:  0.14666666666666617  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 32.90100120234052
maxi score, test score, baseline:  -0.014660000000000119 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.014660000000000119 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  46 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  2.0
from probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.014660000000000119 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  14.976174191670584
actor:  1 policy actor:  1  step number:  49 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  58.58978809854366
maxi score, test score, baseline:  -0.014660000000000119 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.014660000000000119 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.014660000000000119 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  58 total reward:  0.16666666666666585  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  32 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.01412666666666678 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  39.89081404017289
printing an ep nov before normalisation:  38.4750823416983
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.28879482888713
printing an ep nov before normalisation:  40.75877991833487
maxi score, test score, baseline:  -0.01412666666666678 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.01412666666666678 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  63 total reward:  0.2399999999999991  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.01412666666666678 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  8.41345752178313e-06
actor:  0 policy actor:  1  step number:  54 total reward:  0.2733333333333331  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.403]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]] [[41.482]
 [40.624]
 [41.482]
 [41.482]
 [41.482]
 [41.482]
 [41.482]] [[1.447]
 [1.435]
 [1.447]
 [1.447]
 [1.447]
 [1.447]
 [1.447]]
maxi score, test score, baseline:  -0.014420000000000117 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.014420000000000117 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[-0.008]
 [-0.009]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]] [[34.448]
 [31.61 ]
 [29.326]
 [29.326]
 [29.326]
 [29.326]
 [31.43 ]] [[0.284]
 [0.241]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.24 ]]
printing an ep nov before normalisation:  28.384871023008987
maxi score, test score, baseline:  -0.014420000000000117 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.014420000000000117 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.014420000000000117 0.6643333333333334 0.6643333333333334
actor:  1 policy actor:  1  step number:  57 total reward:  0.3333333333333327  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  42.85828262449826
maxi score, test score, baseline:  -0.014420000000000117 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.014420000000000117 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  34.76021371507827
siam score:  -0.8140122
maxi score, test score, baseline:  -0.014420000000000117 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.014420000000000117 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  52.70412789300543
printing an ep nov before normalisation:  37.03485852080934
actor:  1 policy actor:  1  step number:  45 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.578]
 [0.539]
 [0.395]
 [0.391]
 [0.407]
 [0.405]] [[16.327]
 [24.733]
 [25.046]
 [16.294]
 [16.272]
 [16.321]
 [16.25 ]] [[0.476]
 [0.761]
 [0.725]
 [0.486]
 [0.482]
 [0.498]
 [0.495]]
printing an ep nov before normalisation:  18.688028691217557
from probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.014420000000000117 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.708]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]] [[30.785]
 [ 0.039]
 [30.785]
 [30.785]
 [30.785]
 [30.785]
 [30.785]] [[0.142]
 [0.708]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.014420000000000124 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  43.80433422332075
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.41714904348726
printing an ep nov before normalisation:  30.037071704864502
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.66 ]
 [0.655]
 [0.573]
 [0.573]
 [0.573]
 [0.573]] [[29.425]
 [26.309]
 [28.807]
 [29.494]
 [29.494]
 [29.494]
 [29.494]] [[1.199]
 [1.11 ]
 [1.202]
 [1.147]
 [1.147]
 [1.147]
 [1.147]]
maxi score, test score, baseline:  -0.014420000000000124 0.6643333333333334 0.6643333333333334
printing an ep nov before normalisation:  26.424803733825684
maxi score, test score, baseline:  -0.014420000000000124 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  0.09618266376946849
actor:  1 policy actor:  1  step number:  50 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  56.28798102512371
maxi score, test score, baseline:  -0.014420000000000124 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  65 total reward:  0.2533333333333331  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.014246666666666786 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  41 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.014246666666666786 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.014246666666666786 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  50 total reward:  0.39333333333333265  reward:  1.0 rdn_beta:  2.0
from probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  30.917019844055176
printing an ep nov before normalisation:  42.759190783125845
maxi score, test score, baseline:  -0.014246666666666786 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  38.87621453693927
printing an ep nov before normalisation:  25.617237725698747
maxi score, test score, baseline:  -0.014246666666666786 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.014246666666666786 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.014246666666666786 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  54 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  22.69444745385222
actor:  0 policy actor:  0  step number:  46 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  27.705853168742504
maxi score, test score, baseline:  -0.011566666666666786 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  55 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.011566666666666786 0.6643333333333334 0.6643333333333334
printing an ep nov before normalisation:  47.563431547295934
line 256 mcts: sample exp_bonus 48.6524944868555
maxi score, test score, baseline:  -0.011566666666666786 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.011566666666666786 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.011566666666666786 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.011566666666666786 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[0.847]
 [0.874]
 [0.867]
 [0.853]
 [0.835]
 [0.863]
 [0.84 ]] [[30.968]
 [29.376]
 [29.865]
 [29.485]
 [29.01 ]
 [28.483]
 [28.943]] [[0.847]
 [0.874]
 [0.867]
 [0.853]
 [0.835]
 [0.863]
 [0.84 ]]
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([0.5154, 0.0023, 0.0803, 0.0769, 0.1375, 0.1039, 0.0837],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0052, 0.9649, 0.0052, 0.0039, 0.0030, 0.0025, 0.0153],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1253, 0.0055, 0.5415, 0.0774, 0.0786, 0.1054, 0.0661],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1743, 0.0558, 0.0855, 0.3536, 0.0963, 0.1219, 0.1126],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1724, 0.0044, 0.1437, 0.1130, 0.2683, 0.1667, 0.1317],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1764, 0.0016, 0.1330, 0.1096, 0.1105, 0.3628, 0.1061],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1765, 0.0028, 0.1207, 0.1143, 0.0948, 0.1177, 0.3732],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  32.54331156482088
printing an ep nov before normalisation:  29.630939868469277
line 256 mcts: sample exp_bonus 30.9522978881122
actor:  0 policy actor:  0  step number:  43 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  46 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  40 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.014]
 [ 0.078]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]] [[25.848]
 [34.702]
 [25.848]
 [25.848]
 [25.848]
 [25.848]
 [25.848]] [[0.207]
 [0.448]
 [0.207]
 [0.207]
 [0.207]
 [0.207]
 [0.207]]
maxi score, test score, baseline:  -0.008353333333333447 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  40.9384477333567
maxi score, test score, baseline:  -0.008353333333333447 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  30.6454195071618
siam score:  -0.8114131
maxi score, test score, baseline:  -0.008353333333333447 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.008353333333333447 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
line 256 mcts: sample exp_bonus 34.15905540414675
actor:  1 policy actor:  1  step number:  57 total reward:  0.37333333333333274  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.457]
 [0.316]
 [0.312]
 [0.312]
 [0.313]
 [0.313]] [[19.984]
 [33.503]
 [18.817]
 [18.485]
 [18.347]
 [18.209]
 [18.207]] [[0.414]
 [0.683]
 [0.4  ]
 [0.392]
 [0.391]
 [0.391]
 [0.391]]
siam score:  -0.8119325
printing an ep nov before normalisation:  39.67158592472398
printing an ep nov before normalisation:  52.53144803541814
maxi score, test score, baseline:  -0.008353333333333447 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
using explorer policy with actor:  1
siam score:  -0.81282896
maxi score, test score, baseline:  -0.008353333333333447 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  70 total reward:  0.059999999999999276  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.008353333333333447 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.008353333333333447 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
line 256 mcts: sample exp_bonus 45.48093379402252
printing an ep nov before normalisation:  47.74963631204629
maxi score, test score, baseline:  -0.008353333333333447 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  46.82481696275819
printing an ep nov before normalisation:  33.96161457349204
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.008353333333333447 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.008353333333333447 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  25.51090955734253
actor:  1 policy actor:  1  step number:  43 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  24.64602213818937
maxi score, test score, baseline:  -0.008353333333333447 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.008353333333333447 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[0.705]
 [0.757]
 [0.197]
 [0.634]
 [0.647]
 [0.691]
 [0.665]] [[39.542]
 [44.917]
 [26.32 ]
 [22.647]
 [25.792]
 [39.872]
 [28.369]] [[0.705]
 [0.757]
 [0.197]
 [0.634]
 [0.647]
 [0.691]
 [0.665]]
printing an ep nov before normalisation:  54.483015181242386
maxi score, test score, baseline:  -0.008353333333333447 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  47 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.008353333333333447 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.008353333333333447 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.008353333333333447 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
siam score:  -0.81246275
printing an ep nov before normalisation:  16.106835473790035
printing an ep nov before normalisation:  16.735629427040767
maxi score, test score, baseline:  -0.008353333333333447 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  13.615858947454296
Printing some Q and Qe and total Qs values:  [[0.784]
 [0.797]
 [0.778]
 [0.812]
 [0.798]
 [0.819]
 [0.815]] [[13.336]
 [13.764]
 [13.244]
 [13.989]
 [13.473]
 [13.649]
 [14.223]] [[0.784]
 [0.797]
 [0.778]
 [0.812]
 [0.798]
 [0.819]
 [0.815]]
maxi score, test score, baseline:  -0.008353333333333447 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  0 policy actor:  0  step number:  37 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  17.13733848920105
actor:  1 policy actor:  1  step number:  58 total reward:  0.27333333333333265  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.011113333333333452 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.011113333333333452 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
siam score:  -0.81361955
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.81311303
maxi score, test score, baseline:  -0.011113333333333452 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.011113333333333452 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.011113333333333452 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.011113333333333452 0.6643333333333334 0.6643333333333334
printing an ep nov before normalisation:  20.961175715805044
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.662]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]] [[20.396]
 [23.669]
 [20.396]
 [20.396]
 [20.396]
 [20.396]
 [20.396]] [[1.063]
 [1.339]
 [1.063]
 [1.063]
 [1.063]
 [1.063]
 [1.063]]
maxi score, test score, baseline:  -0.011113333333333452 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.011113333333333452 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  23.31286747615104
maxi score, test score, baseline:  -0.011113333333333452 0.6643333333333334 0.6643333333333334
actor:  1 policy actor:  1  step number:  47 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  53 total reward:  0.23999999999999932  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  55.284730368973676
siam score:  -0.8088066
actor:  0 policy actor:  0  step number:  57 total reward:  0.2133333333333325  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.008686666666666783 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[-0.021]
 [ 0.053]
 [-0.021]
 [-0.035]
 [-0.04 ]
 [-0.021]
 [-0.034]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.021]
 [ 0.053]
 [-0.021]
 [-0.035]
 [-0.04 ]
 [-0.021]
 [-0.034]]
actions average: 
K:  0  action  0 :  tensor([0.4115, 0.0190, 0.0970, 0.1047, 0.1512, 0.1002, 0.1164],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0044, 0.9650, 0.0031, 0.0034, 0.0015, 0.0022, 0.0204],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1229, 0.0467, 0.3527, 0.1063, 0.1012, 0.1209, 0.1493],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1833, 0.0037, 0.1126, 0.2945, 0.1268, 0.1334, 0.1457],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1835, 0.0189, 0.1057, 0.1341, 0.3015, 0.1132, 0.1431],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1097, 0.0124, 0.1319, 0.0797, 0.0828, 0.4874, 0.0961],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1338, 0.1865, 0.0834, 0.1042, 0.0856, 0.0895, 0.3171],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  33.011776801237126
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]] [[28.815]
 [28.815]
 [28.815]
 [28.815]
 [28.815]
 [28.815]
 [28.815]] [[0.903]
 [0.903]
 [0.903]
 [0.903]
 [0.903]
 [0.903]
 [0.903]]
printing an ep nov before normalisation:  30.44341750585521
maxi score, test score, baseline:  -0.008686666666666783 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
from probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
siam score:  -0.81648684
actor:  1 policy actor:  1  step number:  53 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  4  action  0 :  tensor([0.5639, 0.0263, 0.0592, 0.0902, 0.1103, 0.0758, 0.0741],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0121, 0.9280, 0.0098, 0.0137, 0.0091, 0.0091, 0.0181],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2431, 0.0303, 0.1261, 0.1487, 0.1593, 0.1460, 0.1466],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1768, 0.0102, 0.1103, 0.3276, 0.1351, 0.1328, 0.1072],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1554, 0.0549, 0.1009, 0.1072, 0.3266, 0.1331, 0.1219],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1675, 0.0038, 0.1331, 0.0974, 0.1465, 0.3627, 0.0889],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1854, 0.0782, 0.1392, 0.1336, 0.1204, 0.1197, 0.2235],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  36.84583965482496
actor:  1 policy actor:  1  step number:  60 total reward:  0.3399999999999992  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.37313452576651
maxi score, test score, baseline:  -0.010940000000000116 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  0 policy actor:  0  step number:  44 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.010700000000000124 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  42.255300539144706
maxi score, test score, baseline:  -0.010700000000000124 0.6643333333333334 0.6643333333333334
printing an ep nov before normalisation:  48.7000928957159
maxi score, test score, baseline:  -0.010700000000000124 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.01070000000000012 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.439]
 [0.327]
 [0.458]
 [0.293]
 [0.306]
 [0.463]] [[44.997]
 [45.892]
 [43.215]
 [46.369]
 [46.978]
 [46.174]
 [45.672]] [[1.376]
 [1.51 ]
 [1.3  ]
 [1.546]
 [1.403]
 [1.387]
 [1.526]]
maxi score, test score, baseline:  -0.01070000000000012 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  26.24079942703247
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.526]
 [0.499]
 [0.524]
 [0.499]
 [0.499]
 [0.499]] [[19.274]
 [14.41 ]
 [19.274]
 [11.986]
 [19.274]
 [19.274]
 [19.274]] [[0.499]
 [0.526]
 [0.499]
 [0.524]
 [0.499]
 [0.499]
 [0.499]]
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.516]
 [0.445]
 [0.445]
 [0.425]
 [0.41 ]
 [0.445]] [[36.957]
 [34.131]
 [33.061]
 [33.061]
 [43.109]
 [42.679]
 [33.061]] [[1.471]
 [1.413]
 [1.288]
 [1.288]
 [1.768]
 [1.731]
 [1.288]]
printing an ep nov before normalisation:  27.247665416986468
actor:  0 policy actor:  1  step number:  56 total reward:  0.1666666666666663  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.00836666666666679 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  49 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.00836666666666679 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.00836666666666679 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  46 total reward:  0.5133333333333336  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.00836666666666679 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[0.706]
 [0.687]
 [0.706]
 [0.594]
 [0.477]
 [0.496]
 [0.706]] [[44.393]
 [43.877]
 [44.393]
 [43.517]
 [44.119]
 [43.841]
 [44.393]] [[1.641]
 [1.606]
 [1.641]
 [1.502]
 [1.404]
 [1.414]
 [1.641]]
printing an ep nov before normalisation:  24.178926524699012
maxi score, test score, baseline:  -0.00836666666666679 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  37.99541087934606
maxi score, test score, baseline:  -0.00836666666666679 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.679]
 [0.526]
 [0.598]
 [0.598]
 [0.655]
 [0.623]] [[23.109]
 [24.608]
 [22.382]
 [22.721]
 [22.721]
 [23.462]
 [24.186]] [[1.415]
 [1.59 ]
 [1.263]
 [1.361]
 [1.361]
 [1.477]
 [1.502]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.00836666666666679 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.00836666666666679 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  35 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.00836666666666679 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  45.56168112454817
maxi score, test score, baseline:  -0.00836666666666679 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.00836666666666679 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  29.94554623419209
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.575]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]] [[27.438]
 [35.77 ]
 [27.438]
 [27.438]
 [27.438]
 [27.438]
 [27.438]] [[0.85 ]
 [1.033]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]]
printing an ep nov before normalisation:  46.883712770351075
maxi score, test score, baseline:  -0.008366666666666786 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  32.920475006103516
maxi score, test score, baseline:  -0.008366666666666786 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.008366666666666786 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  57 total reward:  0.3333333333333328  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.008366666666666786 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.008366666666666786 0.6643333333333334 0.6643333333333334
actor:  1 policy actor:  1  step number:  58 total reward:  0.12666666666666593  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.008366666666666786 0.6643333333333334 0.6643333333333334
printing an ep nov before normalisation:  50.60494486684529
printing an ep nov before normalisation:  27.28254795074463
maxi score, test score, baseline:  -0.008366666666666786 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.008366666666666786 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.008366666666666786 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  41.23494328282899
maxi score, test score, baseline:  -0.008366666666666786 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  41.91599608337623
maxi score, test score, baseline:  -0.008366666666666786 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  23.0534029006958
maxi score, test score, baseline:  -0.008366666666666786 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.008366666666666786 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  53 total reward:  0.37333333333333274  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.008366666666666786 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  32.26241864186787
actor:  0 policy actor:  1  step number:  79 total reward:  0.013333333333332531  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.274]
 [0.018]
 [0.018]
 [0.018]
 [0.018]
 [0.018]] [[30.311]
 [37.488]
 [30.311]
 [30.311]
 [30.311]
 [30.311]
 [30.311]] [[0.28 ]
 [0.672]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]
 [0.28 ]]
maxi score, test score, baseline:  -0.006340000000000122 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  24.023964405059814
Printing some Q and Qe and total Qs values:  [[0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]] [[40.045]
 [40.045]
 [40.045]
 [40.045]
 [40.045]
 [40.045]
 [40.045]] [[1.298]
 [1.298]
 [1.298]
 [1.298]
 [1.298]
 [1.298]
 [1.298]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.789]
 [0.891]
 [0.791]
 [0.792]
 [0.792]
 [0.791]
 [0.785]] [[22.944]
 [32.021]
 [17.308]
 [17.3  ]
 [17.372]
 [17.372]
 [23.273]] [[0.789]
 [0.891]
 [0.791]
 [0.792]
 [0.792]
 [0.791]
 [0.785]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  47 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.119]
 [0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.055]] [[21.585]
 [24.84 ]
 [21.585]
 [21.585]
 [21.585]
 [21.585]
 [21.585]] [[0.888]
 [1.197]
 [0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]]
maxi score, test score, baseline:  -0.003673333333333457 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.003673333333333457 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  48.454706336889586
maxi score, test score, baseline:  -0.003673333333333457 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  23.499553688781663
maxi score, test score, baseline:  -0.003673333333333457 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[0.836]
 [0.943]
 [0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.838]] [[27.433]
 [24.852]
 [27.433]
 [27.433]
 [27.433]
 [27.433]
 [32.873]] [[1.69 ]
 [1.716]
 [1.69 ]
 [1.69 ]
 [1.69 ]
 [1.69 ]
 [1.862]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.29999999999999927  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  55 total reward:  0.2533333333333332  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  26.181771755218506
maxi score, test score, baseline:  -0.003673333333333457 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.003673333333333457 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  35.15723935302566
maxi score, test score, baseline:  -0.003673333333333457 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.003673333333333457 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  33 total reward:  0.5733333333333334  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.003673333333333457 0.6643333333333334 0.6643333333333334
actor:  0 policy actor:  1  step number:  58 total reward:  0.17999999999999972  reward:  1.0 rdn_beta:  0.667
from probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
line 256 mcts: sample exp_bonus 17.736916578665397
maxi score, test score, baseline:  -0.0013133333333334584 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  49.6533143560807
actor:  1 policy actor:  1  step number:  49 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.0013133333333334584 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.0013133333333334584 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.0013133333333334584 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.0013133333333334584 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  22.97637701034546
actor:  0 policy actor:  0  step number:  45 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  1.333
from probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  41.05232378451222
maxi score, test score, baseline:  -0.0015000000000001284 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.0015000000000001284 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  37.62145213884878
printing an ep nov before normalisation:  21.907212827236027
actor:  1 policy actor:  1  step number:  63 total reward:  0.2533333333333325  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.0015000000000001284 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.0015000000000001284 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[0.74 ]
 [0.811]
 [0.773]
 [0.737]
 [0.773]
 [0.737]
 [0.736]] [[30.13 ]
 [28.617]
 [28.856]
 [29.857]
 [28.63 ]
 [29.166]
 [29.2  ]] [[0.74 ]
 [0.811]
 [0.773]
 [0.737]
 [0.773]
 [0.737]
 [0.736]]
maxi score, test score, baseline:  -0.0015000000000001284 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  33.44274044036865
printing an ep nov before normalisation:  35.84355377872386
printing an ep nov before normalisation:  34.05908933437936
printing an ep nov before normalisation:  36.50620075918054
actor:  0 policy actor:  1  step number:  49 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  21.14849328994751
maxi score, test score, baseline:  -0.0014200000000001239 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.0014200000000001239 0.6643333333333334 0.6643333333333334
printing an ep nov before normalisation:  35.563577686275174
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]] [[25.179]
 [25.179]
 [25.179]
 [25.179]
 [25.179]
 [25.179]
 [25.179]] [[0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]]
maxi score, test score, baseline:  -0.0014200000000001239 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.0036200000000001253 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[0.276]
 [0.321]
 [0.276]
 [0.276]
 [0.276]
 [0.276]
 [0.276]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.276]
 [0.321]
 [0.276]
 [0.276]
 [0.276]
 [0.276]
 [0.276]]
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.356]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]] [[25.822]
 [42.3  ]
 [25.822]
 [25.822]
 [25.822]
 [25.822]
 [25.822]] [[0.502]
 [0.777]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]]
maxi score, test score, baseline:  -0.0036200000000001253 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.0036200000000001253 0.6643333333333334 0.6643333333333334
printing an ep nov before normalisation:  43.33462163920764
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.297]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]] [[37.083]
 [45.604]
 [37.083]
 [37.083]
 [37.083]
 [37.083]
 [37.083]] [[0.631]
 [0.823]
 [0.631]
 [0.631]
 [0.631]
 [0.631]
 [0.631]]
printing an ep nov before normalisation:  40.889124603370696
printing an ep nov before normalisation:  50.19711959945398
maxi score, test score, baseline:  -0.0036200000000001253 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.0036200000000001253 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
line 256 mcts: sample exp_bonus 29.811606837845773
actor:  1 policy actor:  1  step number:  53 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  28.730854988098145
actor:  1 policy actor:  1  step number:  45 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0036200000000001253 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
line 256 mcts: sample exp_bonus 31.34772597704044
printing an ep nov before normalisation:  21.871409751173054
actor:  1 policy actor:  1  step number:  58 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.006540000000000123 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  32.39262694905899
maxi score, test score, baseline:  -0.006540000000000123 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  30.6832139448355
maxi score, test score, baseline:  -0.006540000000000123 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.006540000000000123 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
siam score:  -0.81279397
maxi score, test score, baseline:  -0.006540000000000123 0.6643333333333334 0.6643333333333334
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.606]
 [0.504]
 [0.504]
 [0.504]
 [0.504]
 [0.504]] [[29.134]
 [36.383]
 [29.134]
 [29.134]
 [29.134]
 [29.134]
 [29.134]] [[1.086]
 [1.428]
 [1.086]
 [1.086]
 [1.086]
 [1.086]
 [1.086]]
printing an ep nov before normalisation:  0.00020626745566687532
actor:  0 policy actor:  1  step number:  37 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.89 ]
 [0.877]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.89 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.89 ]
 [0.877]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.89 ]]
printing an ep nov before normalisation:  16.997530741785134
maxi score, test score, baseline:  -0.005940000000000125 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3666666666666666  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  52.11810418879358
printing an ep nov before normalisation:  47.48268738331304
line 256 mcts: sample exp_bonus 41.221653533697925
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.742]
 [0.62 ]
 [0.627]
 [0.624]
 [0.625]
 [0.628]] [[29.201]
 [23.04 ]
 [26.47 ]
 [26.798]
 [26.8  ]
 [26.618]
 [26.638]] [[1.909]
 [1.768]
 [1.799]
 [1.82 ]
 [1.818]
 [1.81 ]
 [1.814]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  28.843829179672667
Printing some Q and Qe and total Qs values:  [[0.994]
 [1.025]
 [0.994]
 [0.994]
 [0.994]
 [0.994]
 [0.994]] [[30.205]
 [42.73 ]
 [30.205]
 [30.205]
 [30.205]
 [30.205]
 [30.205]] [[0.994]
 [1.025]
 [0.994]
 [0.994]
 [0.994]
 [0.994]
 [0.994]]
maxi score, test score, baseline:  -0.008753333333333462 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  33.12235965122189
Printing some Q and Qe and total Qs values:  [[0.722]
 [0.893]
 [0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]] [[28.017]
 [31.837]
 [28.017]
 [28.017]
 [28.017]
 [28.017]
 [28.017]] [[0.722]
 [0.893]
 [0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]]
siam score:  -0.8227171
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.984]
 [1.008]
 [0.984]
 [0.984]
 [0.984]
 [0.984]
 [0.894]] [[27.791]
 [28.918]
 [27.791]
 [27.791]
 [27.791]
 [27.791]
 [29.185]] [[0.984]
 [1.008]
 [0.984]
 [0.984]
 [0.984]
 [0.984]
 [0.894]]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  51 total reward:  0.3466666666666659  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  27.294255020497477
printing an ep nov before normalisation:  26.374359130859375
printing an ep nov before normalisation:  33.826194954105326
siam score:  -0.8191037
maxi score, test score, baseline:  -0.008753333333333458 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.008753333333333458 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  36.108027390097284
actions average: 
K:  1  action  0 :  tensor([0.4611, 0.0015, 0.0575, 0.0672, 0.2564, 0.0676, 0.0887],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0044, 0.9378, 0.0112, 0.0080, 0.0019, 0.0062, 0.0305],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1231, 0.0110, 0.3831, 0.1125, 0.0949, 0.1681, 0.1074],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1544, 0.0477, 0.1142, 0.2316, 0.1287, 0.1520, 0.1715],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1258, 0.0041, 0.0730, 0.0761, 0.5596, 0.0878, 0.0736],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1928, 0.0059, 0.1333, 0.1438, 0.1816, 0.1859, 0.1566],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1675, 0.0243, 0.1020, 0.1342, 0.0990, 0.1205, 0.3525],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  68 total reward:  0.0066666666666661545  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.281]
 [0.251]
 [0.317]
 [0.304]
 [0.294]
 [0.304]] [[31.792]
 [31.01 ]
 [30.457]
 [30.791]
 [31.792]
 [30.426]
 [31.792]] [[1.272]
 [1.203]
 [1.14 ]
 [1.227]
 [1.272]
 [1.182]
 [1.272]]
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.419]
 [0.293]
 [0.331]
 [0.287]
 [0.293]
 [0.287]] [[22.148]
 [31.156]
 [21.298]
 [24.108]
 [23.605]
 [23.548]
 [21.506]] [[0.713]
 [1.201]
 [0.693]
 [0.84 ]
 [0.777]
 [0.781]
 [0.696]]
maxi score, test score, baseline:  -0.008753333333333458 0.6643333333333334 0.6643333333333334
siam score:  -0.8193179
maxi score, test score, baseline:  -0.008753333333333458 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  30.261741761707032
maxi score, test score, baseline:  -0.008753333333333458 0.6643333333333334 0.6643333333333334
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]] [[20.872]
 [20.872]
 [20.872]
 [20.872]
 [20.872]
 [20.872]
 [20.872]] [[1.792]
 [1.792]
 [1.792]
 [1.792]
 [1.792]
 [1.792]
 [1.792]]
maxi score, test score, baseline:  -0.008753333333333458 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.008753333333333458 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  28.697369419426987
printing an ep nov before normalisation:  0.0007339388662330748
actor:  1 policy actor:  1  step number:  53 total reward:  0.23999999999999977  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  45 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.003]
 [ 0.027]
 [-0.021]
 [-0.011]
 [-0.016]
 [-0.025]
 [-0.008]] [[26.949]
 [23.571]
 [21.436]
 [19.205]
 [20.234]
 [43.623]
 [21.897]] [[0.426]
 [0.371]
 [0.27 ]
 [0.223]
 [0.244]
 [0.823]
 [0.294]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  43 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.01084666666666679 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.01084666666666679 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.01084666666666679 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.01084666666666679 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  46.037634126345
siam score:  -0.80752105
maxi score, test score, baseline:  -0.01084666666666679 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  28.787017306312144
maxi score, test score, baseline:  -0.010846666666666798 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.010846666666666798 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
maxi score, test score, baseline:  -0.010846666666666798 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  27.806099213413013
actor:  0 policy actor:  1  step number:  49 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  40.73400160793573
printing an ep nov before normalisation:  40.44619463339286
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.53 ]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]] [[43.783]
 [45.699]
 [43.783]
 [43.783]
 [43.783]
 [43.783]
 [43.783]] [[1.384]
 [1.533]
 [1.384]
 [1.384]
 [1.384]
 [1.384]
 [1.384]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  38.645228575685294
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  34.20947906554929
actor:  1 policy actor:  1  step number:  45 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  54 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.331100808102526
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[ 0.308]
 [ 0.386]
 [-0.117]
 [ 0.308]
 [ 0.308]
 [ 0.308]
 [ 0.235]] [[29.806]
 [33.825]
 [33.898]
 [29.806]
 [29.806]
 [29.806]
 [30.279]] [[1.033]
 [1.329]
 [0.83 ]
 [1.033]
 [1.033]
 [1.033]
 [0.985]]
Printing some Q and Qe and total Qs values:  [[ 0.319]
 [ 0.37 ]
 [-0.12 ]
 [ 0.308]
 [ 0.265]
 [ 0.308]
 [ 0.235]] [[29.065]
 [34.373]
 [22.599]
 [29.806]
 [29.841]
 [29.806]
 [30.279]] [[1.264]
 [1.487]
 [0.615]
 [1.277]
 [1.235]
 [1.277]
 [1.219]]
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  34.73878860473633
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  27.011547088623047
actor:  1 policy actor:  1  step number:  56 total reward:  0.37999999999999934  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  56 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
printing an ep nov before normalisation:  52.09783943446866
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
printing an ep nov before normalisation:  38.69603238581678
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.546]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]] [[37.776]
 [42.077]
 [37.776]
 [37.776]
 [37.776]
 [37.776]
 [37.776]] [[1.323]
 [1.501]
 [1.323]
 [1.323]
 [1.323]
 [1.323]
 [1.323]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.08534845443868691, 0.08534845443868691, 0.08534845443868691, 0.5611377203003406, 0.08534845443868691, 0.09746846194491174]
actor:  1 policy actor:  1  step number:  50 total reward:  0.39333333333333276  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  49 total reward:  0.4266666666666662  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]]
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.44]
 [0.44]
 [0.44]
 [0.44]
 [0.44]
 [0.44]
 [0.44]] [[18.985]
 [18.985]
 [18.985]
 [18.985]
 [18.985]
 [18.985]
 [18.985]] [[25.747]
 [25.747]
 [25.747]
 [25.747]
 [25.747]
 [25.747]
 [25.747]]
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
actor:  1 policy actor:  1  step number:  62 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  55.92226147714046
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
actions average: 
K:  4  action  0 :  tensor([0.2978, 0.0764, 0.1205, 0.1043, 0.1137, 0.1045, 0.1827],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0291, 0.8807, 0.0173, 0.0176, 0.0138, 0.0136, 0.0279],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1979, 0.0502, 0.2442, 0.1219, 0.1130, 0.1340, 0.1389],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2013, 0.0506, 0.1300, 0.1890, 0.1415, 0.1620, 0.1257],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1406, 0.0527, 0.1088, 0.1054, 0.4178, 0.0560, 0.1187],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1436, 0.0265, 0.1467, 0.1292, 0.1058, 0.3089, 0.1393],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2989, 0.0105, 0.1328, 0.1182, 0.1377, 0.1395, 0.1624],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.607]
 [0.638]
 [0.544]
 [0.595]
 [0.621]
 [0.638]] [[50.922]
 [47.518]
 [42.617]
 [47.875]
 [48.869]
 [49.169]
 [42.617]] [[1.864]
 [1.752]
 [1.603]
 [1.702]
 [1.789]
 [1.826]
 [1.603]]
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
printing an ep nov before normalisation:  45.98735923174999
line 256 mcts: sample exp_bonus 0.009574894277664291
actor:  1 policy actor:  1  step number:  40 total reward:  0.48666666666666647  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.0705565666313
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.642]
 [0.496]
 [0.491]
 [0.509]
 [0.5  ]
 [0.502]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.513]
 [0.642]
 [0.496]
 [0.491]
 [0.509]
 [0.5  ]
 [0.502]]
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
actions average: 
K:  3  action  0 :  tensor([0.3660, 0.1176, 0.1102, 0.1015, 0.1031, 0.1079, 0.0938],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0109, 0.9258, 0.0090, 0.0126, 0.0077, 0.0083, 0.0257],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1492, 0.0562, 0.3687, 0.1026, 0.1003, 0.1136, 0.1094],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1311, 0.0120, 0.1053, 0.3398, 0.1220, 0.1583, 0.1315],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1667, 0.0486, 0.0857, 0.0998, 0.3735, 0.1145, 0.1112],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1140, 0.0261, 0.1310, 0.0860, 0.0707, 0.4988, 0.0734],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1728, 0.1267, 0.1255, 0.1270, 0.1180, 0.1357, 0.1943],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  40 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
printing an ep nov before normalisation:  46.257653119990394
actor:  1 policy actor:  1  step number:  56 total reward:  0.3266666666666661  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
printing an ep nov before normalisation:  47.895244727376
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
using explorer policy with actor:  0
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.58860676491013
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.1859016658079
printing an ep nov before normalisation:  30.80075358690949
actor:  1 policy actor:  1  step number:  32 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.007993333333333451 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
actor:  0 policy actor:  1  step number:  51 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8136937
printing an ep nov before normalisation:  28.995584947333484
printing an ep nov before normalisation:  59.25791694259009
printing an ep nov before normalisation:  57.83900412412213
maxi score, test score, baseline:  -0.005593333333333455 0.6643333333333334 0.6643333333333334
actor:  1 policy actor:  1  step number:  44 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.005593333333333455 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
printing an ep nov before normalisation:  20.08347511291504
actor:  1 policy actor:  1  step number:  44 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.005593333333333455 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
Printing some Q and Qe and total Qs values:  [[0.783]
 [0.686]
 [0.748]
 [0.852]
 [0.885]
 [0.927]
 [0.954]] [[47.138]
 [45.63 ]
 [44.789]
 [46.029]
 [46.712]
 [44.865]
 [44.282]] [[2.018]
 [1.866]
 [1.897]
 [2.046]
 [2.104]
 [2.079]
 [2.083]]
actions average: 
K:  0  action  0 :  tensor([0.5013, 0.0097, 0.0738, 0.1020, 0.1200, 0.1046, 0.0887],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0032, 0.9751, 0.0029, 0.0027, 0.0012, 0.0016, 0.0133],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1537, 0.0062, 0.3508, 0.1156, 0.1100, 0.1464, 0.1174],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1654, 0.0103, 0.0882, 0.4014, 0.1100, 0.1279, 0.0967],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1740, 0.0036, 0.1042, 0.1282, 0.3244, 0.1502, 0.1154],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1004, 0.0020, 0.0795, 0.0719, 0.0707, 0.6175, 0.0579],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1749, 0.0804, 0.1118, 0.1983, 0.0994, 0.1136, 0.2215],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  51.51235448669853
maxi score, test score, baseline:  -0.005593333333333455 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
actor:  1 policy actor:  1  step number:  50 total reward:  0.4199999999999995  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  -0.005593333333333455 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
maxi score, test score, baseline:  -0.005593333333333455 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
maxi score, test score, baseline:  -0.005593333333333448 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
maxi score, test score, baseline:  -0.005593333333333448 0.6643333333333334 0.6643333333333334
maxi score, test score, baseline:  -0.005593333333333448 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.499]
 [0.436]
 [0.423]
 [0.436]
 [0.399]
 [0.408]] [[33.534]
 [41.209]
 [37.16 ]
 [36.377]
 [37.16 ]
 [35.855]
 [33.2  ]] [[1.013]
 [1.407]
 [1.192]
 [1.149]
 [1.192]
 [1.106]
 [1.014]]
printing an ep nov before normalisation:  55.575166964266835
maxi score, test score, baseline:  -0.005593333333333448 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
actor:  1 policy actor:  1  step number:  31 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.005593333333333448 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
maxi score, test score, baseline:  -0.005593333333333448 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
actor:  1 policy actor:  1  step number:  44 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  1.333
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.079]
 [0.113]
 [0.079]
 [0.079]
 [0.079]
 [0.079]
 [0.079]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.079]
 [0.113]
 [0.079]
 [0.079]
 [0.079]
 [0.079]
 [0.079]]
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.499]
 [0.377]
 [0.398]
 [0.383]
 [0.377]
 [0.408]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.377]
 [0.499]
 [0.377]
 [0.398]
 [0.383]
 [0.377]
 [0.408]]
maxi score, test score, baseline:  -0.00559333333333346 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]] [[37.928]
 [37.928]
 [37.928]
 [37.928]
 [37.928]
 [37.928]
 [37.928]] [[0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]]
printing an ep nov before normalisation:  33.453963316790684
Printing some Q and Qe and total Qs values:  [[0.723]
 [0.786]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]] [[47.193]
 [47.597]
 [47.193]
 [47.193]
 [47.193]
 [47.193]
 [47.193]] [[0.723]
 [0.786]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]]
printing an ep nov before normalisation:  53.87148245463386
printing an ep nov before normalisation:  34.41990613937378
printing an ep nov before normalisation:  35.68084645189264
printing an ep nov before normalisation:  33.419265360723934
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
Printing some Q and Qe and total Qs values:  [[0.936]
 [0.966]
 [0.936]
 [0.936]
 [0.936]
 [0.936]
 [0.936]] [[48.223]
 [49.912]
 [48.223]
 [48.223]
 [48.223]
 [48.223]
 [48.223]] [[0.936]
 [0.966]
 [0.936]
 [0.936]
 [0.936]
 [0.936]
 [0.936]]
maxi score, test score, baseline:  -0.00559333333333346 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
printing an ep nov before normalisation:  34.68881970078327
printing an ep nov before normalisation:  47.610444921789615
printing an ep nov before normalisation:  31.473125077765186
Printing some Q and Qe and total Qs values:  [[0.975]
 [0.975]
 [0.975]
 [0.975]
 [0.975]
 [0.975]
 [0.975]] [[46.593]
 [46.593]
 [46.593]
 [46.593]
 [46.593]
 [46.593]
 [46.593]] [[0.975]
 [0.975]
 [0.975]
 [0.975]
 [0.975]
 [0.975]
 [0.975]]
maxi score, test score, baseline:  -0.00559333333333346 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
line 256 mcts: sample exp_bonus 38.813572509177824
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  43.020298531713145
Printing some Q and Qe and total Qs values:  [[0.852]
 [0.954]
 [0.875]
 [0.856]
 [0.841]
 [0.893]
 [0.893]] [[42.808]
 [42.36 ]
 [43.931]
 [42.942]
 [43.58 ]
 [42.866]
 [40.187]] [[0.852]
 [0.954]
 [0.875]
 [0.856]
 [0.841]
 [0.893]
 [0.893]]
printing an ep nov before normalisation:  48.74799314865258
line 256 mcts: sample exp_bonus 42.5921483724347
printing an ep nov before normalisation:  42.87445405722665
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.866]
 [0.807]
 [0.787]
 [0.766]
 [0.844]
 [0.854]] [[41.033]
 [41.947]
 [42.592]
 [42.104]
 [43.063]
 [40.662]
 [39.204]] [[0.765]
 [0.866]
 [0.807]
 [0.787]
 [0.766]
 [0.844]
 [0.854]]
line 256 mcts: sample exp_bonus 44.61956441279523
line 256 mcts: sample exp_bonus 42.936300897281974
printing an ep nov before normalisation:  43.39546514462539
maxi score, test score, baseline:  -0.00559333333333346 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
printing an ep nov before normalisation:  35.97904290075148
Printing some Q and Qe and total Qs values:  [[1.005]
 [0.998]
 [1.008]
 [1.001]
 [0.982]
 [0.909]
 [0.861]] [[33.566]
 [32.857]
 [34.772]
 [34.095]
 [34.378]
 [33.011]
 [30.168]] [[1.005]
 [0.998]
 [1.008]
 [1.001]
 [0.982]
 [0.909]
 [0.861]]
maxi score, test score, baseline:  -0.00559333333333346 0.6643333333333334 0.6643333333333334
probs:  [0.11078104094636058, 0.11078104094636058, 0.11078104094636058, 0.2784379181072789, 0.11078104094636058, 0.2784379181072789]
Printing some Q and Qe and total Qs values:  [[1.003]
 [1.003]
 [1.003]
 [1.003]
 [1.003]
 [1.003]
 [1.003]] [[15.347]
 [10.945]
 [16.205]
 [16.783]
 [36.501]
 [17.01 ]
 [15.973]] [[1.003]
 [1.003]
 [1.003]
 [1.003]
 [1.003]
 [1.003]
 [1.003]]
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  36.59046603109222
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.726]
 [0.855]
 [0.721]
 [0.721]
 [0.729]
 [0.72 ]
 [0.727]] [[31.469]
 [34.608]
 [27.695]
 [27.812]
 [29.716]
 [28.017]
 [30.108]] [[0.726]
 [0.855]
 [0.721]
 [0.721]
 [0.729]
 [0.72 ]
 [0.727]]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  1 policy actor:  1  step number:  38 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.005020000000000116 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.871714603381655
printing an ep nov before normalisation:  32.59993076324463
actor:  0 policy actor:  0  step number:  50 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.385]
 [0.131]
 [0.194]
 [0.261]
 [0.208]
 [0.261]] [[32.358]
 [30.197]
 [29.195]
 [26.822]
 [32.358]
 [24.762]
 [32.358]] [[1.135]
 [1.153]
 [0.85 ]
 [0.798]
 [1.135]
 [0.712]
 [1.135]]
maxi score, test score, baseline:  -0.0057266666666667835 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.68302706657901
printing an ep nov before normalisation:  42.84870176905788
printing an ep nov before normalisation:  32.526349568056645
siam score:  -0.81279814
maxi score, test score, baseline:  -0.0057266666666667835 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0057266666666667835 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  0.1772469553102951
actor:  0 policy actor:  0  step number:  44 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.002886666666666782 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.002886666666666782 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.748822826702344
printing an ep nov before normalisation:  9.93874192237854
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.002886666666666782 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.002886666666666782 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.002886666666666782 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  -0.002886666666666782 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.692398069934658
Printing some Q and Qe and total Qs values:  [[0.98 ]
 [1.009]
 [0.98 ]
 [0.98 ]
 [0.98 ]
 [0.969]
 [0.98 ]] [[51.835]
 [47.693]
 [51.835]
 [51.835]
 [51.835]
 [53.349]
 [51.835]] [[0.98 ]
 [1.009]
 [0.98 ]
 [0.98 ]
 [0.98 ]
 [0.969]
 [0.98 ]]
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.509]
 [0.426]
 [0.431]
 [0.42 ]
 [0.5  ]
 [0.48 ]] [[39.715]
 [47.941]
 [35.629]
 [35.463]
 [36.12 ]
 [45.817]
 [46.174]] [[0.599]
 [0.756]
 [0.557]
 [0.561]
 [0.556]
 [0.727]
 [0.71 ]]
actor:  0 policy actor:  0  step number:  35 total reward:  0.5733333333333334  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  58 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0025133333333332165 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.029]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]] [[45.168]
 [40.577]
 [45.168]
 [45.168]
 [45.168]
 [45.168]
 [45.168]] [[2.278]
 [1.696]
 [2.278]
 [2.278]
 [2.278]
 [2.278]
 [2.278]]
printing an ep nov before normalisation:  35.34375190734863
printing an ep nov before normalisation:  54.62520509331048
maxi score, test score, baseline:  0.0025133333333332165 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.317]
 [0.289]
 [0.332]
 [0.299]
 [0.285]
 [0.283]] [[28.106]
 [43.839]
 [42.561]
 [28.106]
 [44.071]
 [43.599]
 [45.363]] [[0.332]
 [0.317]
 [0.289]
 [0.332]
 [0.299]
 [0.285]
 [0.283]]
siam score:  -0.80616724
printing an ep nov before normalisation:  41.029719843616846
printing an ep nov before normalisation:  32.45675775727819
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.649]
 [0.532]
 [0.532]
 [0.484]
 [0.562]
 [0.628]] [[29.933]
 [31.605]
 [30.1  ]
 [30.1  ]
 [26.331]
 [27.053]
 [26.81 ]] [[1.603]
 [1.838]
 [1.609]
 [1.609]
 [1.278]
 [1.41 ]
 [1.458]]
maxi score, test score, baseline:  0.0025133333333332165 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.16396645390444
actor:  1 policy actor:  1  step number:  49 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0025133333333332165 0.6983333333333335 0.6983333333333335
actor:  1 policy actor:  1  step number:  42 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0025133333333332165 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.0025133333333332165 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  52 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.005006666666666542 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.005006666666666542 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.005006666666666542 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  42.23303689087321
maxi score, test score, baseline:  0.005006666666666542 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.83492024121398
maxi score, test score, baseline:  0.005006666666666542 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.626]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]] [[42.495]
 [41.241]
 [46.592]
 [46.592]
 [46.592]
 [46.592]
 [46.592]] [[1.32 ]
 [1.467]
 [1.683]
 [1.683]
 [1.683]
 [1.683]
 [1.683]]
maxi score, test score, baseline:  0.005006666666666542 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.853304650197664
printing an ep nov before normalisation:  26.783041457136935
printing an ep nov before normalisation:  39.34481478279661
printing an ep nov before normalisation:  0.0036518891778314355
actor:  1 policy actor:  1  step number:  64 total reward:  0.28666666666666585  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  40 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  39.30592063123659
actor:  0 policy actor:  0  step number:  48 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.007526666666666556 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.007526666666666556 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  0.007526666666666556 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.7321621930032052
printing an ep nov before normalisation:  38.80312619423896
maxi score, test score, baseline:  0.007526666666666556 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5133333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.007526666666666556 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.007526666666666556 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.794282232394636
printing an ep nov before normalisation:  29.6576639378494
actor:  0 policy actor:  0  step number:  55 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
siam score:  -0.8088957
Printing some Q and Qe and total Qs values:  [[-0.011]
 [ 0.04 ]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.011]
 [ 0.04 ]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]]
maxi score, test score, baseline:  0.009846666666666542 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.009846666666666542 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.25660455883806
printing an ep nov before normalisation:  56.977079950786816
maxi score, test score, baseline:  0.009846666666666542 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.009846666666666542 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.009846666666666542 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.05999999999999939  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.009846666666666542 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.009846666666666542 0.6983333333333335 0.6983333333333335
siam score:  -0.81470156
siam score:  -0.81480217
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.4  ]
 [0.329]
 [0.332]
 [0.335]
 [0.338]
 [0.329]] [[26.971]
 [28.677]
 [26.46 ]
 [28.114]
 [29.097]
 [30.118]
 [28.304]] [[1.111]
 [1.271]
 [1.075]
 [1.171]
 [1.229]
 [1.291]
 [1.179]]
printing an ep nov before normalisation:  21.23697435562979
maxi score, test score, baseline:  0.009846666666666542 0.6983333333333335 0.6983333333333335
actor:  1 policy actor:  1  step number:  59 total reward:  0.1466666666666664  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.009846666666666542 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  55 total reward:  0.1866666666666661  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  38.78655856601519
maxi score, test score, baseline:  0.012219999999999884 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.231281948007265
printing an ep nov before normalisation:  49.18245333343542
Printing some Q and Qe and total Qs values:  [[0.157]
 [0.264]
 [0.116]
 [0.116]
 [0.155]
 [0.116]
 [0.116]] [[39.731]
 [42.905]
 [46.668]
 [46.668]
 [51.67 ]
 [46.668]
 [46.668]] [[0.909]
 [1.116]
 [1.086]
 [1.086]
 [1.282]
 [1.086]
 [1.086]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.167599482103086
actions average: 
K:  1  action  0 :  tensor([0.3655, 0.0073, 0.1207, 0.1216, 0.1422, 0.1131, 0.1296],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0044, 0.9770, 0.0042, 0.0025, 0.0018, 0.0014, 0.0087],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1502, 0.0433, 0.3105, 0.1002, 0.1236, 0.1378, 0.1344],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1397, 0.0215, 0.0765, 0.4399, 0.1556, 0.0894, 0.0773],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1355, 0.0288, 0.0713, 0.1083, 0.5213, 0.0704, 0.0644],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0768, 0.0273, 0.1193, 0.0947, 0.0801, 0.5019, 0.0998],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1492, 0.1042, 0.0923, 0.1148, 0.1394, 0.1004, 0.2998],
       grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.028]
 [0.028]
 [0.028]
 [0.028]
 [0.028]
 [0.028]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.028]
 [0.028]
 [0.028]
 [0.028]
 [0.028]
 [0.028]
 [0.028]]
maxi score, test score, baseline:  0.012219999999999884 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5400000000000003  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  49.547504446584156
printing an ep nov before normalisation:  28.13532132463709
actor:  0 policy actor:  0  step number:  54 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.804]
 [0.83 ]
 [0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.804]] [[31.036]
 [38.908]
 [31.036]
 [31.036]
 [31.036]
 [31.036]
 [31.036]] [[0.804]
 [0.83 ]
 [0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.804]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
Printing some Q and Qe and total Qs values:  [[0.18 ]
 [0.275]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.18 ]
 [0.18 ]] [[57.86 ]
 [56.244]
 [57.86 ]
 [57.86 ]
 [57.86 ]
 [57.86 ]
 [57.86 ]] [[1.5  ]
 [1.548]
 [1.5  ]
 [1.5  ]
 [1.5  ]
 [1.5  ]
 [1.5  ]]
printing an ep nov before normalisation:  53.50975216461542
maxi score, test score, baseline:  0.012166666666666534 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.303]
 [0.477]
 [0.303]
 [0.303]
 [0.303]
 [0.303]
 [0.303]] [[54.219]
 [53.106]
 [54.219]
 [54.219]
 [54.219]
 [54.219]
 [54.219]] [[1.528]
 [1.669]
 [1.528]
 [1.528]
 [1.528]
 [1.528]
 [1.528]]
actor:  0 policy actor:  0  step number:  44 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.159]
 [0.254]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]] [[27.059]
 [39.24 ]
 [27.059]
 [27.059]
 [27.059]
 [27.059]
 [27.059]] [[0.582]
 [1.048]
 [0.582]
 [0.582]
 [0.582]
 [0.582]
 [0.582]]
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.375]
 [0.264]
 [0.331]
 [0.262]
 [0.261]
 [0.265]] [[47.435]
 [47.98 ]
 [38.223]
 [37.7  ]
 [38.06 ]
 [37.844]
 [40.547]] [[0.999]
 [1.093]
 [0.737]
 [0.792]
 [0.732]
 [0.725]
 [0.796]]
printing an ep nov before normalisation:  29.299524457432053
maxi score, test score, baseline:  0.01513999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.01513999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.17 ]
 [0.068]
 [0.068]
 [0.068]
 [0.095]
 [0.068]] [[35.085]
 [38.889]
 [35.085]
 [35.085]
 [35.085]
 [41.418]
 [35.085]] [[1.057]
 [1.393]
 [1.057]
 [1.057]
 [1.057]
 [1.474]
 [1.057]]
actor:  1 policy actor:  1  step number:  69 total reward:  0.19999999999999962  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.01513999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.024]
 [-0.016]
 [-0.028]
 [-0.024]
 [-0.028]
 [-0.024]
 [-0.026]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.024]
 [-0.016]
 [-0.028]
 [-0.024]
 [-0.028]
 [-0.024]
 [-0.026]]
Printing some Q and Qe and total Qs values:  [[-0.25]
 [-0.25]
 [-0.25]
 [-0.25]
 [-0.25]
 [-0.25]
 [-0.25]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.25]
 [-0.25]
 [-0.25]
 [-0.25]
 [-0.25]
 [-0.25]
 [-0.25]]
maxi score, test score, baseline:  0.01513999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.01513999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.06189353623594
Printing some Q and Qe and total Qs values:  [[0.133]
 [0.224]
 [0.133]
 [0.133]
 [0.133]
 [0.133]
 [0.133]] [[55.715]
 [48.904]
 [55.715]
 [55.715]
 [54.869]
 [55.715]
 [55.715]] [[2.133]
 [1.893]
 [2.133]
 [2.133]
 [2.092]
 [2.133]
 [2.133]]
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.342]
 [0.191]
 [0.191]
 [0.146]
 [0.191]
 [0.191]] [[44.048]
 [44.776]
 [44.048]
 [44.048]
 [43.936]
 [44.048]
 [44.048]] [[1.48 ]
 [1.662]
 [1.48 ]
 [1.48 ]
 [1.43 ]
 [1.48 ]
 [1.48 ]]
maxi score, test score, baseline:  0.01513999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.93049553151391
actor:  1 policy actor:  1  step number:  51 total reward:  0.2933333333333332  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  0.01513999999999987 0.6983333333333335 0.6983333333333335
printing an ep nov before normalisation:  34.345676155846135
printing an ep nov before normalisation:  25.965456937776707
actor:  1 policy actor:  1  step number:  46 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  49.437923898669624
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.177]
 [0.043]
 [0.043]
 [0.043]
 [0.043]
 [0.043]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.043]
 [0.177]
 [0.043]
 [0.043]
 [0.043]
 [0.043]
 [0.043]]
maxi score, test score, baseline:  0.01513999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.01513999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.01513999999999987 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.01513999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.01513999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.364]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]] [[42.628]
 [50.711]
 [42.628]
 [42.628]
 [42.628]
 [42.628]
 [42.628]] [[1.1  ]
 [1.521]
 [1.1  ]
 [1.1  ]
 [1.1  ]
 [1.1  ]
 [1.1  ]]
maxi score, test score, baseline:  0.01513999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.887852012302375
printing an ep nov before normalisation:  38.61755385725141
printing an ep nov before normalisation:  0.03543719674468093
actor:  1 policy actor:  1  step number:  39 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  66 total reward:  0.03333333333333277  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  27.672503081978768
maxi score, test score, baseline:  0.01513999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.01513999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.92506456375122
actor:  1 policy actor:  1  step number:  82 total reward:  0.13999999999999857  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  44.939085001521754
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.252]] [[30.643]
 [30.643]
 [30.643]
 [30.643]
 [30.643]
 [30.643]
 [30.643]] [[0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]
 [0.709]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [0.276]
 [0.1  ]
 [0.1  ]
 [0.1  ]
 [0.1  ]
 [0.1  ]] [[32.68 ]
 [42.998]
 [32.68 ]
 [32.68 ]
 [32.68 ]
 [32.68 ]
 [32.68 ]] [[0.844]
 [1.48 ]
 [0.844]
 [0.844]
 [0.844]
 [0.844]
 [0.844]]
Printing some Q and Qe and total Qs values:  [[0.164]
 [0.299]
 [0.164]
 [0.164]
 [0.134]
 [0.164]
 [0.164]] [[34.53 ]
 [42.166]
 [34.53 ]
 [34.53 ]
 [41.856]
 [34.53 ]
 [34.53 ]] [[1.015]
 [1.517]
 [1.015]
 [1.015]
 [1.336]
 [1.015]
 [1.015]]
printing an ep nov before normalisation:  29.876952171325684
maxi score, test score, baseline:  0.01513999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.28666666666666596  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.01513999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.80680704
maxi score, test score, baseline:  0.01513999999999987 0.6983333333333335 0.6983333333333335
actor:  1 policy actor:  1  step number:  46 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  1  action  0 :  tensor([0.4814, 0.0132, 0.0923, 0.1026, 0.1149, 0.0970, 0.0986],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0052, 0.9439, 0.0082, 0.0124, 0.0034, 0.0032, 0.0237],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1442, 0.0014, 0.3662, 0.1004, 0.0904, 0.1887, 0.1087],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1316, 0.0103, 0.1412, 0.3445, 0.1202, 0.1310, 0.1212],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1585, 0.0186, 0.1073, 0.1160, 0.3633, 0.1095, 0.1267],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1148, 0.0048, 0.0993, 0.1037, 0.1025, 0.4819, 0.0929],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1624, 0.0786, 0.1347, 0.1447, 0.1205, 0.1299, 0.2294],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  81 total reward:  0.1733333333333319  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  28.532600476861077
maxi score, test score, baseline:  0.01513999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  41 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  50.490228341672605
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.624]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.543]
 [0.624]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]]
printing an ep nov before normalisation:  54.03797245977641
maxi score, test score, baseline:  0.017886666666666533 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.705]
 [0.818]
 [0.707]
 [0.704]
 [0.704]
 [0.701]
 [0.713]] [[23.847]
 [29.55 ]
 [23.086]
 [23.183]
 [22.895]
 [23.071]
 [24.002]] [[0.705]
 [0.818]
 [0.707]
 [0.704]
 [0.704]
 [0.701]
 [0.713]]
maxi score, test score, baseline:  0.017886666666666533 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.2782, 0.0167, 0.1398, 0.1347, 0.1446, 0.1442, 0.1418],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0028,     0.9653,     0.0021,     0.0032,     0.0006,     0.0006,
            0.0254], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1256, 0.0079, 0.5258, 0.0601, 0.0738, 0.1087, 0.0980],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2046, 0.1688, 0.1340, 0.1174, 0.1325, 0.1107, 0.1320],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1960, 0.0062, 0.0926, 0.0920, 0.3961, 0.0980, 0.1191],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1114, 0.0059, 0.1603, 0.0638, 0.0798, 0.4891, 0.0898],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1554, 0.1342, 0.1038, 0.0918, 0.1230, 0.1292, 0.2625],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.925]
 [0.95 ]
 [0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]] [[32.715]
 [37.257]
 [32.715]
 [32.715]
 [32.715]
 [32.715]
 [32.715]] [[0.925]
 [0.95 ]
 [0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]]
actor:  0 policy actor:  0  step number:  50 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.020566666666666535 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.4146, 0.0723, 0.0925, 0.0892, 0.1477, 0.0965, 0.0873],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0027, 0.9697, 0.0030, 0.0035, 0.0013, 0.0020, 0.0177],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1759, 0.0323, 0.3119, 0.1057, 0.1181, 0.1382, 0.1180],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1916, 0.0980, 0.1411, 0.1218, 0.1550, 0.1492, 0.1433],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1308, 0.0160, 0.0845, 0.0721, 0.5086, 0.1038, 0.0842],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1227, 0.0107, 0.1205, 0.0885, 0.1320, 0.4260, 0.0995],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1635, 0.1394, 0.1221, 0.0991, 0.1029, 0.1085, 0.2645],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.020566666666666535 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.232178211212158
Printing some Q and Qe and total Qs values:  [[0.188]
 [0.318]
 [0.188]
 [0.188]
 [0.188]
 [0.188]
 [0.188]] [[30.392]
 [31.47 ]
 [30.392]
 [30.392]
 [30.392]
 [30.392]
 [30.392]] [[0.958]
 [1.132]
 [0.958]
 [0.958]
 [0.958]
 [0.958]
 [0.958]]
printing an ep nov before normalisation:  26.19185104068236
printing an ep nov before normalisation:  22.506240700948844
printing an ep nov before normalisation:  24.457762241363525
printing an ep nov before normalisation:  43.2177982646662
actor:  0 policy actor:  0  step number:  57 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  46 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8041153
UNIT TEST: sample policy line 217 mcts : [0.122 0.184 0.061 0.082 0.306 0.163 0.082]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  49.16791911298108
line 256 mcts: sample exp_bonus 44.31045670530874
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  71 total reward:  0.14666666666666528  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.99 ]
 [1.017]
 [0.99 ]
 [0.99 ]
 [0.99 ]
 [0.99 ]
 [0.99 ]] [[38.659]
 [36.033]
 [38.659]
 [38.659]
 [38.659]
 [38.659]
 [38.659]] [[0.99 ]
 [1.017]
 [0.99 ]
 [0.99 ]
 [0.99 ]
 [0.99 ]
 [0.99 ]]
siam score:  -0.80308884
printing an ep nov before normalisation:  31.20171721706156
printing an ep nov before normalisation:  30.84751589952319
actor:  0 policy actor:  0  step number:  44 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  45.713065324988385
actor:  1 policy actor:  1  step number:  70 total reward:  0.13999999999999935  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  43.67715088319979
printing an ep nov before normalisation:  47.213115068838945
actions average: 
K:  0  action  0 :  tensor([0.4639, 0.0085, 0.0965, 0.1075, 0.1045, 0.0899, 0.1291],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0024,     0.9708,     0.0040,     0.0025,     0.0008,     0.0011,
            0.0185], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0105, 0.0025, 0.2549, 0.0854, 0.0223, 0.5710, 0.0535],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1754, 0.0126, 0.1124, 0.2512, 0.1113, 0.1246, 0.2125],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1526, 0.0025, 0.0557, 0.0680, 0.5732, 0.0667, 0.0813],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1635, 0.0099, 0.1504, 0.1188, 0.1138, 0.2981, 0.1455],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1382, 0.0894, 0.1191, 0.1291, 0.1142, 0.1265, 0.2834],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.424]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]] [[28.961]
 [31.791]
 [28.961]
 [28.961]
 [28.961]
 [28.961]
 [28.961]] [[0.659]
 [0.822]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]]
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.641]
 [0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]] [[35.626]
 [43.873]
 [35.626]
 [35.626]
 [35.626]
 [35.626]
 [35.626]] [[0.643]
 [0.641]
 [0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]]
maxi score, test score, baseline:  0.025859999999999862 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  32.54727394132718
maxi score, test score, baseline:  0.025859999999999862 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.49195937429134
printing an ep nov before normalisation:  26.432435018182172
maxi score, test score, baseline:  0.023033333333333204 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.023033333333333204 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.268883224592486
printing an ep nov before normalisation:  34.73422887661377
Printing some Q and Qe and total Qs values:  [[0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]] [[34.845]
 [34.845]
 [34.845]
 [34.845]
 [34.845]
 [34.845]
 [34.845]] [[0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]]
printing an ep nov before normalisation:  30.62673598222869
actor:  0 policy actor:  0  step number:  43 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.59 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]] [[35.476]
 [39.823]
 [32.141]
 [32.141]
 [32.141]
 [32.141]
 [32.141]] [[0.388]
 [0.59 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]]
printing an ep nov before normalisation:  34.505638754477786
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  52.27313983950557
printing an ep nov before normalisation:  18.27614446528
printing an ep nov before normalisation:  28.035009503948753
maxi score, test score, baseline:  0.02053999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.712]
 [0.831]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]] [[34.036]
 [35.563]
 [34.036]
 [34.036]
 [34.036]
 [34.036]
 [34.036]] [[0.957]
 [1.091]
 [0.957]
 [0.957]
 [0.957]
 [0.957]
 [0.957]]
actor:  0 policy actor:  0  step number:  63 total reward:  0.2533333333333323  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.020566666666666535 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.912318283098625
actor:  0 policy actor:  0  step number:  38 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  0  action  0 :  tensor([0.4250, 0.0065, 0.0916, 0.1111, 0.1264, 0.1068, 0.1326],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0039, 0.9652, 0.0048, 0.0031, 0.0010, 0.0020, 0.0200],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2506, 0.0163, 0.3297, 0.0929, 0.0946, 0.0912, 0.1248],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1515, 0.0173, 0.1102, 0.3684, 0.0960, 0.0961, 0.1606],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1348, 0.0207, 0.0647, 0.1387, 0.4684, 0.0669, 0.1059],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1532, 0.0072, 0.1357, 0.1118, 0.1033, 0.3745, 0.1142],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1166, 0.1509, 0.1039, 0.0943, 0.0794, 0.0887, 0.3661],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  57 total reward:  0.11999999999999933  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.574]
 [0.502]
 [0.494]
 [0.494]
 [0.549]
 [0.498]] [[29.524]
 [32.737]
 [28.539]
 [28.351]
 [28.384]
 [31.053]
 [28.614]] [[1.723]
 [2.045]
 [1.646]
 [1.623]
 [1.626]
 [1.889]
 [1.648]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.2666666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.023486666666666545 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  0.015997407741963343
actor:  0 policy actor:  0  step number:  47 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  32.43733533305703
maxi score, test score, baseline:  0.023699999999999867 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.023699999999999867 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.023699999999999867 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.47979199363337
Printing some Q and Qe and total Qs values:  [[-0.013]
 [ 0.039]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.012]
 [-0.006]] [[37.771]
 [38.068]
 [56.993]
 [56.993]
 [56.993]
 [37.279]
 [56.993]] [[1.956]
 [2.039]
 [3.934]
 [3.934]
 [3.934]
 [1.907]
 [3.934]]
printing an ep nov before normalisation:  54.200746283492364
maxi score, test score, baseline:  0.021353333333333203 0.6983333333333335 0.6983333333333335
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]] [[51.604]
 [51.604]
 [51.604]
 [51.604]
 [51.604]
 [51.604]
 [51.604]] [[1.747]
 [1.747]
 [1.747]
 [1.747]
 [1.747]
 [1.747]
 [1.747]]
maxi score, test score, baseline:  0.021353333333333203 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
actor:  1 policy actor:  1  step number:  43 total reward:  0.52  reward:  1.0 rdn_beta:  1.333
siam score:  -0.7984595
actions average: 
K:  4  action  0 :  tensor([0.4564, 0.1370, 0.0576, 0.0767, 0.1185, 0.0583, 0.0956],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0084, 0.9119, 0.0217, 0.0112, 0.0042, 0.0092, 0.0335],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0468, 0.0188, 0.5409, 0.0646, 0.0687, 0.2243, 0.0358],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1708, 0.0523, 0.1562, 0.1546, 0.1343, 0.1734, 0.1585],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1598, 0.0566, 0.0939, 0.1378, 0.2452, 0.1051, 0.2016],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1175, 0.0802, 0.1037, 0.1091, 0.1164, 0.3088, 0.1641],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2139, 0.2307, 0.0660, 0.0835, 0.0588, 0.0529, 0.2941],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.021353333333333203 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.19067072882349
maxi score, test score, baseline:  0.021353333333333203 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.493624613976685
maxi score, test score, baseline:  0.021353333333333203 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 49.08143212631993
siam score:  -0.7943556
maxi score, test score, baseline:  0.021353333333333203 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.021353333333333203 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.7052602881781
maxi score, test score, baseline:  0.021353333333333203 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.461353865322685
printing an ep nov before normalisation:  36.1462147978152
actor:  1 policy actor:  1  step number:  51 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.021353333333333203 0.6983333333333335 0.6983333333333335
actor:  0 policy actor:  0  step number:  56 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.028]
 [-0.017]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.028]
 [-0.017]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.2399394300658
maxi score, test score, baseline:  0.02129999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.02129999999999987 0.6983333333333335 0.6983333333333335
actor:  1 policy actor:  1  step number:  48 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.02129999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.199]
 [0.254]
 [0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]] [[33.468]
 [41.577]
 [33.468]
 [33.468]
 [33.468]
 [33.468]
 [33.468]] [[0.376]
 [0.529]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]]
line 256 mcts: sample exp_bonus 33.6349083530058
printing an ep nov before normalisation:  35.87113378308178
printing an ep nov before normalisation:  36.86965849400568
printing an ep nov before normalisation:  39.562258045896705
printing an ep nov before normalisation:  33.824617862701416
maxi score, test score, baseline:  0.02129999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.02129999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.79704934
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  0.00026621307014806916
printing an ep nov before normalisation:  26.70243263244629
actor:  1 policy actor:  1  step number:  61 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  49.14319521388706
printing an ep nov before normalisation:  40.399157572575895
maxi score, test score, baseline:  0.02129999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.021299999999999875 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.731957431412877
printing an ep nov before normalisation:  32.423917462883466
maxi score, test score, baseline:  0.021299999999999875 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.015969454865398802
actor:  1 policy actor:  1  step number:  43 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.021299999999999875 0.6983333333333335 0.6983333333333335
Printing some Q and Qe and total Qs values:  [[0.685]
 [0.786]
 [0.705]
 [0.696]
 [0.674]
 [0.674]
 [0.71 ]] [[55.277]
 [48.269]
 [50.837]
 [52.425]
 [42.708]
 [42.708]
 [53.302]] [[0.685]
 [0.786]
 [0.705]
 [0.696]
 [0.674]
 [0.674]
 [0.71 ]]
maxi score, test score, baseline:  0.021299999999999875 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.58750954109978
maxi score, test score, baseline:  0.021299999999999875 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.021299999999999875 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.021299999999999875 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.60092043436578
actor:  0 policy actor:  0  step number:  45 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.366]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]] [[28.836]
 [37.469]
 [28.836]
 [28.836]
 [28.836]
 [28.836]
 [28.836]] [[0.899]
 [1.522]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]]
Printing some Q and Qe and total Qs values:  [[0.101]
 [0.122]
 [0.072]
 [0.084]
 [0.089]
 [0.089]
 [0.106]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.101]
 [0.122]
 [0.072]
 [0.084]
 [0.089]
 [0.089]
 [0.106]]
Printing some Q and Qe and total Qs values:  [[0.704]
 [0.812]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]] [[32.372]
 [35.973]
 [32.372]
 [32.372]
 [32.372]
 [32.372]
 [32.372]] [[0.704]
 [0.812]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]]
printing an ep nov before normalisation:  36.67078162124151
printing an ep nov before normalisation:  23.853889917423192
actor:  1 policy actor:  1  step number:  47 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.02081999999999987 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.02081999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.2572386605399
Printing some Q and Qe and total Qs values:  [[0.761]
 [0.855]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]] [[32.725]
 [44.687]
 [32.725]
 [32.725]
 [32.725]
 [32.725]
 [32.725]] [[0.761]
 [0.855]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]]
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.463]
 [0.463]
 [0.463]
 [0.501]
 [0.463]
 [0.463]] [[39.532]
 [40.59 ]
 [40.59 ]
 [40.59 ]
 [41.382]
 [40.59 ]
 [40.59 ]] [[0.638]
 [0.463]
 [0.463]
 [0.463]
 [0.501]
 [0.463]
 [0.463]]
siam score:  -0.8027853
printing an ep nov before normalisation:  37.99892768758362
maxi score, test score, baseline:  0.02081999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.02081999999999987 0.6983333333333335 0.6983333333333335
printing an ep nov before normalisation:  25.55011749267578
printing an ep nov before normalisation:  25.4524474883355
maxi score, test score, baseline:  0.02081999999999987 0.6983333333333335 0.6983333333333335
line 256 mcts: sample exp_bonus 24.76884216070175
actor:  0 policy actor:  0  step number:  70 total reward:  0.07333333333333236  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  51.56008218184243
actor:  0 policy actor:  0  step number:  50 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.02019333333333319 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.02019333333333319 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.4205, 0.1996, 0.0716, 0.0588, 0.0932, 0.0677, 0.0885],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0052, 0.9689, 0.0054, 0.0025, 0.0015, 0.0017, 0.0148],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2211, 0.0489, 0.2318, 0.1043, 0.1176, 0.1674, 0.1088],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1880, 0.0850, 0.1292, 0.2287, 0.1059, 0.1358, 0.1275],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2007, 0.0514, 0.1158, 0.0996, 0.3198, 0.1261, 0.0866],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1298, 0.0287, 0.1607, 0.1204, 0.0783, 0.4115, 0.0706],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1887, 0.0848, 0.0927, 0.1494, 0.1460, 0.0900, 0.2483],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  46 total reward:  0.44666666666666677  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.02019333333333319 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.02019333333333319 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.020193333333333202 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  51 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  27.499995505090116
printing an ep nov before normalisation:  23.863136373952933
printing an ep nov before normalisation:  52.15629612585895
maxi score, test score, baseline:  0.02023333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.619]
 [0.619]
 [0.619]
 [0.619]
 [0.619]
 [0.619]] [[17.184]
 [17.184]
 [17.184]
 [17.184]
 [17.184]
 [17.184]
 [17.184]] [[1.378]
 [1.378]
 [1.378]
 [1.378]
 [1.378]
 [1.378]
 [1.378]]
actor:  1 policy actor:  1  step number:  66 total reward:  0.2199999999999993  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  34 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  49 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  37.17302287651822
printing an ep nov before normalisation:  50.11894846294724
maxi score, test score, baseline:  0.022633333333333203 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.022633333333333203 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.97703947922054
Printing some Q and Qe and total Qs values:  [[0.89 ]
 [0.933]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.89 ]] [[37.327]
 [40.649]
 [37.327]
 [37.327]
 [37.327]
 [37.327]
 [37.327]] [[0.89 ]
 [0.933]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.89 ]]
actor:  0 policy actor:  0  step number:  46 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.025233333333333198 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.025233333333333198 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.025233333333333198 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.025233333333333198 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  55 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.025233333333333198 0.6983333333333335 0.6983333333333335
using explorer policy with actor:  1
maxi score, test score, baseline:  0.025233333333333198 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.025233333333333198 0.6983333333333335 0.6983333333333335
actor:  1 policy actor:  1  step number:  42 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.473]
 [0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.486]] [[35.696]
 [43.542]
 [35.696]
 [35.696]
 [35.696]
 [35.696]
 [35.696]] [[1.429]
 [1.789]
 [1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.429]]
printing an ep nov before normalisation:  51.457971374779085
actions average: 
K:  2  action  0 :  tensor([0.5213, 0.0114, 0.0750, 0.0886, 0.1202, 0.0935, 0.0900],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0069, 0.9654, 0.0049, 0.0041, 0.0026, 0.0027, 0.0134],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0490, 0.0257, 0.6902, 0.0348, 0.0462, 0.1266, 0.0274],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1681, 0.1277, 0.1167, 0.1829, 0.1489, 0.1169, 0.1387],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1745, 0.0008, 0.0540, 0.1391, 0.4349, 0.0921, 0.1045],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1768, 0.0106, 0.1357, 0.1472, 0.1571, 0.2315, 0.1411],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1301, 0.1699, 0.1346, 0.1179, 0.0927, 0.1885, 0.1663],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  49.310175026024496
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.213]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]] [[29.512]
 [44.208]
 [29.512]
 [29.512]
 [29.512]
 [29.512]
 [29.512]] [[0.667]
 [1.071]
 [0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]]
Printing some Q and Qe and total Qs values:  [[0.245]
 [0.36 ]
 [0.245]
 [0.245]
 [0.242]
 [0.245]
 [0.245]] [[52.534]
 [48.637]
 [52.534]
 [52.534]
 [45.8  ]
 [52.534]
 [52.534]] [[1.16 ]
 [1.188]
 [1.16 ]
 [1.16 ]
 [1.008]
 [1.16 ]
 [1.16 ]]
maxi score, test score, baseline:  0.025233333333333198 0.6983333333333335 0.6983333333333335
actor:  1 policy actor:  1  step number:  46 total reward:  0.4600000000000001  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.025233333333333198 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.233]
 [0.174]
 [0.008]
 [0.038]
 [0.01 ]
 [0.009]] [[24.141]
 [35.98 ]
 [34.92 ]
 [24.082]
 [23.439]
 [23.585]
 [24.069]] [[0.116]
 [0.42 ]
 [0.352]
 [0.103]
 [0.129]
 [0.101]
 [0.104]]
maxi score, test score, baseline:  0.025233333333333198 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  31.638750433921814
maxi score, test score, baseline:  0.025233333333333198 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.01785230636597
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.688]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]] [[32.409]
 [41.549]
 [32.409]
 [32.409]
 [32.409]
 [32.409]
 [32.409]] [[1.013]
 [1.355]
 [1.013]
 [1.013]
 [1.013]
 [1.013]
 [1.013]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  53.34272554329947
siam score:  -0.80448824
actor:  1 policy actor:  1  step number:  47 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.265]
 [0.24 ]
 [0.265]
 [0.265]
 [0.265]
 [0.265]
 [0.265]] [[32.637]
 [45.904]
 [32.637]
 [32.637]
 [32.637]
 [32.637]
 [32.637]] [[1.016]
 [1.555]
 [1.016]
 [1.016]
 [1.016]
 [1.016]
 [1.016]]
maxi score, test score, baseline:  0.025233333333333198 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.075]
 [0.187]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]] [[34.704]
 [38.113]
 [34.704]
 [34.704]
 [34.704]
 [34.704]
 [34.704]] [[0.424]
 [0.6  ]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]]
maxi score, test score, baseline:  0.025233333333333198 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  41 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  30.18095805012267
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.026]
 [ 0.303]
 [-0.026]
 [-0.026]
 [-0.088]
 [-0.026]] [[39.414]
 [39.414]
 [41.372]
 [39.414]
 [39.414]
 [37.19 ]
 [39.414]] [[1.33 ]
 [1.33 ]
 [1.79 ]
 [1.33 ]
 [1.33 ]
 [1.119]
 [1.33 ]]
Printing some Q and Qe and total Qs values:  [[0.25 ]
 [0.394]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]] [[47.726]
 [50.412]
 [47.726]
 [47.726]
 [47.726]
 [47.726]
 [47.726]] [[0.786]
 [0.976]
 [0.786]
 [0.786]
 [0.786]
 [0.786]
 [0.786]]
maxi score, test score, baseline:  0.02821999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 47.19512542891502
printing an ep nov before normalisation:  4.7398945639542944e-05
printing an ep nov before normalisation:  19.766115699437133
maxi score, test score, baseline:  0.028219999999999867 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.84639517166158
Printing some Q and Qe and total Qs values:  [[0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]] [[29.208]
 [29.208]
 [29.208]
 [29.208]
 [29.208]
 [29.208]
 [29.208]] [[0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]]
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.52 ]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]] [[47.566]
 [47.805]
 [47.566]
 [47.566]
 [47.566]
 [47.566]
 [47.566]] [[1.934]
 [1.937]
 [1.934]
 [1.934]
 [1.934]
 [1.934]
 [1.934]]
maxi score, test score, baseline:  0.028219999999999867 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.312552986622702
printing an ep nov before normalisation:  47.41023637078255
printing an ep nov before normalisation:  44.758611683604286
printing an ep nov before normalisation:  49.29981300558831
printing an ep nov before normalisation:  20.53147754989107
actor:  1 policy actor:  1  step number:  42 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  45.342494954691524
maxi score, test score, baseline:  0.028219999999999867 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.028219999999999867 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.29333333333333234  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.028219999999999867 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.028219999999999867 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.731]
 [0.797]
 [0.776]
 [0.776]
 [0.723]
 [0.776]
 [0.752]] [[42.646]
 [46.542]
 [38.514]
 [38.514]
 [34.287]
 [38.514]
 [39.34 ]] [[0.731]
 [0.797]
 [0.776]
 [0.776]
 [0.723]
 [0.776]
 [0.752]]
maxi score, test score, baseline:  0.028219999999999867 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.028219999999999867 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
actor:  0 policy actor:  0  step number:  51 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8074524
siam score:  -0.8075632
printing an ep nov before normalisation:  50.628493683581084
maxi score, test score, baseline:  0.02825999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.33158104330737
printing an ep nov before normalisation:  41.03503378950385
printing an ep nov before normalisation:  46.93243198300411
printing an ep nov before normalisation:  34.96680830372226
maxi score, test score, baseline:  0.02825999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.02825999999999987 0.6983333333333335 0.6983333333333335
actor:  1 policy actor:  1  step number:  59 total reward:  0.30666666666666575  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  54 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  60 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  51.353903291437504
maxi score, test score, baseline:  0.030859999999999877 0.6983333333333335 0.6983333333333335
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  0.030859999999999877 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.622]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]] [[37.754]
 [29.119]
 [37.754]
 [37.754]
 [37.754]
 [37.754]
 [37.754]] [[1.524]
 [1.296]
 [1.524]
 [1.524]
 [1.524]
 [1.524]
 [1.524]]
maxi score, test score, baseline:  0.030859999999999877 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.714044167267993
printing an ep nov before normalisation:  27.92503180307794
siam score:  -0.80906105
maxi score, test score, baseline:  0.030859999999999877 0.6983333333333335 0.6983333333333335
actor:  1 policy actor:  1  step number:  42 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.81926055707262
maxi score, test score, baseline:  0.030859999999999877 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.030859999999999877 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.030859999999999877 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8079278
printing an ep nov before normalisation:  34.0357196423207
Printing some Q and Qe and total Qs values:  [[0.921]
 [0.934]
 [0.962]
 [0.947]
 [0.941]
 [0.934]
 [0.943]] [[33.093]
 [33.479]
 [34.417]
 [33.128]
 [34.305]
 [33.833]
 [32.606]] [[0.921]
 [0.934]
 [0.962]
 [0.947]
 [0.941]
 [0.934]
 [0.943]]
maxi score, test score, baseline:  0.030859999999999877 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  35 total reward:  0.52  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  59 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.031513333333333206 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.031513333333333206 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.031513333333333206 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.107536124663795
printing an ep nov before normalisation:  40.1595878081695
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  0.031513333333333206 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.459]
 [0.476]
 [0.487]
 [0.487]
 [0.478]
 [0.473]] [[36.912]
 [37.087]
 [35.397]
 [36.096]
 [36.081]
 [36.067]
 [36.347]] [[2.179]
 [2.126]
 [1.995]
 [2.067]
 [2.066]
 [2.056]
 [2.075]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.33333333333333337  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.031513333333333206 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.6069, 0.0064, 0.0739, 0.0643, 0.0940, 0.0640, 0.0905],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0024,     0.9718,     0.0047,     0.0033,     0.0006,     0.0014,
            0.0158], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1230, 0.0256, 0.5030, 0.0712, 0.0708, 0.1129, 0.0935],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1936, 0.0398, 0.1499, 0.1609, 0.1586, 0.1612, 0.1360],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1751, 0.0045, 0.1043, 0.1023, 0.3731, 0.0961, 0.1446],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1257, 0.0257, 0.1088, 0.1024, 0.0899, 0.4303, 0.1172],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1883, 0.1098, 0.1257, 0.1307, 0.1231, 0.1118, 0.2105],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.17081055963029
maxi score, test score, baseline:  0.031513333333333206 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  32.91292247517839
printing an ep nov before normalisation:  42.70780463188199
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.363]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]] [[36.657]
 [37.272]
 [36.657]
 [36.657]
 [36.657]
 [36.657]
 [36.657]] [[1.214]
 [1.256]
 [1.214]
 [1.214]
 [1.214]
 [1.214]
 [1.214]]
printing an ep nov before normalisation:  30.618051591459363
maxi score, test score, baseline:  0.03151333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03151333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.84396131511611
printing an ep nov before normalisation:  52.75472416761947
maxi score, test score, baseline:  0.03151333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.03151333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  0.153333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.03151333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03151333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]] [[42.088]
 [42.088]
 [42.088]
 [42.088]
 [42.088]
 [42.088]
 [42.088]] [[0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]]
maxi score, test score, baseline:  0.03151333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  72 total reward:  0.2999999999999988  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  37 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.03157999999999988 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03157999999999988 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.3053163496453
printing an ep nov before normalisation:  23.762296543210507
printing an ep nov before normalisation:  28.919707094796877
printing an ep nov before normalisation:  23.739798069000244
maxi score, test score, baseline:  0.03157999999999988 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.28656898309211
actor:  1 policy actor:  1  step number:  70 total reward:  0.04666666666666586  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.03157999999999988 0.6983333333333335 0.6983333333333335
printing an ep nov before normalisation:  18.201256134274345
maxi score, test score, baseline:  0.03157999999999988 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.03157999999999988 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.67375302007199
actor:  0 policy actor:  0  step number:  55 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.03411333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03411333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.633]
 [0.541]
 [0.571]
 [0.544]
 [0.528]
 [0.541]] [[29.398]
 [21.459]
 [26.84 ]
 [29.517]
 [29.772]
 [29.945]
 [26.84 ]] [[1.296]
 [1.195]
 [1.246]
 [1.348]
 [1.328]
 [1.316]
 [1.246]]
printing an ep nov before normalisation:  28.795876502990723
siam score:  -0.80256927
maxi score, test score, baseline:  0.03411333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  69 total reward:  0.0666666666666661  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.387]
 [0.459]
 [0.438]
 [0.529]
 [0.459]
 [0.475]] [[43.721]
 [44.94 ]
 [40.444]
 [42.332]
 [42.047]
 [40.444]
 [46.363]] [[1.036]
 [0.948]
 [0.94 ]
 [0.953]
 [1.038]
 [0.94 ]
 [1.061]]
maxi score, test score, baseline:  0.03411333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  61 total reward:  0.10666666666666602  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.03411333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.501]
 [0.309]
 [0.298]
 [0.297]
 [0.295]
 [0.298]] [[27.494]
 [30.899]
 [30.181]
 [30.456]
 [32.702]
 [32.655]
 [29.198]] [[0.294]
 [0.501]
 [0.309]
 [0.298]
 [0.297]
 [0.295]
 [0.298]]
maxi score, test score, baseline:  0.03141999999999988 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.56224933936186
Printing some Q and Qe and total Qs values:  [[ 0.002]
 [-0.017]
 [ 0.002]
 [ 0.002]
 [ 0.002]
 [ 0.002]
 [ 0.002]] [[26.122]
 [27.247]
 [26.122]
 [26.122]
 [26.122]
 [26.122]
 [26.122]] [[1.325]
 [1.398]
 [1.325]
 [1.325]
 [1.325]
 [1.325]
 [1.325]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.02660666666666654 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.02660666666666654 0.6983333333333335 0.6983333333333335
printing an ep nov before normalisation:  46.340083612493416
maxi score, test score, baseline:  0.02660666666666654 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.02660666666666654 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.11576295438886
printing an ep nov before normalisation:  26.731971609926084
siam score:  -0.79683423
actor:  1 policy actor:  1  step number:  45 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  53 total reward:  0.38666666666666605  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.003]
 [ 0.006]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[34.465]
 [46.304]
 [34.465]
 [34.465]
 [34.465]
 [34.465]
 [34.465]] [[0.795]
 [1.339]
 [0.795]
 [0.795]
 [0.795]
 [0.795]
 [0.795]]
Printing some Q and Qe and total Qs values:  [[-0.008]
 [ 0.011]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]] [[45.548]
 [36.485]
 [33.003]
 [33.003]
 [33.003]
 [33.003]
 [33.003]] [[0.903]
 [0.678]
 [0.564]
 [0.564]
 [0.564]
 [0.564]
 [0.564]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.02660666666666654 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.93003329910611
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.0764373143514
printing an ep nov before normalisation:  22.32081651687622
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]] [[38.655]
 [38.655]
 [38.655]
 [38.655]
 [38.655]
 [38.655]
 [38.655]] [[0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]]
printing an ep nov before normalisation:  32.383836887023456
printing an ep nov before normalisation:  48.04441404215463
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  24.560426229076104
printing an ep nov before normalisation:  21.696801911675543
actor:  0 policy actor:  0  step number:  39 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.029486666666666536 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.07247772778083
actor:  1 policy actor:  1  step number:  59 total reward:  0.13333333333333297  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  57 total reward:  0.39999999999999936  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.00383225912173657
actor:  0 policy actor:  0  step number:  38 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0325133333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0325133333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.862313270568848
printing an ep nov before normalisation:  27.012258309519623
siam score:  -0.80327296
maxi score, test score, baseline:  0.0325133333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.194]
 [0.233]
 [0.196]
 [0.196]
 [0.201]
 [0.201]
 [0.199]] [[35.539]
 [30.178]
 [36.386]
 [36.386]
 [35.022]
 [33.885]
 [37.259]] [[1.671]
 [1.302]
 [1.738]
 [1.738]
 [1.639]
 [1.552]
 [1.807]]
actor:  1 policy actor:  1  step number:  62 total reward:  0.2599999999999988  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  40 total reward:  0.5933333333333336  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0325133333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.139]
 [0.198]
 [0.139]
 [0.17 ]
 [0.139]
 [0.139]
 [0.139]] [[24.906]
 [29.935]
 [24.906]
 [30.63 ]
 [24.906]
 [24.906]
 [24.906]] [[0.93 ]
 [1.454]
 [0.93 ]
 [1.49 ]
 [0.93 ]
 [0.93 ]
 [0.93 ]]
maxi score, test score, baseline:  0.0325133333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.654469720693545
maxi score, test score, baseline:  0.0325133333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.72404420261207
actor:  1 policy actor:  1  step number:  59 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0325133333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0325133333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.33999999999999997  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  57 total reward:  0.2533333333333324  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  36.09235763549805
actor:  0 policy actor:  0  step number:  44 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0352733333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.64569871894854
maxi score, test score, baseline:  0.0352733333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.20150139520847
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0352733333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0352733333333332 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.0352733333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]] [[38.976]
 [38.976]
 [38.976]
 [38.976]
 [38.976]
 [38.976]
 [38.976]] [[0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]]
maxi score, test score, baseline:  0.0352733333333332 0.6983333333333335 0.6983333333333335
printing an ep nov before normalisation:  36.03592105570632
Printing some Q and Qe and total Qs values:  [[0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]] [[31.881]
 [31.881]
 [31.881]
 [31.881]
 [31.881]
 [31.881]
 [31.881]] [[0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]]
printing an ep nov before normalisation:  37.30531768476857
maxi score, test score, baseline:  0.0352733333333332 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.0352733333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.617344856262207
actor:  1 policy actor:  1  step number:  48 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0352733333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.759]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]] [[23.32 ]
 [27.843]
 [23.894]
 [23.894]
 [23.894]
 [23.894]
 [23.894]] [[0.64 ]
 [0.759]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]]
printing an ep nov before normalisation:  20.446955004471974
actor:  0 policy actor:  0  step number:  62 total reward:  0.04666666666666608  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  14.635987071662631
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.525]
 [0.429]
 [0.437]
 [0.419]
 [0.422]
 [0.423]] [[17.181]
 [15.038]
 [15.824]
 [15.822]
 [15.943]
 [17.649]
 [17.767]] [[1.982]
 [1.899]
 [1.875]
 [1.883]
 [1.875]
 [2.035]
 [2.047]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2733333333333332  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 30.34426223239747
maxi score, test score, baseline:  0.032419999999999866 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
using explorer policy with actor:  1
maxi score, test score, baseline:  0.032419999999999866 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.01105692435299
printing an ep nov before normalisation:  35.97519457558272
printing an ep nov before normalisation:  42.48095422410699
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.425]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.377]] [[52.327]
 [43.203]
 [52.327]
 [52.327]
 [52.327]
 [52.327]
 [52.327]] [[1.044]
 [0.943]
 [1.044]
 [1.044]
 [1.044]
 [1.044]
 [1.044]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.868483995817115
printing an ep nov before normalisation:  36.159508368144294
actor:  0 policy actor:  0  step number:  47 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.697]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]] [[32.297]
 [35.331]
 [32.297]
 [32.297]
 [32.297]
 [32.297]
 [32.297]] [[0.823]
 [0.927]
 [0.823]
 [0.823]
 [0.823]
 [0.823]
 [0.823]]
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.364]
 [0.216]
 [0.342]
 [0.267]
 [0.283]
 [0.349]] [[35.84 ]
 [38.801]
 [34.576]
 [40.864]
 [40.032]
 [41.777]
 [37.285]] [[1.167]
 [1.399]
 [1.031]
 [1.484]
 [1.366]
 [1.473]
 [1.305]]
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.414]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]] [[41.498]
 [44.173]
 [41.498]
 [41.498]
 [41.498]
 [41.498]
 [41.498]] [[1.24 ]
 [1.549]
 [1.24 ]
 [1.24 ]
 [1.24 ]
 [1.24 ]
 [1.24 ]]
printing an ep nov before normalisation:  34.36922763837304
printing an ep nov before normalisation:  53.31756320660536
printing an ep nov before normalisation:  38.10652905515703
actor:  1 policy actor:  1  step number:  44 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.108]
 [0.22 ]
 [0.115]
 [0.134]
 [0.125]
 [0.119]
 [0.134]] [[35.053]
 [38.545]
 [34.621]
 [35.012]
 [33.389]
 [33.419]
 [35.012]] [[0.738]
 [0.95 ]
 [0.734]
 [0.763]
 [0.708]
 [0.703]
 [0.763]]
Printing some Q and Qe and total Qs values:  [[0.112]
 [0.318]
 [0.101]
 [0.094]
 [0.094]
 [0.092]
 [0.096]] [[30.311]
 [35.54 ]
 [29.144]
 [29.049]
 [29.256]
 [31.766]
 [29.299]] [[0.645]
 [1.038]
 [0.592]
 [0.581]
 [0.588]
 [0.677]
 [0.592]]
printing an ep nov before normalisation:  32.06442919743897
printing an ep nov before normalisation:  34.193671975368844
actor:  1 policy actor:  1  step number:  42 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([0.4343, 0.0067, 0.1069, 0.0847, 0.1348, 0.0982, 0.1345],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0103, 0.9209, 0.0170, 0.0224, 0.0041, 0.0040, 0.0213],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0983, 0.0013, 0.4770, 0.0764, 0.1061, 0.1734, 0.0674],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1577, 0.0317, 0.0998, 0.2418, 0.1125, 0.1168, 0.2396],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2454, 0.0070, 0.1556, 0.1298, 0.1870, 0.1489, 0.1263],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2006, 0.0008, 0.0866, 0.0833, 0.0975, 0.4695, 0.0616],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1456, 0.0846, 0.1035, 0.1396, 0.1372, 0.1134, 0.2760],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  34.71061448550327
printing an ep nov before normalisation:  36.24672487320901
printing an ep nov before normalisation:  33.08757914904952
printing an ep nov before normalisation:  24.43095684004407
printing an ep nov before normalisation:  23.75195152872903
maxi score, test score, baseline:  0.030339999999999867 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.03267765045166
actions average: 
K:  1  action  0 :  tensor([0.3191, 0.0659, 0.1105, 0.1087, 0.1419, 0.1051, 0.1487],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0058, 0.9554, 0.0077, 0.0065, 0.0027, 0.0048, 0.0172],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1177, 0.0161, 0.4710, 0.0814, 0.0831, 0.1061, 0.1245],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1997, 0.0551, 0.1149, 0.2837, 0.1078, 0.0978, 0.1411],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1839, 0.0325, 0.1035, 0.0997, 0.3505, 0.1103, 0.1196],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1234, 0.0171, 0.1391, 0.1467, 0.0997, 0.3804, 0.0934],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1833, 0.1172, 0.1066, 0.1596, 0.1351, 0.1034, 0.1948],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  44.50298309326172
actor:  1 policy actor:  1  step number:  46 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.852]
 [0.852]
 [0.852]
 [0.852]
 [0.852]
 [0.852]
 [0.852]] [[37.689]
 [37.689]
 [37.689]
 [37.689]
 [37.689]
 [37.689]
 [37.689]] [[0.852]
 [0.852]
 [0.852]
 [0.852]
 [0.852]
 [0.852]
 [0.852]]
maxi score, test score, baseline:  0.030339999999999867 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.030339999999999867 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.030339999999999867 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  39 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  33.806902531650074
printing an ep nov before normalisation:  31.850557567446035
printing an ep nov before normalisation:  37.943922198989746
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.31754292313435
printing an ep nov before normalisation:  26.02695176120161
actor:  1 policy actor:  1  step number:  35 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.027]
 [ 0.046]
 [-0.036]
 [-0.004]
 [-0.014]
 [-0.082]
 [ 0.002]] [[26.253]
 [34.472]
 [24.66 ]
 [22.106]
 [20.568]
 [22.402]
 [25.194]] [[0.327]
 [0.687]
 [0.262]
 [0.205]
 [0.141]
 [0.137]
 [0.319]]
maxi score, test score, baseline:  0.03053999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03053999999999987 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.03053999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.84388422439845
maxi score, test score, baseline:  0.03053999999999987 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.03053999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.554162049198215
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.548]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.753]] [[17.566]
 [19.835]
 [17.566]
 [17.566]
 [17.566]
 [17.566]
 [18.809]] [[2.14 ]
 [2.314]
 [2.14 ]
 [2.14 ]
 [2.14 ]
 [2.14 ]
 [2.428]]
actions average: 
K:  0  action  0 :  tensor([0.4025, 0.0051, 0.1089, 0.1161, 0.1326, 0.1218, 0.1131],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0023,     0.9572,     0.0013,     0.0017,     0.0010,     0.0007,
            0.0358], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1376, 0.0688, 0.2503, 0.0995, 0.1164, 0.1150, 0.2125],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1616, 0.0106, 0.0920, 0.2847, 0.1601, 0.1439, 0.1472],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2003, 0.0051, 0.1106, 0.0837, 0.3701, 0.0920, 0.1382],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1172, 0.0342, 0.0974, 0.0842, 0.1002, 0.4658, 0.1010],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1643, 0.1228, 0.1096, 0.1067, 0.1216, 0.1103, 0.2648],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  39.269305617977324
maxi score, test score, baseline:  0.03053999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03053999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  0.3399999999999991  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.343]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]] [[34.476]
 [38.101]
 [34.476]
 [34.476]
 [34.476]
 [34.476]
 [34.476]] [[0.871]
 [1.153]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]]
actions average: 
K:  3  action  0 :  tensor([0.4920, 0.0116, 0.0689, 0.1192, 0.1051, 0.1146, 0.0887],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0041, 0.9526, 0.0039, 0.0115, 0.0014, 0.0021, 0.0244],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1611, 0.0093, 0.2891, 0.1179, 0.1385, 0.1839, 0.1003],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2096, 0.0444, 0.1147, 0.1752, 0.1214, 0.1623, 0.1725],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1867, 0.0109, 0.0914, 0.1024, 0.3618, 0.1433, 0.1035],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2127, 0.0103, 0.1192, 0.1416, 0.1425, 0.2319, 0.1418],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0679, 0.3410, 0.0470, 0.2365, 0.0565, 0.0700, 0.1811],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  59 total reward:  0.3066666666666661  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.03053999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.397]
 [0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]] [[32.462]
 [36.849]
 [32.462]
 [32.462]
 [32.462]
 [32.462]
 [32.462]] [[0.782]
 [1.033]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.782]]
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.435]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]] [[28.701]
 [29.607]
 [29.838]
 [29.838]
 [29.838]
 [29.838]
 [29.838]] [[1.139]
 [1.18 ]
 [1.216]
 [1.216]
 [1.216]
 [1.216]
 [1.216]]
printing an ep nov before normalisation:  23.658077716827393
maxi score, test score, baseline:  0.028113333333333206 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.028113333333333206 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.028113333333333206 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.79801726
maxi score, test score, baseline:  0.028113333333333206 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  45 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  1.333
siam score:  -0.7921252
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  13.959803586043641
printing an ep nov before normalisation:  30.008686508093266
printing an ep nov before normalisation:  32.30864189107379
maxi score, test score, baseline:  0.028113333333333206 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.33481345807205
using explorer policy with actor:  1
maxi score, test score, baseline:  0.028113333333333206 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.373]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.351]] [[28.1  ]
 [35.104]
 [28.1  ]
 [28.1  ]
 [28.1  ]
 [28.1  ]
 [36.922]] [[1.13 ]
 [1.744]
 [1.13 ]
 [1.13 ]
 [1.13 ]
 [1.13 ]
 [1.867]]
actor:  0 policy actor:  0  step number:  44 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  36.15910807681942
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.028806666666666533 0.6983333333333335 0.6983333333333335
actor:  1 policy actor:  1  step number:  58 total reward:  0.23333333333333273  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.028806666666666533 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.028806666666666533 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]] [[53.576]
 [46.303]
 [46.303]
 [46.303]
 [46.303]
 [46.303]
 [46.303]] [[2.191]
 [1.94 ]
 [1.94 ]
 [1.94 ]
 [1.94 ]
 [1.94 ]
 [1.94 ]]
printing an ep nov before normalisation:  35.79907283545648
maxi score, test score, baseline:  0.028806666666666533 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.056]
 [ 0.115]
 [-0.006]
 [ 0.025]
 [ 0.01 ]
 [ 0.021]
 [-0.006]] [[45.269]
 [41.957]
 [41.223]
 [46.389]
 [45.188]
 [44.99 ]
 [41.223]] [[1.623]
 [1.516]
 [1.358]
 [1.648]
 [1.573]
 [1.574]
 [1.358]]
printing an ep nov before normalisation:  30.587455760804506
maxi score, test score, baseline:  0.028806666666666533 0.6983333333333335 0.6983333333333335
actor:  1 policy actor:  1  step number:  41 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  23.964977264404297
maxi score, test score, baseline:  0.028806666666666533 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.0933333333333326  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.028806666666666533 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.028806666666666533 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.028806666666666533 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.133198922715124
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  54.86996855439886
maxi score, test score, baseline:  0.028806666666666533 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.028806666666666533 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.028806666666666533 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.24666666666666592  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  55 total reward:  0.34666666666666657  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  58 total reward:  0.3133333333333326  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.26116751371283
maxi score, test score, baseline:  0.028806666666666533 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.530481456338357
printing an ep nov before normalisation:  26.847310066223145
Printing some Q and Qe and total Qs values:  [[0.146]
 [0.242]
 [0.146]
 [0.146]
 [0.146]
 [0.146]
 [0.146]] [[29.985]
 [27.51 ]
 [29.985]
 [29.985]
 [29.985]
 [29.985]
 [29.781]] [[1.155]
 [1.095]
 [1.155]
 [1.155]
 [1.155]
 [1.155]
 [1.143]]
Printing some Q and Qe and total Qs values:  [[ 0.206]
 [ 0.385]
 [-0.146]
 [ 0.206]
 [ 0.206]
 [ 0.206]
 [ 0.206]] [[35.341]
 [39.415]
 [37.284]
 [35.341]
 [35.341]
 [35.341]
 [35.341]] [[0.947]
 [1.286]
 [0.671]
 [0.947]
 [0.947]
 [0.947]
 [0.947]]
maxi score, test score, baseline:  0.028806666666666533 0.6983333333333335 0.6983333333333335
actor:  1 policy actor:  1  step number:  46 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  2  action  0 :  tensor([0.3411, 0.0977, 0.0731, 0.1246, 0.1341, 0.0981, 0.1313],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0106, 0.9292, 0.0042, 0.0118, 0.0164, 0.0013, 0.0265],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1448, 0.0128, 0.3414, 0.1133, 0.0885, 0.1143, 0.1850],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1432, 0.0932, 0.0691, 0.3105, 0.1235, 0.0682, 0.1922],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2449, 0.0053, 0.0776, 0.1273, 0.3485, 0.1028, 0.0935],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0996, 0.0239, 0.1029, 0.0877, 0.1176, 0.4885, 0.0798],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2726, 0.0073, 0.1242, 0.1691, 0.1290, 0.1404, 0.1575],
       grad_fn=<DivBackward0>)
actions average: 
K:  4  action  0 :  tensor([0.4627, 0.0169, 0.1084, 0.1108, 0.1019, 0.0955, 0.1038],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0103, 0.9398, 0.0072, 0.0078, 0.0042, 0.0027, 0.0280],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1447, 0.0055, 0.2648, 0.2020, 0.0894, 0.1183, 0.1754],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2466, 0.0496, 0.1082, 0.1870, 0.1342, 0.1368, 0.1375],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.4092, 0.0532, 0.0305, 0.0151, 0.3946, 0.0079, 0.0897],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1296, 0.0824, 0.0992, 0.1217, 0.0992, 0.3398, 0.1281],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2043, 0.0123, 0.1576, 0.1485, 0.1466, 0.1561, 0.1746],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.028806666666666533 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.547]
 [0.447]
 [0.533]
 [0.458]
 [0.44 ]
 [0.449]] [[32.31 ]
 [37.99 ]
 [32.009]
 [37.553]
 [30.41 ]
 [30.386]
 [31.065]] [[0.961]
 [1.331]
 [0.945]
 [1.296]
 [0.88 ]
 [0.861]
 [0.903]]
maxi score, test score, baseline:  0.028806666666666533 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.028806666666666533 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.656]] [[32.242]
 [32.242]
 [32.242]
 [32.242]
 [32.242]
 [32.242]
 [32.242]] [[1.885]
 [1.885]
 [1.885]
 [1.885]
 [1.885]
 [1.885]
 [1.885]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.737]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]] [[29.769]
 [34.392]
 [29.769]
 [29.769]
 [29.769]
 [29.769]
 [29.769]] [[1.065]
 [1.247]
 [1.065]
 [1.065]
 [1.065]
 [1.065]
 [1.065]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  0.5  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.028806666666666533 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  47.39858369443471
maxi score, test score, baseline:  0.028806666666666533 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  67 total reward:  0.026666666666665506  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.028806666666666533 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.028806666666666533 0.6983333333333335 0.6983333333333335
Printing some Q and Qe and total Qs values:  [[0.74 ]
 [0.856]
 [0.703]
 [0.861]
 [0.74 ]
 [0.74 ]
 [0.822]] [[27.478]
 [33.562]
 [29.909]
 [32.115]
 [27.478]
 [27.478]
 [31.57 ]] [[0.74 ]
 [0.856]
 [0.703]
 [0.861]
 [0.74 ]
 [0.74 ]
 [0.822]]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  36 total reward:  0.48666666666666647  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.028753333333333204 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.102455139160156
printing an ep nov before normalisation:  11.931574666436102
printing an ep nov before normalisation:  30.99307436843713
Printing some Q and Qe and total Qs values:  [[0.631]
 [0.665]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]] [[28.786]
 [30.555]
 [27.817]
 [27.817]
 [27.817]
 [27.817]
 [27.817]] [[1.948]
 [2.158]
 [1.74 ]
 [1.74 ]
 [1.74 ]
 [1.74 ]
 [1.74 ]]
printing an ep nov before normalisation:  31.948830475521298
actor:  1 policy actor:  1  step number:  44 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  51 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.031126666666666542 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  64 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03361999999999987 0.6983333333333335 0.6983333333333335
actor:  1 policy actor:  1  step number:  48 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [ 0.019]
 [-0.024]
 [-0.006]
 [-0.012]
 [-0.013]
 [-0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.01 ]
 [ 0.019]
 [-0.024]
 [-0.006]
 [-0.012]
 [-0.013]
 [-0.002]]
actions average: 
K:  0  action  0 :  tensor([0.4877, 0.0022, 0.0923, 0.1018, 0.1068, 0.0869, 0.1222],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0039, 0.9593, 0.0029, 0.0046, 0.0011, 0.0016, 0.0265],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1301, 0.1112, 0.3769, 0.0863, 0.0834, 0.1178, 0.0943],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1125, 0.1952, 0.0518, 0.3819, 0.1014, 0.0762, 0.0810],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1353, 0.0068, 0.0739, 0.0832, 0.5215, 0.0625, 0.1166],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1288, 0.0031, 0.1493, 0.1009, 0.0773, 0.4484, 0.0921],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1538, 0.0935, 0.1103, 0.1547, 0.1032, 0.1330, 0.2514],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.03361999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.56332015991211
printing an ep nov before normalisation:  42.19650965123455
maxi score, test score, baseline:  0.03361999999999987 0.6983333333333335 0.6983333333333335
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
actor:  0 policy actor:  0  step number:  43 total reward:  0.39999999999999947  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  40.748019218444824
actions average: 
K:  4  action  0 :  tensor([0.4267, 0.0540, 0.0992, 0.1020, 0.1117, 0.1102, 0.0962],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0116, 0.9505, 0.0077, 0.0067, 0.0057, 0.0049, 0.0130],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1836, 0.0707, 0.2947, 0.0909, 0.0925, 0.1552, 0.1123],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1308, 0.0208, 0.0749, 0.3446, 0.1359, 0.1882, 0.1048],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1146, 0.0007, 0.0584, 0.0744, 0.5515, 0.1227, 0.0777],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1588, 0.0146, 0.1060, 0.1053, 0.1052, 0.3767, 0.1335],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1826, 0.0760, 0.0985, 0.1433, 0.1327, 0.1140, 0.2529],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  72 total reward:  0.0733333333333327  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.03348666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.030846666666666536 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.18716655986042
using explorer policy with actor:  1
siam score:  -0.7940308
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]] [[43.521]
 [43.521]
 [43.521]
 [43.521]
 [43.521]
 [43.521]
 [43.521]] [[2.117]
 [2.117]
 [2.117]
 [2.117]
 [2.117]
 [2.117]
 [2.117]]
maxi score, test score, baseline:  0.030846666666666536 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.720471191740366
printing an ep nov before normalisation:  43.08077239296015
siam score:  -0.7943025
printing an ep nov before normalisation:  27.25935935974121
maxi score, test score, baseline:  0.030846666666666536 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.05644970842406
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  59 total reward:  0.29333333333333267  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  18.045180587691434
actor:  1 policy actor:  1  step number:  46 total reward:  0.31333333333333324  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.025]
 [-0.004]
 [-0.024]
 [-0.023]
 [-0.018]
 [-0.016]
 [-0.023]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.025]
 [-0.004]
 [-0.024]
 [-0.023]
 [-0.018]
 [-0.016]
 [-0.023]]
Printing some Q and Qe and total Qs values:  [[-0.013]
 [-0.007]
 [-0.018]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]] [[39.639]
 [32.171]
 [24.343]
 [39.639]
 [39.639]
 [39.639]
 [39.639]] [[0.987]
 [0.716]
 [0.416]
 [0.987]
 [0.987]
 [0.987]
 [0.987]]
maxi score, test score, baseline:  0.030846666666666536 0.6983333333333335 0.6983333333333335
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.83349060352199
maxi score, test score, baseline:  0.030846666666666536 0.6983333333333335 0.6983333333333335
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.537]
 [0.223]
 [0.444]
 [0.402]
 [0.35 ]
 [0.445]] [[33.804]
 [33.437]
 [42.314]
 [35.478]
 [38.009]
 [43.147]
 [35.054]] [[0.754]
 [0.852]
 [0.723]
 [0.801]
 [0.812]
 [0.868]
 [0.794]]
maxi score, test score, baseline:  0.030846666666666536 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.79332395177069
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.625]
 [0.473]
 [0.459]
 [0.459]
 [0.474]
 [0.473]] [[35.791]
 [32.217]
 [38.945]
 [37.65 ]
 [37.65 ]
 [38.141]
 [38.422]] [[1.258]
 [1.239]
 [1.385]
 [1.313]
 [1.313]
 [1.35 ]
 [1.362]]
printing an ep nov before normalisation:  23.133816719055176
maxi score, test score, baseline:  0.030846666666666536 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.976876258850098
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  0.030846666666666536 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.030846666666666536 0.6983333333333335 0.6983333333333335
printing an ep nov before normalisation:  38.842552953469
actor:  1 policy actor:  1  step number:  52 total reward:  0.2733333333333331  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  39.202213989906745
Starting evaluation
siam score:  -0.7881396
Printing some Q and Qe and total Qs values:  [[0.809]
 [0.869]
 [0.809]
 [0.88 ]
 [0.859]
 [0.802]
 [0.876]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.809]
 [0.869]
 [0.809]
 [0.88 ]
 [0.859]
 [0.802]
 [0.876]]
maxi score, test score, baseline:  0.030846666666666536 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.83502724942502
actor:  1 policy actor:  1  step number:  60 total reward:  0.12666666666666637  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.030846666666666536 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.687]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]] [[35.628]
 [35.516]
 [35.628]
 [35.628]
 [35.628]
 [35.628]
 [35.628]] [[0.602]
 [0.687]
 [0.602]
 [0.602]
 [0.602]
 [0.602]
 [0.602]]
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.786]
 [0.695]
 [0.695]
 [0.669]
 [0.695]
 [0.699]] [[34.202]
 [35.558]
 [38.664]
 [38.664]
 [50.021]
 [38.664]
 [32.617]] [[0.674]
 [0.786]
 [0.695]
 [0.695]
 [0.669]
 [0.695]
 [0.699]]
printing an ep nov before normalisation:  41.216468733059244
maxi score, test score, baseline:  0.030846666666666536 0.6983333333333335 0.6983333333333335
printing an ep nov before normalisation:  36.02368779752108
printing an ep nov before normalisation:  37.3046875
Printing some Q and Qe and total Qs values:  [[0.82 ]
 [0.733]
 [0.733]
 [0.733]
 [0.792]
 [0.733]
 [0.733]] [[43.688]
 [34.386]
 [34.386]
 [34.386]
 [44.084]
 [34.386]
 [34.386]] [[0.82 ]
 [0.733]
 [0.733]
 [0.733]
 [0.792]
 [0.733]
 [0.733]]
line 256 mcts: sample exp_bonus 45.511783679801034
Printing some Q and Qe and total Qs values:  [[0.827]
 [0.75 ]
 [0.75 ]
 [0.75 ]
 [0.839]
 [0.75 ]
 [0.75 ]] [[39.401]
 [25.196]
 [25.196]
 [25.196]
 [38.589]
 [25.196]
 [25.196]] [[0.827]
 [0.75 ]
 [0.75 ]
 [0.75 ]
 [0.839]
 [0.75 ]
 [0.75 ]]
Printing some Q and Qe and total Qs values:  [[0.742]
 [0.8  ]
 [0.342]
 [0.711]
 [0.746]
 [0.555]
 [0.684]] [[34.438]
 [38.687]
 [30.21 ]
 [30.847]
 [33.684]
 [30.079]
 [24.67 ]] [[0.742]
 [0.8  ]
 [0.342]
 [0.711]
 [0.746]
 [0.555]
 [0.684]]
Printing some Q and Qe and total Qs values:  [[0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]] [[41.802]
 [41.802]
 [41.802]
 [41.802]
 [41.802]
 [41.802]
 [41.802]] [[0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]]
Printing some Q and Qe and total Qs values:  [[0.828]
 [0.895]
 [0.828]
 [0.828]
 [0.828]
 [0.828]
 [0.828]] [[47.272]
 [51.002]
 [47.272]
 [47.272]
 [47.272]
 [47.272]
 [47.272]] [[0.828]
 [0.895]
 [0.828]
 [0.828]
 [0.828]
 [0.828]
 [0.828]]
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.651]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]] [[42.772]
 [50.975]
 [42.772]
 [42.772]
 [42.772]
 [42.772]
 [42.772]] [[0.561]
 [0.651]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]]
line 256 mcts: sample exp_bonus 39.861062819821704
maxi score, test score, baseline:  0.030846666666666536 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.12079543963663
printing an ep nov before normalisation:  40.11218669240462
printing an ep nov before normalisation:  39.807338005364734
printing an ep nov before normalisation:  33.53992462158203
maxi score, test score, baseline:  0.030846666666666536 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.75553231036932
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]] [[14.434]
 [13.907]
 [13.907]
 [13.907]
 [13.907]
 [13.907]
 [13.907]] [[0.651]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]]
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7133333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.03473999999999988 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.03473999999999988 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.981]
 [0.975]
 [0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]] [[60.654]
 [48.932]
 [60.654]
 [60.654]
 [60.654]
 [60.654]
 [60.654]] [[0.981]
 [0.975]
 [0.981]
 [0.981]
 [0.981]
 [0.981]
 [0.981]]
printing an ep nov before normalisation:  24.278743745353438
printing an ep nov before normalisation:  49.09984074685836
Printing some Q and Qe and total Qs values:  [[0.907]
 [0.963]
 [0.907]
 [0.907]
 [0.907]
 [0.907]
 [0.907]] [[39.837]
 [56.802]
 [39.837]
 [39.837]
 [39.837]
 [39.837]
 [39.837]] [[0.907]
 [0.963]
 [0.907]
 [0.907]
 [0.907]
 [0.907]
 [0.907]]
printing an ep nov before normalisation:  53.00184036691009
printing an ep nov before normalisation:  41.27595565048849
maxi score, test score, baseline:  0.034766666666666536 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.034766666666666536 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.034766666666666536 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.034766666666666536 0.6983333333333335 0.6983333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  1.0191696274750939
actor:  0 policy actor:  0  step number:  57 total reward:  0.03999999999999926  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.693]
 [0.836]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]] [[48.447]
 [47.675]
 [48.447]
 [48.447]
 [48.447]
 [48.447]
 [48.447]] [[0.693]
 [0.836]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]]
actor:  0 policy actor:  0  step number:  57 total reward:  0.03999999999999926  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  57 total reward:  0.03999999999999926  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  58 total reward:  0.01999999999999924  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.586]
 [0.688]
 [0.617]
 [0.758]
 [0.688]
 [0.688]] [[46.311]
 [43.581]
 [37.161]
 [44.355]
 [47.418]
 [37.161]
 [37.161]] [[0.727]
 [0.586]
 [0.688]
 [0.617]
 [0.758]
 [0.688]
 [0.688]]
printing an ep nov before normalisation:  38.46974163800417
printing an ep nov before normalisation:  20.391847689946495
actor:  0 policy actor:  0  step number:  34 total reward:  0.5133333333333333  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  0
actor:  0 policy actor:  0  step number:  48 total reward:  0.4199999999999995  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.015313333333333195 0.2256666666666666 0.2256666666666666
probs:  [0.057886677750281244, 0.0771890514734005, 0.11797573582540585, 0.40452681950000224, 0.1637209418794883, 0.17870077357142194]
printing an ep nov before normalisation:  31.337676186086735
maxi score, test score, baseline:  0.015313333333333195 0.2256666666666666 0.2256666666666666
maxi score, test score, baseline:  0.015313333333333195 0.2256666666666666 0.2256666666666666
probs:  [0.057886677750281244, 0.0771890514734005, 0.11797573582540585, 0.40452681950000224, 0.1637209418794883, 0.17870077357142194]
printing an ep nov before normalisation:  45.754703163020054
maxi score, test score, baseline:  0.015313333333333195 0.2256666666666666 0.2256666666666666
using explorer policy with actor:  1
maxi score, test score, baseline:  0.015313333333333195 0.2256666666666666 0.2256666666666666
printing an ep nov before normalisation:  29.356852850672084
line 256 mcts: sample exp_bonus 25.7962409674131
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  51 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  67 total reward:  0.05333333333333257  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.01741999999999986 0.2256666666666666 0.2256666666666666
probs:  [0.05788849284574455, 0.07718949154199793, 0.11797327040747822, 0.40453954512040136, 0.16371521774902034, 0.1786939823353576]
maxi score, test score, baseline:  0.01741999999999986 0.2256666666666666 0.2256666666666666
probs:  [0.05788849284574455, 0.07718949154199793, 0.11797327040747822, 0.40453954512040136, 0.16371521774902034, 0.1786939823353576]
printing an ep nov before normalisation:  45.08834704010806
actor:  0 policy actor:  0  step number:  50 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  38.62783193706133
maxi score, test score, baseline:  0.02007333333333319 0.2256666666666666 0.2256666666666666
probs:  [0.05788849284574455, 0.07718949154199793, 0.11797327040747822, 0.40453954512040136, 0.16371521774902034, 0.1786939823353576]
printing an ep nov before normalisation:  55.68748690026586
maxi score, test score, baseline:  0.02007333333333319 0.2256666666666666 0.2256666666666666
probs:  [0.05788849284574455, 0.07718949154199793, 0.11797327040747822, 0.40453954512040136, 0.16371521774902034, 0.1786939823353576]
actor:  1 policy actor:  1  step number:  37 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  34.14473381450384
printing an ep nov before normalisation:  20.19722718036709
printing an ep nov before normalisation:  50.676993596292796
actor:  1 policy actor:  1  step number:  52 total reward:  0.4333333333333328  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  1  action  0 :  tensor([0.4351, 0.1138, 0.0801, 0.0831, 0.1240, 0.0719, 0.0920],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0042, 0.9691, 0.0032, 0.0026, 0.0016, 0.0011, 0.0182],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1515, 0.0520, 0.4150, 0.0984, 0.0977, 0.0870, 0.0985],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2491, 0.0107, 0.1303, 0.1753, 0.1517, 0.1215, 0.1614],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1413, 0.0115, 0.0631, 0.0905, 0.5268, 0.0713, 0.0955],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0919, 0.0157, 0.1313, 0.0772, 0.0721, 0.5106, 0.1012],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1634, 0.0860, 0.1234, 0.1342, 0.0921, 0.0830, 0.3180],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  35.40138827284788
maxi score, test score, baseline:  0.02007333333333319 0.2256666666666666 0.2256666666666666
probs:  [0.05772244115918118, 0.07933797975121273, 0.11763446947524944, 0.4033768398683315, 0.16374766595818382, 0.17818060378784134]
printing an ep nov before normalisation:  13.12296158538195
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.406]] [[24.567]
 [24.567]
 [24.567]
 [24.567]
 [24.567]
 [24.567]
 [35.026]] [[1.237]
 [1.237]
 [1.237]
 [1.237]
 [1.237]
 [1.237]
 [1.776]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.2266666666666659  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.019]
 [-0.015]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]] [[30.889]
 [39.394]
 [30.889]
 [30.889]
 [30.889]
 [30.889]
 [30.889]] [[0.32 ]
 [0.494]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]]
printing an ep nov before normalisation:  26.365537254098236
actor:  1 policy actor:  1  step number:  51 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.02007333333333319 0.2256666666666666 0.2256666666666666
maxi score, test score, baseline:  0.02007333333333319 0.2256666666666666 0.2256666666666666
probs:  [0.05758614307249852, 0.07915052353391425, 0.12002474742201555, 0.4024224713550522, 0.16336043485515525, 0.17745567976136414]
maxi score, test score, baseline:  0.02007333333333319 0.2256666666666666 0.2256666666666666
probs:  [0.05758614307249852, 0.07915052353391425, 0.12002474742201555, 0.4024224713550522, 0.16336043485515525, 0.17745567976136414]
actor:  1 policy actor:  1  step number:  42 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.333
siam score:  -0.7833383
printing an ep nov before normalisation:  36.851797103881836
printing an ep nov before normalisation:  37.7516508102417
maxi score, test score, baseline:  0.02007333333333319 0.2256666666666666 0.2256666666666666
probs:  [0.05767123110584576, 0.07916443878189497, 0.11990375815145393, 0.403019014000232, 0.16309641705725395, 0.1771451409033195]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5533333333333337  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  51.240520546356905
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.02007333333333319 0.2256666666666666 0.2256666666666666
probs:  [0.057387512593909173, 0.07877473778121782, 0.11931317254939672, 0.40103239389892653, 0.1622928494957259, 0.1811993336808239]
actor:  1 policy actor:  1  step number:  56 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  0  action  0 :  tensor([0.4708, 0.0079, 0.0857, 0.0991, 0.1128, 0.0805, 0.1431],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0072, 0.9666, 0.0047, 0.0036, 0.0015, 0.0015, 0.0149],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2003, 0.0311, 0.3609, 0.1026, 0.0875, 0.0807, 0.1368],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1280, 0.0269, 0.0450, 0.5953, 0.0668, 0.0525, 0.0855],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2244, 0.0065, 0.1121, 0.1325, 0.2681, 0.1254, 0.1309],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1246, 0.0055, 0.1510, 0.0866, 0.0770, 0.4869, 0.0686],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2399, 0.0772, 0.1055, 0.1275, 0.1589, 0.1148, 0.1761],
       grad_fn=<DivBackward0>)
siam score:  -0.78116953
printing an ep nov before normalisation:  39.74883129488641
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.569556828529215
actor:  1 policy actor:  1  step number:  53 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]
 [0.369]] [[25.829]
 [25.829]
 [25.829]
 [25.829]
 [25.829]
 [25.829]
 [25.829]] [[1.189]
 [1.189]
 [1.189]
 [1.189]
 [1.189]
 [1.189]
 [1.189]]
printing an ep nov before normalisation:  49.67732054906894
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.525]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]] [[19.671]
 [30.524]
 [19.671]
 [19.671]
 [19.671]
 [19.671]
 [19.671]] [[0.498]
 [0.525]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]]
maxi score, test score, baseline:  0.02007333333333319 0.2256666666666666 0.2256666666666666
probs:  [0.057442548382446675, 0.07879889113068572, 0.11906843232259798, 0.4014181333501132, 0.16219640548047531, 0.18107558933368104]
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.313]
 [0.628]
 [0.549]
 [0.549]
 [0.649]
 [0.549]] [[41.367]
 [31.777]
 [39.415]
 [41.367]
 [41.367]
 [40.375]
 [41.367]] [[0.549]
 [0.313]
 [0.628]
 [0.549]
 [0.549]
 [0.649]
 [0.549]]
printing an ep nov before normalisation:  32.423749076145164
printing an ep nov before normalisation:  28.77954475922942
actor:  1 policy actor:  1  step number:  42 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  66 total reward:  0.17999999999999905  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.02007333333333319 0.2256666666666666 0.2256666666666666
printing an ep nov before normalisation:  19.463497400283813
maxi score, test score, baseline:  0.02007333333333319 0.2256666666666666 0.2256666666666666
probs:  [0.05716174817103124, 0.07841345060268397, 0.11848568190924816, 0.3994519455920891, 0.166298153141714, 0.18018902058323355]
maxi score, test score, baseline:  0.02007333333333319 0.2256666666666666 0.2256666666666666
probs:  [0.05716174817103124, 0.07841345060268397, 0.11848568190924816, 0.3994519455920891, 0.166298153141714, 0.18018902058323355]
printing an ep nov before normalisation:  24.14147508266597
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  40.23673378784224
printing an ep nov before normalisation:  20.805330628659306
printing an ep nov before normalisation:  29.562850652288066
actor:  0 policy actor:  0  step number:  58 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  48 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  39.56699623879658
maxi score, test score, baseline:  0.02020666666666652 0.2256666666666666 0.2256666666666666
probs:  [0.05701919367421157, 0.07821777311281942, 0.11818983565070088, 0.3984537664771757, 0.1658827897957753, 0.1822366412893171]
maxi score, test score, baseline:  0.02020666666666652 0.2256666666666666 0.2256666666666666
maxi score, test score, baseline:  0.02020666666666652 0.2256666666666666 0.2256666666666666
probs:  [0.05701919367421157, 0.07821777311281942, 0.11818983565070088, 0.3984537664771757, 0.1658827897957753, 0.1822366412893171]
Printing some Q and Qe and total Qs values:  [[0.14 ]
 [0.216]
 [0.285]
 [0.285]
 [0.166]
 [0.285]
 [0.196]] [[34.729]
 [38.885]
 [44.385]
 [44.385]
 [39.503]
 [44.385]
 [36.198]] [[1.163]
 [1.509]
 [1.934]
 [1.934]
 [1.499]
 [1.934]
 [1.314]]
maxi score, test score, baseline:  0.02020666666666652 0.2256666666666666 0.2256666666666666
probs:  [0.05703102934570591, 0.07823401935226147, 0.11800650517660523, 0.39853664088913876, 0.16591727558347752, 0.18227452965281118]
maxi score, test score, baseline:  0.02020666666666652 0.2256666666666666 0.2256666666666666
probs:  [0.05703102934570591, 0.07823401935226147, 0.11800650517660523, 0.39853664088913876, 0.16591727558347752, 0.18227452965281118]
actor:  1 policy actor:  1  step number:  39 total reward:  0.4933333333333332  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  39.58053870661864
maxi score, test score, baseline:  0.02020666666666652 0.2256666666666666 0.2256666666666666
probs:  [0.057201667633161285, 0.07826727559383787, 0.11778206071320783, 0.3997329278193927, 0.16538239944718403, 0.18163366879321627]
printing an ep nov before normalisation:  71.74851215347603
actor:  1 policy actor:  1  step number:  62 total reward:  0.1933333333333327  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  3  action  0 :  tensor([0.4448, 0.0487, 0.0800, 0.0948, 0.1115, 0.1097, 0.1105],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0105, 0.9263, 0.0151, 0.0047, 0.0047, 0.0047, 0.0340],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1279, 0.0060, 0.3560, 0.1229, 0.1107, 0.1424, 0.1341],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1777, 0.0033, 0.1231, 0.1900, 0.2085, 0.1300, 0.1673],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1042, 0.0044, 0.0742, 0.1601, 0.4328, 0.0866, 0.1378],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1269, 0.0041, 0.1712, 0.0892, 0.0891, 0.3525, 0.1670],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2103, 0.0330, 0.1256, 0.1302, 0.1394, 0.1542, 0.2073],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.02020666666666652 0.2256666666666666 0.2256666666666666
maxi score, test score, baseline:  0.02020666666666652 0.2256666666666666 0.2256666666666666
probs:  [0.057201667633161285, 0.07826727559383787, 0.11778206071320783, 0.3997329278193927, 0.16538239944718403, 0.18163366879321627]
maxi score, test score, baseline:  0.02020666666666652 0.2256666666666666 0.2256666666666666
probs:  [0.057201667633161285, 0.07826727559383787, 0.11778206071320783, 0.3997329278193927, 0.16538239944718403, 0.18163366879321627]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.02020666666666652 0.2256666666666666 0.2256666666666666
probs:  [0.057201667633161285, 0.07826727559383787, 0.11778206071320783, 0.3997329278193927, 0.16538239944718403, 0.18163366879321627]
maxi score, test score, baseline:  0.02020666666666652 0.2256666666666666 0.2256666666666666
probs:  [0.057201667633161285, 0.07826727559383787, 0.11778206071320783, 0.3997329278193927, 0.16538239944718403, 0.18163366879321627]
Printing some Q and Qe and total Qs values:  [[0.71]
 [0.75]
 [0.71]
 [0.71]
 [0.71]
 [0.71]
 [0.71]] [[33.265]
 [50.154]
 [33.265]
 [33.265]
 [33.265]
 [33.265]
 [33.265]] [[0.71]
 [0.75]
 [0.71]
 [0.71]
 [0.71]
 [0.71]
 [0.71]]
maxi score, test score, baseline:  0.02020666666666652 0.2256666666666666 0.2256666666666666
probs:  [0.057201667633161285, 0.07826727559383787, 0.11778206071320783, 0.3997329278193927, 0.16538239944718403, 0.18163366879321627]
maxi score, test score, baseline:  0.02020666666666652 0.2256666666666666 0.2256666666666666
probs:  [0.057201667633161285, 0.07826727559383787, 0.11778206071320783, 0.3997329278193927, 0.16538239944718403, 0.18163366879321627]
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.678]
 [0.539]
 [0.547]
 [0.549]
 [0.544]
 [0.444]] [[15.159]
 [17.964]
 [15.476]
 [14.421]
 [14.228]
 [14.497]
 [42.246]] [[0.973]
 [1.186]
 [0.977]
 [0.955]
 [0.951]
 [0.954]
 [1.639]]
maxi score, test score, baseline:  0.02020666666666652 0.2256666666666666 0.2256666666666666
probs:  [0.057201667633161285, 0.07826727559383787, 0.11778206071320783, 0.3997329278193927, 0.16538239944718403, 0.18163366879321627]
printing an ep nov before normalisation:  19.777173922948005
actor:  0 policy actor:  1  step number:  38 total reward:  0.5  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  59 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  57 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.42794753302907
printing an ep nov before normalisation:  58.63663257306071
printing an ep nov before normalisation:  30.58626413345337
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.515]
 [0.581]
 [0.482]
 [0.581]
 [0.42 ]
 [0.484]] [[51.101]
 [49.656]
 [51.101]
 [52.174]
 [51.101]
 [52.374]
 [49.566]] [[1.726]
 [1.614]
 [1.726]
 [1.661]
 [1.726]
 [1.605]
 [1.581]]
maxi score, test score, baseline:  0.020579999999999855 0.2256666666666666 0.2256666666666666
maxi score, test score, baseline:  0.020579999999999855 0.2256666666666666 0.2256666666666666
probs:  [0.05720060691442393, 0.07826582333321269, 0.11777987399983905, 0.39972550054638817, 0.16538861395946186, 0.18163958124667431]
maxi score, test score, baseline:  0.020579999999999855 0.2256666666666666 0.2256666666666666
probs:  [0.05720060691442393, 0.07826582333321269, 0.11777987399983905, 0.39972550054638817, 0.16538861395946186, 0.18163958124667431]
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]] [[33.31]
 [33.31]
 [33.31]
 [33.31]
 [33.31]
 [33.31]
 [33.31]] [[1.346]
 [1.346]
 [1.346]
 [1.346]
 [1.346]
 [1.346]
 [1.346]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.2079],
        [ 0.6422],
        [-0.0460],
        [ 0.5634],
        [ 0.6509],
        [-0.0000],
        [-0.0000],
        [ 0.6138],
        [ 0.5388],
        [ 0.2251]], dtype=torch.float64)
-0.045546567066 0.16234360235952125
-0.07090238119799999 0.5713459244453863
-0.032346567066 -0.07839619278338276
-0.032346567066 0.5310644744823362
-0.058094434398 0.5927836035289811
0.9117834164999999 0.9117834164999999
-0.47887751999999995 -0.47887751999999995
-0.09703970119800001 0.5168006597258147
-0.09703970119800001 0.4417422446666047
-0.045414567066 0.17966307791389394
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.5  ]
 [0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]] [[24.988]
 [30.113]
 [24.988]
 [24.988]
 [24.988]
 [24.988]
 [24.988]] [[1.254]
 [1.701]
 [1.254]
 [1.254]
 [1.254]
 [1.254]
 [1.254]]
printing an ep nov before normalisation:  24.566502571105957
actor:  1 policy actor:  1  step number:  48 total reward:  0.2999999999999998  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  48 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.0
using another actor
maxi score, test score, baseline:  0.020579999999999855 0.2256666666666666 0.2256666666666666
probs:  [0.057088029788147955, 0.07810906593545938, 0.11955051219076386, 0.3989372418323287, 0.1650491331343671, 0.18126601711893292]
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.532]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]] [[55.086]
 [59.057]
 [55.086]
 [55.086]
 [55.086]
 [55.086]
 [55.086]] [[1.868]
 [2.055]
 [1.868]
 [1.868]
 [1.868]
 [1.868]
 [1.868]]
UNIT TEST: sample policy line 217 mcts : [0.347 0.449 0.02  0.061 0.061 0.041 0.02 ]
maxi score, test score, baseline:  0.020579999999999855 0.2256666666666666 0.2256666666666666
probs:  [0.057088029788147955, 0.07810906593545938, 0.11955051219076386, 0.3989372418323287, 0.1650491331343671, 0.18126601711893292]
maxi score, test score, baseline:  0.020579999999999855 0.2256666666666666 0.2256666666666666
probs:  [0.057088029788147955, 0.07810906593545938, 0.11955051219076386, 0.3989372418323287, 0.1650491331343671, 0.18126601711893292]
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.508]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]] [[25.354]
 [29.081]
 [23.468]
 [23.468]
 [23.468]
 [23.468]
 [23.468]] [[1.667]
 [1.989]
 [1.576]
 [1.576]
 [1.576]
 [1.576]
 [1.576]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.01853999999999986 0.2256666666666666 0.2256666666666666
probs:  [0.057088029788147955, 0.07810906593545938, 0.11955051219076386, 0.3989372418323287, 0.1650491331343671, 0.18126601711893292]
printing an ep nov before normalisation:  74.14246346186376
maxi score, test score, baseline:  0.01853999999999986 0.2256666666666666 0.2256666666666666
probs:  [0.057088029788147955, 0.07810906593545938, 0.11955051219076386, 0.3989372418323287, 0.1650491331343671, 0.18126601711893292]
printing an ep nov before normalisation:  21.433371512572293
actor:  1 policy actor:  1  step number:  49 total reward:  0.4533333333333336  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.01853999999999986 0.2256666666666666 0.2256666666666666
probs:  [0.056955081627330616, 0.07792704889993304, 0.11927175943204052, 0.3980063236743326, 0.16699658295337344, 0.1808432034129898]
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.535450032508464
maxi score, test score, baseline:  0.01853999999999986 0.2256666666666666 0.2256666666666666
probs:  [0.056955081627330616, 0.07792704889993304, 0.11927175943204052, 0.3980063236743326, 0.16699658295337344, 0.1808432034129898]
actor:  1 policy actor:  1  step number:  63 total reward:  0.06666666666666632  reward:  1.0 rdn_beta:  1.667
siam score:  -0.77572787
printing an ep nov before normalisation:  28.528667150069307
maxi score, test score, baseline:  0.01853999999999986 0.2256666666666666 0.2256666666666666
probs:  [0.056955081627330616, 0.07792704889993304, 0.11927175943204052, 0.3980063236743326, 0.16699658295337344, 0.1808432034129898]
maxi score, test score, baseline:  0.016073333333333193 0.2256666666666666 0.2256666666666666
probs:  [0.056955081627330616, 0.07792704889993304, 0.11927175943204052, 0.3980063236743326, 0.16699658295337344, 0.1808432034129898]
maxi score, test score, baseline:  0.016073333333333193 0.2256666666666666 0.2256666666666666
probs:  [0.056955081627330616, 0.07792704889993304, 0.11927175943204052, 0.3980063236743326, 0.16699658295337344, 0.1808432034129898]
actor:  1 policy actor:  1  step number:  46 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.016073333333333193 0.2256666666666666 0.2256666666666666
probs:  [0.05687176359478132, 0.07788566724670436, 0.12038708272710513, 0.39742332742602954, 0.16680448438898635, 0.1806276746163933]
printing an ep nov before normalisation:  43.40969134452941
printing an ep nov before normalisation:  28.312368465913217
actor:  1 policy actor:  1  step number:  61 total reward:  0.14666666666666583  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.016073333333333193 0.2256666666666666 0.2256666666666666
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.505]
 [0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]] [[32.709]
 [36.645]
 [32.709]
 [32.709]
 [32.709]
 [32.709]
 [32.709]] [[0.667]
 [0.902]
 [0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]]
maxi score, test score, baseline:  0.016073333333333193 0.2256666666666666 0.2256666666666666
printing an ep nov before normalisation:  40.27653694152832
maxi score, test score, baseline:  0.016073333333333193 0.2256666666666666 0.2256666666666666
using explorer policy with actor:  1
maxi score, test score, baseline:  0.016073333333333193 0.2256666666666666 0.2256666666666666
probs:  [0.05687972994891476, 0.07790124340010054, 0.12020723379263247, 0.3974790750986574, 0.16685226085891305, 0.18068045690078186]
actor:  1 policy actor:  1  step number:  52 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  50 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  1.333
from probs:  [0.05687972994891476, 0.07790124340010054, 0.12020723379263247, 0.3974790750986574, 0.16685226085891305, 0.18068045690078186]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.016073333333333193 0.2256666666666666 0.2256666666666666
probs:  [0.05704881927265415, 0.07849660927766161, 0.11985252057479828, 0.39866476970892206, 0.16611173133305912, 0.17982554983290475]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.756]
 [0.785]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]] [[39.34 ]
 [35.396]
 [36.891]
 [36.891]
 [36.891]
 [36.891]
 [36.891]] [[0.756]
 [0.785]
 [0.741]
 [0.741]
 [0.741]
 [0.741]
 [0.741]]
printing an ep nov before normalisation:  43.44305082744188
maxi score, test score, baseline:  0.016073333333333193 0.2256666666666666 0.2256666666666666
maxi score, test score, baseline:  0.016073333333333193 0.2256666666666666 0.2256666666666666
maxi score, test score, baseline:  0.016073333333333193 0.2256666666666666 0.2256666666666666
probs:  [0.05704881927265415, 0.07849660927766161, 0.11985252057479828, 0.39866476970892206, 0.16611173133305912, 0.17982554983290475]
actor:  1 policy actor:  1  step number:  50 total reward:  0.2733333333333332  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.016073333333333193 0.2256666666666666 0.2256666666666666
probs:  [0.05704305181377507, 0.07848866843814248, 0.11984038899409119, 0.3986243850940174, 0.16619616468756512, 0.17980734097240875]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.016073333333333193 0.2256666666666666 0.2256666666666666
probs:  [0.05711419134001929, 0.07850185119106182, 0.11974181873301082, 0.3991231107418521, 0.16597231795478315, 0.17954671003927283]
printing an ep nov before normalisation:  55.87507877240782
maxi score, test score, baseline:  0.016073333333333193 0.2256666666666666 0.2256666666666666
probs:  [0.05711419134001929, 0.07850185119106182, 0.11974181873301082, 0.3991231107418521, 0.16597231795478315, 0.17954671003927283]
actor:  1 policy actor:  1  step number:  39 total reward:  0.6800000000000003  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  49.09999579511723
siam score:  -0.77274203
actor:  1 policy actor:  1  step number:  34 total reward:  0.5666666666666668  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.016073333333333193 0.2256666666666666 0.2256666666666666
probs:  [0.0572970481423731, 0.07819308755771034, 0.11848510609763438, 0.4004074426570288, 0.16365294309602313, 0.18196437244923022]
Printing some Q and Qe and total Qs values:  [[0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]] [[45.109]
 [45.109]
 [45.109]
 [45.109]
 [45.109]
 [45.109]
 [45.109]] [[0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]]
printing an ep nov before normalisation:  54.83964709115499
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.526178995947724
maxi score, test score, baseline:  0.016073333333333193 0.2256666666666666 0.2256666666666666
printing an ep nov before normalisation:  53.762704449359866
maxi score, test score, baseline:  0.016073333333333193 0.2256666666666666 0.2256666666666666
probs:  [0.0572970481423731, 0.07819308755771034, 0.11848510609763438, 0.4004074426570288, 0.16365294309602313, 0.18196437244923022]
actor:  0 policy actor:  0  step number:  59 total reward:  0.14666666666666595  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  39 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
siam score:  -0.7781623
actor:  1 policy actor:  1  step number:  46 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.158351169162614
maxi score, test score, baseline:  0.021193333333333193 0.2256666666666666 0.2256666666666666
printing an ep nov before normalisation:  42.81588561911679
printing an ep nov before normalisation:  48.91192444693924
printing an ep nov before normalisation:  42.08997018141753
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.368]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]] [[36.07 ]
 [37.913]
 [36.07 ]
 [36.07 ]
 [36.07 ]
 [36.07 ]
 [36.07 ]] [[0.554]
 [0.635]
 [0.554]
 [0.554]
 [0.554]
 [0.554]
 [0.554]]
maxi score, test score, baseline:  0.021193333333333193 0.2256666666666666 0.2256666666666666
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.439]
 [0.449]
 [0.343]
 [0.346]
 [0.345]
 [0.34 ]] [[21.661]
 [25.661]
 [24.702]
 [18.969]
 [21.59 ]
 [21.526]
 [18.739]] [[0.419]
 [0.56 ]
 [0.562]
 [0.403]
 [0.431]
 [0.429]
 [0.399]]
printing an ep nov before normalisation:  44.139293758406914
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.4905],
        [ 0.0702],
        [-0.0860],
        [-0.0000],
        [-0.0000],
        [ 0.2595],
        [-0.0000],
        [ 0.7169],
        [ 0.4368],
        [ 0.4332]], dtype=torch.float64)
-0.032346567066 0.45813432701445855
-0.09703970119800001 -0.026851816746603988
-0.032346567066 -0.11831344401332955
-0.9042 -0.9042
-0.019602659999999286 -0.019602659999999286
-0.057834381198 0.2016464206026084
0.957165 0.957165
-0.09703970119800001 0.6198658039168589
-0.09703970119800001 0.33973390479051235
-0.09703970119800001 0.33618349275040527
using explorer policy with actor:  1
maxi score, test score, baseline:  0.021193333333333193 0.2256666666666666 0.2256666666666666
probs:  [0.057469957598826456, 0.0782815258787138, 0.1182682114348604, 0.40162010679850535, 0.16309376648905152, 0.1812664318000426]
actor:  1 policy actor:  1  step number:  42 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.021193333333333193 0.2256666666666666 0.2256666666666666
probs:  [0.0570395444771878, 0.07769488529280862, 0.11738140036661361, 0.39860624083884244, 0.1693712208617899, 0.17990670816275758]
maxi score, test score, baseline:  0.021193333333333193 0.2256666666666666 0.2256666666666666
printing an ep nov before normalisation:  53.73016647348125
maxi score, test score, baseline:  0.021193333333333193 0.2256666666666666 0.2256666666666666
printing an ep nov before normalisation:  39.0472182902547
maxi score, test score, baseline:  0.021193333333333193 0.2256666666666666 0.2256666666666666
probs:  [0.0570395444771878, 0.07769488529280862, 0.11738140036661361, 0.39860624083884244, 0.1693712208617899, 0.17990670816275758]
printing an ep nov before normalisation:  42.532536079959556
printing an ep nov before normalisation:  14.22842025756836
printing an ep nov before normalisation:  51.86112548632317
printing an ep nov before normalisation:  15.595460993834509
maxi score, test score, baseline:  0.021193333333333193 0.2256666666666666 0.2256666666666666
probs:  [0.0570395444771878, 0.07769488529280862, 0.11738140036661361, 0.39860624083884244, 0.1693712208617899, 0.17990670816275758]
actor:  1 policy actor:  1  step number:  31 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.710497482654866
printing an ep nov before normalisation:  53.07680058696649
actor:  1 policy actor:  1  step number:  43 total reward:  0.48  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.021193333333333193 0.2256666666666666 0.2256666666666666
probs:  [0.05757296761195172, 0.07786251376907699, 0.11670902132520061, 0.4023461614626419, 0.1675984216908282, 0.17791091414030058]
actor:  1 policy actor:  1  step number:  43 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  47 total reward:  0.3599999999999999  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.476]
 [0.345]
 [0.345]
 [0.404]
 [0.373]
 [0.345]] [[27.577]
 [30.323]
 [32.794]
 [32.794]
 [24.551]
 [25.431]
 [32.794]] [[1.621]
 [2.022]
 [2.157]
 [2.157]
 [1.327]
 [1.392]
 [2.157]]
printing an ep nov before normalisation:  33.49126622579225
actor:  1 policy actor:  1  step number:  48 total reward:  0.38  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  41.803606438976075
using explorer policy with actor:  1
maxi score, test score, baseline:  0.021193333333333193 0.2256666666666666 0.2256666666666666
probs:  [0.05755468680217852, 0.07774434119637702, 0.11639959528170937, 0.40221880204654964, 0.16703845125972755, 0.179044123413458]
maxi score, test score, baseline:  0.021193333333333193 0.2256666666666666 0.2256666666666666
maxi score, test score, baseline:  0.021193333333333193 0.2256666666666666 0.2256666666666666
probs:  [0.05755468680217852, 0.07774434119637702, 0.11639959528170937, 0.40221880204654964, 0.16703845125972755, 0.17904412341345805]
maxi score, test score, baseline:  0.021193333333333193 0.2256666666666666 0.2256666666666666
probs:  [0.05755468680217852, 0.07774434119637702, 0.11639959528170937, 0.40221880204654964, 0.16703845125972755, 0.17904412341345805]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.0
from probs:  [0.05755468680217852, 0.07774434119637702, 0.11639959528170937, 0.40221880204654964, 0.16703845125972755, 0.17904412341345805]
printing an ep nov before normalisation:  35.58691229919605
line 256 mcts: sample exp_bonus 34.18199609112095
actor:  1 policy actor:  1  step number:  42 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  1.333
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  29.44228839951958
printing an ep nov before normalisation:  39.446633062642675
actor:  0 policy actor:  0  step number:  38 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.419]
 [0.377]
 [0.377]
 [0.377]
 [0.377]
 [0.363]] [[44.607]
 [45.06 ]
 [39.844]
 [39.844]
 [39.844]
 [39.844]
 [44.363]] [[1.32 ]
 [1.405]
 [1.189]
 [1.189]
 [1.189]
 [1.189]
 [1.326]]
printing an ep nov before normalisation:  27.399591416433296
actor:  0 policy actor:  0  step number:  35 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.02472666666666653 0.2256666666666666 0.2256666666666666
probs:  [0.057662848568806976, 0.0777533252074011, 0.11644239637466987, 0.4029771426290542, 0.16660879529941475, 0.17855549192065318]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.719]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]] [[50.538]
 [41.513]
 [50.538]
 [50.538]
 [50.538]
 [50.538]
 [50.538]] [[1.779]
 [1.578]
 [1.779]
 [1.779]
 [1.779]
 [1.779]
 [1.779]]
siam score:  -0.7669041
actor:  1 policy actor:  1  step number:  39 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  4  action  0 :  tensor([0.5163, 0.0564, 0.0785, 0.0902, 0.0915, 0.0696, 0.0976],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0179, 0.9115, 0.0084, 0.0133, 0.0071, 0.0097, 0.0321],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1694, 0.0073, 0.3959, 0.1042, 0.0886, 0.1250, 0.1096],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1418, 0.0335, 0.0905, 0.3393, 0.1363, 0.1078, 0.1507],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2130, 0.0005, 0.0731, 0.0828, 0.4296, 0.1260, 0.0751],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0945, 0.0082, 0.2042, 0.0684, 0.0577, 0.5098, 0.0572],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1366, 0.3598, 0.0774, 0.0949, 0.0813, 0.0783, 0.1718],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.02472666666666653 0.2256666666666666 0.2256666666666666
probs:  [0.05781468267829198, 0.07778648351550717, 0.1162470157283262, 0.40404153470189974, 0.166117078348971, 0.17799320502700394]
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.56 ]
 [0.498]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.491]] [[17.026]
 [15.065]
 [15.74 ]
 [15.894]
 [15.623]
 [18.445]
 [15.493]] [[1.589]
 [1.496]
 [1.475]
 [1.487]
 [1.471]
 [1.646]
 [1.453]]
Printing some Q and Qe and total Qs values:  [[0.092]
 [0.168]
 [0.066]
 [0.059]
 [0.09 ]
 [0.041]
 [0.053]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.092]
 [0.168]
 [0.066]
 [0.059]
 [0.09 ]
 [0.041]
 [0.053]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  40.041807393058164
printing an ep nov before normalisation:  26.643242835998535
actor:  0 policy actor:  0  step number:  55 total reward:  0.22666666666666602  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.024779999999999858 0.2256666666666666 0.2256666666666666
probs:  [0.057669136411157916, 0.07759054287095807, 0.11595402852339357, 0.4030223623259179, 0.16569825518272974, 0.18006567468584275]
printing an ep nov before normalisation:  35.00392481300014
maxi score, test score, baseline:  0.024779999999999858 0.2256666666666666 0.2256666666666666
probs:  [0.057669136411157916, 0.07759054287095807, 0.11595402852339357, 0.4030223623259179, 0.16569825518272974, 0.18006567468584275]
actor:  0 policy actor:  0  step number:  41 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  32.32773842672311
actor:  1 policy actor:  1  step number:  43 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.027606666666666526 0.2256666666666666 0.2256666666666666
probs:  [0.057542011627472855, 0.07741940201970923, 0.11790584385831451, 0.4021321844437527, 0.16533244157954288, 0.17966811647120778]
printing an ep nov before normalisation:  25.632538138256137
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 38.83200091829535
line 256 mcts: sample exp_bonus 45.054060567205525
actor:  1 policy actor:  1  step number:  45 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.027606666666666526 0.2256666666666666 0.2256666666666666
probs:  [0.057579423757910274, 0.07742801779624459, 0.11785580697047014, 0.40239444899833915, 0.165213697831019, 0.17952860464601691]
printing an ep nov before normalisation:  33.69802951812744
maxi score, test score, baseline:  0.027606666666666526 0.2256666666666666 0.2256666666666666
probs:  [0.057579423757910274, 0.07742801779624459, 0.11785580697047014, 0.40239444899833915, 0.165213697831019, 0.17952860464601691]
printing an ep nov before normalisation:  45.00482909906954
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.79 ]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]] [[6.192]
 [7.708]
 [8.481]
 [5.718]
 [8.806]
 [5.48 ]
 [8.536]] [[1.147]
 [1.235]
 [1.28 ]
 [1.121]
 [1.299]
 [1.107]
 [1.283]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.027606666666666526 0.2256666666666666 0.2256666666666666
probs:  [0.057518406388957596, 0.07741396588288053, 0.11793741470601334, 0.4019667082387698, 0.16540736312165588, 0.17975614166172293]
maxi score, test score, baseline:  0.027606666666666526 0.2256666666666666 0.2256666666666666
printing an ep nov before normalisation:  46.68768340191311
actor:  1 policy actor:  1  step number:  52 total reward:  0.5000000000000003  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  49.266398004797566
actions average: 
K:  2  action  0 :  tensor([0.3085, 0.0127, 0.1236, 0.1164, 0.1647, 0.1159, 0.1581],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0099, 0.9409, 0.0108, 0.0084, 0.0049, 0.0040, 0.0211],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0550, 0.0018, 0.5990, 0.0362, 0.0644, 0.2016, 0.0420],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1710, 0.0808, 0.1090, 0.2231, 0.1473, 0.1242, 0.1445],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1506, 0.0067, 0.0893, 0.0829, 0.4555, 0.1094, 0.1057],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1283, 0.0510, 0.1359, 0.0896, 0.0718, 0.3979, 0.1255],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2193, 0.0637, 0.1445, 0.1318, 0.1662, 0.1514, 0.1231],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  62.633868067865286
maxi score, test score, baseline:  0.027606666666666526 0.2256666666666666 0.2256666666666666
probs:  [0.05734152502030064, 0.07717575938087486, 0.11757430063839797, 0.4007281156853063, 0.16489792992601926, 0.18228236934910086]
maxi score, test score, baseline:  0.027606666666666526 0.2256666666666666 0.2256666666666666
probs:  [0.05734152502030064, 0.07717575938087486, 0.11757430063839797, 0.4007281156853063, 0.16489792992601926, 0.18228236934910086]
actor:  1 policy actor:  1  step number:  45 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  35.6085439505984
Printing some Q and Qe and total Qs values:  [[0.704]
 [0.735]
 [0.718]
 [0.723]
 [0.725]
 [0.725]
 [0.721]] [[18.173]
 [26.609]
 [17.178]
 [12.648]
 [12.653]
 [12.642]
 [21.214]] [[0.704]
 [0.735]
 [0.718]
 [0.723]
 [0.725]
 [0.725]
 [0.721]]
maxi score, test score, baseline:  0.027606666666666526 0.2256666666666666 0.2256666666666666
probs:  [0.05732820209732369, 0.07717259010530592, 0.11759181240028527, 0.40063472073175, 0.1649396678534907, 0.18233300681184442]
printing an ep nov before normalisation:  21.838207244873047
printing an ep nov before normalisation:  26.339386792934157
printing an ep nov before normalisation:  44.24303919163936
printing an ep nov before normalisation:  42.62667986986074
printing an ep nov before normalisation:  32.9469968575855
maxi score, test score, baseline:  0.027606666666666526 0.2256666666666666 0.2256666666666666
probs:  [0.05732820209732369, 0.07717259010530592, 0.11759181240028527, 0.40063472073175, 0.1649396678534907, 0.18233300681184442]
printing an ep nov before normalisation:  34.406402639053304
printing an ep nov before normalisation:  30.03888813678875
maxi score, test score, baseline:  0.024606666666666523 0.2256666666666666 0.2256666666666666
probs:  [0.05732820209732369, 0.07717259010530592, 0.11759181240028527, 0.40063472073175, 0.1649396678534907, 0.18233300681184442]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.024606666666666523 0.2256666666666666 0.2256666666666666
probs:  [0.057127185224971476, 0.08041360864607036, 0.1171789936627063, 0.39922712229839047, 0.16436044082989865, 0.1816926493379627]
actor:  1 policy actor:  1  step number:  43 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  0.667
using another actor
from probs:  [0.057127185224971476, 0.08041360864607036, 0.1171789936627063, 0.39922712229839047, 0.16436044082989865, 0.1816926493379627]
printing an ep nov before normalisation:  44.06815957749069
maxi score, test score, baseline:  0.024606666666666523 0.2256666666666666 0.2256666666666666
probs:  [0.0569852887368079, 0.08270142231875435, 0.11688758763083874, 0.3982335078216499, 0.16395156828335364, 0.18124062520859552]
printing an ep nov before normalisation:  37.135038508200516
printing an ep nov before normalisation:  31.81001004241374
printing an ep nov before normalisation:  36.051408638355674
siam score:  -0.7804423
actor:  0 policy actor:  0  step number:  38 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.693097510521895
actor:  0 policy actor:  1  step number:  38 total reward:  0.5400000000000003  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.024819999999999866 0.2256666666666666 0.2256666666666666
probs:  [0.0569852887368079, 0.08270142231875435, 0.11688758763083874, 0.3982335078216499, 0.16395156828335364, 0.18124062520859552]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.39999999999999947  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  43.84278315699099
maxi score, test score, baseline:  0.024819999999999866 0.2256666666666666 0.2256666666666666
probs:  [0.056951847919617764, 0.08270057907697129, 0.11693007851228569, 0.39799908787108407, 0.16405371710884525, 0.18136468951119578]
from probs:  [0.056951847919617764, 0.08270057907697129, 0.11693007851228569, 0.39799908787108407, 0.16405371710884525, 0.18136468951119578]
printing an ep nov before normalisation:  34.220490776970244
maxi score, test score, baseline:  0.024819999999999866 0.2256666666666666 0.2256666666666666
maxi score, test score, baseline:  0.024819999999999866 0.2256666666666666 0.2256666666666666
probs:  [0.05695803853982645, 0.0827165654938702, 0.1167641313789866, 0.39804239972517047, 0.1641006533537339, 0.18141821150841242]
maxi score, test score, baseline:  0.024819999999999866 0.2256666666666666 0.2256666666666666
printing an ep nov before normalisation:  34.947702059555816
printing an ep nov before normalisation:  30.76723545947529
printing an ep nov before normalisation:  31.27011215571868
actor:  0 policy actor:  1  step number:  41 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  29.74434311889029
maxi score, test score, baseline:  0.0254333333333332 0.2256666666666666 0.2256666666666666
probs:  [0.05695803853982645, 0.0827165654938702, 0.1167641313789866, 0.39804239972517047, 0.1641006533537339, 0.18141821150841242]
printing an ep nov before normalisation:  30.924319881998265
printing an ep nov before normalisation:  26.739575520686195
siam score:  -0.7760369
maxi score, test score, baseline:  0.0254333333333332 0.2256666666666666 0.2256666666666666
probs:  [0.05695803853982645, 0.0827165654938702, 0.1167641313789866, 0.39804239972517047, 0.1641006533537339, 0.18141821150841242]
maxi score, test score, baseline:  0.0254333333333332 0.2256666666666666 0.2256666666666666
probs:  [0.05695803853982645, 0.0827165654938702, 0.1167641313789866, 0.39804239972517047, 0.1641006533537339, 0.18141821150841242]
actor:  0 policy actor:  0  step number:  54 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0281933333333332 0.2256666666666666 0.2256666666666666
probs:  [0.05695803853982645, 0.0827165654938702, 0.1167641313789866, 0.39804239972517047, 0.1641006533537339, 0.18141821150841242]
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.603]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]] [[22.644]
 [31.619]
 [22.644]
 [22.644]
 [22.644]
 [22.644]
 [22.644]] [[0.553]
 [0.603]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  49 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  9.992343187332153
maxi score, test score, baseline:  0.0281933333333332 0.2256666666666666 0.2256666666666666
probs:  [0.05701123476441832, 0.08276439981751639, 0.11671372019136853, 0.3984157557662802, 0.16391365088030785, 0.18118123858010882]
maxi score, test score, baseline:  0.0281933333333332 0.2256666666666666 0.2256666666666666
probs:  [0.05701123476441832, 0.08276439981751639, 0.11671372019136853, 0.3984157557662802, 0.16391365088030785, 0.18118123858010882]
printing an ep nov before normalisation:  37.51146442793642
actions average: 
K:  1  action  0 :  tensor([0.4730, 0.0065, 0.0839, 0.0982, 0.1098, 0.0966, 0.1321],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0056, 0.9630, 0.0045, 0.0035, 0.0025, 0.0025, 0.0183],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1250, 0.0061, 0.4465, 0.0875, 0.0844, 0.1505, 0.1001],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1286, 0.1875, 0.0947, 0.2209, 0.0896, 0.1445, 0.1342],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1715, 0.0009, 0.0542, 0.0646, 0.5732, 0.0680, 0.0676],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1454, 0.0110, 0.1232, 0.1158, 0.1109, 0.3907, 0.1030],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1607, 0.0831, 0.1284, 0.1208, 0.1068, 0.1309, 0.2693],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0281933333333332 0.2256666666666666 0.2256666666666666
probs:  [0.05701123476441832, 0.08276439981751639, 0.11671372019136853, 0.3984157557662802, 0.16391365088030785, 0.18118123858010882]
actor:  0 policy actor:  0  step number:  46 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  29.58302768555697
maxi score, test score, baseline:  0.031006666666666526 0.2256666666666666 0.2256666666666666
probs:  [0.05701123476441832, 0.08276439981751639, 0.11671372019136853, 0.3984157557662802, 0.16391365088030785, 0.18118123858010882]
actor:  0 policy actor:  0  step number:  52 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  29.775285720825195
printing an ep nov before normalisation:  44.450233811887045
actor:  1 policy actor:  1  step number:  54 total reward:  0.32666666666666655  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.03357999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.05698764068170527, 0.08273012283067889, 0.11666536037883195, 0.3982505405757154, 0.16426019912188886, 0.18110613641117962]
Printing some Q and Qe and total Qs values:  [[0.159]
 [0.218]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]] [[37.912]
 [48.194]
 [37.912]
 [37.912]
 [37.912]
 [37.912]
 [37.912]] [[1.275]
 [1.777]
 [1.275]
 [1.275]
 [1.275]
 [1.275]
 [1.275]]
maxi score, test score, baseline:  0.03357999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.05698764068170527, 0.08273012283067889, 0.11666536037883195, 0.3982505405757154, 0.16426019912188886, 0.18110613641117962]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.03357999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.05669806582920701, 0.08230943457895651, 0.12116102895973443, 0.39622282180316903, 0.16342425621913165, 0.18018439260980146]
printing an ep nov before normalisation:  55.91407398858657
siam score:  -0.7718823
printing an ep nov before normalisation:  41.575256632444855
actor:  1 policy actor:  1  step number:  53 total reward:  0.40000000000000013  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  50 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  59.2009391031974
printing an ep nov before normalisation:  43.635379350293015
printing an ep nov before normalisation:  41.03480602467439
maxi score, test score, baseline:  0.03399333333333321 0.2256666666666666 0.2256666666666666
probs:  [0.056628599861474734, 0.08220851588291861, 0.1210123975211938, 0.39573639335260213, 0.16322372229082963, 0.18119037109098104]
actor:  1 policy actor:  1  step number:  55 total reward:  0.25333333333333274  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.03399333333333321 0.2256666666666666 0.2256666666666666
probs:  [0.05662374973115376, 0.08222737245491295, 0.12102473493262261, 0.39570257636492423, 0.1632289680817236, 0.18119259843466284]
siam score:  -0.7730113
actor:  1 policy actor:  1  step number:  46 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  53.413968086242676
Printing some Q and Qe and total Qs values:  [[-0.012]
 [-0.004]
 [-0.014]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.018]] [[28.916]
 [30.343]
 [26.707]
 [27.533]
 [27.308]
 [27.196]
 [27.208]] [[1.304]
 [1.435]
 [1.112]
 [1.18 ]
 [1.161]
 [1.152]
 [1.151]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  54.0640929531528
maxi score, test score, baseline:  0.03399333333333321 0.2256666666666666 0.2256666666666666
probs:  [0.05649444062328568, 0.08203945561183557, 0.12171572972964895, 0.39479710201275325, 0.16285563528719105, 0.1820976367352855]
printing an ep nov before normalisation:  54.36832047283676
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  66 total reward:  0.2333333333333324  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.03399333333333321 0.2256666666666666 0.2256666666666666
actions average: 
K:  4  action  0 :  tensor([0.3700, 0.0091, 0.1949, 0.0858, 0.0846, 0.1299, 0.1255],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0154,     0.9153,     0.0095,     0.0186,     0.0009,     0.0019,
            0.0385], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0733, 0.1908, 0.3002, 0.0846, 0.0634, 0.1811, 0.1066],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1858, 0.0435, 0.1258, 0.1756, 0.1733, 0.1447, 0.1513],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2487, 0.0110, 0.1329, 0.1234, 0.1891, 0.1277, 0.1674],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1533, 0.1048, 0.1491, 0.1184, 0.1127, 0.2153, 0.1465],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2090, 0.2731, 0.0880, 0.0827, 0.1486, 0.0824, 0.1162],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  1  step number:  59 total reward:  0.1999999999999993  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.03360666666666653 0.2256666666666666 0.2256666666666666
probs:  [0.05640059343619254, 0.08203463245816853, 0.12184917785728158, 0.3941392502209288, 0.1631348310312726, 0.1824415149961559]
printing an ep nov before normalisation:  46.21851961405476
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.413]
 [0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.252]] [[52.548]
 [56.809]
 [52.548]
 [52.548]
 [52.548]
 [52.548]
 [52.548]] [[1.24 ]
 [1.524]
 [1.24 ]
 [1.24 ]
 [1.24 ]
 [1.24 ]
 [1.24 ]]
actor:  0 policy actor:  0  step number:  57 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  1.0
siam score:  -0.7792776
printing an ep nov before normalisation:  50.5039412883656
printing an ep nov before normalisation:  27.797625064849854
actor:  1 policy actor:  1  step number:  43 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.03613999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.05658060821486537, 0.08204352450412768, 0.12159228370370316, 0.39540112313279774, 0.16260233014295583, 0.1817801303015502]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5733333333333337  reward:  1.0 rdn_beta:  1.333
siam score:  -0.7792612
maxi score, test score, baseline:  0.033619999999999865 0.2256666666666666 0.2256666666666666
probs:  [0.056628003050156875, 0.08204586562887724, 0.12152464783410422, 0.3957333528489231, 0.1624621316988035, 0.18160599893913512]
maxi score, test score, baseline:  0.033619999999999865 0.2256666666666666 0.2256666666666666
probs:  [0.056628003050156875, 0.08204586562887724, 0.12152464783410422, 0.3957333528489231, 0.1624621316988035, 0.18160599893913512]
Printing some Q and Qe and total Qs values:  [[0.783]
 [0.8  ]
 [0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]] [[41.844]
 [50.351]
 [41.844]
 [41.844]
 [41.844]
 [41.844]
 [41.844]] [[0.783]
 [0.8  ]
 [0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4266666666666662  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  50.034768972331776
maxi score, test score, baseline:  0.033619999999999865 0.2256666666666666 0.2256666666666666
probs:  [0.05671434317276756, 0.08205013050281722, 0.12140143421308121, 0.3963385823902591, 0.16220672937946876, 0.18128878034160614]
printing an ep nov before normalisation:  41.808899987978364
actor:  1 policy actor:  1  step number:  33 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.959]
 [0.958]
 [0.961]
 [0.958]
 [0.924]
 [0.875]
 [0.943]] [[39.431]
 [38.421]
 [40.022]
 [39.404]
 [42.602]
 [41.578]
 [37.511]] [[0.959]
 [0.958]
 [0.961]
 [0.958]
 [0.924]
 [0.875]
 [0.943]]
printing an ep nov before normalisation:  34.909843339139755
actor:  0 policy actor:  1  step number:  41 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.03401999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.05634310535352049, 0.0815262175858897, 0.12064038778774715, 0.3937389434999547, 0.1675844968885625, 0.1801668488843255]
printing an ep nov before normalisation:  54.36740445689558
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.54335263031613
printing an ep nov before normalisation:  33.98962514796519
maxi score, test score, baseline:  0.03401999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.05634310535352049, 0.0815262175858897, 0.12064038778774715, 0.3937389434999547, 0.1675844968885625, 0.1801668488843255]
printing an ep nov before normalisation:  32.43135971301248
printing an ep nov before normalisation:  44.20637300319457
maxi score, test score, baseline:  0.03401999999999987 0.2256666666666666 0.2256666666666666
maxi score, test score, baseline:  0.03401999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.05634310535352049, 0.0815262175858897, 0.12064038778774715, 0.3937389434999547, 0.1675844968885625, 0.1801668488843255]
maxi score, test score, baseline:  0.03401999999999987 0.2256666666666666 0.2256666666666666
actor:  1 policy actor:  1  step number:  48 total reward:  0.4866666666666668  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.03401999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.05648229153139256, 0.08153569684517087, 0.12044840749575002, 0.3947146011027406, 0.1671507285410908, 0.17966827448385533]
actor:  1 policy actor:  1  step number:  52 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  14.365577941185848
maxi score, test score, baseline:  0.03401999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.056419737568095554, 0.08144533071138625, 0.12031484376350748, 0.3942765702515556, 0.16696531984410945, 0.18057819786134569]
maxi score, test score, baseline:  0.03401999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.056419737568095554, 0.08144533071138625, 0.12031484376350748, 0.3942765702515556, 0.16696531984410945, 0.18057819786134569]
maxi score, test score, baseline:  0.03401999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.056419737568095554, 0.08144533071138625, 0.12031484376350748, 0.3942765702515556, 0.16696531984410945, 0.18057819786134569]
actor:  1 policy actor:  1  step number:  60 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  2.0
from probs:  [0.056419737568095554, 0.08144533071138625, 0.12031484376350748, 0.3942765702515556, 0.16696531984410945, 0.18057819786134569]
maxi score, test score, baseline:  0.03401999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.056419737568095554, 0.08144533071138625, 0.12031484376350748, 0.3942765702515556, 0.16696531984410945, 0.18057819786134569]
printing an ep nov before normalisation:  28.10733650686449
printing an ep nov before normalisation:  39.45058911763173
maxi score, test score, baseline:  0.03401999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.056419737568095554, 0.08144533071138625, 0.12031484376350748, 0.3942765702515556, 0.16696531984410945, 0.18057819786134569]
actor:  1 policy actor:  1  step number:  39 total reward:  0.5733333333333334  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 53.655749075163214
actor:  1 policy actor:  1  step number:  47 total reward:  0.4533333333333335  reward:  1.0 rdn_beta:  2.0
siam score:  -0.77607995
maxi score, test score, baseline:  0.03401999999999987 0.2256666666666666 0.2256666666666666
maxi score, test score, baseline:  0.03401999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.05655046039380353, 0.08129819331740323, 0.11973613652124536, 0.3951937276267667, 0.165868650391792, 0.18135283174898917]
actor:  0 policy actor:  1  step number:  56 total reward:  0.23333333333333306  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.609]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]] [[40.76 ]
 [33.729]
 [33.729]
 [33.729]
 [33.729]
 [33.729]
 [33.729]] [[2.224]
 [1.864]
 [1.864]
 [1.864]
 [1.864]
 [1.864]
 [1.864]]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.03648666666666653 0.2256666666666666 0.2256666666666666
actor:  1 policy actor:  1  step number:  51 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.03648666666666653 0.2256666666666666 0.2256666666666666
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.464]
 [0.364]
 [0.363]
 [0.363]
 [0.362]
 [0.361]] [[28.221]
 [34.559]
 [27.331]
 [27.508]
 [27.525]
 [27.236]
 [26.984]] [[0.995]
 [1.376]
 [0.963]
 [0.969]
 [0.97 ]
 [0.957]
 [0.945]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4266666666666661  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  0
printing an ep nov before normalisation:  25.587775363710197
maxi score, test score, baseline:  0.03648666666666653 0.2256666666666666 0.2256666666666666
probs:  [0.056235110352327616, 0.08083704513916717, 0.11904853606484248, 0.3929855350570036, 0.17059154905657964, 0.18030222433007953]
actor:  0 policy actor:  1  step number:  42 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  12.662804126739502
actor:  1 policy actor:  1  step number:  39 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  64 total reward:  0.16666666666666663  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  50.916498444054255
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
Printing some Q and Qe and total Qs values:  [[0.928]
 [0.836]
 [0.928]
 [0.928]
 [0.928]
 [0.928]
 [0.928]] [[43.011]
 [41.878]
 [43.011]
 [43.011]
 [43.011]
 [43.011]
 [43.011]] [[0.928]
 [0.836]
 [0.928]
 [0.928]
 [0.928]
 [0.928]
 [0.928]]
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([0.4668, 0.0162, 0.0755, 0.0805, 0.2160, 0.0738, 0.0711],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0029,     0.9818,     0.0012,     0.0047,     0.0006,     0.0005,
            0.0084], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0975, 0.0051, 0.5807, 0.0624, 0.0478, 0.1316, 0.0750],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2108, 0.0143, 0.1008, 0.3117, 0.1337, 0.1199, 0.1087],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2499, 0.0032, 0.0880, 0.1010, 0.3942, 0.0743, 0.0893],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1599, 0.0029, 0.1765, 0.1173, 0.1146, 0.3287, 0.1001],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1450, 0.2276, 0.0647, 0.1113, 0.0661, 0.0641, 0.3211],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]] [[65.179]
 [64.201]
 [64.201]
 [64.201]
 [64.201]
 [64.201]
 [64.201]] [[2.445]
 [2.414]
 [2.414]
 [2.414]
 [2.414]
 [2.414]
 [2.414]]
line 256 mcts: sample exp_bonus 34.734729879964185
maxi score, test score, baseline:  0.03673999999999986 0.2256666666666666 0.2256666666666666
actor:  1 policy actor:  1  step number:  41 total reward:  0.5733333333333336  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.03673999999999986 0.2256666666666666 0.2256666666666666
actor:  0 policy actor:  1  step number:  47 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.03945999999999986 0.2256666666666666 0.2256666666666666
probs:  [0.055561868284510096, 0.08012952355483233, 0.11840516973344789, 0.3882691377083633, 0.1728505650329203, 0.18478373568592615]
maxi score, test score, baseline:  0.03945999999999986 0.2256666666666666 0.2256666666666666
probs:  [0.055561868284510096, 0.08012952355483233, 0.11840516973344789, 0.3882691377083633, 0.1728505650329203, 0.18478373568592615]
maxi score, test score, baseline:  0.03945999999999986 0.2256666666666666 0.2256666666666666
probs:  [0.055561868284510096, 0.08012952355483233, 0.11840516973344789, 0.3882691377083633, 0.1728505650329203, 0.18478373568592615]
actor:  1 policy actor:  1  step number:  64 total reward:  0.23333333333333317  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[ 0.018]
 [ 0.128]
 [ 0.006]
 [ 0.013]
 [-0.   ]
 [-0.   ]
 [-0.002]] [[28.651]
 [34.14 ]
 [29.011]
 [30.826]
 [29.16 ]
 [28.801]
 [26.86 ]] [[0.182]
 [0.351]
 [0.174]
 [0.201]
 [0.17 ]
 [0.166]
 [0.143]]
maxi score, test score, baseline:  0.03945999999999986 0.2256666666666666 0.2256666666666666
probs:  [0.05556199558096724, 0.08012953837512862, 0.11840500931936833, 0.38827002999267257, 0.17285015535577627, 0.18478327137608697]
actor:  1 policy actor:  1  step number:  33 total reward:  0.56  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  4  action  0 :  tensor([0.4247, 0.0656, 0.0877, 0.0996, 0.1101, 0.0968, 0.1153],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0336, 0.8008, 0.0304, 0.0308, 0.0195, 0.0246, 0.0602],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.4266, 0.0004, 0.1154, 0.1274, 0.0664, 0.1030, 0.1607],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2224, 0.0083, 0.1453, 0.1563, 0.1247, 0.1357, 0.2074],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1579, 0.0370, 0.0726, 0.0761, 0.5049, 0.0545, 0.0970],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1302, 0.0169, 0.1378, 0.1258, 0.1150, 0.3650, 0.1093],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1781, 0.0212, 0.0989, 0.1206, 0.1153, 0.1375, 0.3285],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.03945999999999986 0.2256666666666666 0.2256666666666666
probs:  [0.05558090624094811, 0.08013174001993677, 0.11838117883592875, 0.38840258423304047, 0.17278929533385884, 0.1847142953362872]
printing an ep nov before normalisation:  25.487031936645508
maxi score, test score, baseline:  0.03945999999999986 0.2256666666666666 0.2256666666666666
probs:  [0.055616114605394305, 0.08018253776000904, 0.11845626434050455, 0.3886491288886053, 0.17289892909748072, 0.18419702530800605]
maxi score, test score, baseline:  0.03945999999999986 0.2256666666666666 0.2256666666666666
maxi score, test score, baseline:  0.03945999999999986 0.2256666666666666 0.2256666666666666
probs:  [0.055616114605394305, 0.08018253776000904, 0.11845626434050455, 0.3886491288886053, 0.17289892909748072, 0.18419702530800605]
Printing some Q and Qe and total Qs values:  [[-0.147]
 [-0.149]
 [-0.15 ]
 [-0.146]
 [-0.132]
 [-0.15 ]
 [-0.152]] [[14.168]
 [13.383]
 [13.821]
 [14.652]
 [14.988]
 [15.173]
 [14.98 ]] [[0.631]
 [0.586]
 [0.61 ]
 [0.659]
 [0.692]
 [0.684]
 [0.671]]
maxi score, test score, baseline:  0.03945999999999986 0.2256666666666666 0.2256666666666666
probs:  [0.055616114605394305, 0.08018253776000904, 0.11845626434050455, 0.3886491288886053, 0.17289892909748072, 0.18419702530800605]
printing an ep nov before normalisation:  33.43197504679362
printing an ep nov before normalisation:  37.778336935573165
maxi score, test score, baseline:  0.03945999999999986 0.2256666666666666 0.2256666666666666
probs:  [0.055616114605394305, 0.08018253776000904, 0.11845626434050455, 0.3886491288886053, 0.17289892909748072, 0.18419702530800605]
printing an ep nov before normalisation:  39.01695970584019
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.022]
 [-0.006]
 [-0.003]] [[40.556]
 [40.556]
 [40.556]
 [40.556]
 [44.648]
 [43.318]
 [40.556]] [[0.801]
 [0.801]
 [0.801]
 [0.801]
 [0.908]
 [0.883]
 [0.801]]
using explorer policy with actor:  1
siam score:  -0.76490647
printing an ep nov before normalisation:  33.303469313071105
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.437]
 [0.408]
 [0.408]
 [0.406]
 [0.405]
 [0.409]] [[31.309]
 [32.788]
 [27.413]
 [27.463]
 [27.379]
 [27.08 ]
 [27.645]] [[1.788]
 [1.907]
 [1.46 ]
 [1.464]
 [1.456]
 [1.432]
 [1.479]]
line 256 mcts: sample exp_bonus 34.30387002326896
maxi score, test score, baseline:  0.03945999999999986 0.2256666666666666 0.2256666666666666
probs:  [0.05560517896263875, 0.08022739450257935, 0.11848609613648614, 0.38857289227366426, 0.1729073885787388, 0.18420104954589267]
maxi score, test score, baseline:  0.03945999999999986 0.2256666666666666 0.2256666666666666
probs:  [0.05560517896263875, 0.08022739450257935, 0.11848609613648614, 0.38857289227366426, 0.1729073885787388, 0.18420104954589267]
actor:  1 policy actor:  1  step number:  66 total reward:  0.21999999999999909  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.03945999999999986 0.2256666666666666 0.2256666666666666
probs:  [0.05560517896263875, 0.08022739450257935, 0.11848609613648614, 0.38857289227366426, 0.17290738857873883, 0.1842010495458927]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.797]
 [0.844]
 [0.797]
 [0.797]
 [0.797]
 [0.797]
 [0.797]] [[48.196]
 [47.702]
 [48.196]
 [48.196]
 [48.196]
 [48.196]
 [48.196]] [[0.797]
 [0.844]
 [0.797]
 [0.797]
 [0.797]
 [0.797]
 [0.797]]
maxi score, test score, baseline:  0.03945999999999986 0.2256666666666666 0.2256666666666666
probs:  [0.05576689597955181, 0.08024568141225183, 0.1182815172496451, 0.3897064464594135, 0.17238579306118473, 0.18361366583795294]
maxi score, test score, baseline:  0.03945999999999986 0.2256666666666666 0.2256666666666666
probs:  [0.05576689597955181, 0.08024568141225183, 0.1182815172496451, 0.3897064464594135, 0.17238579306118473, 0.18361366583795294]
printing an ep nov before normalisation:  44.13549570347698
printing an ep nov before normalisation:  41.27996444586032
printing an ep nov before normalisation:  32.0864902349264
actor:  0 policy actor:  0  step number:  34 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  51.64008724731362
maxi score, test score, baseline:  0.04261999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.05576689597955181, 0.08024568141225183, 0.1182815172496451, 0.3897064464594135, 0.17238579306118473, 0.18361366583795294]
printing an ep nov before normalisation:  40.01567840825909
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.639]
 [0.5  ]
 [0.466]
 [0.466]
 [0.466]
 [0.466]] [[19.379]
 [16.386]
 [21.372]
 [19.379]
 [19.379]
 [19.379]
 [19.379]] [[1.647]
 [1.637]
 [1.802]
 [1.647]
 [1.647]
 [1.647]
 [1.647]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.34666666666666657  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.04261999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.05573537773762776, 0.08020029516032617, 0.11821458248775506, 0.3894857408479314, 0.17285428542904313, 0.1835097183373165]
Printing some Q and Qe and total Qs values:  [[-0.009]
 [ 0.085]
 [-0.009]
 [-0.009]
 [-0.027]
 [-0.009]
 [-0.009]] [[28.888]
 [39.058]
 [28.888]
 [28.888]
 [26.01 ]
 [28.888]
 [28.888]] [[0.735]
 [1.297]
 [0.735]
 [0.735]
 [0.586]
 [0.735]
 [0.735]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.04261999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.055650949633776904, 0.08019062026796978, 0.11832106127011995, 0.3888939433796803, 0.17312771728954, 0.18381570815891315]
Printing some Q and Qe and total Qs values:  [[-0.039]
 [-0.003]
 [-0.012]
 [-0.025]
 [-0.091]
 [-0.01 ]
 [-0.023]] [[28.434]
 [38.27 ]
 [26.422]
 [23.965]
 [26.974]
 [25.068]
 [25.89 ]] [[0.501]
 [0.925]
 [0.448]
 [0.339]
 [0.391]
 [0.397]
 [0.417]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.04261999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.055650949633776904, 0.08019062026796978, 0.11832106127011995, 0.3888939433796803, 0.17312771728954, 0.18381570815891315]
from probs:  [0.055650949633776904, 0.08019062026796978, 0.11832106127011995, 0.3888939433796803, 0.17312771728954, 0.18381570815891315]
line 256 mcts: sample exp_bonus 30.75870079087349
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.769]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]] [[46.061]
 [48.645]
 [46.061]
 [46.061]
 [46.061]
 [46.061]
 [46.061]] [[0.695]
 [0.769]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]]
printing an ep nov before normalisation:  42.8726863861084
maxi score, test score, baseline:  0.04261999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.05555789240402931, 0.08017995653587434, 0.11843842290755852, 0.38824166018895057, 0.17342909574413046, 0.1841529722194569]
printing an ep nov before normalisation:  41.96048045573878
printing an ep nov before normalisation:  38.596343032511804
maxi score, test score, baseline:  0.04261999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.05555789240402931, 0.08017995653587434, 0.11843842290755852, 0.38824166018895057, 0.17342909574413046, 0.1841529722194569]
printing an ep nov before normalisation:  22.576472784725564
printing an ep nov before normalisation:  28.386482081166488
maxi score, test score, baseline:  0.04261999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.05555789240402931, 0.08017995653587434, 0.11843842290755852, 0.38824166018895057, 0.17342909574413046, 0.1841529722194569]
maxi score, test score, baseline:  0.04261999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.05555789240402931, 0.08017995653587434, 0.11843842290755852, 0.38824166018895057, 0.17342909574413046, 0.1841529722194569]
printing an ep nov before normalisation:  27.736389549795888
printing an ep nov before normalisation:  16.78489999345925
printing an ep nov before normalisation:  34.41010431110758
actor:  0 policy actor:  0  step number:  47 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  45 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  44.52964873237861
maxi score, test score, baseline:  0.045886666666666534 0.2256666666666666 0.2256666666666666
probs:  [0.05555789240402931, 0.08017995653587434, 0.11843842290755852, 0.38824166018895057, 0.17342909574413046, 0.1841529722194569]
maxi score, test score, baseline:  0.045886666666666534 0.2256666666666666 0.2256666666666666
probs:  [0.05555789240402931, 0.08017995653587434, 0.11843842290755852, 0.38824166018895057, 0.17342909574413046, 0.1841529722194569]
printing an ep nov before normalisation:  32.815760994099726
maxi score, test score, baseline:  0.045886666666666534 0.2256666666666666 0.2256666666666666
probs:  [0.05555789240402931, 0.08017995653587434, 0.11843842290755852, 0.38824166018895057, 0.17342909574413046, 0.1841529722194569]
maxi score, test score, baseline:  0.045886666666666534 0.2256666666666666 0.2256666666666666
maxi score, test score, baseline:  0.045886666666666534 0.2256666666666666 0.2256666666666666
probs:  [0.05555789240402931, 0.08017995653587434, 0.11843842290755852, 0.38824166018895057, 0.17342909574413046, 0.1841529722194569]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.045886666666666534 0.2256666666666666 0.2256666666666666
actor:  1 policy actor:  1  step number:  44 total reward:  0.4600000000000001  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.015]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]] [[35.014]
 [35.415]
 [35.415]
 [35.415]
 [35.415]
 [35.415]
 [35.415]] [[1.015]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]]
maxi score, test score, baseline:  0.045886666666666534 0.2256666666666666 0.2256666666666666
probs:  [0.05566606993617612, 0.08019235295283934, 0.11830199189256645, 0.38899992890192836, 0.17307874814031454, 0.18376090817617521]
actor:  0 policy actor:  0  step number:  38 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 34.88300174474716
maxi score, test score, baseline:  0.04628666666666654 0.2256666666666666 0.2256666666666666
probs:  [0.05566606993617612, 0.08019235295283934, 0.11830199189256645, 0.38899992890192836, 0.17307874814031454, 0.18376090817617521]
printing an ep nov before normalisation:  21.824025633981417
printing an ep nov before normalisation:  52.4559055100103
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.656]
 [0.579]
 [0.614]
 [0.583]
 [0.591]
 [0.605]] [[18.11 ]
 [16.565]
 [17.726]
 [17.892]
 [17.687]
 [17.901]
 [17.754]] [[1.427]
 [1.41 ]
 [1.386]
 [1.429]
 [1.389]
 [1.407]
 [1.414]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.77788819451305
actor:  1 policy actor:  1  step number:  62 total reward:  0.2999999999999994  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.04628666666666654 0.2256666666666666 0.2256666666666666
Printing some Q and Qe and total Qs values:  [[-0.021]
 [-0.017]
 [-0.027]
 [-0.02 ]
 [-0.023]
 [-0.025]
 [-0.025]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.021]
 [-0.017]
 [-0.027]
 [-0.02 ]
 [-0.023]
 [-0.025]
 [-0.025]]
maxi score, test score, baseline:  0.04628666666666654 0.2256666666666666 0.2256666666666666
probs:  [0.05566366893522588, 0.08018889154105019, 0.11834008391606471, 0.38898311598063995, 0.17307127072087441, 0.18375296890614495]
printing an ep nov before normalisation:  36.82369305494289
printing an ep nov before normalisation:  18.366017112076666
maxi score, test score, baseline:  0.04628666666666654 0.2256666666666666 0.2256666666666666
probs:  [0.05564173623363923, 0.08018637567849164, 0.11836777269519205, 0.388829379066602, 0.17314229067220296, 0.1838324456538722]
actor:  1 policy actor:  1  step number:  52 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.04628666666666654 0.2256666666666666 0.2256666666666666
probs:  [0.055560474822579364, 0.08006918226252174, 0.11819468380973858, 0.3882603492348598, 0.17435178978978555, 0.18356352008051505]
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.417]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]] [[38.889]
 [41.066]
 [38.889]
 [38.889]
 [38.889]
 [38.889]
 [38.889]] [[1.562]
 [1.75 ]
 [1.562]
 [1.562]
 [1.562]
 [1.562]
 [1.562]]
actor:  1 policy actor:  1  step number:  62 total reward:  0.3399999999999991  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  45.27414730273901
actions average: 
K:  2  action  0 :  tensor([0.4669, 0.0210, 0.0888, 0.0849, 0.1496, 0.0810, 0.1078],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0157, 0.9341, 0.0093, 0.0080, 0.0072, 0.0076, 0.0180],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1049, 0.0148, 0.4600, 0.0863, 0.0811, 0.1483, 0.1046],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1359, 0.1275, 0.0824, 0.3199, 0.1213, 0.1244, 0.0886],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1653, 0.0167, 0.0937, 0.0992, 0.4128, 0.1136, 0.0988],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1960, 0.0114, 0.1258, 0.1086, 0.1281, 0.3295, 0.1006],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1143, 0.1420, 0.1186, 0.1146, 0.1085, 0.1039, 0.2982],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  27.368356262155352
maxi score, test score, baseline:  0.04628666666666654 0.2256666666666666 0.2256666666666666
probs:  [0.05553240244304507, 0.08002869689549306, 0.11813488892281447, 0.3880637735023607, 0.17426355287033243, 0.18397668536595424]
printing an ep nov before normalisation:  30.635905727479578
actor:  1 policy actor:  1  step number:  47 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  49 total reward:  0.4399999999999996  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  64.91413304729838
printing an ep nov before normalisation:  46.88340482728081
printing an ep nov before normalisation:  29.154694080352783
printing an ep nov before normalisation:  51.04494836594068
actor:  1 policy actor:  1  step number:  52 total reward:  0.3799999999999992  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.106]
 [-0.109]
 [-0.098]
 [-0.105]
 [-0.109]
 [-0.109]
 [-0.109]] [[29.967]
 [30.971]
 [31.006]
 [31.387]
 [30.971]
 [30.971]
 [30.971]] [[1.434]
 [1.483]
 [1.496]
 [1.51 ]
 [1.483]
 [1.483]
 [1.483]]
maxi score, test score, baseline:  0.04628666666666654 0.2256666666666666 0.2256666666666666
probs:  [0.055626558305298654, 0.07996689011711465, 0.11783046821176356, 0.3887241416160788, 0.17459887765830429, 0.18325306409144015]
printing an ep nov before normalisation:  51.65678176173307
actions average: 
K:  2  action  0 :  tensor([0.4872, 0.0138, 0.1021, 0.0856, 0.1108, 0.1030, 0.0975],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0092, 0.9518, 0.0061, 0.0061, 0.0022, 0.0059, 0.0186],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1828, 0.0039, 0.3140, 0.1231, 0.1044, 0.1243, 0.1475],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2500, 0.0124, 0.1202, 0.2243, 0.1298, 0.1273, 0.1359],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1001, 0.0527, 0.0448, 0.0538, 0.5999, 0.0725, 0.0762],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1385, 0.0191, 0.1197, 0.1105, 0.1156, 0.3617, 0.1349],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0777, 0.2773, 0.0493, 0.0950, 0.0569, 0.0642, 0.3796],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.375]
 [0.222]
 [0.282]
 [0.222]
 [0.222]
 [0.222]] [[35.009]
 [38.491]
 [35.009]
 [37.302]
 [35.009]
 [35.009]
 [35.009]] [[0.989]
 [1.296]
 [0.989]
 [1.151]
 [0.989]
 [0.989]
 [0.989]]
printing an ep nov before normalisation:  39.624441636681524
maxi score, test score, baseline:  0.04628666666666654 0.2256666666666666 0.2256666666666666
actor:  1 policy actor:  1  step number:  48 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  23.91002927856426
maxi score, test score, baseline:  0.04628666666666654 0.2256666666666666 0.2256666666666666
probs:  [0.055495824513272306, 0.07977881423032007, 0.11755319153263165, 0.38780868092152093, 0.17654182700836163, 0.18282166179389345]
actor:  1 policy actor:  1  step number:  53 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  65 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.265]
 [0.216]
 [0.216]
 [0.184]
 [0.185]
 [0.216]] [[38.448]
 [34.925]
 [39.017]
 [39.017]
 [37.979]
 [37.846]
 [39.017]] [[1.775]
 [1.626]
 [1.863]
 [1.863]
 [1.758]
 [1.75 ]
 [1.863]]
printing an ep nov before normalisation:  44.598147289804274
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.807]
 [0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]] [[41.594]
 [38.723]
 [38.579]
 [38.579]
 [38.579]
 [38.579]
 [38.579]] [[2.054]
 [1.986]
 [1.924]
 [1.924]
 [1.924]
 [1.924]
 [1.924]]
maxi score, test score, baseline:  0.04628666666666654 0.2256666666666666 0.2256666666666666
maxi score, test score, baseline:  0.04628666666666654 0.2256666666666666 0.2256666666666666
probs:  [0.05553133107621609, 0.07979742236238281, 0.11735209886092347, 0.3880574866958025, 0.17649309816493966, 0.18276856283973542]
printing an ep nov before normalisation:  27.33307942477952
actor:  1 policy actor:  1  step number:  52 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  44 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.04628666666666654 0.2256666666666666 0.2256666666666666
probs:  [0.05552070377462964, 0.0797378674293273, 0.11721682257043994, 0.3879833034282499, 0.17623857609298538, 0.1833027267043679]
line 256 mcts: sample exp_bonus 42.73822055069273
maxi score, test score, baseline:  0.04628666666666654 0.2256666666666666 0.2256666666666666
probs:  [0.05552070377462964, 0.0797378674293273, 0.11721682257043994, 0.3879833034282499, 0.17623857609298538, 0.1833027267043679]
maxi score, test score, baseline:  0.04628666666666654 0.2256666666666666 0.2256666666666666
maxi score, test score, baseline:  0.04628666666666654 0.2256666666666666 0.2256666666666666
probs:  [0.05552070377462964, 0.0797378674293273, 0.11721682257043994, 0.3879833034282499, 0.17623857609298538, 0.1833027267043679]
printing an ep nov before normalisation:  0.0
actor:  1 policy actor:  1  step number:  53 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.04628666666666654 0.2256666666666666 0.2256666666666666
probs:  [0.055504148394327706, 0.0800127357263291, 0.11718182614958657, 0.3878673745962346, 0.1761859380024229, 0.1832479771310991]
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.277445316314697
maxi score, test score, baseline:  0.04628666666666654 0.2256666666666666 0.2256666666666666
probs:  [0.055504148394327706, 0.0800127357263291, 0.11718182614958657, 0.3878673745962346, 0.1761859380024229, 0.1832479771310991]
actor:  1 policy actor:  1  step number:  37 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  1.667
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [ 0.7420],
        [ 0.1917],
        [-0.0000],
        [ 0.2057],
        [ 0.7430],
        [ 0.0000],
        [ 0.0731],
        [ 0.8827],
        [-0.4748]], dtype=torch.float64)
-0.93815238 -0.93815238
-0.09703970119800001 0.644979984029069
-0.084359833866 0.10734486550233481
-0.42370509060000006 -0.42370509060000006
-0.045546567066 0.16010964756062768
-0.09703970119800001 0.6459801569189442
-0.742939270326 -0.742939270326
-0.045026434398 0.028067484322356978
-0.084359833866 0.7983443088456621
-0.057834381198 -0.5326787034934376
maxi score, test score, baseline:  0.04628666666666654 0.2256666666666666 0.2256666666666666
probs:  [0.055337236802878526, 0.07977194479181654, 0.11682899171189136, 0.38669857838052685, 0.17866725719713694, 0.18269599111574977]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.043686666666666533 0.2256666666666666 0.2256666666666666
probs:  [0.05511189030623063, 0.07944685416721776, 0.11635263171950849, 0.3851205925125695, 0.1779384692294489, 0.18602956206502472]
maxi score, test score, baseline:  0.043686666666666533 0.2256666666666666 0.2256666666666666
probs:  [0.05511189030623063, 0.07944685416721776, 0.11635263171950849, 0.3851205925125695, 0.1779384692294489, 0.18602956206502472]
printing an ep nov before normalisation:  41.6947572761233
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.462]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]] [[38.517]
 [42.47 ]
 [38.517]
 [38.517]
 [38.517]
 [38.517]
 [38.517]] [[1.721]
 [2.048]
 [1.721]
 [1.721]
 [1.721]
 [1.721]
 [1.721]]
printing an ep nov before normalisation:  47.59758611626444
siam score:  -0.775998
maxi score, test score, baseline:  0.043686666666666533 0.2256666666666666 0.2256666666666666
probs:  [0.05511189030623063, 0.07944685416721776, 0.11635263171950849, 0.3851205925125695, 0.1779384692294489, 0.18602956206502472]
actor:  1 policy actor:  1  step number:  40 total reward:  0.54  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.043686666666666533 0.2256666666666666 0.2256666666666666
probs:  [0.05490612967052439, 0.07915001860698233, 0.11591767428144868, 0.3836797563773015, 0.17727302344814247, 0.18907339761560069]
siam score:  -0.78128797
maxi score, test score, baseline:  0.043686666666666533 0.2256666666666666 0.2256666666666666
probs:  [0.05490612967052439, 0.07915001860698233, 0.11591767428144868, 0.3836797563773015, 0.17727302344814247, 0.18907339761560069]
actor:  1 policy actor:  1  step number:  62 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.074]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]
 [-0.073]] [[39.516]
 [34.92 ]
 [34.92 ]
 [34.92 ]
 [34.92 ]
 [34.92 ]
 [34.92 ]] [[1.014]
 [0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  45.10914022095106
printing an ep nov before normalisation:  54.523623656733115
printing an ep nov before normalisation:  31.809022769084883
maxi score, test score, baseline:  0.043686666666666533 0.2256666666666666 0.2256666666666666
probs:  [0.05499509587962479, 0.07885083707892745, 0.11569162074238509, 0.38430060031967117, 0.1771690008957194, 0.18899284508367206]
maxi score, test score, baseline:  0.043686666666666533 0.2256666666666666 0.2256666666666666
probs:  [0.05499509587962479, 0.07885083707892745, 0.11569162074238509, 0.38430060031967117, 0.1771690008957194, 0.18899284508367206]
printing an ep nov before normalisation:  42.685611065976325
maxi score, test score, baseline:  0.043686666666666533 0.2256666666666666 0.2256666666666666
probs:  [0.05499509587962479, 0.07885083707892745, 0.11569162074238509, 0.38430060031967117, 0.1771690008957194, 0.18899284508367206]
printing an ep nov before normalisation:  44.189413010506634
maxi score, test score, baseline:  0.043686666666666533 0.2256666666666666 0.2256666666666666
probs:  [0.05499509587962479, 0.07885083707892745, 0.11569162074238509, 0.38430060031967117, 0.1771690008957194, 0.18899284508367206]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.92101224279028
printing an ep nov before normalisation:  47.15492736560735
printing an ep nov before normalisation:  41.213600332313085
printing an ep nov before normalisation:  25.608158346899838
maxi score, test score, baseline:  0.043686666666666533 0.2256666666666666 0.2256666666666666
probs:  [0.05499509587962479, 0.07885083707892745, 0.11569162074238509, 0.38430060031967117, 0.1771690008957194, 0.18899284508367206]
actor:  1 policy actor:  1  step number:  50 total reward:  0.4866666666666668  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.99941986518053
maxi score, test score, baseline:  0.043686666666666533 0.2256666666666666 0.2256666666666666
probs:  [0.055080963574525584, 0.07886450438405793, 0.11559378788838306, 0.3849024769159095, 0.17688510420190431, 0.1886731630352196]
printing an ep nov before normalisation:  10.602860450744629
printing an ep nov before normalisation:  33.84105819449409
printing an ep nov before normalisation:  32.738850842008794
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]
 [0.551]] [[33.779]
 [33.779]
 [33.779]
 [33.779]
 [33.779]
 [33.779]
 [33.779]] [[1.843]
 [1.843]
 [1.843]
 [1.843]
 [1.843]
 [1.843]
 [1.843]]
siam score:  -0.7895911
maxi score, test score, baseline:  0.043686666666666533 0.2256666666666666 0.2256666666666666
probs:  [0.055080963574525584, 0.07886450438405793, 0.11559378788838306, 0.3849024769159095, 0.17688510420190431, 0.1886731630352196]
actor:  1 policy actor:  1  step number:  57 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.456470581614326
actor:  1 policy actor:  1  step number:  53 total reward:  0.4  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  0  action  0 :  tensor([0.5241, 0.0161, 0.0700, 0.0697, 0.1762, 0.0576, 0.0863],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0077, 0.9432, 0.0037, 0.0161, 0.0040, 0.0044, 0.0210],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1434, 0.0222, 0.4187, 0.0905, 0.1014, 0.1155, 0.1084],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2784, 0.0022, 0.1422, 0.1417, 0.1154, 0.1290, 0.1911],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1810, 0.0016, 0.0824, 0.1094, 0.3887, 0.0855, 0.1515],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1287, 0.0082, 0.0917, 0.0989, 0.0967, 0.4160, 0.1597],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2060, 0.0771, 0.1331, 0.1243, 0.1396, 0.1335, 0.1863],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.043686666666666533 0.2256666666666666 0.2256666666666666
Printing some Q and Qe and total Qs values:  [[-0.   ]
 [-0.013]
 [ 0.023]
 [-0.   ]
 [-0.   ]
 [-0.012]
 [-0.   ]] [[26.185]
 [27.838]
 [26.694]
 [26.185]
 [26.185]
 [37.084]
 [26.185]] [[0.405]
 [0.437]
 [0.442]
 [0.405]
 [0.405]
 [0.687]
 [0.405]]
printing an ep nov before normalisation:  34.443383866290944
printing an ep nov before normalisation:  48.42656886804001
maxi score, test score, baseline:  0.043686666666666533 0.2256666666666666 0.2256666666666666
actor:  1 policy actor:  1  step number:  57 total reward:  0.3333333333333325  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.043686666666666533 0.2256666666666666 0.2256666666666666
probs:  [0.055139526825459396, 0.07886634453997649, 0.11561484056988819, 0.38531300761883885, 0.1766531679053826, 0.18841311254045454]
maxi score, test score, baseline:  0.043686666666666533 0.2256666666666666 0.2256666666666666
probs:  [0.055139526825459396, 0.07886634453997649, 0.11561484056988819, 0.38531300761883885, 0.1766531679053826, 0.18841311254045454]
printing an ep nov before normalisation:  38.36573219902353
maxi score, test score, baseline:  0.043686666666666533 0.2256666666666666 0.2256666666666666
probs:  [0.055139526825459396, 0.07886634453997649, 0.11561484056988819, 0.38531300761883885, 0.1766531679053826, 0.18841311254045454]
Printing some Q and Qe and total Qs values:  [[1.006]
 [1.005]
 [1.005]
 [1.005]
 [1.005]
 [1.005]
 [1.005]] [[11.251]
 [ 5.984]
 [ 5.41 ]
 [21.965]
 [ 5.535]
 [11.72 ]
 [ 7.646]] [[1.006]
 [1.005]
 [1.005]
 [1.005]
 [1.005]
 [1.005]
 [1.005]]
actor:  0 policy actor:  0  step number:  33 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.044219999999999864 0.2256666666666666 0.2256666666666666
probs:  [0.055139526825459396, 0.07886634453997649, 0.11561484056988819, 0.38531300761883885, 0.1766531679053826, 0.18841311254045454]
printing an ep nov before normalisation:  28.15345048904419
maxi score, test score, baseline:  0.044219999999999864 0.2256666666666666 0.2256666666666666
probs:  [0.055139526825459396, 0.07886634453997649, 0.11561484056988819, 0.38531300761883885, 0.1766531679053826, 0.18841311254045454]
printing an ep nov before normalisation:  45.31404503247239
actor:  1 policy actor:  1  step number:  61 total reward:  0.25333333333333263  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.044219999999999864 0.2256666666666666 0.2256666666666666
probs:  [0.055137937509304416, 0.0788640696766269, 0.11561150391946451, 0.3853018784854943, 0.17664806765334276, 0.18843654275576713]
maxi score, test score, baseline:  0.044219999999999864 0.2256666666666666 0.2256666666666666
probs:  [0.055137937509304416, 0.0788640696766269, 0.11561150391946451, 0.3853018784854943, 0.17664806765334276, 0.18843654275576713]
printing an ep nov before normalisation:  43.67312226642412
actor:  1 policy actor:  1  step number:  58 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  18.091715312468825
maxi score, test score, baseline:  0.044219999999999864 0.2256666666666666 0.2256666666666666
probs:  [0.05505963782485232, 0.07875199563655344, 0.11544711958694483, 0.38475358755774797, 0.17781916804721126, 0.18816849134669017]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
actor:  1 policy actor:  1  step number:  51 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  40 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.678]
 [0.641]
 [0.653]
 [0.64 ]
 [0.636]
 [0.655]] [[31.76 ]
 [29.662]
 [30.473]
 [29.424]
 [30.421]
 [29.435]
 [29.373]] [[0.623]
 [0.678]
 [0.641]
 [0.653]
 [0.64 ]
 [0.636]
 [0.655]]
siam score:  -0.77633697
maxi score, test score, baseline:  0.04472666666666654 0.2256666666666666 0.2256666666666666
probs:  [0.054882552248215435, 0.07868367175554711, 0.1161131529624819, 0.38351255016565267, 0.1782056200791107, 0.18860245278899218]
line 256 mcts: sample exp_bonus 34.91309255626866
printing an ep nov before normalisation:  3.844725142698735e-06
printing an ep nov before normalisation:  40.16754150390625
printing an ep nov before normalisation:  31.987406572825737
printing an ep nov before normalisation:  32.143094539642334
actor:  0 policy actor:  0  step number:  51 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  0.667
from probs:  [0.054882552248215435, 0.07868367175554711, 0.1161131529624819, 0.38351255016565267, 0.1782056200791107, 0.18860245278899218]
printing an ep nov before normalisation:  28.932220935821533
from probs:  [0.054882552248215435, 0.07868367175554711, 0.1161131529624819, 0.38351255016565267, 0.1782056200791107, 0.18860245278899218]
printing an ep nov before normalisation:  38.212723195122265
maxi score, test score, baseline:  0.047206666666666536 0.2256666666666666 0.2256666666666666
probs:  [0.05488911375146403, 0.07857333525026285, 0.11612705272573383, 0.38355849675318077, 0.17822696139385222, 0.18862504012550624]
actor:  1 policy actor:  1  step number:  63 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.047206666666666536 0.2256666666666666 0.2256666666666666
probs:  [0.05490828733889376, 0.07857656253410755, 0.11610499553993106, 0.3836928905958628, 0.17816309308091255, 0.18855417091029228]
printing an ep nov before normalisation:  39.44647292590481
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  0.047206666666666536 0.2256666666666666 0.2256666666666666
probs:  [0.05490828733889376, 0.07857656253410755, 0.11610499553993106, 0.3836928905958628, 0.17816309308091255, 0.18855417091029228]
printing an ep nov before normalisation:  27.91560114407316
maxi score, test score, baseline:  0.04469999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.05490828733889376, 0.07857656253410755, 0.11610499553993106, 0.3836928905958628, 0.17816309308091255, 0.18855417091029228]
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.479]
 [0.438]
 [0.434]
 [0.45 ]
 [0.434]
 [0.434]] [[28.435]
 [25.459]
 [26.665]
 [39.681]
 [28.023]
 [39.681]
 [39.681]] [[2.11 ]
 [1.836]
 [1.921]
 [3.271]
 [2.074]
 [3.271]
 [3.271]]
Printing some Q and Qe and total Qs values:  [[0.144]
 [0.167]
 [0.144]
 [0.144]
 [0.144]
 [0.144]
 [0.144]] [[44.705]
 [45.332]
 [44.705]
 [44.705]
 [44.705]
 [44.705]
 [44.705]] [[1.701]
 [1.763]
 [1.701]
 [1.701]
 [1.701]
 [1.701]
 [1.701]]
actor:  0 policy actor:  1  step number:  52 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  2.0
siam score:  -0.77341485
maxi score, test score, baseline:  0.04748666666666653 0.2256666666666666 0.2256666666666666
probs:  [0.05490828733889376, 0.07857656253410755, 0.11610499553993106, 0.3836928905958628, 0.17816309308091255, 0.18855417091029228]
actor:  1 policy actor:  1  step number:  42 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.04748666666666653 0.2256666666666666 0.2256666666666666
probs:  [0.05480722727459848, 0.07843183446858405, 0.11589102746240526, 0.3829852224732945, 0.17967815431943246, 0.18820653400168513]
actions average: 
K:  1  action  0 :  tensor([0.4928, 0.0083, 0.0979, 0.0842, 0.1272, 0.0906, 0.0990],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0055, 0.9717, 0.0046, 0.0042, 0.0017, 0.0024, 0.0098],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.2359, 0.0176, 0.4376, 0.0805, 0.0652, 0.0856, 0.0776],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2050, 0.0145, 0.1099, 0.2672, 0.1272, 0.1282, 0.1479],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2091, 0.0046, 0.0738, 0.0713, 0.4663, 0.1003, 0.0746],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([    0.0706,     0.0002,     0.1544,     0.0388,     0.0392,     0.6564,
            0.0405], grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1282, 0.0597, 0.0791, 0.1254, 0.0749, 0.0664, 0.4663],
       grad_fn=<DivBackward0>)
actions average: 
K:  0  action  0 :  tensor([0.3979, 0.0110, 0.1131, 0.1153, 0.1471, 0.1121, 0.1036],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0065,     0.9625,     0.0021,     0.0038,     0.0012,     0.0005,
            0.0234], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1326, 0.0081, 0.5020, 0.0818, 0.0628, 0.1274, 0.0853],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1339, 0.0309, 0.1180, 0.3808, 0.0771, 0.1347, 0.1246],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2311, 0.0281, 0.1362, 0.1445, 0.1422, 0.1363, 0.1815],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1150, 0.0157, 0.1363, 0.0965, 0.0675, 0.5021, 0.0669],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2063, 0.0741, 0.1284, 0.1151, 0.0864, 0.0994, 0.2902],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  58.707891858477495
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.614]
 [0.533]
 [0.524]
 [0.521]
 [0.522]
 [0.524]] [[26.655]
 [30.67 ]
 [25.508]
 [24.419]
 [25.19 ]
 [25.601]
 [25.158]] [[1.417]
 [1.917]
 [1.296]
 [1.174]
 [1.251]
 [1.294]
 [1.25 ]]
printing an ep nov before normalisation:  56.05460623729868
printing an ep nov before normalisation:  36.7200371896857
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.41 ]
 [0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.344]] [[37.298]
 [40.737]
 [37.298]
 [37.298]
 [37.298]
 [37.298]
 [37.298]] [[0.98 ]
 [1.168]
 [0.98 ]
 [0.98 ]
 [0.98 ]
 [0.98 ]
 [0.98 ]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  45 total reward:  0.5333333333333337  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 26.81209542329794
maxi score, test score, baseline:  0.04748666666666653 0.2256666666666666 0.2256666666666666
probs:  [0.05452412855525818, 0.07802640893983527, 0.11529164046270692, 0.3810028376349897, 0.17874848137484273, 0.1924065030323672]
printing an ep nov before normalisation:  20.896647189433395
actor:  1 policy actor:  1  step number:  48 total reward:  0.4066666666666666  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  50.52852729428868
maxi score, test score, baseline:  0.04748666666666653 0.2256666666666666 0.2256666666666666
printing an ep nov before normalisation:  46.551321049601725
printing an ep nov before normalisation:  30.480852127075195
maxi score, test score, baseline:  0.04748666666666653 0.2256666666666666 0.2256666666666666
probs:  [0.05459494542688155, 0.07803952080867717, 0.11521325524047972, 0.38149920939003457, 0.17851429096599453, 0.19213877816793257]
maxi score, test score, baseline:  0.04748666666666653 0.2256666666666666 0.2256666666666666
probs:  [0.05459494542688155, 0.07803952080867717, 0.11521325524047972, 0.38149920939003457, 0.17851429096599453, 0.19213877816793257]
actor:  1 policy actor:  1  step number:  46 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.04748666666666653 0.2256666666666666 0.2256666666666666
maxi score, test score, baseline:  0.04748666666666653 0.2256666666666666 0.2256666666666666
probs:  [0.05450816515614957, 0.077915383534745, 0.1166220287395351, 0.3808915340535505, 0.17823005540709957, 0.1918328331089202]
maxi score, test score, baseline:  0.04748666666666653 0.2256666666666666 0.2256666666666666
probs:  [0.05450816515614957, 0.077915383534745, 0.1166220287395351, 0.3808915340535505, 0.17823005540709957, 0.1918328331089202]
maxi score, test score, baseline:  0.04748666666666653 0.2256666666666666 0.2256666666666666
maxi score, test score, baseline:  0.04748666666666653 0.2256666666666666 0.2256666666666666
probs:  [0.05450816515614957, 0.077915383534745, 0.1166220287395351, 0.3808915340535505, 0.17823005540709957, 0.1918328331089202]
actor:  1 policy actor:  1  step number:  52 total reward:  0.33999999999999997  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.04748666666666653 0.2256666666666666 0.2256666666666666
probs:  [0.054443122584454724, 0.077903097215212, 0.11669698121539143, 0.3804356370201958, 0.17844386285228622, 0.19207729911245972]
printing an ep nov before normalisation:  30.861080074910916
actor:  1 policy actor:  1  step number:  44 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.04748666666666653 0.2256666666666666 0.2256666666666666
maxi score, test score, baseline:  0.04748666666666653 0.2256666666666666 0.2256666666666666
probs:  [0.054503972727628754, 0.07791459159932358, 0.11662685992762162, 0.3808621484423209, 0.17824383672613126, 0.19184859057697395]
maxi score, test score, baseline:  0.04748666666666653 0.2256666666666666 0.2256666666666666
maxi score, test score, baseline:  0.04748666666666653 0.2256666666666666 0.2256666666666666
probs:  [0.054503972727628754, 0.07791459159932358, 0.11662685992762162, 0.3808621484423209, 0.17824383672613126, 0.19184859057697395]
printing an ep nov before normalisation:  39.05938959500344
printing an ep nov before normalisation:  42.41978143511659
actor:  1 policy actor:  1  step number:  39 total reward:  0.56  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.477110661268085
Printing some Q and Qe and total Qs values:  [[0.215]
 [0.277]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]] [[34.727]
 [37.19 ]
 [34.727]
 [34.727]
 [34.727]
 [34.727]
 [34.727]] [[1.508]
 [1.748]
 [1.508]
 [1.508]
 [1.508]
 [1.508]
 [1.508]]
maxi score, test score, baseline:  0.04748666666666653 0.2256666666666666 0.2256666666666666
probs:  [0.054743411793096476, 0.07795982082059144, 0.11635093986010443, 0.38254042709538677, 0.17745675445783288, 0.19094864597298802]
printing an ep nov before normalisation:  33.24512871330772
siam score:  -0.77326035
maxi score, test score, baseline:  0.04748666666666654 0.2256666666666666 0.2256666666666666
probs:  [0.054743411793096476, 0.07795982082059144, 0.11635093986010443, 0.38254042709538677, 0.17745675445783288, 0.19094864597298802]
printing an ep nov before normalisation:  69.86981873810021
Printing some Q and Qe and total Qs values:  [[-0.022]
 [-0.008]
 [-0.022]
 [-0.022]
 [-0.022]
 [-0.009]
 [-0.028]] [[39.958]
 [37.189]
 [39.958]
 [39.958]
 [39.958]
 [25.255]
 [30.418]] [[0.728]
 [0.659]
 [0.728]
 [0.728]
 [0.728]
 [0.302]
 [0.437]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.04748666666666654 0.2256666666666666 0.2256666666666666
probs:  [0.05496064155008432, 0.07800085477937996, 0.11610061291743223, 0.38406303604497993, 0.17674267845883956, 0.190132176249284]
actor:  1 policy actor:  1  step number:  45 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  2.0
from probs:  [0.05496064155008432, 0.07800085477937996, 0.11610061291743223, 0.38406303604497993, 0.17674267845883956, 0.190132176249284]
printing an ep nov before normalisation:  45.829611316425975
maxi score, test score, baseline:  0.04748666666666654 0.2256666666666666 0.2256666666666666
probs:  [0.054856144940301395, 0.07785244539019642, 0.11587958847657001, 0.38333129693939866, 0.17640607515505555, 0.19167444909847803]
printing an ep nov before normalisation:  27.33508825302124
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.667]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]] [[47.353]
 [45.875]
 [47.353]
 [47.353]
 [47.353]
 [47.353]
 [47.353]] [[1.219]
 [1.268]
 [1.219]
 [1.219]
 [1.219]
 [1.219]
 [1.219]]
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.473]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]] [[25.449]
 [36.166]
 [25.449]
 [25.449]
 [25.449]
 [25.449]
 [25.449]] [[0.977]
 [1.482]
 [0.977]
 [0.977]
 [0.977]
 [0.977]
 [0.977]]
maxi score, test score, baseline:  0.04748666666666654 0.2256666666666666 0.2256666666666666
probs:  [0.054856144940301395, 0.07785244539019642, 0.11587958847657001, 0.38333129693939866, 0.17640607515505555, 0.19167444909847803]
maxi score, test score, baseline:  0.04748666666666654 0.2256666666666666 0.2256666666666666
probs:  [0.054856144940301395, 0.07785244539019642, 0.11587958847657001, 0.38333129693939866, 0.17640607515505555, 0.19167444909847803]
maxi score, test score, baseline:  0.04748666666666654 0.2256666666666666 0.2256666666666666
probs:  [0.054856144940301395, 0.07785244539019642, 0.11587958847657001, 0.38333129693939866, 0.17640607515505555, 0.19167444909847803]
actions average: 
K:  0  action  0 :  tensor([0.3667, 0.0166, 0.1114, 0.1176, 0.1286, 0.1151, 0.1439],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0126, 0.9292, 0.0041, 0.0102, 0.0103, 0.0032, 0.0306],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1356, 0.0282, 0.3531, 0.0855, 0.0844, 0.1942, 0.1190],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2221, 0.0018, 0.1078, 0.3117, 0.1098, 0.1028, 0.1440],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1204, 0.0203, 0.0800, 0.0935, 0.4958, 0.0936, 0.0964],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1651, 0.0025, 0.1039, 0.1131, 0.1346, 0.3593, 0.1216],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1708, 0.0587, 0.0873, 0.1125, 0.1041, 0.0915, 0.3751],
       grad_fn=<DivBackward0>)
siam score:  -0.77689666
maxi score, test score, baseline:  0.04748666666666654 0.2256666666666666 0.2256666666666666
probs:  [0.054856144940301395, 0.07785244539019642, 0.11587958847657001, 0.38333129693939866, 0.17640607515505555, 0.19167444909847803]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3333333333333327  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  65 total reward:  0.13333333333333242  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  34.73133821949715
Printing some Q and Qe and total Qs values:  [[-0.011]
 [-0.019]
 [-0.017]
 [-0.016]
 [-0.017]
 [-0.019]
 [-0.018]] [[30.53 ]
 [22.281]
 [22.277]
 [22.624]
 [22.564]
 [23.076]
 [21.985]] [[0.37 ]
 [0.199]
 [0.201]
 [0.208]
 [0.207]
 [0.215]
 [0.194]]
actor:  0 policy actor:  1  step number:  60 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  1.0
from probs:  [0.05487717228790409, 0.07774862702816651, 0.1158762300076395, 0.383478639266469, 0.17637852946046206, 0.19164080194935873]
maxi score, test score, baseline:  0.05011333333333321 0.2256666666666666 0.2256666666666666
probs:  [0.05487717228790409, 0.07774862702816651, 0.1158762300076395, 0.383478639266469, 0.17637852946046206, 0.19164080194935873]
printing an ep nov before normalisation:  47.82150240393471
using explorer policy with actor:  0
printing an ep nov before normalisation:  52.177239379832486
maxi score, test score, baseline:  0.05011333333333321 0.2256666666666666 0.2256666666666666
probs:  [0.05487717228790409, 0.07774862702816651, 0.1158762300076395, 0.383478639266469, 0.17637852946046206, 0.19164080194935873]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.575]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]] [[36.288]
 [53.91 ]
 [36.288]
 [36.288]
 [36.288]
 [36.288]
 [36.288]] [[0.691]
 [1.031]
 [0.691]
 [0.691]
 [0.691]
 [0.691]
 [0.691]]
maxi score, test score, baseline:  0.05011333333333321 0.2256666666666666 0.2256666666666666
probs:  [0.05512799021327152, 0.0777980874483598, 0.11559001964950213, 0.38523666895393516, 0.17555966421817018, 0.1906875695167612]
printing an ep nov before normalisation:  28.344993591308594
printing an ep nov before normalisation:  48.57799018355512
actor:  0 policy actor:  1  step number:  41 total reward:  0.52  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.526]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]] [[51.009]
 [53.976]
 [51.009]
 [51.009]
 [51.009]
 [51.009]
 [51.009]] [[1.249]
 [1.373]
 [1.249]
 [1.249]
 [1.249]
 [1.249]
 [1.249]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  46.98805798890789
printing an ep nov before normalisation:  44.5955753326416
printing an ep nov before normalisation:  63.85701319225846
printing an ep nov before normalisation:  59.86104528342967
printing an ep nov before normalisation:  63.26752414171704
Printing some Q and Qe and total Qs values:  [[0.684]
 [0.749]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]] [[52.591]
 [50.668]
 [52.591]
 [52.591]
 [52.591]
 [52.591]
 [52.591]] [[0.684]
 [0.749]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]]
printing an ep nov before normalisation:  49.537111659447035
maxi score, test score, baseline:  0.05033999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.05502195133271782, 0.07957495884888817, 0.11536740010719271, 0.3844941266583004, 0.17522141341249992, 0.19032014964040092]
actor:  1 policy actor:  1  step number:  48 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.05033999999999987 0.2256666666666666 0.2256666666666666
probs:  [0.05497040453914138, 0.0795003539715289, 0.1161975457756568, 0.38413316778892154, 0.17505698556899663, 0.19014154235575467]
printing an ep nov before normalisation:  47.51191408896296
printing an ep nov before normalisation:  39.53761337519129
printing an ep nov before normalisation:  36.35982699019543
printing an ep nov before normalisation:  31.08056996329158
actor:  0 policy actor:  1  step number:  51 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.017]
 [-0.015]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]
 [-0.017]] [[ 9.636]
 [20.32 ]
 [ 9.636]
 [ 9.636]
 [ 9.636]
 [ 9.636]
 [ 9.636]] [[0.301]
 [0.993]
 [0.301]
 [0.301]
 [0.301]
 [0.301]
 [0.301]]
printing an ep nov before normalisation:  41.617618708114975
siam score:  -0.77731
maxi score, test score, baseline:  0.049579999999999874 0.2256666666666666 0.2256666666666666
probs:  [0.05496571787434066, 0.07949971585734414, 0.11620296435714084, 0.38410031825262325, 0.1750721186189873, 0.19015916503956373]
printing an ep nov before normalisation:  56.41249325859417
printing an ep nov before normalisation:  41.79886983831293
siam score:  -0.7786745
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.381]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]] [[48.905]
 [50.743]
 [48.905]
 [48.905]
 [48.905]
 [48.905]
 [48.905]] [[1.382]
 [1.479]
 [1.382]
 [1.382]
 [1.382]
 [1.382]
 [1.382]]
printing an ep nov before normalisation:  21.507001142116728
actor:  1 policy actor:  1  step number:  50 total reward:  0.32666666666666655  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  56.31133670156516
line 256 mcts: sample exp_bonus 42.42701546851841
printing an ep nov before normalisation:  36.80182456970215
printing an ep nov before normalisation:  50.86718246917375
actor:  1 policy actor:  1  step number:  42 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  56 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.058579424953365
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.812]
 [0.621]
 [0.621]
 [0.599]
 [0.621]
 [0.621]] [[23.522]
 [37.695]
 [23.522]
 [23.522]
 [32.676]
 [23.522]
 [23.522]] [[0.621]
 [0.812]
 [0.621]
 [0.621]
 [0.599]
 [0.621]
 [0.621]]
printing an ep nov before normalisation:  26.65209240383572
printing an ep nov before normalisation:  52.172099380747774
printing an ep nov before normalisation:  29.66287612915039
Printing some Q and Qe and total Qs values:  [[0.245]
 [0.333]
 [0.327]
 [0.225]
 [0.224]
 [0.231]
 [0.299]] [[34.594]
 [41.231]
 [40.071]
 [33.056]
 [33.019]
 [32.586]
 [40.049]] [[0.425]
 [0.589]
 [0.57 ]
 [0.388]
 [0.387]
 [0.388]
 [0.542]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.878]
 [0.877]
 [0.771]
 [0.856]
 [0.883]
 [0.872]
 [0.854]] [[42.434]
 [41.071]
 [34.781]
 [41.526]
 [43.114]
 [41.968]
 [39.971]] [[0.878]
 [0.877]
 [0.771]
 [0.856]
 [0.883]
 [0.872]
 [0.854]]
printing an ep nov before normalisation:  37.92440163126555
printing an ep nov before normalisation:  47.246090921782375
printing an ep nov before normalisation:  29.01798725128174
printing an ep nov before normalisation:  29.882875946027234
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  0.0
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  0.0498333333333332 0.2256666666666666 0.2256666666666666
probs:  [0.055104510035692666, 0.0794921224011062, 0.1159763757048113, 0.38507326671739384, 0.17449427871484025, 0.1898594464261558]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  0.050433333333333205 0.2256666666666666 0.2256666666666666
probs:  [0.055104510035692666, 0.0794921224011062, 0.1159763757048113, 0.38507326671739384, 0.17449427871484025, 0.1898594464261558]
actor:  0 policy actor:  0  step number:  29 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  30 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  30 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  14.207281458214267
actor:  0 policy actor:  0  step number:  36 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  36 total reward:  0.47333333333333305  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  31.829914994471693
actor:  0 policy actor:  0  step number:  44 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  33.75286538488885
actor:  1 policy actor:  1  step number:  49 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  76 total reward:  0.03333333333333266  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.022]
 [ 0.025]
 [-0.023]
 [-0.023]
 [-0.024]
 [-0.024]
 [-0.023]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.022]
 [ 0.025]
 [-0.023]
 [-0.023]
 [-0.024]
 [-0.024]
 [-0.023]]
maxi score, test score, baseline:  0.0518733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
from probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Printing some Q and Qe and total Qs values:  [[0.772]
 [0.805]
 [0.772]
 [0.772]
 [0.772]
 [0.686]
 [0.772]] [[ 0.   ]
 [35.338]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [27.414]
 [ 0.   ]] [[0.772]
 [0.805]
 [0.772]
 [0.772]
 [0.772]
 [0.686]
 [0.772]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  0.0518733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Printing some Q and Qe and total Qs values:  [[0.829]
 [0.873]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]] [[38.938]
 [41.736]
 [38.938]
 [38.938]
 [38.938]
 [38.938]
 [38.938]] [[0.829]
 [0.873]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]]
printing an ep nov before normalisation:  29.74633158859504
printing an ep nov before normalisation:  8.685549994052764
printing an ep nov before normalisation:  24.5729020179456
actor:  1 policy actor:  1  step number:  54 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0518733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  0 policy actor:  1  step number:  50 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  58 total reward:  0.04666666666666608  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.007]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]] [[40.021]
 [35.853]
 [35.853]
 [35.853]
 [35.853]
 [35.853]
 [35.853]] [[1.198]
 [1.002]
 [1.002]
 [1.002]
 [1.002]
 [1.002]
 [1.002]]
Printing some Q and Qe and total Qs values:  [[0.153]
 [0.225]
 [0.153]
 [0.153]
 [0.153]
 [0.153]
 [0.153]] [[31.427]
 [43.143]
 [31.427]
 [31.427]
 [31.427]
 [31.427]
 [31.427]] [[0.679]
 [1.139]
 [0.679]
 [0.679]
 [0.679]
 [0.679]
 [0.679]]
printing an ep nov before normalisation:  11.17138345426745
printing an ep nov before normalisation:  50.88701330794398
maxi score, test score, baseline:  0.0540733333333332 0.639 0.639
maxi score, test score, baseline:  0.0540733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  37.378748267392204
maxi score, test score, baseline:  0.0540733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  39.02387387675841
line 256 mcts: sample exp_bonus 26.395324093481463
actor:  1 policy actor:  1  step number:  45 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0540733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.0540733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.527]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]] [[24.756]
 [32.494]
 [24.756]
 [24.756]
 [24.756]
 [24.756]
 [24.756]] [[1.071]
 [1.512]
 [1.071]
 [1.071]
 [1.071]
 [1.071]
 [1.071]]
maxi score, test score, baseline:  0.0540733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  30.878321887340817
maxi score, test score, baseline:  0.0540733333333332 0.639 0.639
Printing some Q and Qe and total Qs values:  [[0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]] [[35.264]
 [35.264]
 [35.264]
 [35.264]
 [35.264]
 [35.264]
 [35.264]] [[0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]
 [0.61]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0540733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  25.408419675534937
actor:  0 policy actor:  1  step number:  47 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.056953333333333196 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.056953333333333196 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  39 total reward:  0.52  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.056953333333333196 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.056953333333333196 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.185]
 [0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]] [[23.386]
 [41.998]
 [23.386]
 [23.386]
 [23.386]
 [23.386]
 [23.386]] [[0.342]
 [1.025]
 [0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]]
actor:  0 policy actor:  1  step number:  46 total reward:  0.4600000000000001  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.647552617975435
maxi score, test score, baseline:  0.05771333333333321 0.639 0.639
maxi score, test score, baseline:  0.05771333333333321 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.05771333333333321 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.05771333333333321 0.639 0.639
printing an ep nov before normalisation:  24.60245074882442
actor:  1 policy actor:  1  step number:  45 total reward:  0.5200000000000001  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  58 total reward:  0.2599999999999991  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.05771333333333321 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  35.10095834732056
printing an ep nov before normalisation:  50.077135117332375
printing an ep nov before normalisation:  49.52027621648003
line 256 mcts: sample exp_bonus 30.11816132068634
maxi score, test score, baseline:  0.05771333333333321 0.639 0.639
actor:  0 policy actor:  0  step number:  41 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  18.715191804826198
maxi score, test score, baseline:  0.05793999999999988 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  62 total reward:  0.37999999999999934  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  52.36201809720604
maxi score, test score, baseline:  0.05793999999999988 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.05793999999999988 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  60.5119935508662
printing an ep nov before normalisation:  42.91062391227959
actions average: 
K:  4  action  0 :  tensor([0.5293, 0.0451, 0.0572, 0.0534, 0.1949, 0.0667, 0.0532],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0102, 0.9251, 0.0264, 0.0067, 0.0028, 0.0149, 0.0139],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1194, 0.0429, 0.2904, 0.0959, 0.0792, 0.1561, 0.2161],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1476, 0.0584, 0.1347, 0.2263, 0.1904, 0.1426, 0.1000],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2337, 0.0189, 0.0531, 0.0712, 0.5077, 0.0575, 0.0577],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2250, 0.0529, 0.1153, 0.1095, 0.0839, 0.3040, 0.1094],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2306, 0.0060, 0.0981, 0.1552, 0.1235, 0.1329, 0.2538],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  13.43820333480835
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.643]
 [0.653]
 [0.622]
 [0.584]
 [0.649]
 [0.627]] [[45.35 ]
 [46.283]
 [43.853]
 [42.357]
 [43.333]
 [45.593]
 [41.391]] [[1.451]
 [1.485]
 [1.413]
 [1.332]
 [1.327]
 [1.467]
 [1.305]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.3266666666666661  reward:  1.0 rdn_beta:  1.333
from probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.05572666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actions average: 
K:  0  action  0 :  tensor([0.4507, 0.0096, 0.0980, 0.1163, 0.1006, 0.1152, 0.1096],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0036, 0.9546, 0.0056, 0.0052, 0.0021, 0.0062, 0.0228],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2554, 0.0013, 0.2572, 0.1269, 0.1023, 0.1235, 0.1334],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1147, 0.0066, 0.0844, 0.4274, 0.0894, 0.1772, 0.1002],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1093, 0.0087, 0.0716, 0.0798, 0.5483, 0.0944, 0.0880],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0677, 0.0014, 0.1359, 0.0718, 0.0546, 0.6199, 0.0486],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1262, 0.0205, 0.1604, 0.0801, 0.0768, 0.1040, 0.4321],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]] [[34.148]
 [34.148]
 [34.148]
 [34.148]
 [34.148]
 [34.148]
 [34.148]] [[1.562]
 [1.562]
 [1.562]
 [1.562]
 [1.562]
 [1.562]
 [1.562]]
printing an ep nov before normalisation:  37.152259349823
siam score:  -0.77225196
maxi score, test score, baseline:  0.05572666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  40 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.05572666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  0 policy actor:  0  step number:  46 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.058673333333333216 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  25.845394134521484
printing an ep nov before normalisation:  20.27794482106969
actions average: 
K:  3  action  0 :  tensor([0.5799, 0.0481, 0.0774, 0.0709, 0.0711, 0.0802, 0.0724],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0197, 0.8899, 0.0247, 0.0136, 0.0063, 0.0066, 0.0392],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.2481, 0.0562, 0.1670, 0.1301, 0.1457, 0.1488, 0.1042],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1361, 0.0532, 0.1002, 0.2507, 0.1178, 0.1596, 0.1824],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1555, 0.0386, 0.0729, 0.0641, 0.5415, 0.0658, 0.0616],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1436, 0.0039, 0.0983, 0.0964, 0.0848, 0.4630, 0.1099],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0813, 0.1341, 0.1298, 0.1309, 0.0821, 0.1437, 0.2981],
       grad_fn=<DivBackward0>)
siam score:  -0.7663181
maxi score, test score, baseline:  0.058673333333333216 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  18.96376325508596
printing an ep nov before normalisation:  19.416157287320964
printing an ep nov before normalisation:  0.03884746628529001
maxi score, test score, baseline:  0.058673333333333216 0.639 0.639
maxi score, test score, baseline:  0.058673333333333216 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  41.186267836761274
printing an ep nov before normalisation:  26.955843853251466
actor:  0 policy actor:  0  step number:  57 total reward:  0.37333333333333285  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.013]
 [-0.004]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.013]
 [-0.004]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]]
maxi score, test score, baseline:  0.06141999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]] [[32.914]
 [32.914]
 [32.914]
 [32.914]
 [32.914]
 [32.914]
 [32.914]] [[1.405]
 [1.405]
 [1.405]
 [1.405]
 [1.405]
 [1.405]
 [1.405]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06141999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Printing some Q and Qe and total Qs values:  [[-0.085]
 [-0.089]
 [-0.089]
 [-0.089]
 [-0.089]
 [-0.089]
 [-0.089]] [[40.535]
 [30.784]
 [30.784]
 [30.784]
 [30.784]
 [30.784]
 [30.784]] [[1.364]
 [0.868]
 [0.868]
 [0.868]
 [0.868]
 [0.868]
 [0.868]]
printing an ep nov before normalisation:  38.55499022189626
printing an ep nov before normalisation:  8.042991565102824
actor:  1 policy actor:  1  step number:  46 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06141999999999987 0.639 0.639
printing an ep nov before normalisation:  24.27281979253941
maxi score, test score, baseline:  0.06141999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  51.75669899862251
maxi score, test score, baseline:  0.06141999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  42 total reward:  0.43333333333333335  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.656063870655366
maxi score, test score, baseline:  0.06141999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06141999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06141999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  0.0002202529168471301
actor:  1 policy actor:  1  step number:  71 total reward:  0.11999999999999955  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.181]
 [0.486]
 [0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.181]
 [0.486]
 [0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]]
maxi score, test score, baseline:  0.06141999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  84 total reward:  0.13999999999999913  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.06141999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5733333333333336  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  41.76854734704766
maxi score, test score, baseline:  0.06141999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actions average: 
K:  1  action  0 :  tensor([0.7346, 0.0015, 0.0472, 0.0499, 0.0642, 0.0469, 0.0556],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0069, 0.9683, 0.0047, 0.0031, 0.0015, 0.0014, 0.0141],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1940, 0.0094, 0.3574, 0.0958, 0.1092, 0.1169, 0.1173],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1635, 0.0191, 0.1044, 0.3239, 0.1112, 0.1391, 0.1388],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2020, 0.0060, 0.0827, 0.1013, 0.4090, 0.0844, 0.1148],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2258, 0.0055, 0.1197, 0.1057, 0.1416, 0.2757, 0.1259],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1933, 0.1326, 0.1250, 0.0949, 0.0994, 0.1283, 0.2265],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.004940702861195
maxi score, test score, baseline:  0.06141999999999987 0.639 0.639
printing an ep nov before normalisation:  30.305727462612033
actor:  0 policy actor:  1  step number:  46 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.22704887390137
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.633]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]
 [0.56 ]] [[29.388]
 [33.398]
 [29.388]
 [29.388]
 [29.388]
 [29.388]
 [29.388]] [[1.298]
 [1.566]
 [1.298]
 [1.298]
 [1.298]
 [1.298]
 [1.298]]
actor:  1 policy actor:  1  step number:  34 total reward:  0.6066666666666668  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0640733333333332 0.639 0.639
maxi score, test score, baseline:  0.0640733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.0640733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  30.271184564375414
maxi score, test score, baseline:  0.0640733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  57 total reward:  0.2666666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0640733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  39.933481024102434
printing an ep nov before normalisation:  32.86672400618635
line 256 mcts: sample exp_bonus 32.00639182119982
Printing some Q and Qe and total Qs values:  [[-0.069]
 [-0.076]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]] [[43.459]
 [53.904]
 [44.524]
 [44.524]
 [44.524]
 [44.524]
 [44.524]] [[1.494]
 [1.869]
 [1.518]
 [1.518]
 [1.518]
 [1.518]
 [1.518]]
printing an ep nov before normalisation:  24.232884952285804
maxi score, test score, baseline:  0.0640733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.0640733333333332 0.639 0.639
maxi score, test score, baseline:  0.0640733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  55 total reward:  0.27999999999999925  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 38.87810132303382
printing an ep nov before normalisation:  36.577294268753
maxi score, test score, baseline:  0.0640733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  29.483466876926013
actor:  1 policy actor:  1  step number:  64 total reward:  0.16666666666666574  reward:  1.0 rdn_beta:  1.0
siam score:  -0.7737949
printing an ep nov before normalisation:  23.587119579315186
maxi score, test score, baseline:  0.06128666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06128666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06128666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06128666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.658]
 [0.566]
 [0.554]
 [0.553]
 [0.589]
 [0.555]] [[27.085]
 [37.913]
 [25.146]
 [25.622]
 [25.496]
 [25.643]
 [24.731]] [[0.886]
 [1.537]
 [0.808]
 [0.82 ]
 [0.813]
 [0.856]
 [0.776]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.06128666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  43 total reward:  0.4933333333333335  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  31.439804787283872
siam score:  -0.7718911
actor:  1 policy actor:  1  step number:  60 total reward:  0.25999999999999923  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06128666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  47.30854801255049
maxi score, test score, baseline:  0.06128666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  22.621568959737438
maxi score, test score, baseline:  0.06128666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  0.06359743933813888
maxi score, test score, baseline:  0.06128666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  0 policy actor:  1  step number:  43 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  0.2533333333333332  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.015]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[34.869]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[ 0.847]
 [-0.507]
 [-0.507]
 [-0.507]
 [-0.507]
 [-0.507]
 [-0.507]]
maxi score, test score, baseline:  0.06424666666666654 0.639 0.639
maxi score, test score, baseline:  0.06424666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  48.88432675557414
maxi score, test score, baseline:  0.06424666666666654 0.639 0.639
printing an ep nov before normalisation:  11.767146275896087
actions average: 
K:  0  action  0 :  tensor([0.3861, 0.0126, 0.1026, 0.1250, 0.1779, 0.1061, 0.0899],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0071, 0.9401, 0.0044, 0.0032, 0.0030, 0.0026, 0.0397],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1431, 0.0144, 0.4279, 0.0736, 0.0881, 0.1758, 0.0771],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2046, 0.0476, 0.1352, 0.2121, 0.1337, 0.1187, 0.1481],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1862, 0.0093, 0.1294, 0.1283, 0.2993, 0.1114, 0.1361],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1328, 0.0022, 0.1119, 0.0831, 0.0900, 0.4883, 0.0917],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2108, 0.0123, 0.1200, 0.1328, 0.1324, 0.1149, 0.2767],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.06424666666666654 0.639 0.639
maxi score, test score, baseline:  0.06424666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06424666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  40.58069181549374
Printing some Q and Qe and total Qs values:  [[0.521]
 [0.649]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]] [[37.166]
 [35.851]
 [37.166]
 [37.166]
 [37.166]
 [37.166]
 [37.166]] [[1.591]
 [1.659]
 [1.591]
 [1.591]
 [1.591]
 [1.591]
 [1.591]]
siam score:  -0.7780155
printing an ep nov before normalisation:  36.48038503069101
maxi score, test score, baseline:  0.06424666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  54 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06424666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Printing some Q and Qe and total Qs values:  [[-0.006]
 [-0.006]
 [-0.003]
 [-0.006]
 [-0.006]
 [-0.007]
 [-0.006]] [[31.634]
 [31.634]
 [30.144]
 [31.634]
 [31.634]
 [31.829]
 [31.634]] [[0.274]
 [0.274]
 [0.259]
 [0.274]
 [0.274]
 [0.277]
 [0.274]]
maxi score, test score, baseline:  0.06424666666666654 0.639 0.639
maxi score, test score, baseline:  0.06424666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06424666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  33.56059509078306
printing an ep nov before normalisation:  34.93427781166424
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.154]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]] [[38.325]
 [40.875]
 [38.325]
 [38.325]
 [38.325]
 [38.325]
 [38.325]] [[0.358]
 [0.425]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]]
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.411]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]] [[39.449]
 [40.116]
 [39.449]
 [39.449]
 [39.449]
 [39.449]
 [39.449]] [[1.682]
 [1.771]
 [1.682]
 [1.682]
 [1.682]
 [1.682]
 [1.682]]
printing an ep nov before normalisation:  34.81997017620295
maxi score, test score, baseline:  0.06424666666666654 0.639 0.639
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]] [[32.505]
 [27.897]
 [27.897]
 [27.897]
 [27.897]
 [27.897]
 [27.897]] [[2.545]
 [2.288]
 [2.288]
 [2.288]
 [2.288]
 [2.288]
 [2.288]]
printing an ep nov before normalisation:  39.375643121839445
maxi score, test score, baseline:  0.06424666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  52 total reward:  0.4066666666666662  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  68 total reward:  0.1533333333333322  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.428]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.33 ]
 [0.428]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]]
maxi score, test score, baseline:  0.06357999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  39 total reward:  0.52  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.10569496917861
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.8  ]
 [0.866]
 [0.8  ]
 [0.8  ]
 [0.8  ]
 [0.8  ]
 [0.8  ]] [[49.853]
 [43.254]
 [49.853]
 [49.853]
 [49.853]
 [49.853]
 [49.853]] [[1.484]
 [1.415]
 [1.484]
 [1.484]
 [1.484]
 [1.484]
 [1.484]]
printing an ep nov before normalisation:  45.34886040324906
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06357999999999987 0.639 0.639
maxi score, test score, baseline:  0.06357999999999987 0.639 0.639
maxi score, test score, baseline:  0.06357999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  45 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  40.43657820733045
printing an ep nov before normalisation:  34.792782414769015
actor:  1 policy actor:  1  step number:  46 total reward:  0.44666666666666677  reward:  1.0 rdn_beta:  1.0
siam score:  -0.7723797
printing an ep nov before normalisation:  26.32775853853792
actor:  1 policy actor:  1  step number:  44 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.06357999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06357999999999987 0.639 0.639
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.921558581996436
printing an ep nov before normalisation:  36.218201924045246
actor:  1 policy actor:  1  step number:  36 total reward:  0.5666666666666668  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06357999999999987 0.639 0.639
maxi score, test score, baseline:  0.06357999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  37.82080411911011
printing an ep nov before normalisation:  44.05882620263888
from probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06357999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06357999999999987 0.639 0.639
actor:  1 policy actor:  1  step number:  50 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  44 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.06357999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  0 policy actor:  0  step number:  34 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06383333333333321 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  46.06469221236279
printing an ep nov before normalisation:  38.44103614859838
actor:  1 policy actor:  1  step number:  35 total reward:  0.52  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06383333333333321 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  47.349732656986106
maxi score, test score, baseline:  0.06383333333333321 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06383333333333321 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06383333333333321 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  39.12417533987973
maxi score, test score, baseline:  0.06383333333333321 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  29.80721950531006
actor:  0 policy actor:  0  step number:  55 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0637933333333332 0.639 0.639
maxi score, test score, baseline:  0.0637933333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]]
maxi score, test score, baseline:  0.0637933333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  56 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0637933333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.0637933333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.0637933333333332 0.639 0.639
printing an ep nov before normalisation:  48.168230487173574
siam score:  -0.7786064
printing an ep nov before normalisation:  44.63726159178585
maxi score, test score, baseline:  0.0637933333333332 0.639 0.639
printing an ep nov before normalisation:  53.19807613273679
actor:  1 policy actor:  1  step number:  42 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0637933333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  0 policy actor:  0  step number:  42 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.06660666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06660666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06660666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06660666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3733333333333333  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.10197951494288
Printing some Q and Qe and total Qs values:  [[-0.014]
 [ 0.016]
 [-0.019]
 [-0.026]
 [-0.025]
 [-0.025]
 [-0.025]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.014]
 [ 0.016]
 [-0.019]
 [-0.026]
 [-0.025]
 [-0.025]
 [-0.025]]
printing an ep nov before normalisation:  45.50581539303451
maxi score, test score, baseline:  0.06660666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  46 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06660666666666654 0.639 0.639
maxi score, test score, baseline:  0.06660666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  18.000685760639005
actor:  1 policy actor:  1  step number:  54 total reward:  0.29999999999999993  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.161]
 [0.271]
 [0.241]
 [0.241]
 [0.303]
 [0.241]
 [0.241]] [[27.58 ]
 [30.934]
 [34.191]
 [34.191]
 [42.935]
 [34.191]
 [34.191]] [[0.535]
 [0.761]
 [0.842]
 [0.842]
 [1.204]
 [0.842]
 [0.842]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.019]
 [ 0.075]
 [-0.007]
 [-0.011]
 [-0.011]
 [-0.207]
 [ 0.007]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.019]
 [ 0.075]
 [-0.007]
 [-0.011]
 [-0.011]
 [-0.207]
 [ 0.007]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  43.768578631811465
siam score:  -0.78524595
Printing some Q and Qe and total Qs values:  [[0.811]
 [0.866]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]] [[37.31]
 [40.72]
 [37.31]
 [37.31]
 [37.31]
 [37.31]
 [37.31]] [[0.811]
 [0.866]
 [0.811]
 [0.811]
 [0.811]
 [0.811]
 [0.811]]
maxi score, test score, baseline:  0.06660666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  43.333716006107075
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.24109867853027
printing an ep nov before normalisation:  44.23331240050594
actor:  1 policy actor:  1  step number:  49 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  53 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06701999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06701999999999987 0.639 0.639
printing an ep nov before normalisation:  42.78039176532893
printing an ep nov before normalisation:  46.56607616409278
maxi score, test score, baseline:  0.06701999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  39.611893482988265
maxi score, test score, baseline:  0.06701999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
actor:  0 policy actor:  1  step number:  42 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.16920020433087
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.003]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]
 [-0.015]] [[60.309]
 [47.169]
 [33.238]
 [33.238]
 [33.238]
 [33.238]
 [33.238]] [[1.014]
 [0.735]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]]
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]] [[44.7]
 [44.7]
 [44.7]
 [44.7]
 [44.7]
 [44.7]
 [44.7]] [[1.579]
 [1.579]
 [1.579]
 [1.579]
 [1.579]
 [1.579]
 [1.579]]
printing an ep nov before normalisation:  44.32422135895718
printing an ep nov before normalisation:  32.85598278045654
maxi score, test score, baseline:  0.06679333333333319 0.639 0.639
maxi score, test score, baseline:  0.06679333333333319 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06679333333333319 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
siam score:  -0.78110194
maxi score, test score, baseline:  0.06679333333333319 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06679333333333319 0.639 0.639
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.24943174190817
printing an ep nov before normalisation:  37.30062677112886
maxi score, test score, baseline:  0.06679333333333319 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]] [[26.733]
 [26.733]
 [26.733]
 [26.733]
 [26.733]
 [26.733]
 [26.733]] [[2.082]
 [2.082]
 [2.082]
 [2.082]
 [2.082]
 [2.082]
 [2.082]]
printing an ep nov before normalisation:  26.73588514328003
maxi score, test score, baseline:  0.06679333333333319 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  60 total reward:  0.19333333333333302  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  21.2592000761894
Printing some Q and Qe and total Qs values:  [[-0.008]
 [ 0.009]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.004]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.008]
 [ 0.009]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.004]]
maxi score, test score, baseline:  0.06679333333333319 0.639 0.639
printing an ep nov before normalisation:  45.13140366330528
maxi score, test score, baseline:  0.06679333333333319 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  69 total reward:  0.013333333333332531  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  82 total reward:  0.01999999999999924  reward:  1.0 rdn_beta:  1.333
siam score:  -0.77869034
maxi score, test score, baseline:  0.06679333333333319 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  0 policy actor:  0  step number:  42 total reward:  0.4466666666666662  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  3  action  0 :  tensor([0.5546, 0.0025, 0.0889, 0.0672, 0.1547, 0.0756, 0.0565],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0105, 0.9034, 0.0105, 0.0371, 0.0027, 0.0073, 0.0285],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1214, 0.1015, 0.4727, 0.0586, 0.0567, 0.1212, 0.0678],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1010, 0.1831, 0.0952, 0.2730, 0.0710, 0.1753, 0.1014],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1988, 0.0152, 0.1267, 0.1392, 0.2387, 0.1482, 0.1331],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1235, 0.0134, 0.1389, 0.1478, 0.1169, 0.3678, 0.0916],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1638, 0.1529, 0.1407, 0.1247, 0.1092, 0.1573, 0.1515],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  35.555600919713754
maxi score, test score, baseline:  0.06713999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06713999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06713999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06441999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Printing some Q and Qe and total Qs values:  [[-0.09 ]
 [-0.091]
 [-0.091]
 [-0.091]
 [-0.091]
 [-0.091]
 [-0.091]] [[26.998]
 [26.419]
 [26.419]
 [26.419]
 [26.419]
 [26.419]
 [26.419]] [[0.779]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]]
actor:  0 policy actor:  0  step number:  59 total reward:  0.3333333333333327  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  32.63628959655762
Printing some Q and Qe and total Qs values:  [[ 0.04 ]
 [-0.002]
 [-0.108]
 [-0.016]
 [ 0.013]
 [-0.056]
 [-0.045]] [[33.212]
 [40.834]
 [26.71 ]
 [27.262]
 [28.969]
 [27.276]
 [25.289]] [[1.032]
 [1.439]
 [0.501]
 [0.626]
 [0.756]
 [0.587]
 [0.481]]
printing an ep nov before normalisation:  37.55759989656835
actions average: 
K:  4  action  0 :  tensor([0.4663, 0.0021, 0.1041, 0.0842, 0.1455, 0.0893, 0.1085],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0266, 0.8857, 0.0129, 0.0125, 0.0081, 0.0107, 0.0433],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2699, 0.0796, 0.1349, 0.1098, 0.1469, 0.1341, 0.1248],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1428, 0.0330, 0.1299, 0.2527, 0.1072, 0.2040, 0.1304],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0826, 0.0071, 0.0749, 0.0893, 0.5109, 0.1701, 0.0652],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2364, 0.0062, 0.1485, 0.0949, 0.1026, 0.3052, 0.1062],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1354, 0.0908, 0.1753, 0.1437, 0.0860, 0.1496, 0.2192],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  32.30904080762276
printing an ep nov before normalisation:  45.019311308485506
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.491]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]] [[35.884]
 [35.435]
 [35.884]
 [35.884]
 [35.884]
 [35.884]
 [35.884]] [[1.422]
 [1.461]
 [1.422]
 [1.422]
 [1.422]
 [1.422]
 [1.422]]
siam score:  -0.77174604
maxi score, test score, baseline:  0.06456666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
siam score:  -0.7737903
maxi score, test score, baseline:  0.06456666666666654 0.639 0.639
maxi score, test score, baseline:  0.06456666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  1  action  0 :  tensor([0.4437, 0.0378, 0.0743, 0.1067, 0.1539, 0.0997, 0.0839],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0022,     0.9671,     0.0024,     0.0044,     0.0009,     0.0017,
            0.0214], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1520, 0.0037, 0.4766, 0.0841, 0.0955, 0.1066, 0.0815],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.3319, 0.0019, 0.1012, 0.1569, 0.1695, 0.1271, 0.1116],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1948, 0.0024, 0.0685, 0.0973, 0.4654, 0.1021, 0.0695],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0604, 0.1074, 0.0644, 0.0518, 0.0580, 0.6201, 0.0378],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1198, 0.0273, 0.0884, 0.1348, 0.0986, 0.1257, 0.4054],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.5436648563751
maxi score, test score, baseline:  0.06456666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  55 total reward:  0.35999999999999943  reward:  1.0 rdn_beta:  2.0
from probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06456666666666654 0.639 0.639
printing an ep nov before normalisation:  33.038459635302395
maxi score, test score, baseline:  0.06456666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  0.028539706560053446
actor:  1 policy actor:  1  step number:  43 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06456666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  0 policy actor:  0  step number:  50 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  48.11097803142505
maxi score, test score, baseline:  0.06713999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06713999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  41.55869414078175
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]
 [0.503]] [[42.664]
 [42.664]
 [42.664]
 [42.664]
 [42.664]
 [42.664]
 [42.664]] [[0.8]
 [0.8]
 [0.8]
 [0.8]
 [0.8]
 [0.8]
 [0.8]]
maxi score, test score, baseline:  0.06713999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06713999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06713999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  63 total reward:  0.17333333333333267  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.071]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.   ]
 [-0.   ]] [[23.989]
 [17.337]
 [15.628]
 [15.628]
 [15.628]
 [15.628]
 [15.628]] [[1.224]
 [0.81 ]
 [0.789]
 [0.789]
 [0.789]
 [0.789]
 [0.789]]
Printing some Q and Qe and total Qs values:  [[0.167]
 [0.204]
 [0.167]
 [0.167]
 [0.167]
 [0.167]
 [0.167]] [[31.399]
 [40.049]
 [31.399]
 [31.399]
 [31.399]
 [31.399]
 [31.399]] [[1.44 ]
 [2.079]
 [1.44 ]
 [1.44 ]
 [1.44 ]
 [1.44 ]
 [1.44 ]]
printing an ep nov before normalisation:  51.65077044683704
line 256 mcts: sample exp_bonus 37.20209429232236
maxi score, test score, baseline:  0.06713999999999987 0.639 0.639
actor:  1 policy actor:  1  step number:  65 total reward:  0.10666666666666635  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06713999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  34.97942014523026
maxi score, test score, baseline:  0.06713999999999987 0.639 0.639
printing an ep nov before normalisation:  43.02699395298863
maxi score, test score, baseline:  0.06713999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  41 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  17.458369160084708
actor:  1 policy actor:  1  step number:  44 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06713999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.0641533333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  56.208858561934804
printing an ep nov before normalisation:  24.77314376727646
maxi score, test score, baseline:  0.0641533333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.0641533333333332 0.639 0.639
maxi score, test score, baseline:  0.0641533333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  57 total reward:  0.173333333333333  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0641533333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.138]
 [0.115]
 [0.1  ]
 [0.115]
 [0.115]
 [0.115]] [[ 0.   ]
 [34.038]
 [ 0.   ]
 [34.11 ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.364]
 [ 0.905]
 [-0.364]
 [ 0.869]
 [-0.364]
 [-0.364]
 [-0.364]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.2123818397522
actor:  1 policy actor:  1  step number:  65 total reward:  0.14666666666666572  reward:  1.0 rdn_beta:  0.333
siam score:  -0.77925783
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.545]
 [0.486]
 [0.467]
 [0.537]
 [0.466]
 [0.491]] [[27.057]
 [28.993]
 [28.134]
 [27.421]
 [29.095]
 [27.225]
 [28.239]] [[0.876]
 [1.01 ]
 [0.924]
 [0.883]
 [1.006]
 [0.875]
 [0.932]]
printing an ep nov before normalisation:  25.946111004428715
maxi score, test score, baseline:  0.0641533333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
from probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.0641533333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  28.441474034793735
maxi score, test score, baseline:  0.0641533333333332 0.639 0.639
maxi score, test score, baseline:  0.0641533333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  80 total reward:  0.16666666666666552  reward:  1.0 rdn_beta:  0.667
from probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.0641533333333332 0.639 0.639
printing an ep nov before normalisation:  37.44185874165872
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.626]] [[43.374]
 [43.374]
 [43.374]
 [43.374]
 [43.374]
 [43.374]
 [43.374]] [[1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]
 [1.485]]
maxi score, test score, baseline:  0.0641533333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  0 policy actor:  0  step number:  52 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  0
maxi score, test score, baseline:  0.06651333333333319 0.639 0.639
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  55 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06651333333333319 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  32.69331011534928
actor:  0 policy actor:  1  step number:  60 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  1.333
from probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06657999999999988 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Printing some Q and Qe and total Qs values:  [[0.881]
 [0.911]
 [0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.881]] [[44.769]
 [41.494]
 [44.769]
 [44.769]
 [44.769]
 [44.769]
 [44.769]] [[0.881]
 [0.911]
 [0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.881]]
printing an ep nov before normalisation:  40.323931068026326
line 256 mcts: sample exp_bonus 37.88522973845302
actions average: 
K:  1  action  0 :  tensor([0.4058, 0.0030, 0.0903, 0.1067, 0.1609, 0.1268, 0.1065],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0115, 0.9353, 0.0068, 0.0091, 0.0042, 0.0036, 0.0293],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1330, 0.0066, 0.4815, 0.0765, 0.0827, 0.1572, 0.0625],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2126, 0.0098, 0.0987, 0.2135, 0.1614, 0.1869, 0.1170],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1150, 0.0795, 0.0658, 0.0794, 0.4964, 0.0843, 0.0796],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1755, 0.0007, 0.0989, 0.1291, 0.1196, 0.3321, 0.1441],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1209, 0.1536, 0.1316, 0.1352, 0.0868, 0.1497, 0.2222],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  34 total reward:  0.5933333333333335  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  33 total reward:  0.5333333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0644733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  33.65494130292337
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.139]
 [0.058]
 [0.058]
 [0.058]
 [0.058]
 [0.058]] [[19.965]
 [23.765]
 [19.965]
 [19.965]
 [19.965]
 [19.965]
 [19.965]] [[1.07 ]
 [1.423]
 [1.07 ]
 [1.07 ]
 [1.07 ]
 [1.07 ]
 [1.07 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0644733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  42 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  1.333
using another actor
maxi score, test score, baseline:  0.0644733333333332 0.639 0.639
maxi score, test score, baseline:  0.0644733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.0644733333333332 0.639 0.639
using explorer policy with actor:  0
printing an ep nov before normalisation:  36.14310016113885
printing an ep nov before normalisation:  35.85859300025973
maxi score, test score, baseline:  0.0644733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.0644733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.0644733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  46.52603522944771
actor:  0 policy actor:  1  step number:  43 total reward:  0.4933333333333335  reward:  1.0 rdn_beta:  2.0
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.06480666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  70 total reward:  0.2599999999999989  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  49 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.06480666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06480666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.222]
 [0.187]
 [0.186]
 [0.186]
 [0.24 ]
 [0.24 ]] [[38.062]
 [28.961]
 [26.314]
 [26.372]
 [26.191]
 [38.062]
 [38.062]] [[1.316]
 [0.889]
 [0.735]
 [0.737]
 [0.729]
 [1.316]
 [1.316]]
line 256 mcts: sample exp_bonus 37.710360917180864
maxi score, test score, baseline:  0.06480666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  0 policy actor:  0  step number:  52 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  52 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.06500666666666653 0.639 0.639
maxi score, test score, baseline:  0.06500666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06500666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  56 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.002]
 [ 0.016]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [ 0.016]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]]
printing an ep nov before normalisation:  0.04130473453784589
printing an ep nov before normalisation:  40.02403867174131
maxi score, test score, baseline:  0.06500666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  47 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06500666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06500666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  28.550585110982258
printing an ep nov before normalisation:  42.53466833635193
maxi score, test score, baseline:  0.06500666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  46.88732032203623
maxi score, test score, baseline:  0.06500666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2999999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  23.76673131171575
Printing some Q and Qe and total Qs values:  [[0.201]
 [0.287]
 [0.201]
 [0.201]
 [0.201]
 [0.201]
 [0.201]] [[24.055]
 [28.719]
 [24.055]
 [24.055]
 [24.055]
 [24.055]
 [24.055]] [[1.068]
 [1.482]
 [1.068]
 [1.068]
 [1.068]
 [1.068]
 [1.068]]
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.351]
 [0.285]
 [0.285]
 [0.285]
 [0.285]
 [0.285]] [[35.546]
 [38.317]
 [35.546]
 [35.546]
 [35.546]
 [35.546]
 [35.546]] [[1.228]
 [1.418]
 [1.228]
 [1.228]
 [1.228]
 [1.228]
 [1.228]]
printing an ep nov before normalisation:  36.65148331172368
printing an ep nov before normalisation:  31.49174740262761
actor:  1 policy actor:  1  step number:  54 total reward:  0.35333333333333283  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  0.0007178480126412978
maxi score, test score, baseline:  0.06500666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06500666666666653 0.639 0.639
actor:  1 policy actor:  1  step number:  65 total reward:  0.23999999999999932  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  16.038600183783625
from probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Printing some Q and Qe and total Qs values:  [[-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.002]
 [-0.009]] [[35.352]
 [35.352]
 [35.352]
 [35.352]
 [35.352]
 [34.989]
 [35.352]] [[0.887]
 [0.887]
 [0.887]
 [0.887]
 [0.887]
 [0.88 ]
 [0.887]]
printing an ep nov before normalisation:  36.04974049604756
siam score:  -0.7760301
maxi score, test score, baseline:  0.06500666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06500666666666653 0.639 0.639
maxi score, test score, baseline:  0.06500666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06500666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06500666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  64 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  56 total reward:  0.15333333333333266  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0673133333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  25.06733536634112
printing an ep nov before normalisation:  36.42609493206159
printing an ep nov before normalisation:  35.5078281909716
maxi score, test score, baseline:  0.0673133333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.0673133333333332 0.639 0.639
actor:  1 policy actor:  1  step number:  37 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.83793981615906
Printing some Q and Qe and total Qs values:  [[0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.158]] [[30.328]
 [30.328]
 [30.328]
 [30.328]
 [30.328]
 [30.328]
 [30.328]] [[1.73]
 [1.73]
 [1.73]
 [1.73]
 [1.73]
 [1.73]
 [1.73]]
printing an ep nov before normalisation:  24.75503444671631
printing an ep nov before normalisation:  32.32813514870762
printing an ep nov before normalisation:  37.95872461807956
printing an ep nov before normalisation:  32.45371852811921
using explorer policy with actor:  1
printing an ep nov before normalisation:  18.25450009293094
actor:  1 policy actor:  1  step number:  45 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0673133333333332 0.639 0.639
maxi score, test score, baseline:  0.0673133333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  34.00122337417515
printing an ep nov before normalisation:  31.778499157041868
printing an ep nov before normalisation:  41.806853977460406
maxi score, test score, baseline:  0.0673133333333332 0.639 0.639
actor:  0 policy actor:  0  step number:  34 total reward:  0.6066666666666668  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.406]
 [0.485]
 [0.406]
 [0.406]
 [0.406]
 [0.406]
 [0.406]] [[39.013]
 [38.13 ]
 [39.013]
 [39.013]
 [39.013]
 [39.013]
 [39.013]] [[2.068]
 [2.075]
 [2.068]
 [2.068]
 [2.068]
 [2.068]
 [2.068]]
actor:  0 policy actor:  1  step number:  50 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  58 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  2.0
siam score:  -0.780262
printing an ep nov before normalisation:  42.676610779499406
Printing some Q and Qe and total Qs values:  [[0.153]
 [0.346]
 [0.209]
 [0.285]
 [0.236]
 [0.274]
 [0.299]] [[36.932]
 [36.963]
 [41.009]
 [37.764]
 [38.679]
 [37.669]
 [40.197]] [[1.516]
 [1.711]
 [1.861]
 [1.707]
 [1.723]
 [1.689]
 [1.893]]
maxi score, test score, baseline:  0.06720666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  45 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06720666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  60 total reward:  0.0733333333333328  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  3  action  0 :  tensor([0.5652, 0.0087, 0.0721, 0.1010, 0.0999, 0.0787, 0.0744],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0120, 0.9079, 0.0080, 0.0092, 0.0044, 0.0041, 0.0544],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1509, 0.1542, 0.1889, 0.1002, 0.1031, 0.1947, 0.1080],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0898, 0.0081, 0.0488, 0.5845, 0.1155, 0.0913, 0.0620],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2783, 0.0075, 0.0414, 0.0673, 0.4877, 0.0416, 0.0763],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1504, 0.0375, 0.1213, 0.1281, 0.1014, 0.3550, 0.1063],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1174, 0.3775, 0.0668, 0.1092, 0.0741, 0.0607, 0.1942],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.241]] [[28.86]
 [28.86]
 [28.86]
 [28.86]
 [28.86]
 [28.86]
 [28.86]] [[0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]]
maxi score, test score, baseline:  0.06720666666666654 0.639 0.639
maxi score, test score, baseline:  0.06720666666666654 0.639 0.639
maxi score, test score, baseline:  0.06720666666666654 0.639 0.639
printing an ep nov before normalisation:  40.61270235979781
printing an ep nov before normalisation:  29.183784392573006
actor:  1 policy actor:  1  step number:  50 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06720666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  61 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.572]
 [0.512]
 [0.504]
 [0.507]
 [0.508]
 [0.501]] [[18.105]
 [16.192]
 [17.508]
 [17.47 ]
 [17.433]
 [17.571]
 [17.638]] [[2.229]
 [2.091]
 [2.154]
 [2.143]
 [2.143]
 [2.157]
 [2.155]]
printing an ep nov before normalisation:  35.86091398927798
actor:  1 policy actor:  1  step number:  68 total reward:  0.08666666666666567  reward:  1.0 rdn_beta:  2.0
siam score:  -0.77832776
printing an ep nov before normalisation:  38.205108642578125
maxi score, test score, baseline:  0.06720666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  41.52555465698242
printing an ep nov before normalisation:  34.819109364476844
maxi score, test score, baseline:  0.06720666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06720666666666654 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  0 policy actor:  0  step number:  60 total reward:  0.28666666666666574  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.06737999999999986 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06737999999999986 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06737999999999986 0.639 0.639
maxi score, test score, baseline:  0.06737999999999986 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  57 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  43.967609152871994
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06737999999999986 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.732]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]] [[38.522]
 [34.714]
 [38.522]
 [38.522]
 [38.522]
 [38.522]
 [38.522]] [[1.258]
 [1.498]
 [1.258]
 [1.258]
 [1.258]
 [1.258]
 [1.258]]
siam score:  -0.774796
printing an ep nov before normalisation:  35.9590302051273
maxi score, test score, baseline:  0.06737999999999986 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06737999999999986 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  13.316423892974854
actor:  0 policy actor:  0  step number:  62 total reward:  0.04666666666666608  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  59 total reward:  0.1999999999999994  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.016]
 [ 0.052]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.016]
 [ 0.052]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]]
Printing some Q and Qe and total Qs values:  [[0.802]
 [0.852]
 [0.77 ]
 [0.831]
 [0.786]
 [0.771]
 [0.855]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.802]
 [0.852]
 [0.77 ]
 [0.831]
 [0.786]
 [0.771]
 [0.855]]
printing an ep nov before normalisation:  33.44665286321779
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.782]
 [0.717]
 [0.711]
 [0.751]
 [0.714]
 [0.726]] [[28.299]
 [32.917]
 [29.438]
 [28.261]
 [33.639]
 [29.252]
 [28.453]] [[0.713]
 [0.782]
 [0.717]
 [0.711]
 [0.751]
 [0.714]
 [0.726]]
maxi score, test score, baseline:  0.0671133333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.394]
 [0.414]
 [0.414]
 [0.414]
 [0.414]
 [0.414]] [[36.917]
 [42.712]
 [36.917]
 [36.917]
 [36.917]
 [36.917]
 [36.917]] [[1.083]
 [1.299]
 [1.083]
 [1.083]
 [1.083]
 [1.083]
 [1.083]]
maxi score, test score, baseline:  0.0671133333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  53 total reward:  0.2799999999999999  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  49 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.013]
 [-0.007]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]] [[30.778]
 [35.499]
 [30.778]
 [30.778]
 [30.778]
 [30.778]
 [30.778]] [[0.476]
 [0.83 ]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]]
maxi score, test score, baseline:  0.0674733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Printing some Q and Qe and total Qs values:  [[0.066]
 [0.236]
 [0.132]
 [0.132]
 [0.077]
 [0.123]
 [0.159]] [[34.226]
 [46.058]
 [43.041]
 [43.041]
 [36.372]
 [36.316]
 [33.956]] [[0.802]
 [1.53 ]
 [1.285]
 [1.285]
 [0.915]
 [0.958]
 [0.882]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.73368938163083
printing an ep nov before normalisation:  46.09330421212584
maxi score, test score, baseline:  0.0674733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.0674733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  38.598415964286474
printing an ep nov before normalisation:  44.05926023291665
maxi score, test score, baseline:  0.0674733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 36.659322171398365
maxi score, test score, baseline:  0.0674733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  52 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  43.379960011026874
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0674733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.473]
 [0.485]
 [0.485]
 [0.485]
 [0.397]
 [0.485]] [[49.998]
 [46.884]
 [45.481]
 [45.481]
 [45.481]
 [47.832]
 [45.481]] [[1.724]
 [1.68 ]
 [1.636]
 [1.636]
 [1.636]
 [1.642]
 [1.636]]
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.416]
 [0.52 ]
 [0.52 ]] [[49.086]
 [49.086]
 [49.086]
 [49.086]
 [49.608]
 [49.086]
 [49.086]] [[1.831]
 [1.831]
 [1.831]
 [1.831]
 [1.749]
 [1.831]
 [1.831]]
maxi score, test score, baseline:  0.0674733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  62 total reward:  0.17999999999999905  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  48.3066058763852
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0674733333333332 0.639 0.639
printing an ep nov before normalisation:  40.50580415581824
maxi score, test score, baseline:  0.0674733333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.0674733333333332 0.639 0.639
actor:  1 policy actor:  1  step number:  62 total reward:  0.33999999999999897  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  40 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
siam score:  -0.77753127
maxi score, test score, baseline:  0.06741999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06741999999999987 0.639 0.639
printing an ep nov before normalisation:  44.906138066659146
maxi score, test score, baseline:  0.06741999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06741999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  25.822833333080446
Printing some Q and Qe and total Qs values:  [[0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]] [[39.987]
 [39.987]
 [39.987]
 [39.987]
 [39.987]
 [39.987]
 [39.987]] [[0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]]
maxi score, test score, baseline:  0.06741999999999987 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  45.061380079308115
actor:  1 policy actor:  1  step number:  51 total reward:  0.36  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  36.83590050392157
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.565]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]] [[23.933]
 [32.723]
 [23.933]
 [23.933]
 [23.933]
 [23.933]
 [23.933]] [[1.014]
 [1.39 ]
 [1.014]
 [1.014]
 [1.014]
 [1.014]
 [1.014]]
maxi score, test score, baseline:  0.06453999999999988 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  46.15904775324062
Printing some Q and Qe and total Qs values:  [[-0.068]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.068]] [[40.982]
 [40.982]
 [40.982]
 [40.982]
 [40.982]
 [40.982]
 [40.982]] [[0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]]
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]] [[24.887]
 [24.887]
 [24.887]
 [24.887]
 [24.887]
 [24.887]
 [24.887]] [[1.333]
 [1.333]
 [1.333]
 [1.333]
 [1.333]
 [1.333]
 [1.333]]
actions average: 
K:  4  action  0 :  tensor([0.4525, 0.0127, 0.0866, 0.1096, 0.0968, 0.0992, 0.1426],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0091, 0.9385, 0.0056, 0.0112, 0.0152, 0.0068, 0.0138],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0880, 0.0408, 0.4575, 0.0708, 0.0598, 0.2138, 0.0695],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0899, 0.0973, 0.0667, 0.4432, 0.0913, 0.0649, 0.1467],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1776, 0.0768, 0.0892, 0.1195, 0.2910, 0.0935, 0.1525],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1433, 0.0276, 0.1064, 0.1055, 0.0836, 0.4303, 0.1034],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1766, 0.1024, 0.1101, 0.1511, 0.1525, 0.1221, 0.1852],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.06453999999999988 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  66 total reward:  0.08666666666666578  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.584]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]
 [0.34 ]] [[38.574]
 [40.719]
 [38.574]
 [38.574]
 [38.574]
 [38.574]
 [38.574]] [[1.288]
 [1.64 ]
 [1.288]
 [1.288]
 [1.288]
 [1.288]
 [1.288]]
printing an ep nov before normalisation:  36.82741373143135
maxi score, test score, baseline:  0.06453999999999988 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.175]
 [0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]] [[34.375]
 [36.337]
 [34.375]
 [34.375]
 [34.375]
 [34.375]
 [34.375]] [[0.735]
 [0.863]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]]
printing an ep nov before normalisation:  43.72973995319106
printing an ep nov before normalisation:  11.482541116589537
actor:  1 policy actor:  1  step number:  56 total reward:  0.2599999999999999  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[ 0.037]
 [ 0.06 ]
 [ 0.037]
 [ 0.037]
 [ 0.037]
 [-0.007]
 [ 0.037]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.037]
 [ 0.06 ]
 [ 0.037]
 [ 0.037]
 [ 0.037]
 [-0.007]
 [ 0.037]]
printing an ep nov before normalisation:  31.405298875862652
printing an ep nov before normalisation:  22.500360986812712
actor:  1 policy actor:  1  step number:  85 total reward:  0.013333333333331754  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.536]
 [0.487]
 [0.487]
 [0.487]
 [0.449]
 [0.487]] [[31.427]
 [33.463]
 [31.427]
 [31.427]
 [31.427]
 [33.095]
 [31.427]] [[0.487]
 [0.536]
 [0.487]
 [0.487]
 [0.487]
 [0.449]
 [0.487]]
maxi score, test score, baseline:  0.06453999999999988 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
from probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.06113999999999986 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  47.78515588165106
maxi score, test score, baseline:  0.06113999999999986 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Printing some Q and Qe and total Qs values:  [[ 0.423]
 [ 0.529]
 [-0.003]
 [ 0.483]
 [ 0.431]
 [ 0.483]
 [ 0.439]] [[37.468]
 [38.192]
 [40.312]
 [30.659]
 [43.223]
 [30.659]
 [38.254]] [[ 0.423]
 [ 0.529]
 [-0.003]
 [ 0.483]
 [ 0.431]
 [ 0.483]
 [ 0.439]]
printing an ep nov before normalisation:  43.04900173030087
maxi score, test score, baseline:  0.06113999999999986 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.05773999999999986 0.639 0.639
siam score:  -0.775614
maxi score, test score, baseline:  0.054339999999999854 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Printing some Q and Qe and total Qs values:  [[-0.012]
 [-0.004]
 [-0.015]
 [-0.01 ]
 [-0.013]
 [-0.01 ]
 [-0.014]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.012]
 [-0.004]
 [-0.015]
 [-0.01 ]
 [-0.013]
 [-0.01 ]
 [-0.014]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.054339999999999854 0.639 0.639
Printing some Q and Qe and total Qs values:  [[0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]] [[36.485]
 [36.485]
 [36.485]
 [36.485]
 [36.485]
 [36.485]
 [36.485]] [[0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]]
Printing some Q and Qe and total Qs values:  [[0.771]
 [0.858]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]] [[40.895]
 [44.194]
 [40.895]
 [40.895]
 [40.895]
 [40.895]
 [40.895]] [[0.771]
 [0.858]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]]
Printing some Q and Qe and total Qs values:  [[0.706]
 [0.736]
 [0.784]
 [0.706]
 [0.706]
 [0.706]
 [0.665]] [[33.63 ]
 [36.663]
 [37.643]
 [33.63 ]
 [33.63 ]
 [33.63 ]
 [32.705]] [[0.706]
 [0.736]
 [0.784]
 [0.706]
 [0.706]
 [0.706]
 [0.665]]
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.261]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]] [[31.813]
 [32.184]
 [31.813]
 [31.813]
 [31.813]
 [31.813]
 [31.813]] [[1.379]
 [1.417]
 [1.379]
 [1.379]
 [1.379]
 [1.379]
 [1.379]]
maxi score, test score, baseline:  0.054339999999999854 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.054339999999999854 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.184]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]] [[26.88 ]
 [36.329]
 [26.88 ]
 [26.88 ]
 [26.88 ]
 [26.88 ]
 [26.88 ]] [[0.547]
 [0.909]
 [0.547]
 [0.547]
 [0.547]
 [0.547]
 [0.547]]
maxi score, test score, baseline:  0.054339999999999854 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  45.11241146293491
maxi score, test score, baseline:  0.054339999999999854 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  44 total reward:  0.45999999999999963  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  57 total reward:  0.3333333333333325  reward:  1.0 rdn_beta:  1.333
siam score:  -0.7749298
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.45932009990634
Printing some Q and Qe and total Qs values:  [[0.13 ]
 [0.114]
 [0.135]
 [0.135]
 [0.135]
 [0.135]
 [0.135]] [[36.545]
 [40.222]
 [36.783]
 [36.783]
 [36.783]
 [36.783]
 [36.783]] [[1.328]
 [1.56 ]
 [1.349]
 [1.349]
 [1.349]
 [1.349]
 [1.349]]
siam score:  -0.77398115
maxi score, test score, baseline:  0.05360666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.05360666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.749]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]] [[11.336]
 [13.01 ]
 [11.336]
 [11.336]
 [11.336]
 [11.336]
 [11.336]] [[1.363]
 [1.57 ]
 [1.363]
 [1.363]
 [1.363]
 [1.363]
 [1.363]]
printing an ep nov before normalisation:  44.825601539608854
actor:  1 policy actor:  1  step number:  66 total reward:  0.23333333333333228  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.087]
 [0.274]
 [0.087]
 [0.087]
 [0.319]
 [0.087]
 [0.087]] [[32.494]
 [36.779]
 [32.494]
 [32.494]
 [36.509]
 [32.494]
 [32.494]] [[0.767]
 [1.142]
 [0.767]
 [0.767]
 [1.175]
 [0.767]
 [0.767]]
maxi score, test score, baseline:  0.05360666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  47.43358486883104
printing an ep nov before normalisation:  45.95784909551568
using explorer policy with actor:  1
maxi score, test score, baseline:  0.05360666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.05360666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  28.223185539245605
printing an ep nov before normalisation:  26.678459318671315
using explorer policy with actor:  1
maxi score, test score, baseline:  0.05360666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.05020666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.05020666666666653 0.639 0.639
printing an ep nov before normalisation:  38.60912881267645
using explorer policy with actor:  1
maxi score, test score, baseline:  0.05020666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.05020666666666653 0.639 0.639
maxi score, test score, baseline:  0.05020666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  38.222694046221484
printing an ep nov before normalisation:  48.66930263427658
actions average: 
K:  1  action  0 :  tensor([0.4535, 0.0045, 0.0967, 0.0984, 0.1493, 0.0984, 0.0993],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0060, 0.9616, 0.0027, 0.0043, 0.0027, 0.0023, 0.0203],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1587, 0.0568, 0.2754, 0.1136, 0.1198, 0.1517, 0.1241],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2223, 0.1147, 0.0895, 0.1589, 0.1166, 0.1169, 0.1813],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1305, 0.0033, 0.0596, 0.0627, 0.6151, 0.0657, 0.0632],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1408, 0.0129, 0.1051, 0.1026, 0.1269, 0.4091, 0.1026],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0968, 0.1289, 0.1066, 0.1475, 0.1118, 0.1320, 0.2764],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.05020666666666653 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  40.38771818689592
printing an ep nov before normalisation:  49.19561171431615
printing an ep nov before normalisation:  47.13451301024071
Printing some Q and Qe and total Qs values:  [[0.936]
 [0.951]
 [0.936]
 [0.936]
 [0.936]
 [0.936]
 [0.936]] [[41.54 ]
 [37.343]
 [41.54 ]
 [41.54 ]
 [41.54 ]
 [41.54 ]
 [41.54 ]] [[0.936]
 [0.951]
 [0.936]
 [0.936]
 [0.936]
 [0.936]
 [0.936]]
actor:  0 policy actor:  1  step number:  64 total reward:  0.0999999999999992  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  50 total reward:  0.4199999999999995  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  34.24506280959736
printing an ep nov before normalisation:  34.51527620620816
printing an ep nov before normalisation:  38.6673148216199
actor:  1 policy actor:  1  step number:  67 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  35.15215372268418
maxi score, test score, baseline:  0.049006666666666525 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  31.157651343820916
maxi score, test score, baseline:  0.049006666666666525 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  30.81420411170931
maxi score, test score, baseline:  0.049006666666666525 0.639 0.639
maxi score, test score, baseline:  0.049006666666666525 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  34.29607772078554
using another actor
from probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.049006666666666525 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  31.866432112457026
printing an ep nov before normalisation:  44.161438888309654
maxi score, test score, baseline:  0.049006666666666525 0.639 0.639
actor:  1 policy actor:  1  step number:  57 total reward:  0.21333333333333315  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.049006666666666525 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  0.0006700430233763655
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
actor:  1 policy actor:  1  step number:  66 total reward:  0.03333333333333266  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.20408325381215
maxi score, test score, baseline:  0.049006666666666525 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  53.283190557811835
maxi score, test score, baseline:  0.049006666666666525 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  0 policy actor:  1  step number:  50 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0481533333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  49.36936452332754
maxi score, test score, baseline:  0.0481533333333332 0.639 0.639
maxi score, test score, baseline:  0.0481533333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  32.07999080963818
actor:  1 policy actor:  1  step number:  59 total reward:  0.38666666666666594  reward:  1.0 rdn_beta:  2.0
from probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0481533333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Printing some Q and Qe and total Qs values:  [[0.783]
 [0.837]
 [0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]] [[37.949]
 [40.046]
 [31.08 ]
 [31.08 ]
 [31.08 ]
 [31.08 ]
 [31.08 ]] [[0.783]
 [0.837]
 [0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]]
Printing some Q and Qe and total Qs values:  [[0.805]
 [0.861]
 [0.816]
 [0.816]
 [0.813]
 [0.824]
 [0.817]] [[27.774]
 [42.371]
 [28.45 ]
 [28.45 ]
 [28.75 ]
 [28.515]
 [27.959]] [[0.805]
 [0.861]
 [0.816]
 [0.816]
 [0.813]
 [0.824]
 [0.817]]
printing an ep nov before normalisation:  52.71096229553223
printing an ep nov before normalisation:  31.700242332614714
printing an ep nov before normalisation:  48.60829287459747
actor:  0 policy actor:  1  step number:  57 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  46 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0431133333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  44.329188404063956
printing an ep nov before normalisation:  42.89047836594335
printing an ep nov before normalisation:  39.83158826828003
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.262]] [[46.471]
 [46.471]
 [46.471]
 [46.471]
 [46.471]
 [46.471]
 [46.471]] [[1.642]
 [1.642]
 [1.642]
 [1.642]
 [1.642]
 [1.642]
 [1.642]]
printing an ep nov before normalisation:  45.76519472536189
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.404]
 [0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]] [[44.081]
 [49.348]
 [44.081]
 [44.081]
 [44.081]
 [44.081]
 [44.081]] [[1.569]
 [1.894]
 [1.569]
 [1.569]
 [1.569]
 [1.569]
 [1.569]]
UNIT TEST: sample policy line 217 mcts : [0.02  0.939 0.    0.    0.    0.02  0.02 ]
printing an ep nov before normalisation:  46.0498743939917
printing an ep nov before normalisation:  39.948649292421905
maxi score, test score, baseline:  0.0397133333333332 0.639 0.639
maxi score, test score, baseline:  0.0397133333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.47077372785457
siam score:  -0.7615218
actor:  1 policy actor:  1  step number:  55 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  40.987577014356155
maxi score, test score, baseline:  0.0397133333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  30.573902130126953
printing an ep nov before normalisation:  37.701170444488525
actor:  1 policy actor:  1  step number:  56 total reward:  0.35333333333333283  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0397133333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.0397133333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  40.19385650752875
maxi score, test score, baseline:  0.0397133333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0397133333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
line 256 mcts: sample exp_bonus 20.95841407775879
maxi score, test score, baseline:  0.0397133333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.0397133333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  46 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0397133333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  44.59661662987441
printing an ep nov before normalisation:  45.267633145243856
actor:  1 policy actor:  1  step number:  54 total reward:  0.20666666666666633  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 36.80408651998292
maxi score, test score, baseline:  0.0397133333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
maxi score, test score, baseline:  0.0397133333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  1.333
from probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Starting evaluation
printing an ep nov before normalisation:  41.90908620696638
line 256 mcts: sample exp_bonus 0.0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.546]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.497]
 [0.546]
 [0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.581]
 [0.535]
 [0.525]
 [0.527]
 [0.523]
 [0.523]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.513]
 [0.581]
 [0.535]
 [0.525]
 [0.527]
 [0.523]
 [0.523]]
printing an ep nov before normalisation:  31.99510752470488
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  26.904016070895725
printing an ep nov before normalisation:  47.43517603755436
printing an ep nov before normalisation:  47.06818659656033
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.512]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]] [[44.137]
 [37.831]
 [44.137]
 [44.137]
 [44.137]
 [44.137]
 [44.137]] [[0.491]
 [0.512]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]]
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.557]
 [0.531]
 [0.531]
 [0.525]
 [0.531]
 [0.501]] [[32.165]
 [32.377]
 [32.165]
 [32.165]
 [28.232]
 [32.165]
 [28.274]] [[0.531]
 [0.557]
 [0.531]
 [0.531]
 [0.525]
 [0.531]
 [0.501]]
siam score:  -0.75607985
printing an ep nov before normalisation:  49.69311401341971
actor:  1 policy actor:  1  step number:  48 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  47.478042391609506
printing an ep nov before normalisation:  42.492638511355935
printing an ep nov before normalisation:  45.0714854267563
maxi score, test score, baseline:  0.0397133333333332 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
Printing some Q and Qe and total Qs values:  [[0.726]
 [0.752]
 [0.729]
 [0.735]
 [0.735]
 [0.729]
 [0.735]] [[28.094]
 [32.16 ]
 [30.276]
 [27.4  ]
 [25.321]
 [30.276]
 [24.857]] [[0.726]
 [0.752]
 [0.729]
 [0.735]
 [0.735]
 [0.729]
 [0.735]]
printing an ep nov before normalisation:  14.595924250669803
printing an ep nov before normalisation:  33.18363913187099
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  12.925387065681504
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.724]
 [0.817]
 [0.745]
 [0.76 ]
 [0.762]
 [0.787]
 [0.777]] [[32.352]
 [35.734]
 [34.416]
 [34.712]
 [34.923]
 [34.844]
 [34.215]] [[0.724]
 [0.817]
 [0.745]
 [0.76 ]
 [0.762]
 [0.787]
 [0.777]]
printing an ep nov before normalisation:  32.651249059826235
actor:  0 policy actor:  0  step number:  30 total reward:  0.5933333333333334  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  11.871966299843534
actor:  0 policy actor:  0  step number:  31 total reward:  0.56  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  27.277729511260986
actions average: 
K:  0  action  0 :  tensor([0.5196, 0.0057, 0.1107, 0.0873, 0.1134, 0.0789, 0.0843],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0033, 0.9651, 0.0030, 0.0030, 0.0014, 0.0014, 0.0228],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0410, 0.0053, 0.7514, 0.0246, 0.0279, 0.0778, 0.0719],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1524, 0.0108, 0.0969, 0.3718, 0.1037, 0.1594, 0.1049],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1144, 0.0018, 0.0306, 0.0233, 0.7837, 0.0252, 0.0210],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1305, 0.0476, 0.1195, 0.0869, 0.0993, 0.4260, 0.0901],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2360, 0.0038, 0.1280, 0.1145, 0.1292, 0.1084, 0.2801],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  20.320568084716797
maxi score, test score, baseline:  0.04229999999999985 0.639 0.639
probs:  [0.07354950211316366, 0.07354950211316366, 0.07354950211316366, 0.4842702656241082, 0.07354950211316366, 0.22153172592323706]
printing an ep nov before normalisation:  33.807229692564974
printing an ep nov before normalisation:  14.018669128417969
printing an ep nov before normalisation:  11.788956325321232
printing an ep nov before normalisation:  10.241403579711914
Printing some Q and Qe and total Qs values:  [[0.63 ]
 [0.642]
 [0.625]
 [0.625]
 [0.65 ]
 [0.625]
 [0.625]] [[13.514]
 [19.5  ]
 [15.179]
 [15.179]
 [10.481]
 [15.179]
 [15.179]] [[0.63 ]
 [0.642]
 [0.625]
 [0.625]
 [0.65 ]
 [0.625]
 [0.625]]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  13.057150898965249
actor:  0 policy actor:  1  step number:  58 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.026579999999999854 0.08566666666666674 0.08566666666666674
probs:  [0.05096027217323711, 0.0814531098140037, 0.1315968391442182, 0.3560019173831703, 0.1705283801690051, 0.20945948131636566]
Printing some Q and Qe and total Qs values:  [[0.121]
 [0.334]
 [0.287]
 [0.287]
 [0.287]
 [0.287]
 [0.287]] [[38.429]
 [40.369]
 [45.26 ]
 [45.26 ]
 [45.26 ]
 [45.26 ]
 [45.26 ]] [[0.344]
 [0.581]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]]
printing an ep nov before normalisation:  30.67582368850708
printing an ep nov before normalisation:  44.85766512914911
printing an ep nov before normalisation:  42.81944926572383
printing an ep nov before normalisation:  43.05449590965986
printing an ep nov before normalisation:  25.884342193603516
Printing some Q and Qe and total Qs values:  [[0.705]
 [0.772]
 [0.749]
 [0.749]
 [0.749]
 [0.405]
 [0.749]] [[38.111]
 [37.319]
 [33.03 ]
 [33.03 ]
 [33.03 ]
 [40.756]
 [33.03 ]] [[0.705]
 [0.772]
 [0.749]
 [0.749]
 [0.749]
 [0.405]
 [0.749]]
maxi score, test score, baseline:  0.026579999999999854 0.08566666666666674 0.08566666666666674
probs:  [0.05096027217323711, 0.0814531098140037, 0.1315968391442182, 0.3560019173831703, 0.1705283801690051, 0.20945948131636566]
actor:  1 policy actor:  1  step number:  60 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  27.769890722655592
printing an ep nov before normalisation:  28.320621547142323
maxi score, test score, baseline:  0.026579999999999854 0.08566666666666674 0.08566666666666674
maxi score, test score, baseline:  0.026579999999999854 0.08566666666666674 0.08566666666666674
probs:  [0.050914474406919055, 0.0813798364421085, 0.13237871456631023, 0.3556812615000996, 0.17037484542479084, 0.20927086765977176]
Printing some Q and Qe and total Qs values:  [[0.753]
 [0.809]
 [0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]] [[39.097]
 [38.297]
 [39.097]
 [39.097]
 [39.097]
 [39.097]
 [39.097]] [[0.753]
 [0.809]
 [0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]]
Printing some Q and Qe and total Qs values:  [[0.193]
 [0.296]
 [0.267]
 [0.189]
 [0.189]
 [0.189]
 [0.189]] [[50.182]
 [45.805]
 [48.41 ]
 [47.618]
 [47.618]
 [47.618]
 [47.618]] [[1.457]
 [1.348]
 [1.446]
 [1.329]
 [1.329]
 [1.329]
 [1.329]]
printing an ep nov before normalisation:  58.0786245313559
maxi score, test score, baseline:  0.026579999999999854 0.08566666666666674 0.08566666666666674
probs:  [0.050902859058477815, 0.08143655488309152, 0.13240985044748663, 0.3556003821571766, 0.17038692131237795, 0.20926343214138948]
maxi score, test score, baseline:  0.026579999999999854 0.08566666666666674 0.08566666666666674
probs:  [0.050902859058477815, 0.08143655488309152, 0.13240985044748663, 0.3556003821571766, 0.17038692131237795, 0.20926343214138948]
actor:  0 policy actor:  0  step number:  51 total reward:  0.3466666666666661  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.026446666666666514 0.08566666666666674 0.08566666666666674
probs:  [0.050902859058477815, 0.08143655488309152, 0.13240985044748663, 0.3556003821571766, 0.17038692131237795, 0.20926343214138948]
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]] [[40.046]
 [40.046]
 [40.046]
 [40.046]
 [40.046]
 [40.046]
 [40.046]] [[0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]]
printing an ep nov before normalisation:  42.987968253283945
actor:  0 policy actor:  1  step number:  48 total reward:  0.4333333333333329  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.374]
 [0.46 ]
 [0.397]
 [0.408]
 [0.423]
 [0.374]] [[39.832]
 [50.275]
 [47.074]
 [43.577]
 [47.736]
 [49.049]
 [50.275]] [[1.088]
 [1.413]
 [1.373]
 [1.172]
 [1.347]
 [1.414]
 [1.413]]
maxi score, test score, baseline:  0.02643333333333319 0.08566666666666674 0.08566666666666674
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.36  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  46.86012534892022
printing an ep nov before normalisation:  44.72742952734587
actor:  1 policy actor:  1  step number:  47 total reward:  0.546666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.02643333333333319 0.08566666666666674 0.08566666666666674
probs:  [0.051094609473988015, 0.0814479551888487, 0.1321201722496025, 0.35694452394209697, 0.16987292801887216, 0.20851981112659176]
printing an ep nov before normalisation:  34.85429286956787
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]] [[53.602]
 [53.602]
 [53.602]
 [53.602]
 [53.602]
 [53.602]
 [53.602]] [[1.639]
 [1.639]
 [1.639]
 [1.639]
 [1.639]
 [1.639]
 [1.639]]
printing an ep nov before normalisation:  36.04392124200888
actor:  1 policy actor:  1  step number:  46 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.78621162497666
maxi score, test score, baseline:  0.02643333333333319 0.08566666666666674 0.08566666666666674
probs:  [0.05120111613012734, 0.0814938119156321, 0.13206477930150948, 0.35769090745521376, 0.16974210015853233, 0.2078072850389851]
maxi score, test score, baseline:  0.02643333333333319 0.08566666666666674 0.08566666666666674
maxi score, test score, baseline:  0.02643333333333319 0.08566666666666674 0.08566666666666674
printing an ep nov before normalisation:  39.380478858947754
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  73 total reward:  0.1599999999999987  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.02643333333333319 0.08566666666666674 0.08566666666666674
probs:  [0.051158826610037164, 0.08149134960435556, 0.13212880498100946, 0.35739446395404123, 0.16985566195511922, 0.20797089289543733]
maxi score, test score, baseline:  0.02643333333333319 0.08566666666666674 0.08566666666666674
probs:  [0.051158826610037164, 0.08149134960435556, 0.13212880498100946, 0.35739446395404123, 0.16985566195511922, 0.20797089289543733]
printing an ep nov before normalisation:  23.797421799457318
maxi score, test score, baseline:  0.02643333333333319 0.08566666666666674 0.08566666666666674
probs:  [0.051158826610037164, 0.08149134960435556, 0.13212880498100946, 0.35739446395404123, 0.16985566195511922, 0.20797089289543733]
using explorer policy with actor:  1
siam score:  -0.77525884
printing an ep nov before normalisation:  25.02543745734944
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.44000000000000017  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.02643333333333319 0.08566666666666674 0.08566666666666674
probs:  [0.051259669861589315, 0.08149722121214234, 0.13197612985748364, 0.3581013607572207, 0.16958486340277074, 0.20758075490879335]
printing an ep nov before normalisation:  28.11913291465063
printing an ep nov before normalisation:  35.945186614990234
maxi score, test score, baseline:  0.02643333333333319 0.08566666666666674 0.08566666666666674
actor:  0 policy actor:  0  step number:  49 total reward:  0.37333333333333285  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.026673333333333188 0.08566666666666674 0.08566666666666674
probs:  [0.051259669861589315, 0.08149722121214234, 0.13197612985748364, 0.3581013607572207, 0.16958486340277074, 0.20758075490879335]
maxi score, test score, baseline:  0.026673333333333188 0.08566666666666674 0.08566666666666674
probs:  [0.05122051598721256, 0.08149494147413176, 0.1320354082178595, 0.3578268976850468, 0.16969000492029282, 0.20773223171545654]
printing an ep nov before normalisation:  40.84922902490459
printing an ep nov before normalisation:  39.52486038208008
maxi score, test score, baseline:  0.026673333333333188 0.08566666666666674 0.08566666666666674
probs:  [0.05122051598721256, 0.08149494147413176, 0.1320354082178595, 0.3578268976850468, 0.16969000492029282, 0.20773223171545654]
printing an ep nov before normalisation:  37.92302074495316
actor:  1 policy actor:  1  step number:  55 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.026673333333333188 0.08566666666666674 0.08566666666666674
printing an ep nov before normalisation:  36.388665685129546
maxi score, test score, baseline:  0.026673333333333188 0.08566666666666674 0.08566666666666674
printing an ep nov before normalisation:  52.4741259316951
maxi score, test score, baseline:  0.026673333333333188 0.08566666666666674 0.08566666666666674
probs:  [0.0513064611303289, 0.08149994563815655, 0.13190528860013823, 0.358429360874032, 0.16945921287442894, 0.20739973088291547]
maxi score, test score, baseline:  0.026673333333333188 0.08566666666666674 0.08566666666666674
probs:  [0.0513064611303289, 0.08149994563815655, 0.13190528860013823, 0.358429360874032, 0.16945921287442894, 0.20739973088291547]
actor:  1 policy actor:  1  step number:  53 total reward:  0.31999999999999995  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.026673333333333188 0.08566666666666674 0.08566666666666674
maxi score, test score, baseline:  0.026673333333333188 0.08566666666666674 0.08566666666666674
probs:  [0.05131586827212515, 0.0815004933698706, 0.13189104633291998, 0.35849530359563675, 0.16943395148766333, 0.20736333694178427]
using explorer policy with actor:  0
printing an ep nov before normalisation:  19.579984466971112
actor:  0 policy actor:  1  step number:  64 total reward:  0.21999999999999986  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026593333333333188 0.08566666666666674 0.08566666666666674
probs:  [0.05131586827212515, 0.0815004933698706, 0.13189104633291998, 0.35849530359563675, 0.16943395148766333, 0.20736333694178427]
Printing some Q and Qe and total Qs values:  [[-0.011]
 [ 0.008]
 [-0.014]
 [-0.009]
 [-0.009]
 [-0.014]
 [-0.018]] [[37.458]
 [36.344]
 [37.184]
 [38.231]
 [37.888]
 [36.739]
 [37.36 ]] [[1.079]
 [1.034]
 [1.06 ]
 [1.125]
 [1.105]
 [1.035]
 [1.066]]
maxi score, test score, baseline:  0.026593333333333188 0.08566666666666674 0.08566666666666674
maxi score, test score, baseline:  0.026593333333333188 0.08566666666666674 0.08566666666666674
probs:  [0.05131586827212515, 0.0815004933698706, 0.13189104633291998, 0.35849530359563675, 0.16943395148766333, 0.20736333694178427]
Printing some Q and Qe and total Qs values:  [[0.504]
 [0.649]
 [0.477]
 [0.52 ]
 [0.474]
 [0.516]
 [0.58 ]] [[41.73 ]
 [39.989]
 [38.99 ]
 [40.956]
 [41.674]
 [41.031]
 [42.702]] [[1.751]
 [1.799]
 [1.571]
 [1.723]
 [1.718]
 [1.724]
 [1.881]]
maxi score, test score, baseline:  0.026593333333333188 0.08566666666666674 0.08566666666666674
probs:  [0.05131586827212515, 0.0815004933698706, 0.13189104633291998, 0.35849530359563675, 0.16943395148766333, 0.20736333694178427]
maxi score, test score, baseline:  0.026593333333333188 0.08566666666666674 0.08566666666666674
probs:  [0.05131586827212515, 0.0815004933698706, 0.13189104633291998, 0.35849530359563675, 0.16943395148766333, 0.20736333694178427]
printing an ep nov before normalisation:  53.97669356536263
printing an ep nov before normalisation:  53.24164368657759
printing an ep nov before normalisation:  37.70541891462774
actor:  1 policy actor:  1  step number:  41 total reward:  0.5733333333333336  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  49 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.026473333333333186 0.08566666666666674 0.08566666666666674
probs:  [0.0511647458782363, 0.08126024754139684, 0.1315020168393797, 0.35743720004093393, 0.17188432251347452, 0.20675146718657864]
printing an ep nov before normalisation:  32.55166300628535
printing an ep nov before normalisation:  40.10240664614388
Printing some Q and Qe and total Qs values:  [[0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]] [[33.277]
 [33.277]
 [33.277]
 [33.277]
 [33.277]
 [33.277]
 [33.277]] [[0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]]
maxi score, test score, baseline:  0.026473333333333186 0.08566666666666674 0.08566666666666674
probs:  [0.0511647458782363, 0.08126024754139684, 0.1315020168393797, 0.35743720004093393, 0.17188432251347452, 0.20675146718657864]
actions average: 
K:  2  action  0 :  tensor([0.5993, 0.0294, 0.0586, 0.0564, 0.1205, 0.0811, 0.0548],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0010,     0.9780,     0.0015,     0.0035,     0.0009,     0.0010,
            0.0142], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0204, 0.0089, 0.8349, 0.0188, 0.0208, 0.0642, 0.0320],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1832, 0.0096, 0.1247, 0.2330, 0.1360, 0.1701, 0.1434],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1184, 0.0026, 0.0542, 0.1331, 0.5284, 0.1135, 0.0497],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1265, 0.0117, 0.1403, 0.1407, 0.1501, 0.3223, 0.1085],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2119, 0.1811, 0.0773, 0.0950, 0.0814, 0.0931, 0.2602],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.026473333333333186 0.08566666666666674 0.08566666666666674
probs:  [0.0511647458782363, 0.08126024754139684, 0.1315020168393797, 0.35743720004093393, 0.17188432251347452, 0.20675146718657864]
actor:  1 policy actor:  1  step number:  64 total reward:  0.27333333333333243  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  31.25253094714493
actor:  0 policy actor:  0  step number:  35 total reward:  0.5466666666666667  reward:  1.0 rdn_beta:  2.0
siam score:  -0.7714964
actor:  0 policy actor:  0  step number:  46 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  50.40320679296522
maxi score, test score, baseline:  0.029779999999999866 0.08566666666666674 0.08566666666666674
probs:  [0.051192870893635875, 0.08126208726767885, 0.13145997560808353, 0.35763435129997156, 0.17180701154014988, 0.20664370339048022]
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.788]
 [0.727]
 [0.727]
 [0.698]
 [0.727]
 [0.727]] [[32.165]
 [35.094]
 [32.165]
 [32.165]
 [34.955]
 [32.165]
 [32.165]] [[0.727]
 [0.788]
 [0.727]
 [0.727]
 [0.698]
 [0.727]
 [0.727]]
maxi score, test score, baseline:  0.029779999999999866 0.08566666666666674 0.08566666666666674
probs:  [0.051192870893635875, 0.08126208726767885, 0.13145997560808353, 0.35763435129997156, 0.17180701154014988, 0.20664370339048022]
maxi score, test score, baseline:  0.029779999999999866 0.08566666666666674 0.08566666666666674
Printing some Q and Qe and total Qs values:  [[0.833]
 [0.887]
 [0.871]
 [0.852]
 [0.851]
 [0.846]
 [0.847]] [[27.492]
 [30.2  ]
 [29.424]
 [29.487]
 [29.4  ]
 [27.798]
 [27.53 ]] [[0.833]
 [0.887]
 [0.871]
 [0.852]
 [0.851]
 [0.846]
 [0.847]]
maxi score, test score, baseline:  0.029779999999999866 0.08566666666666674 0.08566666666666674
probs:  [0.051192870893635875, 0.08126208726767885, 0.13145997560808353, 0.35763435129997156, 0.17180701154014988, 0.20664370339048022]
maxi score, test score, baseline:  0.029779999999999866 0.08566666666666674 0.08566666666666674
probs:  [0.051192870893635875, 0.08126208726767885, 0.13145997560808353, 0.35763435129997156, 0.17180701154014988, 0.20664370339048022]
maxi score, test score, baseline:  0.029779999999999866 0.08566666666666674 0.08566666666666674
actor:  0 policy actor:  1  step number:  47 total reward:  0.4133333333333328  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.315]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]] [[32.195]
 [35.711]
 [32.195]
 [32.195]
 [32.195]
 [32.195]
 [32.195]] [[1.436]
 [1.746]
 [1.436]
 [1.436]
 [1.436]
 [1.436]
 [1.436]]
actor:  1 policy actor:  1  step number:  64 total reward:  0.019999999999999574  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.03001999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.051200511587252126, 0.08112470469679005, 0.1314796276732026, 0.3576878486677275, 0.17183270123723962, 0.20667460613778807]
printing an ep nov before normalisation:  38.69730444542256
printing an ep nov before normalisation:  35.25127227526319
actor:  1 policy actor:  1  step number:  37 total reward:  0.52  reward:  1.0 rdn_beta:  1.667
from probs:  [0.051200511587252126, 0.08112470469679005, 0.1314796276732026, 0.3576878486677275, 0.17183270123723965, 0.20667460613778807]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.242]
 [0.142]
 [0.108]
 [0.142]
 [0.113]
 [0.151]] [[38.761]
 [38.816]
 [38.761]
 [32.854]
 [38.761]
 [33.36 ]
 [35.839]] [[0.791]
 [0.893]
 [0.791]
 [0.577]
 [0.791]
 [0.598]
 [0.712]]
maxi score, test score, baseline:  0.02733999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.051081272307659124, 0.08093559408488929, 0.13117294110716543, 0.3568529784436717, 0.17376487059175952, 0.20619234346485502]
maxi score, test score, baseline:  0.02733999999999986 0.08566666666666674 0.08566666666666674
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.788]
 [0.852]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]] [[35.599]
 [40.556]
 [35.599]
 [35.599]
 [35.599]
 [35.599]
 [35.599]] [[0.788]
 [0.852]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]]
printing an ep nov before normalisation:  32.87416131402346
printing an ep nov before normalisation:  36.21317670980099
printing an ep nov before normalisation:  31.770884712027225
actor:  1 policy actor:  1  step number:  52 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.02733999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.051036412683941386, 0.08086444780725449, 0.1319373545798139, 0.3565388876152792, 0.1736119883737434, 0.20601090893996746]
actor:  0 policy actor:  1  step number:  41 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  58 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.02727333333333319 0.08566666666666674 0.08566666666666674
probs:  [0.050996829403901274, 0.0808016696830074, 0.13261185851111487, 0.3562617398331997, 0.1734770880079498, 0.20585081456082702]
maxi score, test score, baseline:  0.02727333333333319 0.08566666666666674 0.08566666666666674
maxi score, test score, baseline:  0.02727333333333319 0.08566666666666674 0.08566666666666674
probs:  [0.050996829403901274, 0.0808016696830074, 0.13261185851111487, 0.3562617398331997, 0.1734770880079498, 0.20585081456082702]
maxi score, test score, baseline:  0.02727333333333319 0.08566666666666674 0.08566666666666674
maxi score, test score, baseline:  0.02727333333333319 0.08566666666666674 0.08566666666666674
probs:  [0.050996829403901274, 0.0808016696830074, 0.13261185851111487, 0.3562617398331997, 0.1734770880079498, 0.20585081456082702]
printing an ep nov before normalisation:  33.327059745788574
printing an ep nov before normalisation:  33.66512252001695
maxi score, test score, baseline:  0.02727333333333319 0.08566666666666674 0.08566666666666674
probs:  [0.050996829403901274, 0.0808016696830074, 0.13261185851111487, 0.3562617398331997, 0.1734770880079498, 0.20585081456082702]
maxi score, test score, baseline:  0.02727333333333319 0.08566666666666674 0.08566666666666674
probs:  [0.050996829403901274, 0.0808016696830074, 0.13261185851111487, 0.3562617398331997, 0.1734770880079498, 0.20585081456082702]
actor:  1 policy actor:  1  step number:  35 total reward:  0.6266666666666668  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  43.1601715330748
maxi score, test score, baseline:  0.02727333333333319 0.08566666666666674 0.08566666666666674
probs:  [0.05078779631784753, 0.08457654227706811, 0.13206741384363258, 0.35479816591708374, 0.17276470035985586, 0.20500538128451226]
maxi score, test score, baseline:  0.02449999999999986 0.08566666666666674 0.08566666666666674
siam score:  -0.7686715
line 256 mcts: sample exp_bonus 44.35664174642149
actor:  0 policy actor:  0  step number:  31 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.42000341415405
actor:  1 policy actor:  1  step number:  64 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.588]
 [0.486]
 [0.486]
 [0.486]
 [0.518]
 [0.486]] [[42.432]
 [40.808]
 [42.432]
 [42.432]
 [42.432]
 [46.538]
 [42.432]] [[1.494]
 [1.519]
 [1.494]
 [1.494]
 [1.494]
 [1.723]
 [1.494]]
maxi score, test score, baseline:  0.02516666666666653 0.08566666666666674 0.08566666666666674
probs:  [0.05078494224482953, 0.08457178441631723, 0.1320599801647611, 0.35477818273211204, 0.17281127244158617, 0.20499383800039395]
printing an ep nov before normalisation:  0.0
siam score:  -0.77470374
maxi score, test score, baseline:  0.022406666666666533 0.08566666666666674 0.08566666666666674
probs:  [0.05078494224482953, 0.08457178441631723, 0.1320599801647611, 0.35477818273211204, 0.17281127244158617, 0.20499383800039395]
printing an ep nov before normalisation:  43.913793291661385
printing an ep nov before normalisation:  16.518027337387693
maxi score, test score, baseline:  0.02001999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.05078494224482953, 0.08457178441631723, 0.1320599801647611, 0.35477818273211204, 0.17281127244158617, 0.20499383800039395]
actor:  1 policy actor:  1  step number:  59 total reward:  0.2666666666666665  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]] [[42.4]
 [42.4]
 [42.4]
 [42.4]
 [42.4]
 [42.4]
 [42.4]] [[2.099]
 [2.099]
 [2.099]
 [2.099]
 [2.099]
 [2.099]
 [2.099]]
maxi score, test score, baseline:  0.02001999999999986 0.08566666666666674 0.08566666666666674
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.0573],
        [ 0.3791],
        [-0.0000],
        [-0.0000],
        [ 0.3389],
        [ 0.0000],
        [ 0.1012],
        [-0.0042],
        [ 0.1835],
        [-0.0000]], dtype=torch.float64)
-0.032346567066 0.024976309294584498
-0.032346567066 0.34680255493856027
-0.79068 -0.79068
0.8857780965 0.8857780965
-0.045546567066 0.2933158616968344
-0.9438 -0.9438
-0.09703970119800001 0.004197967617435572
-0.032346567066 -0.03650764415418977
-0.08423175439800001 0.09922916183288091
0.9147253499999999 0.9147253499999999
siam score:  -0.7716119
printing an ep nov before normalisation:  31.257313167970242
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  39.446085481590984
Printing some Q and Qe and total Qs values:  [[0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]] [[43.296]
 [43.296]
 [43.296]
 [43.296]
 [43.296]
 [43.296]
 [43.296]] [[2.125]
 [2.125]
 [2.125]
 [2.125]
 [2.125]
 [2.125]
 [2.125]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  24.73710144672499
maxi score, test score, baseline:  0.02001999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.050811180468127985, 0.08457081391792233, 0.13202076717257877, 0.3549621052330368, 0.17273924221941309, 0.204895890988921]
line 256 mcts: sample exp_bonus 33.42857958056555
maxi score, test score, baseline:  0.02001999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.050811180468127985, 0.08457081391792233, 0.13202076717257877, 0.3549621052330368, 0.17273924221941309, 0.204895890988921]
actor:  1 policy actor:  1  step number:  49 total reward:  0.4266666666666662  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.45 ]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]] [[34.199]
 [39.002]
 [34.199]
 [34.199]
 [34.199]
 [34.199]
 [34.199]] [[1.428]
 [1.678]
 [1.428]
 [1.428]
 [1.428]
 [1.428]
 [1.428]]
line 256 mcts: sample exp_bonus 34.186432013456646
maxi score, test score, baseline:  0.02001999999999986 0.08566666666666674 0.08566666666666674
printing an ep nov before normalisation:  37.05066513907489
printing an ep nov before normalisation:  27.69673579506375
actor:  1 policy actor:  1  step number:  49 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.004]
 [ 0.064]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.004]
 [ 0.064]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]
 [-0.004]]
printing an ep nov before normalisation:  37.5030830590846
printing an ep nov before normalisation:  46.41466206397436
maxi score, test score, baseline:  0.02001999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.0509442446284554, 0.08456589214587451, 0.13182190294997848, 0.3558948472435185, 0.1723739491535438, 0.2043991638786293]
printing an ep nov before normalisation:  42.90767750249932
maxi score, test score, baseline:  0.02001999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.0509442446284554, 0.08456589214587451, 0.13182190294997848, 0.3558948472435185, 0.1723739491535438, 0.2043991638786293]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.02001999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.0509442446284554, 0.08456589214587451, 0.13182190294997848, 0.3558948472435185, 0.1723739491535438, 0.2043991638786293]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.02001999999999986 0.08566666666666674 0.08566666666666674
actor:  1 policy actor:  1  step number:  51 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.02001999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.050917501304738065, 0.08442402297889272, 0.13189436225093587, 0.3557073200962996, 0.1724961486361881, 0.20456064473294575]
maxi score, test score, baseline:  0.02001999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.050917501304738065, 0.08442402297889272, 0.13189436225093587, 0.3557073200962996, 0.1724961486361881, 0.20456064473294575]
printing an ep nov before normalisation:  36.2239832228485
maxi score, test score, baseline:  0.02001999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.050917501304738065, 0.08442402297889272, 0.13189436225093587, 0.3557073200962996, 0.1724961486361881, 0.20456064473294575]
printing an ep nov before normalisation:  36.483122882087535
actor:  1 policy actor:  1  step number:  59 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.02001999999999986 0.08566666666666674 0.08566666666666674
maxi score, test score, baseline:  0.02001999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.05083944896689653, 0.08429447315737862, 0.1316918534083283, 0.35516082436497715, 0.1737669478637612, 0.20424645223865823]
actions average: 
K:  0  action  0 :  tensor([0.3635, 0.0035, 0.1344, 0.1067, 0.1330, 0.1329, 0.1259],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0033, 0.9476, 0.0041, 0.0107, 0.0013, 0.0012, 0.0318],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2020, 0.0126, 0.3871, 0.0946, 0.0856, 0.0980, 0.1202],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1352, 0.1127, 0.0835, 0.2402, 0.1131, 0.0992, 0.2161],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1917, 0.0007, 0.0839, 0.0834, 0.4536, 0.0904, 0.0963],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1838, 0.0108, 0.1223, 0.0940, 0.1150, 0.3711, 0.1029],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2008, 0.0404, 0.1402, 0.1608, 0.1263, 0.1326, 0.1990],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0171133333333332 0.08566666666666674 0.08566666666666674
probs:  [0.05083944896689653, 0.08429447315737862, 0.1316918534083283, 0.35516082436497715, 0.1737669478637612, 0.20424645223865823]
actor:  1 policy actor:  1  step number:  39 total reward:  0.4933333333333332  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  48.173778495265594
maxi score, test score, baseline:  0.0171133333333332 0.08566666666666674 0.08566666666666674
probs:  [0.05097120890198666, 0.08429071478092265, 0.13149609956635588, 0.3560844218380019, 0.17340075786468787, 0.20375679704804506]
maxi score, test score, baseline:  0.0171133333333332 0.08566666666666674 0.08566666666666674
probs:  [0.05097120890198666, 0.08429071478092265, 0.13149609956635588, 0.3560844218380019, 0.17340075786468787, 0.20375679704804506]
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.725]
 [0.68 ]
 [0.735]
 [0.735]
 [0.735]
 [0.668]] [[38.275]
 [41.296]
 [40.74 ]
 [38.275]
 [38.275]
 [38.275]
 [36.791]] [[0.735]
 [0.725]
 [0.68 ]
 [0.735]
 [0.735]
 [0.735]
 [0.668]]
maxi score, test score, baseline:  0.0171133333333332 0.08566666666666674 0.08566666666666674
actor:  0 policy actor:  0  step number:  53 total reward:  0.3199999999999994  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.583]
 [0.551]
 [0.586]
 [0.56 ]
 [0.462]
 [0.589]] [[36.633]
 [42.174]
 [41.658]
 [40.472]
 [44.426]
 [45.859]
 [43.266]] [[1.615]
 [1.945]
 [1.884]
 [1.855]
 [2.045]
 [2.025]
 [2.011]]
maxi score, test score, baseline:  0.0197533333333332 0.08566666666666674 0.08566666666666674
probs:  [0.05097120890198666, 0.08429071478092265, 0.13149609956635588, 0.3560844218380019, 0.17340075786468787, 0.20375679704804506]
printing an ep nov before normalisation:  60.70679367489953
printing an ep nov before normalisation:  31.381301879882812
printing an ep nov before normalisation:  38.65857124328613
printing an ep nov before normalisation:  32.55758047103882
maxi score, test score, baseline:  0.0197533333333332 0.08566666666666674 0.08566666666666674
probs:  [0.05097120890198666, 0.08429071478092265, 0.13149609956635588, 0.3560844218380019, 0.17340075786468787, 0.20375679704804506]
actions average: 
K:  3  action  0 :  tensor([0.3685, 0.0506, 0.1020, 0.1031, 0.1107, 0.1010, 0.1642],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0127, 0.9431, 0.0087, 0.0064, 0.0029, 0.0040, 0.0221],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1077, 0.0013, 0.4210, 0.0886, 0.1357, 0.1151, 0.1307],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1502, 0.0048, 0.1087, 0.2862, 0.1744, 0.1735, 0.1022],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1246, 0.0012, 0.0581, 0.0374, 0.6671, 0.0802, 0.0313],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2582, 0.0289, 0.1206, 0.1331, 0.1509, 0.1683, 0.1399],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1176, 0.3010, 0.1113, 0.1062, 0.0947, 0.0800, 0.1892],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  53 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  61 total reward:  0.37333333333333263  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.01929999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.050919970061440015, 0.08420589370959854, 0.13136370089494476, 0.3557256639613897, 0.17322612411599983, 0.20455864725662717]
Printing some Q and Qe and total Qs values:  [[-0.117]
 [-0.111]
 [-0.086]
 [ 0.165]
 [-0.12 ]
 [ 0.165]
 [ 0.165]] [[44.032]
 [48.818]
 [50.831]
 [38.664]
 [53.31 ]
 [38.664]
 [38.664]] [[0.666]
 [0.848]
 [0.946]
 [0.752]
 [1.003]
 [0.752]
 [0.752]]
maxi score, test score, baseline:  0.01929999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.050919970061440015, 0.08420589370959854, 0.13136370089494476, 0.3557256639613897, 0.17322612411599983, 0.20455864725662717]
maxi score, test score, baseline:  0.01929999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.050919970061440015, 0.08420589370959854, 0.13136370089494476, 0.3557256639613897, 0.17322612411599983, 0.20455864725662717]
printing an ep nov before normalisation:  38.88457163797071
printing an ep nov before normalisation:  38.60986798975078
using another actor
maxi score, test score, baseline:  0.01929999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.05091997006144001, 0.08420589370959852, 0.13136370089494478, 0.35572566396138966, 0.17322612411599983, 0.20455864725662715]
maxi score, test score, baseline:  0.01929999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.05091997006144001, 0.08420589370959852, 0.13136370089494478, 0.35572566396138966, 0.17322612411599983, 0.20455864725662715]
maxi score, test score, baseline:  0.01929999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.05091997006144001, 0.08420589370959852, 0.13136370089494478, 0.35572566396138966, 0.17322612411599983, 0.20455864725662715]
printing an ep nov before normalisation:  36.96400859218303
printing an ep nov before normalisation:  42.62774797714349
printing an ep nov before normalisation:  25.108705601738926
maxi score, test score, baseline:  0.01929999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.05091997006144001, 0.08420589370959852, 0.13136370089494478, 0.35572566396138966, 0.17322612411599983, 0.20455864725662715]
actor:  0 policy actor:  0  step number:  55 total reward:  0.2133333333333326  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  53 total reward:  0.3733333333333325  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  31.451788747541816
actor:  0 policy actor:  1  step number:  52 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.782]
 [0.728]
 [0.469]
 [0.728]
 [0.759]
 [0.728]
 [0.728]] [[37.186]
 [35.231]
 [40.174]
 [35.231]
 [32.655]
 [35.231]
 [35.231]] [[0.782]
 [0.728]
 [0.469]
 [0.728]
 [0.759]
 [0.728]
 [0.728]]
line 256 mcts: sample exp_bonus 37.01302337697242
printing an ep nov before normalisation:  34.63009595870972
printing an ep nov before normalisation:  37.57925686240947
printing an ep nov before normalisation:  28.863097989748557
actions average: 
K:  2  action  0 :  tensor([0.4741, 0.0134, 0.0883, 0.1047, 0.1060, 0.1100, 0.1035],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0036,     0.9700,     0.0017,     0.0019,     0.0007,     0.0009,
            0.0211], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1870, 0.0120, 0.4049, 0.0965, 0.0795, 0.1006, 0.1195],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2034, 0.0249, 0.0987, 0.3905, 0.0964, 0.0925, 0.0937],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2220, 0.0202, 0.1012, 0.1005, 0.3283, 0.1138, 0.1139],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1442, 0.0069, 0.1006, 0.0717, 0.0511, 0.5522, 0.0733],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1260, 0.1346, 0.1022, 0.1239, 0.1013, 0.1379, 0.2742],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.016459999999999867 0.08566666666666674 0.08566666666666674
probs:  [0.05076928070210835, 0.08412355505448806, 0.13137819804840958, 0.3546697895797191, 0.17332658331569803, 0.20573259329957688]
Printing some Q and Qe and total Qs values:  [[0.76 ]
 [0.876]
 [0.812]
 [0.866]
 [0.819]
 [0.814]
 [0.898]] [[28.894]
 [31.687]
 [34.03 ]
 [30.298]
 [25.596]
 [39.658]
 [30.437]] [[0.76 ]
 [0.876]
 [0.812]
 [0.866]
 [0.819]
 [0.814]
 [0.898]]
actor:  0 policy actor:  0  step number:  51 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  38.6970387377381
Printing some Q and Qe and total Qs values:  [[0.25 ]
 [0.363]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]
 [0.25 ]] [[41.963]
 [43.338]
 [41.963]
 [41.963]
 [41.963]
 [41.963]
 [41.963]] [[1.582]
 [1.779]
 [1.582]
 [1.582]
 [1.582]
 [1.582]
 [1.582]]
printing an ep nov before normalisation:  39.81642224047437
actions average: 
K:  4  action  0 :  tensor([0.5871, 0.1079, 0.0413, 0.0428, 0.1113, 0.0531, 0.0565],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0145, 0.9094, 0.0133, 0.0081, 0.0068, 0.0064, 0.0416],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1244, 0.1893, 0.2732, 0.1071, 0.0908, 0.1084, 0.1068],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0582, 0.1050, 0.0685, 0.2828, 0.1624, 0.1605, 0.1627],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1841, 0.1760, 0.1014, 0.0800, 0.2560, 0.1243, 0.0782],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1632, 0.0909, 0.1407, 0.0845, 0.0819, 0.3271, 0.1118],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1682, 0.1699, 0.1714, 0.1186, 0.0922, 0.1010, 0.1787],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.179]
 [0.23 ]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]] [[27.148]
 [34.124]
 [27.148]
 [27.148]
 [27.148]
 [27.148]
 [27.148]] [[0.358]
 [0.473]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.5333333333333337  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.016073333333333197 0.08566666666666674 0.08566666666666674
siam score:  -0.77071244
Printing some Q and Qe and total Qs values:  [[0.732]
 [0.806]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]] [[11.489]
 [27.722]
 [11.489]
 [11.489]
 [11.489]
 [11.489]
 [11.489]] [[0.732]
 [0.806]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.4733333333333335  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  25.022801290879723
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.135]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]] [[32.001]
 [33.738]
 [32.001]
 [32.001]
 [32.001]
 [32.001]
 [32.001]] [[0.753]
 [0.863]
 [0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]]
printing an ep nov before normalisation:  37.94787566473107
printing an ep nov before normalisation:  36.29610300064087
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.211]
 [0.153]
 [0.155]
 [0.18 ]
 [0.18 ]
 [0.18 ]] [[29.203]
 [35.356]
 [29.555]
 [30.68 ]
 [30.085]
 [30.085]
 [30.085]] [[0.759]
 [1.164]
 [0.823]
 [0.88 ]
 [0.876]
 [0.876]
 [0.876]]
actions average: 
K:  3  action  0 :  tensor([0.4333, 0.1054, 0.1156, 0.0801, 0.1199, 0.0698, 0.0758],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0232, 0.9074, 0.0110, 0.0134, 0.0115, 0.0120, 0.0216],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1172, 0.0263, 0.5815, 0.0418, 0.0478, 0.1303, 0.0551],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1181, 0.0959, 0.0719, 0.3288, 0.1180, 0.1473, 0.1199],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2476, 0.0023, 0.1038, 0.0996, 0.3370, 0.1075, 0.1022],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2078, 0.0010, 0.0993, 0.0930, 0.0724, 0.4596, 0.0669],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1611, 0.1909, 0.1123, 0.1340, 0.1068, 0.0879, 0.2070],
       grad_fn=<DivBackward0>)
using another actor
from probs:  [0.050554292855561546, 0.08383844366144962, 0.13064646073135228, 0.3531649441603243, 0.17263930787474688, 0.20915655071656536]
maxi score, test score, baseline:  0.016073333333333197 0.08566666666666674 0.08566666666666674
probs:  [0.050499910947238755, 0.08383922558806821, 0.13072482041390154, 0.35278374829305736, 0.1727872648410618, 0.20936502991667227]
printing an ep nov before normalisation:  50.850060271289706
Printing some Q and Qe and total Qs values:  [[0.847]
 [0.911]
 [0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.847]] [[40.162]
 [39.53 ]
 [40.162]
 [40.162]
 [40.162]
 [40.162]
 [40.162]] [[0.847]
 [0.911]
 [0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.847]]
maxi score, test score, baseline:  0.016073333333333197 0.08566666666666674 0.08566666666666674
probs:  [0.050499910947238755, 0.08383922558806821, 0.13072482041390154, 0.35278374829305736, 0.1727872648410618, 0.20936502991667227]
actor:  0 policy actor:  0  step number:  69 total reward:  0.1733333333333329  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  46 total reward:  0.5133333333333336  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.018419999999999864 0.08566666666666674 0.08566666666666674
probs:  [0.050388052664233754, 0.08365332473998534, 0.13043479235914074, 0.3520005543530424, 0.17462292436745894, 0.2089003515161388]
actions average: 
K:  3  action  0 :  tensor([0.4866, 0.0233, 0.0912, 0.0698, 0.1755, 0.0630, 0.0907],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0128, 0.9163, 0.0188, 0.0094, 0.0076, 0.0060, 0.0291],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1197, 0.0296, 0.3959, 0.0957, 0.1194, 0.1026, 0.1371],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1594, 0.0190, 0.1497, 0.3033, 0.1331, 0.0984, 0.1372],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1056, 0.1162, 0.0733, 0.0540, 0.5282, 0.0813, 0.0414],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0415, 0.0023, 0.2598, 0.0382, 0.0585, 0.5550, 0.0448],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1415, 0.2127, 0.0860, 0.1619, 0.0906, 0.0518, 0.2556],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018419999999999864 0.08566666666666674 0.08566666666666674
probs:  [0.050388052664233754, 0.08365332473998534, 0.13043479235914074, 0.3520005543530424, 0.17462292436745894, 0.2089003515161388]
maxi score, test score, baseline:  0.018419999999999864 0.08566666666666674 0.08566666666666674
probs:  [0.050388052664233754, 0.08365332473998534, 0.13043479235914074, 0.3520005543530424, 0.17462292436745894, 0.2089003515161388]
printing an ep nov before normalisation:  27.370419257464036
siam score:  -0.7734681
actor:  0 policy actor:  1  step number:  68 total reward:  0.07333333333333258  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.017633333333333195 0.08566666666666674 0.08566666666666674
printing an ep nov before normalisation:  44.39714294081439
actions average: 
K:  3  action  0 :  tensor([0.5390, 0.0136, 0.0984, 0.0764, 0.1024, 0.0891, 0.0811],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0087, 0.9466, 0.0031, 0.0052, 0.0037, 0.0030, 0.0296],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1037, 0.0173, 0.4132, 0.0798, 0.0969, 0.2086, 0.0806],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1833, 0.0047, 0.1050, 0.3345, 0.1460, 0.0880, 0.1386],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2622, 0.0248, 0.1181, 0.0924, 0.2617, 0.1288, 0.1120],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1602, 0.0141, 0.1590, 0.0924, 0.1298, 0.3382, 0.1063],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0754, 0.4236, 0.0638, 0.0749, 0.1174, 0.0975, 0.1473],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  47 total reward:  0.3199999999999994  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  25.124481759443636
maxi score, test score, baseline:  0.017633333333333195 0.08566666666666674 0.08566666666666674
maxi score, test score, baseline:  0.017633333333333195 0.08566666666666674 0.08566666666666674
probs:  [0.050365067658050724, 0.08361512522513652, 0.13083219997568357, 0.3518396210983013, 0.17454311816565127, 0.20880486787717664]
actor:  1 policy actor:  1  step number:  52 total reward:  0.27333333333333254  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  39 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.258]
 [0.239]
 [0.274]
 [0.232]
 [0.224]
 [0.233]] [[16.821]
 [28.767]
 [16.052]
 [28.122]
 [16.09 ]
 [16.041]
 [16.02 ]] [[0.298]
 [0.753]
 [0.278]
 [0.747]
 [0.272]
 [0.263]
 [0.271]]
maxi score, test score, baseline:  0.017633333333333195 0.08566666666666674 0.08566666666666674
Printing some Q and Qe and total Qs values:  [[ 0.003]
 [ 0.106]
 [-0.018]
 [-0.009]
 [-0.005]
 [-0.   ]
 [ 0.016]] [[34.85 ]
 [36.73 ]
 [34.984]
 [35.688]
 [35.362]
 [35.355]
 [35.621]] [[1.282]
 [1.525]
 [1.271]
 [1.332]
 [1.312]
 [1.316]
 [1.352]]
printing an ep nov before normalisation:  30.440311431884766
maxi score, test score, baseline:  0.017633333333333195 0.08566666666666674 0.08566666666666674
probs:  [0.05040044497526606, 0.0836148588661551, 0.13078131746002422, 0.35208760108048465, 0.17444537805249502, 0.2086703995655748]
maxi score, test score, baseline:  0.017633333333333195 0.08566666666666674 0.08566666666666674
printing an ep nov before normalisation:  34.06249523162842
Printing some Q and Qe and total Qs values:  [[0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]] [[27.11]
 [27.11]
 [27.11]
 [27.11]
 [27.11]
 [27.11]
 [27.11]] [[1.508]
 [1.508]
 [1.508]
 [1.508]
 [1.508]
 [1.508]
 [1.508]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.4133333333333329  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.017633333333333195 0.08566666666666674 0.08566666666666674
probs:  [0.05035382078600076, 0.08353742741970871, 0.13042776207967618, 0.35176115398551033, 0.1742836988703018, 0.20963613685880222]
printing an ep nov before normalisation:  46.15155588743368
using explorer policy with actor:  0
siam score:  -0.77757293
maxi score, test score, baseline:  0.017633333333333195 0.08566666666666674 0.08566666666666674
probs:  [0.05035382078600076, 0.08353742741970871, 0.13042776207967618, 0.35176115398551033, 0.1742836988703018, 0.20963613685880222]
Printing some Q and Qe and total Qs values:  [[0.787]
 [0.814]
 [0.759]
 [0.759]
 [0.75 ]
 [0.759]
 [0.747]] [[31.135]
 [31.712]
 [31.384]
 [31.384]
 [30.498]
 [31.384]
 [29.454]] [[0.787]
 [0.814]
 [0.759]
 [0.759]
 [0.75 ]
 [0.759]
 [0.747]]
printing an ep nov before normalisation:  40.82497081812758
maxi score, test score, baseline:  0.017633333333333195 0.08566666666666674 0.08566666666666674
probs:  [0.05035382078600076, 0.08353742741970871, 0.13042776207967618, 0.35176115398551033, 0.1742836988703018, 0.20963613685880222]
printing an ep nov before normalisation:  23.6511752963525
maxi score, test score, baseline:  0.017633333333333195 0.08566666666666674 0.08566666666666674
probs:  [0.050274642351775034, 0.08353783700662681, 0.1305406340938269, 0.3512061476432791, 0.1745017555698637, 0.20993898333462846]
maxi score, test score, baseline:  0.017633333333333195 0.08566666666666674 0.08566666666666674
actor:  0 policy actor:  1  step number:  63 total reward:  0.23999999999999921  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.428]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]] [[40.607]
 [38.121]
 [40.607]
 [40.607]
 [40.607]
 [40.607]
 [40.607]] [[1.728]
 [1.608]
 [1.728]
 [1.728]
 [1.728]
 [1.728]
 [1.728]]
Printing some Q and Qe and total Qs values:  [[-0.001]
 [ 0.018]
 [ 0.001]
 [-0.   ]
 [-0.001]
 [-0.   ]
 [ 0.016]] [[48.253]
 [49.36 ]
 [33.544]
 [33.874]
 [33.748]
 [33.278]
 [47.961]] [[0.446]
 [0.481]
 [0.233]
 [0.236]
 [0.234]
 [0.227]
 [0.458]]
printing an ep nov before normalisation:  52.32615939826373
maxi score, test score, baseline:  0.01733999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.050274642351775034, 0.08353783700662681, 0.1305406340938269, 0.3512061476432791, 0.1745017555698637, 0.20993898333462846]
printing an ep nov before normalisation:  40.15049934387207
maxi score, test score, baseline:  0.01733999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.050274642351775034, 0.08353783700662681, 0.1305406340938269, 0.3512061476432791, 0.1745017555698637, 0.20993898333462846]
maxi score, test score, baseline:  0.01733999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.050274642351775034, 0.08353783700662681, 0.1305406340938269, 0.3512061476432791, 0.1745017555698637, 0.20993898333462846]
printing an ep nov before normalisation:  30.312947188112666
maxi score, test score, baseline:  0.01733999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.050274642351775034, 0.08353783700662681, 0.1305406340938269, 0.3512061476432791, 0.1745017555698637, 0.20993898333462846]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.01733999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.050274642351775034, 0.08353783700662681, 0.1305406340938269, 0.3512061476432791, 0.1745017555698637, 0.20993898333462846]
maxi score, test score, baseline:  0.01733999999999986 0.08566666666666674 0.08566666666666674
actor:  1 policy actor:  1  step number:  54 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.333
using another actor
actor:  1 policy actor:  1  step number:  56 total reward:  0.29999999999999916  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  32.42640508946869
actor:  1 policy actor:  1  step number:  55 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  39.42666221757812
siam score:  -0.7797996
Printing some Q and Qe and total Qs values:  [[0.847]
 [0.865]
 [0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.847]] [[38.731]
 [33.594]
 [38.731]
 [38.731]
 [38.731]
 [38.731]
 [38.731]] [[0.847]
 [0.865]
 [0.847]
 [0.847]
 [0.847]
 [0.847]
 [0.847]]
printing an ep nov before normalisation:  35.26599245581835
printing an ep nov before normalisation:  40.70805464684853
printing an ep nov before normalisation:  31.73170443569936
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.365]
 [0.196]
 [0.199]
 [0.184]
 [0.196]
 [0.234]] [[33.972]
 [33.307]
 [32.088]
 [40.955]
 [40.539]
 [32.088]
 [37.443]] [[0.731]
 [0.852]
 [0.642]
 [0.944]
 [0.915]
 [0.642]
 [0.86 ]]
printing an ep nov before normalisation:  46.61950933569481
printing an ep nov before normalisation:  45.33751489141495
Printing some Q and Qe and total Qs values:  [[0.196]
 [0.301]
 [0.196]
 [0.196]
 [0.196]
 [0.196]
 [0.196]] [[40.588]
 [41.312]
 [40.588]
 [40.588]
 [40.588]
 [40.588]
 [40.588]] [[1.229]
 [1.37 ]
 [1.229]
 [1.229]
 [1.229]
 [1.229]
 [1.229]]
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.206]
 [0.206]
 [0.206]
 [0.268]
 [0.206]
 [0.206]] [[38.631]
 [35.142]
 [35.142]
 [35.142]
 [40.171]
 [35.142]
 [35.142]] [[1.184]
 [0.968]
 [0.968]
 [0.968]
 [1.28 ]
 [0.968]
 [0.968]]
printing an ep nov before normalisation:  37.722061465588105
actions average: 
K:  0  action  0 :  tensor([0.4912, 0.0216, 0.0969, 0.0819, 0.1305, 0.0834, 0.0946],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0035, 0.9694, 0.0028, 0.0025, 0.0018, 0.0014, 0.0186],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1146, 0.0283, 0.4521, 0.0621, 0.0645, 0.2012, 0.0772],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2217, 0.0153, 0.1305, 0.2219, 0.1429, 0.1118, 0.1560],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1350, 0.0127, 0.0727, 0.0699, 0.5931, 0.0631, 0.0536],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1094, 0.0034, 0.1119, 0.0673, 0.0690, 0.5373, 0.1017],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2141, 0.0530, 0.1190, 0.0991, 0.0979, 0.1116, 0.3052],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.161]] [[26.88]
 [26.88]
 [26.88]
 [26.88]
 [26.88]
 [26.88]
 [26.88]] [[0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]]
line 256 mcts: sample exp_bonus 47.505963837528604
printing an ep nov before normalisation:  46.32049611118724
printing an ep nov before normalisation:  47.1006160807511
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  39.09194400470662
maxi score, test score, baseline:  0.015033333333333194 0.08566666666666674 0.08566666666666674
maxi score, test score, baseline:  0.015033333333333194 0.08566666666666674 0.08566666666666674
probs:  [0.05024603946252991, 0.08375651670453158, 0.13004942238537473, 0.3510057893705803, 0.17503108796429057, 0.20991114411269296]
printing an ep nov before normalisation:  28.24660893313954
maxi score, test score, baseline:  0.015033333333333194 0.08566666666666674 0.08566666666666674
probs:  [0.05024603946252991, 0.08375651670453158, 0.13004942238537473, 0.3510057893705803, 0.17503108796429057, 0.20991114411269296]
actor:  1 policy actor:  1  step number:  66 total reward:  0.20666666666666578  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.004]
 [-0.   ]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.004]
 [-0.003]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.004]
 [-0.   ]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.004]
 [-0.003]]
maxi score, test score, baseline:  0.015033333333333194 0.08566666666666674 0.08566666666666674
maxi score, test score, baseline:  0.015033333333333194 0.08566666666666674 0.08566666666666674
probs:  [0.05023713246644655, 0.08374165359316749, 0.1300263312239914, 0.3509434256681266, 0.17517759863133292, 0.20987385841693496]
actor:  0 policy actor:  0  step number:  42 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.01800666666666653 0.08566666666666674 0.08566666666666674
probs:  [0.05023713246644655, 0.08374165359316749, 0.1300263312239914, 0.3509434256681266, 0.17517759863133292, 0.20987385841693496]
maxi score, test score, baseline:  0.01800666666666653 0.08566666666666674 0.08566666666666674
probs:  [0.05023713246644655, 0.08374165359316749, 0.1300263312239914, 0.3509434256681266, 0.17517759863133292, 0.20987385841693496]
actor:  1 policy actor:  1  step number:  54 total reward:  0.21999999999999986  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.01800666666666653 0.08566666666666674 0.08566666666666674
probs:  [0.05022615869840643, 0.08372334165781492, 0.1299978820087424, 0.3508665911440687, 0.17535810551159664, 0.20982792097937078]
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.403828843029395
maxi score, test score, baseline:  0.01800666666666653 0.08566666666666674 0.08566666666666674
maxi score, test score, baseline:  0.01800666666666653 0.08566666666666674 0.08566666666666674
probs:  [0.05022615869840643, 0.08372334165781492, 0.1299978820087424, 0.3508665911440687, 0.17535810551159664, 0.20982792097937078]
printing an ep nov before normalisation:  46.94584161747489
siam score:  -0.77284795
maxi score, test score, baseline:  0.01800666666666653 0.08566666666666674 0.08566666666666674
probs:  [0.05023389464489445, 0.08358194218793359, 0.13001793725375832, 0.3509207555627416, 0.17538516582195943, 0.20986030452871263]
printing an ep nov before normalisation:  28.194660099820144
printing an ep nov before normalisation:  25.045881515225446
printing an ep nov before normalisation:  23.6246435489959
printing an ep nov before normalisation:  28.995336862089506
maxi score, test score, baseline:  0.01800666666666653 0.08566666666666674 0.08566666666666674
probs:  [0.05024543040213742, 0.08360115638534836, 0.12981777589970891, 0.3510015249406044, 0.17542551785538524, 0.20990859451681576]
maxi score, test score, baseline:  0.01800666666666653 0.08566666666666674 0.08566666666666674
probs:  [0.05024543040213742, 0.08360115638534836, 0.12981777589970891, 0.3510015249406044, 0.17542551785538524, 0.20990859451681576]
printing an ep nov before normalisation:  34.536147117614746
maxi score, test score, baseline:  0.01800666666666653 0.08566666666666674 0.08566666666666674
maxi score, test score, baseline:  0.01800666666666653 0.08566666666666674 0.08566666666666674
probs:  [0.05024543040213742, 0.08360115638534836, 0.12981777589970891, 0.3510015249406044, 0.17542551785538524, 0.20990859451681576]
printing an ep nov before normalisation:  51.08605707749264
siam score:  -0.7754123
printing an ep nov before normalisation:  24.578342858570412
actor:  1 policy actor:  1  step number:  86 total reward:  0.0999999999999992  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  26.98965912457662
actor:  0 policy actor:  0  step number:  51 total reward:  0.3199999999999994  reward:  1.0 rdn_beta:  0.333
from probs:  [0.050245595574979936, 0.08360115521590075, 0.1298175442517421, 0.35100268272959884, 0.17542505876532202, 0.20990796346245633]
actor:  1 policy actor:  1  step number:  63 total reward:  0.33333333333333337  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.017766666666666528 0.08566666666666674 0.08566666666666674
probs:  [0.050081418237004245, 0.08360231761518852, 0.13004779606528982, 0.3498518717842072, 0.175881381870975, 0.21053521442733525]
maxi score, test score, baseline:  0.017766666666666528 0.08566666666666674 0.08566666666666674
probs:  [0.050081418237004245, 0.08360231761518852, 0.13004779606528982, 0.3498518717842072, 0.175881381870975, 0.21053521442733525]
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]] [[50.257]
 [50.257]
 [50.257]
 [50.257]
 [50.257]
 [50.257]
 [50.257]] [[1.676]
 [1.676]
 [1.676]
 [1.676]
 [1.676]
 [1.676]
 [1.676]]
maxi score, test score, baseline:  0.017766666666666528 0.08566666666666674 0.08566666666666674
probs:  [0.05008913668859373, 0.08346081039707151, 0.13006787165329814, 0.34990591350927, 0.1759085400840533, 0.2105677276677133]
maxi score, test score, baseline:  0.017766666666666528 0.08566666666666674 0.08566666666666674
printing an ep nov before normalisation:  41.87457571194806
using explorer policy with actor:  1
maxi score, test score, baseline:  0.017766666666666528 0.08566666666666674 0.08566666666666674
printing an ep nov before normalisation:  34.81224268616332
maxi score, test score, baseline:  0.017766666666666528 0.08566666666666674 0.08566666666666674
maxi score, test score, baseline:  0.017766666666666528 0.08566666666666674 0.08566666666666674
probs:  [0.05008913668859373, 0.08346081039707151, 0.13006787165329814, 0.34990591350927, 0.1759085400840533, 0.2105677276677133]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3533333333333327  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.017766666666666528 0.08566666666666674 0.08566666666666674
probs:  [0.050055919629606364, 0.08340540359405813, 0.13064587052979784, 0.34967334002631434, 0.17579166224199458, 0.21042780397822872]
maxi score, test score, baseline:  0.017766666666666528 0.08566666666666674 0.08566666666666674
maxi score, test score, baseline:  0.017766666666666528 0.08566666666666674 0.08566666666666674
maxi score, test score, baseline:  0.017766666666666528 0.08566666666666674 0.08566666666666674
probs:  [0.050055919629606364, 0.08340540359405813, 0.13064587052979784, 0.34967334002631434, 0.17579166224199458, 0.21042780397822872]
actor:  1 policy actor:  1  step number:  61 total reward:  0.3466666666666661  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  0
printing an ep nov before normalisation:  29.645839702832138
maxi score, test score, baseline:  0.017766666666666528 0.08566666666666674 0.08566666666666674
probs:  [0.050014479078292165, 0.08333627982050834, 0.1305375327058571, 0.3493831887110395, 0.17564584914598363, 0.21108267053831928]
actor:  0 policy actor:  1  step number:  53 total reward:  0.4133333333333329  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.020593333333333193 0.08566666666666674 0.08566666666666674
printing an ep nov before normalisation:  33.21539998704667
actor:  1 policy actor:  1  step number:  55 total reward:  0.3066666666666664  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.493]
 [1.016]
 [0.492]
 [0.524]
 [1.016]
 [0.487]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.552]
 [0.493]
 [1.016]
 [0.492]
 [0.524]
 [1.016]
 [0.487]]
maxi score, test score, baseline:  0.020593333333333196 0.08566666666666674 0.08566666666666674
maxi score, test score, baseline:  0.020593333333333196 0.08566666666666674 0.08566666666666674
probs:  [0.05003309134323731, 0.08333629677052674, 0.13051120887768253, 0.34951365220759784, 0.17559435250807678, 0.2110113982928788]
printing an ep nov before normalisation:  60.065605988508494
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.134]
 [0.01 ]
 [0.01 ]
 [0.01 ]
 [0.01 ]
 [0.01 ]] [[50.316]
 [53.957]
 [50.316]
 [50.316]
 [50.316]
 [50.316]
 [50.316]] [[0.916]
 [1.146]
 [0.916]
 [0.916]
 [0.916]
 [0.916]
 [0.916]]
actions average: 
K:  4  action  0 :  tensor([0.6010, 0.0054, 0.0961, 0.0706, 0.0892, 0.0696, 0.0682],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0132, 0.9007, 0.0170, 0.0267, 0.0070, 0.0055, 0.0299],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1341, 0.0742, 0.4040, 0.0993, 0.1076, 0.1024, 0.0783],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1588, 0.1212, 0.1329, 0.2114, 0.1247, 0.1157, 0.1353],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1259, 0.2170, 0.1133, 0.0748, 0.2863, 0.0825, 0.1003],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1094, 0.0180, 0.2033, 0.0630, 0.0752, 0.4809, 0.0502],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1758, 0.0707, 0.1512, 0.1067, 0.1645, 0.1518, 0.1794],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.020593333333333196 0.08566666666666674 0.08566666666666674
actor:  1 policy actor:  1  step number:  39 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.49169770876124
line 256 mcts: sample exp_bonus 0.0
siam score:  -0.77682084
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  33 total reward:  0.56  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.353]
 [0.325]
 [0.324]
 [0.327]
 [0.329]
 [0.324]] [[29.631]
 [28.684]
 [22.23 ]
 [22.263]
 [22.165]
 [22.469]
 [22.416]] [[1.079]
 [1.009]
 [0.747]
 [0.747]
 [0.746]
 [0.759]
 [0.753]]
printing an ep nov before normalisation:  40.47929852945583
printing an ep nov before normalisation:  37.97122698990178
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  40 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.018633333333333193 0.08566666666666674 0.08566666666666674
probs:  [0.05025427430885849, 0.08335558236855825, 0.13001519080747867, 0.3510639533897658, 0.17505433282743116, 0.21025666629790757]
printing an ep nov before normalisation:  42.50534257137076
maxi score, test score, baseline:  0.018633333333333193 0.08566666666666674 0.08566666666666674
probs:  [0.05025427430885849, 0.08335558236855825, 0.13001519080747867, 0.3510639533897658, 0.17505433282743116, 0.21025666629790757]
actor:  1 policy actor:  1  step number:  59 total reward:  0.10666666666666602  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.018633333333333193 0.08566666666666674 0.08566666666666674
probs:  [0.05024443259792455, 0.08340418606149813, 0.13004338062973997, 0.35099543299511815, 0.17506281774321272, 0.21024974997250656]
printing an ep nov before normalisation:  27.735471901349335
printing an ep nov before normalisation:  43.731963795428484
printing an ep nov before normalisation:  43.790139919494266
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.573]
 [0.573]
 [0.601]
 [0.573]
 [0.573]
 [0.573]] [[27.609]
 [27.609]
 [27.609]
 [32.051]
 [27.609]
 [27.609]
 [27.609]] [[1.786]
 [1.786]
 [1.786]
 [2.223]
 [1.786]
 [1.786]
 [1.786]]
actions average: 
K:  0  action  0 :  tensor([0.3981, 0.0208, 0.1031, 0.0953, 0.1727, 0.1042, 0.1059],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0019,     0.9888,     0.0013,     0.0010,     0.0005,     0.0005,
            0.0060], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1044, 0.0069, 0.6228, 0.0567, 0.0531, 0.0837, 0.0724],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1840, 0.0984, 0.1220, 0.1396, 0.1208, 0.1225, 0.2128],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1619, 0.0556, 0.0813, 0.1042, 0.4022, 0.0867, 0.1082],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0963, 0.0061, 0.1290, 0.0786, 0.0680, 0.5387, 0.0832],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1390, 0.2361, 0.0950, 0.0933, 0.1084, 0.1039, 0.2242],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.725]
 [0.617]
 [0.61 ]
 [0.609]
 [0.618]
 [0.608]] [[15.309]
 [15.836]
 [15.33 ]
 [14.767]
 [14.39 ]
 [12.548]
 [13.223]] [[1.67 ]
 [1.849]
 [1.705]
 [1.658]
 [1.63 ]
 [1.508]
 [1.546]]
maxi score, test score, baseline:  0.018633333333333193 0.08566666666666674 0.08566666666666674
probs:  [0.05024443259792455, 0.08340418606149813, 0.13004338062973997, 0.35099543299511815, 0.17506281774321272, 0.21024974997250656]
actor:  1 policy actor:  1  step number:  53 total reward:  0.173333333333333  reward:  1.0 rdn_beta:  2.0
from probs:  [0.05024443259792455, 0.08340418606149813, 0.13004338062973997, 0.35099543299511815, 0.17506281774321272, 0.21024974997250656]
printing an ep nov before normalisation:  50.55425686593264
maxi score, test score, baseline:  0.018633333333333193 0.08566666666666674 0.08566666666666674
probs:  [0.05023977312697425, 0.0833964433099647, 0.13003130124394346, 0.35096280890974746, 0.17504655233243666, 0.2103231210769335]
printing an ep nov before normalisation:  15.927697924895972
printing an ep nov before normalisation:  26.697952138635436
maxi score, test score, baseline:  0.018633333333333193 0.08566666666666674 0.08566666666666674
probs:  [0.050251221879334756, 0.08341546796873837, 0.12983267590428219, 0.35104296931387513, 0.17508651795557367, 0.2103711469781959]
Printing some Q and Qe and total Qs values:  [[-0.006]
 [ 0.005]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[22.35 ]
 [40.864]
 [22.35 ]
 [22.35 ]
 [22.35 ]
 [22.35 ]
 [22.35 ]] [[0.261]
 [0.705]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]]
maxi score, test score, baseline:  0.018633333333333193 0.08566666666666674 0.08566666666666674
printing an ep nov before normalisation:  33.370585678150036
printing an ep nov before normalisation:  39.34256937720967
maxi score, test score, baseline:  0.018633333333333193 0.08566666666666674 0.08566666666666674
probs:  [0.050251221879334756, 0.08341546796873837, 0.12983267590428219, 0.35104296931387513, 0.17508651795557367, 0.2103711469781959]
printing an ep nov before normalisation:  35.62807423312841
maxi score, test score, baseline:  0.018633333333333193 0.08566666666666674 0.08566666666666674
probs:  [0.050251221879334756, 0.08341546796873837, 0.12983267590428219, 0.35104296931387513, 0.17508651795557367, 0.2103711469781959]
actor:  1 policy actor:  1  step number:  69 total reward:  0.15999999999999892  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]]
maxi score, test score, baseline:  0.018633333333333193 0.08566666666666674 0.08566666666666674
probs:  [0.0502549510104532, 0.08334731720188773, 0.12984232645496058, 0.3510690794652773, 0.1750995357101914, 0.21038679015722966]
actor:  0 policy actor:  1  step number:  46 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  36.225104814611775
siam score:  -0.77599126
printing an ep nov before normalisation:  37.98868449494076
printing an ep nov before normalisation:  26.55888319015503
printing an ep nov before normalisation:  21.840462684631348
maxi score, test score, baseline:  0.021446666666666524 0.08566666666666674 0.08566666666666674
probs:  [0.05026636002727967, 0.08336625890774854, 0.1296444074137737, 0.35114896165424525, 0.17513936262342114, 0.21043464937353162]
printing an ep nov before normalisation:  33.14957861302286
actor:  0 policy actor:  1  step number:  42 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  1.0
siam score:  -0.7768681
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.442]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]] [[44.952]
 [43.12 ]
 [44.952]
 [44.952]
 [44.952]
 [44.952]
 [44.952]] [[0.408]
 [0.442]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]]
maxi score, test score, baseline:  0.02195333333333319 0.08566666666666674 0.08566666666666674
probs:  [0.05026636002727967, 0.08336625890774854, 0.1296444074137737, 0.35114896165424525, 0.17513936262342114, 0.21043464937353162]
siam score:  -0.77710235
printing an ep nov before normalisation:  45.60059935471071
printing an ep nov before normalisation:  26.34426071642913
actor:  0 policy actor:  0  step number:  47 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.022433333333333194 0.08566666666666674 0.08566666666666674
probs:  [0.05026636002727967, 0.08336625890774854, 0.1296444074137737, 0.35114896165424525, 0.17513936262342114, 0.21043464937353162]
using explorer policy with actor:  0
using explorer policy with actor:  0
printing an ep nov before normalisation:  0.1808426991506451
actor:  0 policy actor:  1  step number:  43 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.791]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]] [[29.034]
 [32.08 ]
 [29.034]
 [29.034]
 [29.034]
 [29.034]
 [29.034]] [[2.012]
 [2.458]
 [2.012]
 [2.012]
 [2.012]
 [2.012]
 [2.012]]
maxi score, test score, baseline:  0.025366666666666524 0.08566666666666674 0.08566666666666674
probs:  [0.05026636002727967, 0.08336625890774854, 0.1296444074137737, 0.35114896165424525, 0.17513936262342114, 0.21043464937353162]
actor:  1 policy actor:  1  step number:  41 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.667
using another actor
printing an ep nov before normalisation:  23.999748601806793
printing an ep nov before normalisation:  31.43345204126756
siam score:  -0.77767944
actor:  1 policy actor:  1  step number:  64 total reward:  0.21999999999999986  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  63 total reward:  0.4133333333333338  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.025366666666666524 0.08566666666666674 0.08566666666666674
line 256 mcts: sample exp_bonus 38.14175159562447
maxi score, test score, baseline:  0.025366666666666524 0.08566666666666674 0.08566666666666674
maxi score, test score, baseline:  0.025366666666666524 0.08566666666666674 0.08566666666666674
probs:  [0.050102697005679826, 0.08309453901614074, 0.12922160936040522, 0.3500030469258026, 0.17631807401814314, 0.21126003367382842]
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.0018986340808169189
maxi score, test score, baseline:  0.025366666666666524 0.08566666666666674 0.08566666666666674
actor:  0 policy actor:  1  step number:  64 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  45.292368859289766
Printing some Q and Qe and total Qs values:  [[0.68 ]
 [0.722]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]] [[29.299]
 [31.871]
 [29.299]
 [29.299]
 [29.299]
 [29.299]
 [29.299]] [[0.68 ]
 [0.722]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]]
printing an ep nov before normalisation:  29.75437414273273
printing an ep nov before normalisation:  21.491673950576676
maxi score, test score, baseline:  0.027859999999999864 0.08566666666666674 0.08566666666666674
probs:  [0.05010269700567983, 0.08309453901614075, 0.12922160936040522, 0.3500030469258026, 0.17631807401814317, 0.2112600336738284]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.027859999999999864 0.08566666666666674 0.08566666666666674
probs:  [0.05014423347572654, 0.08309487880339558, 0.129164350596949, 0.3502941967931963, 0.17620200622806426, 0.2111003341026684]
printing an ep nov before normalisation:  24.558408530889825
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.674810460175216
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.435]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]] [[35.059]
 [40.502]
 [35.059]
 [35.059]
 [35.059]
 [35.059]
 [35.059]] [[1.459]
 [1.812]
 [1.459]
 [1.459]
 [1.459]
 [1.459]
 [1.459]]
maxi score, test score, baseline:  0.027859999999999864 0.08566666666666674 0.08566666666666674
probs:  [0.05014423347572654, 0.08309487880339558, 0.129164350596949, 0.3502941967931963, 0.17620200622806426, 0.2111003341026684]
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]] [[47.276]
 [44.646]
 [44.646]
 [44.646]
 [44.646]
 [44.646]
 [44.646]] [[2.204]
 [2.02 ]
 [2.02 ]
 [2.02 ]
 [2.02 ]
 [2.02 ]
 [2.02 ]]
maxi score, test score, baseline:  0.027859999999999864 0.08566666666666674 0.08566666666666674
probs:  [0.05014423347572653, 0.08309487880339557, 0.12916435059694897, 0.35029419679319623, 0.17620200622806426, 0.21110033410266846]
actor:  0 policy actor:  0  step number:  68 total reward:  0.01999999999999902  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  54 total reward:  0.3133333333333327  reward:  1.0 rdn_beta:  1.333
using another actor
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.271]
 [0.681]
 [0.271]
 [0.271]
 [0.271]
 [0.271]
 [0.271]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.271]
 [0.681]
 [0.271]
 [0.271]
 [0.271]
 [0.271]
 [0.271]]
maxi score, test score, baseline:  0.026499999999999857 0.08566666666666674 0.08566666666666674
probs:  [0.049938511543177376, 0.08309319590423157, 0.12944794194355, 0.34885218900307075, 0.1767768670685473, 0.211891294537423]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 42.63873385274612
printing an ep nov before normalisation:  44.28829301936222
maxi score, test score, baseline:  0.023099999999999857 0.08566666666666674 0.08566666666666674
probs:  [0.049938511543177376, 0.08309319590423157, 0.12944794194355, 0.34885218900307075, 0.1767768670685473, 0.211891294537423]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5400000000000001  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  17.39942597079719
actor:  1 policy actor:  1  step number:  53 total reward:  0.2799999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.023099999999999857 0.08566666666666674 0.08566666666666674
maxi score, test score, baseline:  0.023099999999999857 0.08566666666666674 0.08566666666666674
probs:  [0.049912834645509836, 0.08317644465996217, 0.12968348353806772, 0.3486718113484778, 0.17716790204788305, 0.21138752376009942]
maxi score, test score, baseline:  0.023099999999999857 0.08566666666666674 0.08566666666666674
probs:  [0.049912834645509836, 0.08317644465996217, 0.12968348353806772, 0.3486718113484778, 0.17716790204788305, 0.21138752376009942]
printing an ep nov before normalisation:  31.512432908649895
maxi score, test score, baseline:  0.019699999999999867 0.08566666666666674 0.08566666666666674
maxi score, test score, baseline:  0.019699999999999867 0.08566666666666674 0.08566666666666674
probs:  [0.049912834645509836, 0.08317644465996217, 0.12968348353806772, 0.3486718113484778, 0.17716790204788305, 0.21138752376009942]
maxi score, test score, baseline:  0.019699999999999867 0.08566666666666674 0.08566666666666674
probs:  [0.049912834645509836, 0.08317644465996217, 0.12968348353806772, 0.3486718113484778, 0.17716790204788305, 0.21138752376009942]
actor:  1 policy actor:  1  step number:  39 total reward:  0.5600000000000003  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  40.89415974443866
printing an ep nov before normalisation:  25.48343632110322
maxi score, test score, baseline:  0.019699999999999867 0.08566666666666674 0.08566666666666674
probs:  [0.049776960262250765, 0.08294977662269629, 0.1293298736566982, 0.3477204686756852, 0.17941201963032918, 0.21081090115234047]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3599999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.019699999999999867 0.08566666666666674 0.08566666666666674
probs:  [0.04971572693163913, 0.08294901666488079, 0.12941366367702695, 0.34729125490077434, 0.17958710838908834, 0.21104322943659037]
maxi score, test score, baseline:  0.019699999999999867 0.08566666666666674 0.08566666666666674
probs:  [0.04971572693163913, 0.08294901666488079, 0.12941366367702695, 0.34729125490077434, 0.17958710838908834, 0.21104322943659037]
printing an ep nov before normalisation:  45.869898838239415
printing an ep nov before normalisation:  23.566330839748087
maxi score, test score, baseline:  0.019699999999999867 0.08566666666666674 0.08566666666666674
probs:  [0.04971572693163913, 0.08294901666488079, 0.12941366367702695, 0.34729125490077434, 0.17958710838908834, 0.21104322943659037]
printing an ep nov before normalisation:  32.03394236752787
actor:  1 policy actor:  1  step number:  46 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.019699999999999867 0.08566666666666674 0.08566666666666674
probs:  [0.04967945628168877, 0.08288843515899852, 0.12909182614772952, 0.34703730144399414, 0.1804140366689139, 0.2108889442986752]
actor:  0 policy actor:  1  step number:  40 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.01905999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.0495429729392374, 0.08288649766281941, 0.12927708122872048, 0.34608062540896356, 0.18080722317329032, 0.21140559958696886]
maxi score, test score, baseline:  0.01905999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.0495429729392374, 0.08288649766281941, 0.12927708122872048, 0.34608062540896356, 0.18080722317329032, 0.21140559958696886]
actor:  1 policy actor:  1  step number:  65 total reward:  0.2133333333333327  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  47.2007309466674
maxi score, test score, baseline:  0.01905999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.0495451313400901, 0.08289011263242119, 0.12928272271096097, 0.346095737673605, 0.18077146689561877, 0.21141482874730397]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.01905999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.0495451313400901, 0.08289011263242119, 0.12928272271096097, 0.346095737673605, 0.18077146689561877, 0.21141482874730397]
siam score:  -0.77279496
Printing some Q and Qe and total Qs values:  [[-0.006]
 [-0.029]
 [-0.004]
 [-0.008]
 [-0.016]
 [-0.01 ]
 [-0.006]] [[28.399]
 [26.592]
 [28.038]
 [29.101]
 [31.401]
 [30.825]
 [26.951]] [[0.571]
 [0.464]
 [0.556]
 [0.602]
 [0.702]
 [0.681]
 [0.504]]
using explorer policy with actor:  0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.428]
 [0.386]
 [0.383]
 [0.427]
 [0.458]
 [0.385]] [[24.089]
 [27.081]
 [24.743]
 [24.915]
 [25.444]
 [25.957]
 [24.956]] [[1.002]
 [1.192]
 [1.02 ]
 [1.027]
 [1.101]
 [1.16 ]
 [1.032]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4333333333333328  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.01565999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.04965206539316102, 0.08275154374536921, 0.12916751966940965, 0.3468452294884737, 0.1805128238549116, 0.2110708178486748]
printing an ep nov before normalisation:  40.1313472106538
printing an ep nov before normalisation:  23.811026031665552
actor:  1 policy actor:  1  step number:  62 total reward:  0.1933333333333327  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  35.000781315906465
maxi score, test score, baseline:  0.01565999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.049664463760898586, 0.0826882875589367, 0.12913054984709013, 0.34693153522862324, 0.1805049319811743, 0.21108023162327708]
printing an ep nov before normalisation:  32.007930895297804
printing an ep nov before normalisation:  42.44727722204613
printing an ep nov before normalisation:  46.56711433117932
actions average: 
K:  4  action  0 :  tensor([0.2885, 0.0055, 0.1570, 0.1440, 0.1525, 0.1407, 0.1116],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0104, 0.9430, 0.0099, 0.0072, 0.0043, 0.0039, 0.0213],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1556, 0.0612, 0.2942, 0.1319, 0.1566, 0.1017, 0.0988],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1789, 0.0262, 0.0893, 0.3050, 0.1738, 0.1139, 0.1128],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2775, 0.0107, 0.1232, 0.1003, 0.3020, 0.0767, 0.1096],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2208, 0.0129, 0.2093, 0.1145, 0.1518, 0.1680, 0.1227],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2445, 0.0076, 0.1522, 0.1307, 0.1914, 0.1253, 0.1483],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.01565999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.0496757073156452, 0.082707027497647, 0.12893301432770116, 0.34701025819220843, 0.18054587622696644, 0.21112811643983184]
printing an ep nov before normalisation:  35.190097393710325
actor:  1 policy actor:  1  step number:  58 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.01565999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.04962352794489439, 0.0826200587202586, 0.12879735919311625, 0.3466449186891696, 0.18140824406989803, 0.21090589138266316]
maxi score, test score, baseline:  0.01565999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.04962352794489439, 0.0826200587202586, 0.12879735919311625, 0.3466449186891696, 0.18140824406989803, 0.21090589138266316]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  36.22687473941989
maxi score, test score, baseline:  0.01565999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.04962352794489439, 0.0826200587202586, 0.12879735919311625, 0.3466449186891696, 0.18140824406989803, 0.21090589138266316]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.01565999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.04962352794489439, 0.0826200587202586, 0.12879735919311625, 0.3466449186891696, 0.18140824406989803, 0.21090589138266316]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3599999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  40.16857994111989
printing an ep nov before normalisation:  33.994610483125534
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.461]
 [0.413]
 [0.466]
 [0.396]
 [0.413]
 [0.415]] [[27.207]
 [34.424]
 [27.207]
 [32.167]
 [25.731]
 [27.207]
 [28.49 ]] [[0.985]
 [1.335]
 [0.985]
 [1.245]
 [0.905]
 [0.985]
 [1.041]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.01565999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.04954298546067167, 0.08319922025020846, 0.12858796606744563, 0.3460809917821171, 0.18111322989802267, 0.2114756065415344]
printing an ep nov before normalisation:  41.57007813579009
printing an ep nov before normalisation:  40.868539421618046
printing an ep nov before normalisation:  0.0036418254398995487
maxi score, test score, baseline:  0.01565999999999986 0.08566666666666674 0.08566666666666674
printing an ep nov before normalisation:  31.151119673040746
actor:  1 policy actor:  1  step number:  55 total reward:  0.22666666666666646  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.01565999999999986 0.08566666666666674 0.08566666666666674
probs:  [0.04955929496164426, 0.0831993009850989, 0.1285661607136741, 0.3461953126223622, 0.181066097283925, 0.2114138334332957]
actor:  0 policy actor:  0  step number:  57 total reward:  0.27999999999999925  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.014819999999999861 0.08566666666666674 0.08566666666666674
printing an ep nov before normalisation:  29.999619919948437
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.419]
 [0.336]
 [0.379]
 [0.403]
 [0.297]
 [0.405]] [[37.681]
 [33.119]
 [31.064]
 [29.07 ]
 [36.317]
 [32.263]
 [29.057]] [[0.421]
 [0.419]
 [0.336]
 [0.379]
 [0.403]
 [0.297]
 [0.405]]
actions average: 
K:  4  action  0 :  tensor([0.4051, 0.1382, 0.0855, 0.0845, 0.1325, 0.0763, 0.0780],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0140, 0.9230, 0.0086, 0.0100, 0.0140, 0.0073, 0.0231],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1276, 0.1102, 0.2197, 0.1152, 0.1561, 0.1310, 0.1401],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2012, 0.0043, 0.1426, 0.1362, 0.1721, 0.1653, 0.1784],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1333, 0.0018, 0.0864, 0.0984, 0.5030, 0.0999, 0.0771],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1512, 0.0737, 0.1403, 0.1331, 0.1819, 0.2187, 0.1011],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1610, 0.1524, 0.1279, 0.1545, 0.1693, 0.1066, 0.1284],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  66 total reward:  0.3399999999999992  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.014819999999999861 0.08566666666666674 0.08566666666666674
probs:  [0.04960355193784974, 0.0830594775774214, 0.12853671631898098, 0.3465054709403384, 0.1809876784455492, 0.2113071047798603]
maxi score, test score, baseline:  0.011419999999999857 0.08566666666666674 0.08566666666666674
using explorer policy with actor:  1
maxi score, test score, baseline:  0.011419999999999857 0.08566666666666674 0.08566666666666674
probs:  [0.049614692968788364, 0.08307815309496851, 0.12834060727882235, 0.346583476166184, 0.1810284080149251, 0.21135466247631163]
maxi score, test score, baseline:  0.011419999999999857 0.08566666666666674 0.08566666666666674
probs:  [0.049614692968788364, 0.08307815309496851, 0.12834060727882235, 0.346583476166184, 0.1810284080149251, 0.21135466247631163]
actor:  1 policy actor:  1  step number:  64 total reward:  0.03333333333333288  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.165]
 [0.006]
 [0.414]
 [0.475]
 [0.378]
 [0.378]] [[41.001]
 [41.79 ]
 [45.396]
 [38.439]
 [43.171]
 [32.57 ]
 [32.57 ]] [[1.394]
 [1.126]
 [1.12 ]
 [1.234]
 [1.494]
 [0.949]
 [0.949]]
printing an ep nov before normalisation:  35.484239706473794
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  30.34865481606523
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.065]
 [0.072]
 [0.041]
 [0.072]
 [0.072]
 [0.057]] [[41.176]
 [40.146]
 [34.341]
 [41.298]
 [34.341]
 [34.341]
 [41.391]] [[1.434]
 [1.377]
 [0.969]
 [1.435]
 [0.969]
 [0.969]
 [1.458]]
siam score:  -0.7772713
maxi score, test score, baseline:  0.011419999999999857 0.08566666666666674 0.08566666666666674
probs:  [0.0494990297251894, 0.08309574753624148, 0.12831343558185637, 0.3457726509831853, 0.18143605848011285, 0.21188307769341458]
maxi score, test score, baseline:  0.011419999999999857 0.08566666666666674 0.08566666666666674
probs:  [0.0494990297251894, 0.08309574753624148, 0.12831343558185637, 0.3457726509831853, 0.18143605848011285, 0.21188307769341458]
maxi score, test score, baseline:  0.011419999999999857 0.08566666666666674 0.08566666666666674
probs:  [0.04927960835402499, 0.0830939923932382, 0.1286046365442205, 0.34423462352533285, 0.18207142999572326, 0.21271570918746036]
Printing some Q and Qe and total Qs values:  [[-0.024]
 [-0.04 ]
 [ 0.021]
 [-0.021]
 [-0.012]
 [-0.021]
 [-0.021]] [[32.889]
 [30.388]
 [25.318]
 [27.951]
 [26.261]
 [27.951]
 [27.951]] [[1.378]
 [1.255]
 [1.1  ]
 [1.17 ]
 [1.107]
 [1.17 ]
 [1.17 ]]
printing an ep nov before normalisation:  33.88426885503739
maxi score, test score, baseline:  0.011419999999999857 0.08566666666666674 0.08566666666666674
probs:  [0.04927960835402499, 0.0830939923932382, 0.1286046365442205, 0.34423462352533285, 0.18207142999572326, 0.21271570918746036]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.011419999999999857 0.08566666666666674 0.08566666666666674
probs:  [0.04927960835402499, 0.0830939923932382, 0.1286046365442205, 0.34423462352533285, 0.18207142999572326, 0.21271570918746036]
maxi score, test score, baseline:  0.011419999999999857 0.08566666666666674 0.08566666666666674
probs:  [0.04927960835402499, 0.0830939923932382, 0.1286046365442205, 0.34423462352533285, 0.18207142999572326, 0.21271570918746036]
printing an ep nov before normalisation:  48.98186079402416
printing an ep nov before normalisation:  23.73069408984315
maxi score, test score, baseline:  0.011419999999999857 0.08566666666666674 0.08566666666666674
probs:  [0.04927960835402499, 0.0830939923932382, 0.1286046365442205, 0.34423462352533285, 0.18207142999572326, 0.21271570918746036]
actor:  1 policy actor:  1  step number:  57 total reward:  0.4133333333333329  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.011419999999999857 0.08566666666666674 0.08566666666666674
probs:  [0.04921505278503797, 0.08298502039458863, 0.12843588465064562, 0.34378263376059726, 0.18183244751424638, 0.21374896089488404]
maxi score, test score, baseline:  0.011419999999999857 0.08566666666666674 0.08566666666666674
probs:  [0.04921505278503797, 0.08298502039458863, 0.12843588465064562, 0.34378263376059726, 0.18183244751424638, 0.21374896089488404]
printing an ep nov before normalisation:  53.304905703462644
printing an ep nov before normalisation:  22.813713550567627
maxi score, test score, baseline:  0.011419999999999857 0.08566666666666674 0.08566666666666674
probs:  [0.04921505278503797, 0.08298502039458863, 0.12843588465064562, 0.34378263376059726, 0.18183244751424638, 0.21374896089488404]
Printing some Q and Qe and total Qs values:  [[-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]]
maxi score, test score, baseline:  0.011419999999999857 0.08566666666666674 0.08566666666666674
probs:  [0.04920475173016999, 0.08303853450332123, 0.12846757752997232, 0.3437109373888929, 0.1818385043871559, 0.21373969446048768]
actions average: 
K:  1  action  0 :  tensor([0.4845, 0.0786, 0.0783, 0.0794, 0.1014, 0.0884, 0.0894],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0093, 0.9396, 0.0070, 0.0091, 0.0031, 0.0045, 0.0274],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1408, 0.0193, 0.3223, 0.1108, 0.1292, 0.1849, 0.0928],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1915, 0.0074, 0.1122, 0.3475, 0.1091, 0.1114, 0.1210],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1408, 0.0017, 0.0666, 0.0752, 0.5394, 0.0571, 0.1193],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1479, 0.0055, 0.1209, 0.1035, 0.1149, 0.3994, 0.1079],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1204, 0.2668, 0.1334, 0.0765, 0.0778, 0.0639, 0.2611],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.011419999999999857 0.08566666666666674 0.08566666666666674
probs:  [0.04920475173016999, 0.08303853450332123, 0.12846757752997232, 0.3437109373888929, 0.1818385043871559, 0.21373969446048768]
actions average: 
K:  3  action  0 :  tensor([0.4621, 0.0396, 0.1026, 0.0956, 0.1206, 0.0884, 0.0911],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0062, 0.9484, 0.0095, 0.0101, 0.0034, 0.0041, 0.0182],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1646, 0.0200, 0.3046, 0.1254, 0.1366, 0.1454, 0.1034],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1659, 0.0090, 0.1303, 0.2421, 0.1959, 0.1427, 0.1141],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2011, 0.0100, 0.1215, 0.1256, 0.2798, 0.1042, 0.1578],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0479, 0.0282, 0.1147, 0.0758, 0.0423, 0.6486, 0.0425],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1431, 0.0137, 0.1294, 0.2031, 0.1490, 0.1883, 0.1735],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.011419999999999857 0.08566666666666674 0.08566666666666674
probs:  [0.04920475173016999, 0.08303853450332123, 0.12846757752997232, 0.3437109373888929, 0.1818385043871559, 0.21373969446048768]
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.564]
 [0.363]
 [0.391]
 [0.391]
 [0.391]
 [0.391]] [[21.538]
 [25.51 ]
 [22.715]
 [21.538]
 [21.538]
 [21.538]
 [21.538]] [[1.439]
 [2.05 ]
 [1.541]
 [1.439]
 [1.439]
 [1.439]
 [1.439]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  61 total reward:  0.19999999999999918  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.011419999999999857 0.08566666666666674 0.08566666666666674
probs:  [0.049130005830296745, 0.0829437239808177, 0.12827209824666477, 0.34318759874862576, 0.18305224477083906, 0.21341432842275598]
maxi score, test score, baseline:  0.011419999999999857 0.08566666666666674 0.08566666666666674
actor:  1 policy actor:  1  step number:  47 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.011419999999999857 0.08566666666666674 0.08566666666666674
probs:  [0.049250889737263076, 0.08294521727402449, 0.12811354459087204, 0.34403492515138423, 0.1827002715305379, 0.2129551517159184]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.518]
 [0.457]
 [0.457]
 [0.457]
 [0.457]
 [0.457]] [[41.867]
 [44.961]
 [41.867]
 [41.867]
 [41.867]
 [41.867]
 [41.867]] [[1.265]
 [1.444]
 [1.265]
 [1.265]
 [1.265]
 [1.265]
 [1.265]]
printing an ep nov before normalisation:  33.02582076517249
actor:  1 policy actor:  1  step number:  59 total reward:  0.30666666666666587  reward:  1.0 rdn_beta:  1.0
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  30.002563045434847
printing an ep nov before normalisation:  39.60648776574956
actor:  1 policy actor:  1  step number:  47 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.011419999999999857 0.08566666666666674 0.08566666666666674
probs:  [0.04919302534214837, 0.08365070351461727, 0.12833686416902704, 0.3436297824278237, 0.18248519347160935, 0.21270443107477424]
printing an ep nov before normalisation:  36.405222169675284
Printing some Q and Qe and total Qs values:  [[ 0.106]
 [ 0.096]
 [ 0.122]
 [ 0.105]
 [ 0.064]
 [-0.006]
 [-0.006]] [[47.95 ]
 [47.821]
 [48.108]
 [49.345]
 [49.932]
 [45.743]
 [45.743]] [[1.216]
 [1.201]
 [1.239]
 [1.276]
 [1.26 ]
 [1.009]
 [1.009]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.6909],
        [0.6235],
        [0.7227],
        [-0.0000],
        [0.8807],
        [-0.0000],
        [-0.0000],
        [0.4179],
        [0.3060],
        [0.6144]], dtype=torch.float64)
-0.071422513866 0.6195254561781972
-0.08423175439800001 0.5392588747153029
-0.08397170119799999 0.6387536847050747
-0.9372 -0.9372
-0.071551887066 0.8091665975675341
-0.9452970149699998 -0.9452970149699998
-0.9510897 -0.9510897
-0.07129443439800001 0.34664602430260605
-0.032346567066 0.2736430664664186
-0.058351887066 0.5560641575675274
actor:  0 policy actor:  0  step number:  47 total reward:  0.37333333333333285  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.010766666666666522 0.08566666666666674 0.08566666666666674
probs:  [0.04919302534214837, 0.08365070351461727, 0.12833686416902704, 0.3436297824278237, 0.18248519347160935, 0.21270443107477424]
printing an ep nov before normalisation:  43.9324599871657
maxi score, test score, baseline:  0.010766666666666522 0.08566666666666674 0.08566666666666674
probs:  [0.04919302534214837, 0.08365070351461727, 0.12833686416902704, 0.3436297824278237, 0.18248519347160935, 0.21270443107477424]
maxi score, test score, baseline:  0.010766666666666522 0.08566666666666674 0.08566666666666674
probs:  [0.04919302534214837, 0.08365070351461727, 0.12833686416902704, 0.3436297824278237, 0.18248519347160935, 0.21270443107477424]
maxi score, test score, baseline:  0.010766666666666522 0.08566666666666674 0.08566666666666674
Printing some Q and Qe and total Qs values:  [[0.496]
 [0.544]
 [0.496]
 [0.496]
 [0.496]
 [0.496]
 [0.496]] [[35.624]
 [40.949]
 [35.624]
 [35.624]
 [35.624]
 [35.624]
 [35.624]] [[1.849]
 [2.259]
 [1.849]
 [1.849]
 [1.849]
 [1.849]
 [1.849]]
UNIT TEST: sample policy line 217 mcts : [0.041 0.714 0.02  0.02  0.02  0.163 0.02 ]
maxi score, test score, baseline:  0.010766666666666522 0.08566666666666674 0.08566666666666674
probs:  [0.04919302534214837, 0.08365070351461727, 0.12833686416902704, 0.3436297824278237, 0.18248519347160935, 0.21270443107477424]
siam score:  -0.7810808
printing an ep nov before normalisation:  39.40452937233642
printing an ep nov before normalisation:  43.6530759861083
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.580900036495304
printing an ep nov before normalisation:  34.7032904624939
actor:  1 policy actor:  1  step number:  45 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  33 total reward:  0.5866666666666669  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 29.633664849541553
actor:  1 policy actor:  1  step number:  56 total reward:  0.3266666666666658  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.010539999999999855 0.08566666666666674 0.08566666666666674
probs:  [0.049053127930643446, 0.08341254906493749, 0.1279712858757751, 0.34265027821666866, 0.1826684529986836, 0.21424430591329166]
Printing some Q and Qe and total Qs values:  [[0.126]
 [0.309]
 [0.104]
 [0.115]
 [0.131]
 [0.118]
 [0.133]] [[28.854]
 [37.518]
 [25.351]
 [24.657]
 [25.528]
 [23.681]
 [22.929]] [[1.012]
 [1.682]
 [0.794]
 [0.766]
 [0.831]
 [0.714]
 [0.687]]
printing an ep nov before normalisation:  26.61265888820397
printing an ep nov before normalisation:  45.4196357400704
maxi score, test score, baseline:  0.010539999999999855 0.08566666666666674 0.08566666666666674
probs:  [0.049053127930643446, 0.08341254906493749, 0.1279712858757751, 0.34265027821666866, 0.1826684529986836, 0.21424430591329166]
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([0.5220, 0.0139, 0.0846, 0.0832, 0.1079, 0.0964, 0.0920],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0021,     0.9693,     0.0031,     0.0036,     0.0006,     0.0007,
            0.0208], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2546, 0.0103, 0.2282, 0.1123, 0.1480, 0.1330, 0.1136],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1835, 0.0600, 0.0875, 0.3095, 0.1232, 0.1244, 0.1118],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1615, 0.0097, 0.0891, 0.0944, 0.4564, 0.0792, 0.1098],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1676, 0.0022, 0.1912, 0.0959, 0.1055, 0.3434, 0.0942],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1289, 0.0384, 0.2272, 0.0805, 0.0825, 0.0897, 0.3528],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  31.240677094417308
actor:  0 policy actor:  0  step number:  54 total reward:  0.40666666666666684  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.46407703293297
maxi score, test score, baseline:  0.00995333333333319 0.08566666666666674 0.08566666666666674
probs:  [0.049053127930643446, 0.08341254906493749, 0.1279712858757751, 0.34265027821666866, 0.1826684529986836, 0.21424430591329166]
actions average: 
K:  2  action  0 :  tensor([0.6164, 0.0021, 0.0697, 0.0599, 0.1067, 0.0717, 0.0734],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0072, 0.9225, 0.0076, 0.0155, 0.0035, 0.0037, 0.0401],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1162, 0.0527, 0.3851, 0.0852, 0.0913, 0.1250, 0.1446],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1532, 0.0179, 0.0964, 0.2968, 0.1587, 0.1516, 0.1254],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1594, 0.0974, 0.0895, 0.0901, 0.3892, 0.0896, 0.0849],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1006, 0.0017, 0.1093, 0.0721, 0.0725, 0.5755, 0.0682],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1271, 0.2131, 0.1085, 0.0760, 0.0838, 0.0909, 0.3006],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  54 total reward:  0.21999999999999975  reward:  1.0 rdn_beta:  2.0
siam score:  -0.7801648
printing an ep nov before normalisation:  23.906405438227566
maxi score, test score, baseline:  0.00995333333333319 0.08566666666666674 0.08566666666666674
actor:  1 policy actor:  1  step number:  52 total reward:  0.3666666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.00995333333333319 0.08566666666666674 0.08566666666666674
probs:  [0.049065231432102945, 0.08341294303693891, 0.12773315778568348, 0.34273511435319876, 0.18263502102447784, 0.214418532367598]
maxi score, test score, baseline:  0.00995333333333319 0.08566666666666674 0.08566666666666674
maxi score, test score, baseline:  0.00995333333333319 0.08566666666666674 0.08566666666666674
probs:  [0.0490761269865723, 0.08343148670157939, 0.12753908086084542, 0.3428114005702067, 0.18267565819545106, 0.21446624668534525]
maxi score, test score, baseline:  0.00995333333333319 0.08566666666666674 0.08566666666666674
actions average: 
K:  0  action  0 :  tensor([0.4902, 0.0069, 0.1034, 0.1000, 0.1236, 0.0817, 0.0943],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0044, 0.9537, 0.0037, 0.0038, 0.0030, 0.0024, 0.0290],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1243, 0.0078, 0.3703, 0.1066, 0.1057, 0.1778, 0.1075],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1987, 0.0066, 0.1429, 0.2479, 0.1442, 0.1388, 0.1208],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2107, 0.0028, 0.0919, 0.1358, 0.3895, 0.0790, 0.0903],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1156, 0.0184, 0.1483, 0.0842, 0.0679, 0.4647, 0.1009],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1260, 0.3233, 0.0807, 0.1021, 0.0969, 0.0847, 0.1864],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.7154656324474
maxi score, test score, baseline:  0.00995333333333319 0.08566666666666674 0.08566666666666674
probs:  [0.0490761269865723, 0.08343148670157939, 0.12753908086084542, 0.3428114005702067, 0.18267565819545106, 0.21446624668534525]
maxi score, test score, baseline:  0.00995333333333319 0.08566666666666674 0.08566666666666674
actor:  1 policy actor:  1  step number:  62 total reward:  0.23333333333333262  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  42 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  35.774323457274946
printing an ep nov before normalisation:  45.42486272162269
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.64092882140221
printing an ep nov before normalisation:  40.021145710013286
maxi score, test score, baseline:  0.00995333333333319 0.08566666666666674 0.08566666666666674
probs:  [0.04897075593720645, 0.08332780305238714, 0.12868230330266006, 0.3420732897297464, 0.18257684902970467, 0.2143689989482953]
maxi score, test score, baseline:  0.00995333333333319 0.08566666666666674 0.08566666666666674
probs:  [0.04897075593720645, 0.08332780305238714, 0.12868230330266006, 0.3420732897297464, 0.18257684902970467, 0.2143689989482953]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.00995333333333319 0.08566666666666674 0.08566666666666674
probs:  [0.04897075593720645, 0.08332780305238714, 0.12868230330266006, 0.3420732897297464, 0.18257684902970467, 0.2143689989482953]
actor:  1 policy actor:  1  step number:  44 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  3  action  0 :  tensor([0.2936, 0.0586, 0.1789, 0.1237, 0.1158, 0.1251, 0.1044],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0058, 0.9681, 0.0037, 0.0063, 0.0035, 0.0026, 0.0100],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1759, 0.0058, 0.2181, 0.1782, 0.1411, 0.1584, 0.1225],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1743, 0.2068, 0.1189, 0.1316, 0.1198, 0.1247, 0.1240],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1403, 0.0139, 0.0743, 0.0554, 0.5736, 0.0537, 0.0889],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1104, 0.0168, 0.1104, 0.0850, 0.0889, 0.5175, 0.0710],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1912, 0.0115, 0.1209, 0.1412, 0.1508, 0.1357, 0.2487],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0065533333333331915 0.08566666666666674 0.08566666666666674
probs:  [0.048905269554025396, 0.08439679661643666, 0.12854347073622482, 0.34161471523215914, 0.18238842566547492, 0.21415132219567906]
maxi score, test score, baseline:  0.0065533333333331915 0.08566666666666674 0.08566666666666674
probs:  [0.048905269554025396, 0.08439679661643666, 0.12854347073622482, 0.34161471523215914, 0.18238842566547492, 0.21415132219567906]
printing an ep nov before normalisation:  38.844806542033844
printing an ep nov before normalisation:  28.197904304301563
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]] [[20.94]
 [20.94]
 [20.94]
 [20.94]
 [20.94]
 [20.94]
 [20.94]] [[1.253]
 [1.253]
 [1.253]
 [1.253]
 [1.253]
 [1.253]
 [1.253]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0065533333333331915 0.08566666666666674 0.08566666666666674
probs:  [0.048879462246205464, 0.08439756806615079, 0.12857730257653022, 0.34143382269099093, 0.1824625807073991, 0.2142492637127236]
maxi score, test score, baseline:  0.0065533333333331915 0.08566666666666674 0.08566666666666674
probs:  [0.048879462246205464, 0.08439756806615079, 0.12857730257653022, 0.34143382269099093, 0.1824625807073991, 0.2142492637127236]
Printing some Q and Qe and total Qs values:  [[0.179]
 [0.214]
 [0.179]
 [0.179]
 [0.208]
 [0.179]
 [0.179]] [[39.399]
 [35.943]
 [39.399]
 [39.399]
 [41.955]
 [39.399]
 [39.399]] [[1.057]
 [0.941]
 [1.057]
 [1.057]
 [1.197]
 [1.057]
 [1.057]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.4933333333333335  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  0.0065533333333331915 0.08566666666666674 0.08566666666666674
probs:  [0.04878151503144618, 0.08422825329852687, 0.1283192161960788, 0.34074803757489025, 0.182096221085826, 0.21582675681323196]
maxi score, test score, baseline:  0.0065533333333331915 0.08566666666666674 0.08566666666666674
probs:  [0.04878151503144618, 0.08422825329852687, 0.1283192161960788, 0.34074803757489025, 0.182096221085826, 0.21582675681323196]
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.407]
 [0.372]
 [0.344]
 [0.365]
 [0.371]
 [0.394]] [[35.703]
 [37.22 ]
 [36.258]
 [32.957]
 [36.497]
 [37.059]
 [37.353]] [[0.876]
 [0.94 ]
 [0.875]
 [0.749]
 [0.875]
 [0.898]
 [0.93 ]]
printing an ep nov before normalisation:  38.80602391460229
actions average: 
K:  3  action  0 :  tensor([0.5880, 0.0239, 0.0833, 0.0633, 0.0953, 0.0628, 0.0833],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0072, 0.9610, 0.0029, 0.0039, 0.0028, 0.0014, 0.0208],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0017, 0.0009, 0.6850, 0.1105, 0.0026, 0.1677, 0.0315],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1412, 0.1603, 0.1668, 0.1209, 0.1341, 0.1346, 0.1421],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1789, 0.0050, 0.1162, 0.0899, 0.4274, 0.0876, 0.0949],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1530, 0.0076, 0.1293, 0.1352, 0.1524, 0.3097, 0.1128],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1781, 0.1436, 0.1298, 0.1077, 0.1146, 0.1070, 0.2192],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0065533333333331915 0.08566666666666674 0.08566666666666674
probs:  [0.04878151503144618, 0.08422825329852687, 0.1283192161960788, 0.34074803757489025, 0.182096221085826, 0.21582675681323196]
maxi score, test score, baseline:  0.0065533333333331915 0.08566666666666674 0.08566666666666674
probs:  [0.04878151503144618, 0.08422825329852687, 0.1283192161960788, 0.34074803757489025, 0.182096221085826, 0.21582675681323196]
printing an ep nov before normalisation:  32.43866856915911
actor:  1 policy actor:  1  step number:  41 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.0
from probs:  [0.04878151503144618, 0.08422825329852687, 0.1283192161960788, 0.34074803757489025, 0.182096221085826, 0.21582675681323196]
using another actor
printing an ep nov before normalisation:  16.44578284086302
actor:  0 policy actor:  1  step number:  64 total reward:  0.09999999999999909  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0019533333333331807 0.08566666666666674 0.08566666666666674
probs:  [0.04872405452243904, 0.08412892517823263, 0.12934800768935115, 0.34034572330860385, 0.18188129705366238, 0.21557199224771095]
actions average: 
K:  0  action  0 :  tensor([0.6094, 0.0047, 0.0678, 0.0539, 0.1250, 0.0591, 0.0802],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0088, 0.9615, 0.0097, 0.0029, 0.0020, 0.0023, 0.0128],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1109, 0.0072, 0.5406, 0.0692, 0.0618, 0.0977, 0.1126],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0625, 0.0274, 0.0740, 0.6161, 0.0839, 0.0721, 0.0641],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1601, 0.0021, 0.0855, 0.1029, 0.4870, 0.0828, 0.0797],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1494, 0.0030, 0.0963, 0.0713, 0.0724, 0.5149, 0.0927],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2155, 0.0104, 0.1581, 0.1207, 0.1346, 0.1259, 0.2348],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  27.272803633228573
printing an ep nov before normalisation:  30.025930757234253
maxi score, test score, baseline:  0.0019533333333331807 0.08566666666666674 0.08566666666666674
probs:  [0.04872405452243904, 0.08412892517823263, 0.12934800768935115, 0.34034572330860385, 0.18188129705366238, 0.21557199224771095]
maxi score, test score, baseline:  0.0019533333333331807 0.08566666666666674 0.08566666666666674
probs:  [0.04872405452243904, 0.08412892517823263, 0.12934800768935115, 0.34034572330860385, 0.18188129705366238, 0.21557199224771095]
printing an ep nov before normalisation:  45.64347999959112
printing an ep nov before normalisation:  43.67321152665151
printing an ep nov before normalisation:  33.698892126234895
maxi score, test score, baseline:  0.0019533333333331807 0.08566666666666674 0.08566666666666674
actor:  1 policy actor:  1  step number:  39 total reward:  0.6000000000000003  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0019533333333331807 0.08566666666666674 0.08566666666666674
probs:  [0.04856932050858051, 0.08386144688735567, 0.1289365324661109, 0.3392623409590445, 0.18448441626304526, 0.21488594291586313]
actor:  0 policy actor:  1  step number:  40 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]] [[50.624]
 [50.624]
 [50.624]
 [50.624]
 [50.624]
 [50.624]
 [50.624]] [[1.508]
 [1.508]
 [1.508]
 [1.508]
 [1.508]
 [1.508]
 [1.508]]
maxi score, test score, baseline:  0.0016066666666665114 0.08566666666666674 0.08566666666666674
probs:  [0.04856932050858051, 0.08386144688735567, 0.1289365324661109, 0.3392623409590445, 0.18448441626304526, 0.21488594291586313]
actor:  0 policy actor:  0  step number:  51 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  1.333
siam score:  -0.76777893
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 42.13223544755853
maxi score, test score, baseline:  0.0005666666666665105 0.08566666666666674 0.08566666666666674
probs:  [0.04856932050858051, 0.08386144688735567, 0.1289365324661109, 0.3392623409590445, 0.18448441626304526, 0.21488594291586313]
actor:  0 policy actor:  1  step number:  42 total reward:  0.5533333333333336  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.00031333333333318907 0.08566666666666674 0.08566666666666674
probs:  [0.04856932050858051, 0.08386144688735567, 0.1289365324661109, 0.3392623409590445, 0.18448441626304526, 0.21488594291586313]
actor:  1 policy actor:  1  step number:  52 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.00031333333333318907 0.08566666666666674 0.08566666666666674
using explorer policy with actor:  1
using explorer policy with actor:  0
maxi score, test score, baseline:  0.00031333333333318907 0.08566666666666674 0.08566666666666674
probs:  [0.048527583102922836, 0.08378929823915243, 0.1296865542739306, 0.3389701132356219, 0.1843255607529631, 0.21470089039540902]
Printing some Q and Qe and total Qs values:  [[0.212]
 [0.286]
 [0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]] [[32.801]
 [36.349]
 [32.801]
 [32.801]
 [32.801]
 [32.801]
 [32.801]] [[1.403]
 [1.731]
 [1.403]
 [1.403]
 [1.403]
 [1.403]
 [1.403]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  0.00031333333333318907 0.08566666666666674 0.08566666666666674
probs:  [0.04848381812543907, 0.08371364466379613, 0.1304730115779805, 0.3386636893088302, 0.18415898816148626, 0.21450684816246773]
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.545]
 [0.647]
 [0.647]
 [0.647]
 [0.39 ]
 [0.647]] [[35.817]
 [41.176]
 [35.817]
 [35.817]
 [35.817]
 [46.226]
 [35.817]] [[1.713]
 [1.912]
 [1.713]
 [1.713]
 [1.713]
 [2.04 ]
 [1.713]]
actor:  0 policy actor:  1  step number:  45 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.00030000000000015133 0.08566666666666674 0.08566666666666674
probs:  [0.04848381812543907, 0.08371364466379613, 0.1304730115779805, 0.3386636893088302, 0.18415898816148626, 0.21450684816246773]
printing an ep nov before normalisation:  40.27978160487659
actor:  1 policy actor:  1  step number:  35 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.00030000000000015133 0.08566666666666674 0.08566666666666674
probs:  [0.04832258832247454, 0.08343493753884035, 0.13003838074454138, 0.3375348261817197, 0.18354533615828203, 0.21712393105414193]
maxi score, test score, baseline:  -0.00030000000000015133 0.08566666666666674 0.08566666666666674
probs:  [0.04832258832247454, 0.08343493753884035, 0.13003838074454138, 0.3375348261817197, 0.18354533615828203, 0.21712393105414193]
printing an ep nov before normalisation:  25.768304100832047
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.285]
 [0.255]
 [0.258]
 [0.243]
 [0.237]
 [0.24 ]] [[39.084]
 [40.609]
 [35.918]
 [34.493]
 [32.945]
 [27.902]
 [31.674]] [[0.281]
 [0.285]
 [0.255]
 [0.258]
 [0.243]
 [0.237]
 [0.24 ]]
printing an ep nov before normalisation:  39.16040442333236
siam score:  -0.76883364
maxi score, test score, baseline:  -0.00030000000000015133 0.08566666666666674 0.08566666666666674
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.00030000000000015133 0.08566666666666674 0.08566666666666674
probs:  [0.04832258832247454, 0.08343493753884035, 0.13003838074454138, 0.3375348261817197, 0.18354533615828203, 0.21712393105414193]
actor:  1 policy actor:  1  step number:  56 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.00030000000000015133 0.08566666666666674 0.08566666666666674
printing an ep nov before normalisation:  0.0
actor:  1 policy actor:  1  step number:  49 total reward:  0.48  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.008]
 [0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]] [[46.778]
 [55.925]
 [46.778]
 [46.778]
 [46.778]
 [46.778]
 [46.778]] [[0.968]
 [1.293]
 [0.968]
 [0.968]
 [0.968]
 [0.968]
 [0.968]]
printing an ep nov before normalisation:  57.99697284914531
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.7734842
maxi score, test score, baseline:  -0.00030000000000015133 0.08566666666666674 0.08566666666666674
probs:  [0.048227764141087974, 0.08328007964935616, 0.12980384212824184, 0.3368708671800187, 0.1832193134626147, 0.2185981334386808]
maxi score, test score, baseline:  -0.00368666666666682 0.08566666666666674 0.08566666666666674
probs:  [0.048227764141087974, 0.08328007964935616, 0.12980384212824184, 0.3368708671800187, 0.1832193134626147, 0.2185981334386808]
maxi score, test score, baseline:  -0.00368666666666682 0.08566666666666674 0.08566666666666674
maxi score, test score, baseline:  -0.00368666666666682 0.08566666666666674 0.08566666666666674
probs:  [0.048227764141087974, 0.08328007964935616, 0.12980384212824184, 0.3368708671800187, 0.1832193134626147, 0.2185981334386808]
maxi score, test score, baseline:  -0.00368666666666682 0.08566666666666674 0.08566666666666674
probs:  [0.048227764141087974, 0.08328007964935616, 0.12980384212824184, 0.3368708671800187, 0.1832193134626147, 0.2185981334386808]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5400000000000001  reward:  1.0 rdn_beta:  2.0
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[ 0.377]
 [ 0.459]
 [-0.129]
 [ 0.377]
 [ 0.366]
 [ 0.377]
 [ 0.159]] [[36.058]
 [38.938]
 [39.302]
 [36.058]
 [39.12 ]
 [36.058]
 [35.097]] [[1.651]
 [1.928]
 [1.365]
 [1.651]
 [1.848]
 [1.651]
 [1.367]]
printing an ep nov before normalisation:  45.329815523379224
printing an ep nov before normalisation:  45.312565170170416
maxi score, test score, baseline:  -0.00368666666666682 0.08566666666666674 0.08566666666666674
probs:  [0.04810936310303853, 0.08307538543192473, 0.12948461386868224, 0.3360418731124722, 0.18276858487895972, 0.22052017960492265]
maxi score, test score, baseline:  -0.00368666666666682 0.08566666666666674 0.08566666666666674
probs:  [0.04810936310303853, 0.08307538543192473, 0.12948461386868224, 0.3360418731124722, 0.18276858487895972, 0.22052017960492265]
maxi score, test score, baseline:  -0.00368666666666682 0.08566666666666674 0.08566666666666674
probs:  [0.04810936310303853, 0.08307538543192473, 0.12948461386868224, 0.3360418731124722, 0.18276858487895972, 0.22052017960492265]
printing an ep nov before normalisation:  19.489409158697608
maxi score, test score, baseline:  -0.00368666666666682 0.08566666666666674 0.08566666666666674
probs:  [0.04810936310303853, 0.08307538543192473, 0.12948461386868224, 0.3360418731124722, 0.18276858487895972, 0.22052017960492265]
actor:  1 policy actor:  1  step number:  53 total reward:  0.42666666666666675  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.172]
 [0.016]
 [0.172]
 [0.172]
 [0.172]
 [0.172]
 [0.172]] [[49.718]
 [49.399]
 [49.718]
 [49.718]
 [49.718]
 [49.718]
 [49.718]] [[0.997]
 [0.831]
 [0.997]
 [0.997]
 [0.997]
 [0.997]
 [0.997]]
actor:  0 policy actor:  0  step number:  52 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.00466000000000015 0.08566666666666674 0.08566666666666674
printing an ep nov before normalisation:  40.068512046967015
maxi score, test score, baseline:  -0.00466000000000015 0.08566666666666674 0.08566666666666674
probs:  [0.04804292110038789, 0.08296051909324054, 0.12930547553326524, 0.3355766742684359, 0.182515653737111, 0.22159875626755932]
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.378]
 [0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.325]] [[38.071]
 [34.344]
 [38.071]
 [38.071]
 [38.071]
 [38.071]
 [38.071]] [[1.223]
 [1.122]
 [1.223]
 [1.223]
 [1.223]
 [1.223]
 [1.223]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  54 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.00466000000000015 0.08566666666666674 0.08566666666666674
probs:  [0.048140498183626486, 0.08275961837828806, 0.12910848024354477, 0.3362587223945353, 0.1823231423953435, 0.22140953840466182]
printing an ep nov before normalisation:  35.97678031855008
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  49.15101731189556
maxi score, test score, baseline:  -0.00466000000000015 0.08566666666666674 0.08566666666666674
actions average: 
K:  0  action  0 :  tensor([0.4451, 0.0103, 0.0980, 0.0908, 0.1667, 0.0864, 0.1027],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0048, 0.9689, 0.0031, 0.0039, 0.0032, 0.0018, 0.0143],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2156, 0.0022, 0.3641, 0.0847, 0.1117, 0.1209, 0.1009],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1671, 0.0184, 0.1320, 0.3183, 0.1241, 0.0850, 0.1550],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1522, 0.0009, 0.0623, 0.0594, 0.6159, 0.0470, 0.0623],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1658, 0.0048, 0.1287, 0.0859, 0.0905, 0.4347, 0.0895],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1249, 0.1376, 0.0830, 0.1695, 0.0953, 0.1170, 0.2728],
       grad_fn=<DivBackward0>)
siam score:  -0.7690279
maxi score, test score, baseline:  -0.00466000000000015 0.08566666666666674 0.08566666666666674
probs:  [0.04811403413088042, 0.08271407050353895, 0.12965391705573812, 0.3360734329044079, 0.18215698577207973, 0.22128755963335492]
maxi score, test score, baseline:  -0.00466000000000015 0.08566666666666674 0.08566666666666674
probs:  [0.04811403413088042, 0.08271407050353895, 0.12965391705573812, 0.3360734329044079, 0.18215698577207973, 0.22128755963335492]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.667
from probs:  [0.04811403413088042, 0.08271407050353895, 0.12965391705573812, 0.3360734329044079, 0.18215698577207973, 0.22128755963335492]
printing an ep nov before normalisation:  51.38416538451623
maxi score, test score, baseline:  -0.00466000000000015 0.08566666666666674 0.08566666666666674
probs:  [0.04805027311801763, 0.08260432999419154, 0.12948179885157302, 0.33562700673995344, 0.18324292051209334, 0.22099367078417106]
maxi score, test score, baseline:  -0.00466000000000015 0.08566666666666674 0.08566666666666674
probs:  [0.04805027311801763, 0.08260432999419154, 0.12948179885157302, 0.33562700673995344, 0.18324292051209334, 0.22099367078417106]
actions average: 
K:  1  action  0 :  tensor([0.4906, 0.0191, 0.1034, 0.0996, 0.1040, 0.0897, 0.0936],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0147, 0.9104, 0.0053, 0.0304, 0.0019, 0.0022, 0.0351],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0714, 0.0311, 0.6841, 0.0377, 0.0226, 0.0913, 0.0618],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2261, 0.0353, 0.1375, 0.1996, 0.1429, 0.1358, 0.1229],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1879, 0.0046, 0.1216, 0.1439, 0.2718, 0.1301, 0.1400],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0784, 0.0143, 0.1087, 0.0343, 0.0293, 0.7038, 0.0311],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1441, 0.1952, 0.0730, 0.1371, 0.0539, 0.0601, 0.3366],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.00466000000000015 0.08566666666666674 0.08566666666666674
maxi score, test score, baseline:  -0.00466000000000015 0.08566666666666674 0.08566666666666674
probs:  [0.048052392929408896, 0.08260797844912275, 0.12948752112782697, 0.3356418487135401, 0.18320681730844737, 0.22100344147165382]
actor:  1 policy actor:  1  step number:  59 total reward:  0.18666666666666587  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  45 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.00518000000000015 0.08566666666666674 0.08566666666666674
probs:  [0.04804264980682276, 0.08260776913247939, 0.12950024576833882, 0.3355735571716597, 0.1832343629782506, 0.22104141514244866]
maxi score, test score, baseline:  -0.00518000000000015 0.08566666666666674 0.08566666666666674
probs:  [0.04804264980682276, 0.08260776913247939, 0.12950024576833882, 0.3355735571716597, 0.1832343629782506, 0.22104141514244866]
siam score:  -0.76775956
printing an ep nov before normalisation:  32.728741789434615
printing an ep nov before normalisation:  22.739357948303223
actor:  1 policy actor:  1  step number:  42 total reward:  0.5400000000000001  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.00518000000000015 0.08566666666666674 0.08566666666666674
maxi score, test score, baseline:  -0.00518000000000015 0.08566666666666674 0.08566666666666674
probs:  [0.04810644391344454, 0.08260913965493218, 0.12941692986357023, 0.33602070313185495, 0.18305400484554127, 0.2207927785906568]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.00518000000000015 0.08566666666666674 0.08566666666666674
probs:  [0.04812946265505094, 0.0826487136472595, 0.12947896339669404, 0.3361818702889707, 0.18314177477998622, 0.2204192152320386]
line 256 mcts: sample exp_bonus 77.99741293074811
Starting evaluation
maxi score, test score, baseline:  -0.00518000000000015 0.08566666666666674 0.08566666666666674
probs:  [0.04812946265505094, 0.0826487136472595, 0.12947896339669404, 0.3361818702889707, 0.18314177477998622, 0.2204192152320386]
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]]
Printing some Q and Qe and total Qs values:  [[0.807]
 [0.867]
 [0.807]
 [0.851]
 [0.843]
 [0.82 ]
 [0.88 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.807]
 [0.867]
 [0.807]
 [0.851]
 [0.843]
 [0.82 ]
 [0.88 ]]
Printing some Q and Qe and total Qs values:  [[1.004]
 [0.983]
 [1.004]
 [1.004]
 [1.004]
 [1.004]
 [1.004]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.004]
 [0.983]
 [1.004]
 [1.004]
 [1.004]
 [1.004]
 [1.004]]
printing an ep nov before normalisation:  31.691275119326964
Printing some Q and Qe and total Qs values:  [[0.866]
 [0.885]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]] [[29.374]
 [33.287]
 [29.374]
 [29.374]
 [29.374]
 [29.374]
 [29.374]] [[0.866]
 [0.885]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]]
printing an ep nov before normalisation:  39.630429225348045
Printing some Q and Qe and total Qs values:  [[0.236]
 [0.312]
 [0.236]
 [0.236]
 [0.242]
 [0.236]
 [0.236]] [[37.972]
 [39.374]
 [37.972]
 [37.972]
 [42.675]
 [37.972]
 [37.972]] [[0.661]
 [0.766]
 [0.661]
 [0.661]
 [0.769]
 [0.661]
 [0.661]]
printing an ep nov before normalisation:  30.592754152086044
printing an ep nov before normalisation:  36.25712708162925
printing an ep nov before normalisation:  34.900125424502896
printing an ep nov before normalisation:  34.70480328475424
printing an ep nov before normalisation:  45.45128741380835
printing an ep nov before normalisation:  54.33023378169138
Printing some Q and Qe and total Qs values:  [[0.805]
 [0.818]
 [0.805]
 [0.805]
 [0.805]
 [0.805]
 [0.805]] [[36.942]
 [38.812]
 [36.942]
 [36.942]
 [36.942]
 [36.942]
 [36.942]] [[0.805]
 [0.818]
 [0.805]
 [0.805]
 [0.805]
 [0.805]
 [0.805]]
maxi score, test score, baseline:  -0.00518000000000015 0.08566666666666674 0.08566666666666674
Printing some Q and Qe and total Qs values:  [[0.781]
 [0.849]
 [0.725]
 [0.785]
 [0.741]
 [0.745]
 [0.881]] [[36.445]
 [35.154]
 [37.558]
 [37.417]
 [38.944]
 [40.374]
 [34.469]] [[0.781]
 [0.849]
 [0.725]
 [0.785]
 [0.741]
 [0.745]
 [0.881]]
printing an ep nov before normalisation:  34.685871287255935
printing an ep nov before normalisation:  33.2267037150667
actor:  1 policy actor:  1  step number:  46 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.006]
 [1.006]
 [1.006]
 [1.006]
 [1.006]
 [1.006]
 [1.006]] [[35.118]
 [35.118]
 [35.118]
 [35.118]
 [35.118]
 [35.118]
 [35.118]] [[1.006]
 [1.006]
 [1.006]
 [1.006]
 [1.006]
 [1.006]
 [1.006]]
Printing some Q and Qe and total Qs values:  [[0.97 ]
 [0.978]
 [0.935]
 [0.901]
 [0.931]
 [0.917]
 [0.892]] [[32.773]
 [31.973]
 [31.073]
 [30.372]
 [31.234]
 [29.1  ]
 [28.79 ]] [[0.97 ]
 [0.978]
 [0.935]
 [0.901]
 [0.931]
 [0.917]
 [0.892]]
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  34.25672811241651
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  0.10508030240742983
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  34.09559707221582
maxi score, test score, baseline:  0.03060666666666652 0.08566666666666674 0.08566666666666674
probs:  [0.047985767317342824, 0.08247966317822222, 0.12927551510329957, 0.3351754273244778, 0.18493465770638792, 0.2201489693702696]
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  29 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  73.69704246520996
maxi score, test score, baseline:  0.0318733333333332 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 41.818564752482715
printing an ep nov before normalisation:  29.0988365673454
Printing some Q and Qe and total Qs values:  [[-0.106]
 [-0.102]
 [-0.108]
 [-0.109]
 [-0.11 ]
 [-0.108]
 [-0.11 ]] [[43.933]
 [43.177]
 [44.357]
 [44.296]
 [43.067]
 [44.831]
 [43.067]] [[1.027]
 [0.994]
 [1.047]
 [1.043]
 [0.981]
 [1.07 ]
 [0.981]]
printing an ep nov before normalisation:  32.31438303204397
maxi score, test score, baseline:  0.0318733333333332 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0016997850480038323
maxi score, test score, baseline:  0.0318733333333332 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.471]
 [0.574]
 [0.331]
 [0.475]
 [0.571]
 [0.38 ]] [[40.878]
 [45.471]
 [46.643]
 [47.083]
 [45.515]
 [48.582]
 [44.818]] [[1.587]
 [1.722]
 [1.897]
 [1.681]
 [1.73 ]
 [2.012]
 [1.592]]
maxi score, test score, baseline:  0.0318733333333332 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3533333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0318733333333332 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.706]
 [0.804]
 [0.71 ]
 [0.71 ]
 [0.713]
 [0.71 ]
 [0.71 ]] [[33.973]
 [40.527]
 [42.741]
 [42.741]
 [36.996]
 [42.741]
 [42.741]] [[0.706]
 [0.804]
 [0.71 ]
 [0.71 ]
 [0.713]
 [0.71 ]
 [0.71 ]]
printing an ep nov before normalisation:  41.49323335741271
siam score:  -0.7729307
printing an ep nov before normalisation:  38.65139161001872
actions average: 
K:  2  action  0 :  tensor([0.5052, 0.0340, 0.0736, 0.0689, 0.1650, 0.0915, 0.0617],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0108, 0.9133, 0.0057, 0.0179, 0.0065, 0.0027, 0.0431],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1882, 0.0020, 0.3350, 0.1133, 0.1201, 0.1244, 0.1170],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1156, 0.0264, 0.1031, 0.4503, 0.1019, 0.0764, 0.1263],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1853, 0.0064, 0.0578, 0.0878, 0.4815, 0.1172, 0.0641],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0909, 0.0054, 0.1796, 0.0564, 0.0829, 0.5376, 0.0472],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1803, 0.3019, 0.0839, 0.0874, 0.0747, 0.1086, 0.1632],
       grad_fn=<DivBackward0>)
siam score:  -0.77090883
printing an ep nov before normalisation:  40.849357441467475
maxi score, test score, baseline:  0.0318733333333332 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3066666666666664  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  56 total reward:  0.12666666666666626  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.03412666666666653 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.03412666666666653 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.59865075341635
maxi score, test score, baseline:  0.03412666666666653 0.6903333333333335 0.6903333333333335
maxi score, test score, baseline:  0.03412666666666653 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.29823589324951
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.016057570048524
actor:  0 policy actor:  0  step number:  50 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0366733333333332 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.17999999999999916  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  29.569163167126778
Printing some Q and Qe and total Qs values:  [[0.805]
 [0.805]
 [0.805]
 [0.805]
 [0.805]
 [0.805]
 [0.805]] [[33.744]
 [33.744]
 [33.744]
 [33.744]
 [33.744]
 [33.744]
 [33.744]] [[0.805]
 [0.805]
 [0.805]
 [0.805]
 [0.805]
 [0.805]
 [0.805]]
maxi score, test score, baseline:  0.0366733333333332 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0366733333333332 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.439867668603846
maxi score, test score, baseline:  0.0366733333333332 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0366733333333332 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  52 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  45 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.84494829038145
maxi score, test score, baseline:  0.04233999999999986 0.6903333333333335 0.6903333333333335
actor:  1 policy actor:  1  step number:  59 total reward:  0.2266666666666658  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.04233999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04233999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  36 total reward:  0.5133333333333333  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  27.659950256347656
printing an ep nov before normalisation:  44.95464083173442
printing an ep nov before normalisation:  40.66783982706819
printing an ep nov before normalisation:  19.175241613393155
actor:  1 policy actor:  1  step number:  56 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  41 total reward:  0.5200000000000001  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  36.33496207599549
printing an ep nov before normalisation:  68.66608268283879
printing an ep nov before normalisation:  9.075363581486044
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]] [[28.278]
 [28.278]
 [28.278]
 [28.278]
 [28.278]
 [28.278]
 [28.278]] [[0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.633]]
maxi score, test score, baseline:  0.042953333333333184 0.6903333333333335 0.6903333333333335
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.029]
 [-0.017]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]
 [-0.029]] [[43.157]
 [51.219]
 [43.157]
 [43.157]
 [43.157]
 [43.157]
 [43.157]] [[1.523]
 [1.983]
 [1.523]
 [1.523]
 [1.523]
 [1.523]
 [1.523]]
printing an ep nov before normalisation:  31.262798309326172
actor:  0 policy actor:  0  step number:  42 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  0.667
siam score:  -0.776669
maxi score, test score, baseline:  0.04565999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04565999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.3600000000000001  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  16.279597282409668
maxi score, test score, baseline:  0.04565999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.642584559067313
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.526]
 [0.447]
 [0.451]
 [0.428]
 [0.414]
 [0.424]] [[35.892]
 [41.629]
 [36.295]
 [37.134]
 [36.7  ]
 [37.424]
 [36.154]] [[0.637]
 [0.77 ]
 [0.64 ]
 [0.652]
 [0.625]
 [0.618]
 [0.616]]
printing an ep nov before normalisation:  47.81980641899639
printing an ep nov before normalisation:  26.150426527164363
maxi score, test score, baseline:  0.04565999999999986 0.6903333333333335 0.6903333333333335
maxi score, test score, baseline:  0.04565999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04565999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04565999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.569]] [[33.56]
 [33.56]
 [33.56]
 [33.56]
 [33.56]
 [33.56]
 [33.56]] [[0.84]
 [0.84]
 [0.84]
 [0.84]
 [0.84]
 [0.84]
 [0.84]]
maxi score, test score, baseline:  0.04565999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.94210147857666
printing an ep nov before normalisation:  24.32805140468279
actor:  1 policy actor:  1  step number:  56 total reward:  0.23333333333333262  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  72 total reward:  0.1799999999999996  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04565999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.62331814757872
printing an ep nov before normalisation:  38.58254986726511
printing an ep nov before normalisation:  32.21985340118408
printing an ep nov before normalisation:  41.121453001617475
maxi score, test score, baseline:  0.04565999999999986 0.6903333333333335 0.6903333333333335
maxi score, test score, baseline:  0.04565999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.04060734703416
maxi score, test score, baseline:  0.04565999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.889336109161377
printing an ep nov before normalisation:  30.353912727278512
maxi score, test score, baseline:  0.04565999999999986 0.6903333333333335 0.6903333333333335
Printing some Q and Qe and total Qs values:  [[-0.019]
 [ 0.058]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]
 [-0.019]] [[24.679]
 [39.824]
 [24.679]
 [24.679]
 [24.679]
 [24.679]
 [24.679]] [[0.103]
 [0.325]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]]
maxi score, test score, baseline:  0.04565999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  59 total reward:  0.10666666666666613  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.04505999999999987 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.68846484342894
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.79622478664019
maxi score, test score, baseline:  0.04505999999999987 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04505999999999987 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.04505999999999987 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 40.737531570544995
printing an ep nov before normalisation:  37.78896044051109
maxi score, test score, baseline:  0.04505999999999987 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04505999999999987 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.04505999999999987 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.135]
 [0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]] [[47.643]
 [55.626]
 [47.643]
 [47.643]
 [47.643]
 [47.643]
 [47.643]] [[0.965]
 [1.269]
 [0.965]
 [0.965]
 [0.965]
 [0.965]
 [0.965]]
printing an ep nov before normalisation:  40.469465255737305
actor:  1 policy actor:  1  step number:  43 total reward:  0.44000000000000006  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  47.85061961931605
printing an ep nov before normalisation:  50.47472217463107
maxi score, test score, baseline:  0.04505999999999987 0.6903333333333335 0.6903333333333335
Printing some Q and Qe and total Qs values:  [[0.276]
 [0.398]
 [0.276]
 [0.276]
 [0.276]
 [0.276]
 [0.276]] [[31.562]
 [36.317]
 [31.562]
 [31.562]
 [31.562]
 [31.562]
 [31.562]] [[1.533]
 [1.988]
 [1.533]
 [1.533]
 [1.533]
 [1.533]
 [1.533]]
maxi score, test score, baseline:  0.04505999999999987 0.6903333333333335 0.6903333333333335
printing an ep nov before normalisation:  46.23240063819034
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.36 ]
 [0.263]
 [0.263]
 [0.263]
 [0.263]
 [0.263]] [[43.006]
 [47.614]
 [43.006]
 [43.006]
 [43.006]
 [43.006]
 [43.006]] [[1.354]
 [1.64 ]
 [1.354]
 [1.354]
 [1.354]
 [1.354]
 [1.354]]
printing an ep nov before normalisation:  28.303198847443795
maxi score, test score, baseline:  0.04505999999999987 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.90886654550704
maxi score, test score, baseline:  0.04505999999999987 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  13.753770621796146
printing an ep nov before normalisation:  22.36838416146825
actor:  1 policy actor:  1  step number:  61 total reward:  0.3466666666666658  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  19.062583647066607
maxi score, test score, baseline:  0.04505999999999987 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  0.3466666666666658  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.381]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]] [[37.027]
 [41.71 ]
 [37.027]
 [37.027]
 [37.027]
 [37.027]
 [37.027]] [[0.836]
 [0.964]
 [0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.836]]
printing an ep nov before normalisation:  45.977679530011905
maxi score, test score, baseline:  0.04505999999999987 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.57352077281677
maxi score, test score, baseline:  0.04505999999999987 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.935]
 [0.603]] [[37.779]
 [37.779]
 [37.779]
 [37.779]
 [37.779]
 [37.065]
 [37.779]] [[0.906]
 [0.906]
 [0.906]
 [0.906]
 [0.906]
 [1.228]
 [0.906]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7725083
actor:  1 policy actor:  1  step number:  61 total reward:  0.2666666666666665  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[ 0.033]
 [ 0.094]
 [ 0.033]
 [-0.006]
 [ 0.063]
 [ 0.033]
 [ 0.027]] [[25.481]
 [38.854]
 [25.481]
 [30.301]
 [41.347]
 [25.481]
 [36.883]] [[0.341]
 [0.613]
 [0.341]
 [0.378]
 [0.622]
 [0.341]
 [0.516]]
printing an ep nov before normalisation:  28.059873580932617
maxi score, test score, baseline:  0.04505999999999987 0.6903333333333335 0.6903333333333335
printing an ep nov before normalisation:  38.62306693207388
maxi score, test score, baseline:  0.04505999999999987 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04505999999999987 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 28.23472023010254
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.497]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]
 [0.45 ]] [[48.785]
 [45.446]
 [48.785]
 [48.785]
 [48.785]
 [48.785]
 [48.785]] [[1.111]
 [1.087]
 [1.111]
 [1.111]
 [1.111]
 [1.111]
 [1.111]]
maxi score, test score, baseline:  0.04505999999999987 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.28666666666666674  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  35.0640078263667
printing an ep nov before normalisation:  43.946638107299805
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.225119902414114
printing an ep nov before normalisation:  39.63189402038354
maxi score, test score, baseline:  0.04505999999999987 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04505999999999987 0.6903333333333335 0.6903333333333335
maxi score, test score, baseline:  0.04505999999999987 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  56 total reward:  0.39333333333333276  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.04505999999999987 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]] [[27.015]
 [27.015]
 [27.015]
 [27.015]
 [27.015]
 [27.015]
 [27.015]] [[0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]]
printing an ep nov before normalisation:  46.9940632107392
actor:  0 policy actor:  0  step number:  57 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  1  action  0 :  tensor([0.4585, 0.0467, 0.0965, 0.1005, 0.1314, 0.0810, 0.0854],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0081, 0.9410, 0.0107, 0.0053, 0.0041, 0.0082, 0.0225],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1508, 0.0104, 0.4731, 0.0855, 0.0986, 0.0916, 0.0899],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1199, 0.0101, 0.0718, 0.4749, 0.1168, 0.0918, 0.1147],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1281, 0.0059, 0.0695, 0.0733, 0.5763, 0.0903, 0.0566],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1119, 0.0049, 0.1419, 0.0557, 0.1038, 0.5206, 0.0613],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1501, 0.0850, 0.1080, 0.1095, 0.1135, 0.1053, 0.3286],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.04761999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04761999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.0901504905606
maxi score, test score, baseline:  0.04761999999999986 0.6903333333333335 0.6903333333333335
Printing some Q and Qe and total Qs values:  [[0.273]
 [0.326]
 [0.132]
 [0.211]
 [0.269]
 [0.069]
 [0.266]] [[48.946]
 [44.865]
 [53.405]
 [51.03 ]
 [52.783]
 [57.951]
 [47.654]] [[1.702]
 [1.544]
 [1.793]
 [1.749]
 [1.897]
 [1.965]
 [1.629]]
maxi score, test score, baseline:  0.04761999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77739
printing an ep nov before normalisation:  52.70002350077674
printing an ep nov before normalisation:  42.99932001416028
maxi score, test score, baseline:  0.04761999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.562]
 [0.562]
 [0.55 ]
 [0.51 ]
 [0.562]
 [0.562]] [[34.349]
 [31.379]
 [31.379]
 [31.598]
 [31.827]
 [31.379]
 [31.379]] [[2.181]
 [1.939]
 [1.939]
 [1.945]
 [1.923]
 [1.939]
 [1.939]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  46 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  33.05647850036621
printing an ep nov before normalisation:  48.41177354494324
printing an ep nov before normalisation:  32.32368694104629
maxi score, test score, baseline:  0.04761999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.987819858601117
using explorer policy with actor:  1
maxi score, test score, baseline:  0.04761999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  31 total reward:  0.56  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.553]
 [0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]] [[27.653]
 [39.718]
 [27.653]
 [27.653]
 [27.653]
 [27.653]
 [27.653]] [[1.098]
 [1.654]
 [1.098]
 [1.098]
 [1.098]
 [1.098]
 [1.098]]
printing an ep nov before normalisation:  42.43809888424188
actor:  1 policy actor:  1  step number:  34 total reward:  0.54  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  35.72424622296639
maxi score, test score, baseline:  0.04820666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.231753627775497
actor:  1 policy actor:  1  step number:  52 total reward:  0.28666666666666607  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.181]
 [0.333]
 [0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.181]
 [0.333]
 [0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]]
maxi score, test score, baseline:  0.04820666666666652 0.6903333333333335 0.6903333333333335
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.888458972920056
printing an ep nov before normalisation:  42.98286281742317
maxi score, test score, baseline:  0.04820666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.166422693401145
printing an ep nov before normalisation:  31.87577247619629
maxi score, test score, baseline:  0.04820666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.38365005220832
actor:  1 policy actor:  1  step number:  63 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.04820666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04820666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04820666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2599999999999999  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.755]
 [0.72 ]
 [0.755]
 [0.682]
 [0.683]
 [0.755]
 [0.755]] [[34.923]
 [30.045]
 [34.923]
 [28.534]
 [28.469]
 [34.923]
 [34.923]] [[0.755]
 [0.72 ]
 [0.755]
 [0.682]
 [0.683]
 [0.755]
 [0.755]]
printing an ep nov before normalisation:  25.91771759112887
printing an ep nov before normalisation:  46.85657044748435
actor:  1 policy actor:  1  step number:  63 total reward:  0.10666666666666591  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  28.75049114227295
maxi score, test score, baseline:  0.04820666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.662]
 [0.712]
 [0.628]
 [0.629]
 [0.625]
 [0.626]
 [0.628]] [[25.759]
 [21.885]
 [18.484]
 [18.421]
 [19.488]
 [18.492]
 [18.318]] [[1.883]
 [1.75 ]
 [1.504]
 [1.502]
 [1.548]
 [1.502]
 [1.497]]
maxi score, test score, baseline:  0.04820666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.06584644317627
actor:  1 policy actor:  1  step number:  43 total reward:  0.5200000000000001  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.04820666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  58 total reward:  0.16666666666666585  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.050539999999999856 0.6903333333333335 0.6903333333333335
maxi score, test score, baseline:  0.050539999999999856 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  40 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  46.066142213846824
maxi score, test score, baseline:  0.050366666666666525 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.050366666666666525 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.050366666666666525 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  46 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.05025999999999986 0.6903333333333335 0.6903333333333335
actor:  1 policy actor:  1  step number:  37 total reward:  0.6266666666666669  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.05025999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.05025999999999986 0.6903333333333335 0.6903333333333335
actor:  1 policy actor:  1  step number:  56 total reward:  0.3533333333333326  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.05025999999999986 0.6903333333333335 0.6903333333333335
maxi score, test score, baseline:  0.05025999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.85010706800851
actions average: 
K:  1  action  0 :  tensor([0.4473, 0.0040, 0.1100, 0.1131, 0.1296, 0.0847, 0.1113],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0184, 0.8756, 0.0234, 0.0210, 0.0160, 0.0107, 0.0348],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1163, 0.0036, 0.4803, 0.0916, 0.1264, 0.1050, 0.0768],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2157, 0.0036, 0.1420, 0.2548, 0.1582, 0.0907, 0.1349],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2398, 0.0115, 0.0837, 0.0799, 0.4068, 0.0648, 0.1135],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.2475, 0.0049, 0.1233, 0.1101, 0.1113, 0.2886, 0.1143],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1787, 0.2061, 0.0934, 0.0864, 0.0963, 0.1063, 0.2327],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.05025999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  0.00010781651496927225
actor:  1 policy actor:  1  step number:  64 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  49 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.05025999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.185]
 [0.194]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]] [[28.275]
 [36.451]
 [28.275]
 [28.275]
 [28.275]
 [28.275]
 [28.275]] [[0.887]
 [1.292]
 [0.887]
 [0.887]
 [0.887]
 [0.887]
 [0.887]]
printing an ep nov before normalisation:  36.59941165609222
siam score:  -0.7684465
printing an ep nov before normalisation:  40.61659952886663
actor:  0 policy actor:  0  step number:  49 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  53 total reward:  0.44000000000000006  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  5.129543723114693e-06
maxi score, test score, baseline:  0.04964666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04964666666666652 0.6903333333333335 0.6903333333333335
Printing some Q and Qe and total Qs values:  [[0.53]
 [0.48]
 [0.53]
 [0.53]
 [0.53]
 [0.53]
 [0.53]] [[38.746]
 [44.162]
 [38.746]
 [38.746]
 [38.746]
 [38.746]
 [38.746]] [[1.765]
 [2.032]
 [1.765]
 [1.765]
 [1.765]
 [1.765]
 [1.765]]
maxi score, test score, baseline:  0.04964666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.86442748381771
printing an ep nov before normalisation:  40.416077773930475
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.69887555684248
printing an ep nov before normalisation:  41.060562534914865
actor:  0 policy actor:  0  step number:  44 total reward:  0.35333333333333283  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.049793333333333196 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  36.408967590052484
actor:  1 policy actor:  1  step number:  63 total reward:  0.10666666666666591  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.049793333333333196 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.049793333333333196 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.855]
 [0.921]
 [0.878]
 [0.855]
 [0.855]
 [0.855]
 [0.855]] [[36.252]
 [34.864]
 [36.208]
 [36.252]
 [36.252]
 [36.252]
 [36.252]] [[0.855]
 [0.921]
 [0.878]
 [0.855]
 [0.855]
 [0.855]
 [0.855]]
maxi score, test score, baseline:  0.049793333333333196 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.049793333333333196 0.6903333333333335 0.6903333333333335
maxi score, test score, baseline:  0.049793333333333196 0.6903333333333335 0.6903333333333335
maxi score, test score, baseline:  0.049793333333333196 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.42479789916803
maxi score, test score, baseline:  0.049793333333333196 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.049793333333333196 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.049793333333333196 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4666666666666668  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.32220813506819
maxi score, test score, baseline:  0.049793333333333196 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.049793333333333196 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  71 total reward:  0.07999999999999918  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.049793333333333196 0.6903333333333335 0.6903333333333335
maxi score, test score, baseline:  0.049793333333333196 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.40666666666666673  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  2  action  0 :  tensor([0.4281, 0.0070, 0.1157, 0.1180, 0.1413, 0.1001, 0.0898],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0018, 0.9791, 0.0018, 0.0037, 0.0012, 0.0015, 0.0109],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0754, 0.0088, 0.6075, 0.0635, 0.0860, 0.1010, 0.0578],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0930, 0.0667, 0.0779, 0.4903, 0.0856, 0.0880, 0.0985],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2161, 0.0148, 0.0865, 0.1037, 0.3970, 0.0821, 0.0997],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1508, 0.0031, 0.1334, 0.1240, 0.1230, 0.3589, 0.1068],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1625, 0.0304, 0.1717, 0.1852, 0.1720, 0.1284, 0.1497],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.049793333333333196 0.6903333333333335 0.6903333333333335
maxi score, test score, baseline:  0.049793333333333196 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.89027937300305
siam score:  -0.76290053
printing an ep nov before normalisation:  33.22783082832808
actor:  0 policy actor:  0  step number:  52 total reward:  0.13999999999999946  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.049339999999999856 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.805748894293657
actor:  1 policy actor:  1  step number:  55 total reward:  0.1999999999999994  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 39.97412328158645
Printing some Q and Qe and total Qs values:  [[-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]] [[25.074]
 [25.074]
 [25.074]
 [25.074]
 [25.074]
 [25.074]
 [25.074]] [[0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]]
maxi score, test score, baseline:  0.049339999999999856 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[38.924]
 [38.924]
 [38.924]
 [38.924]
 [38.924]
 [38.924]
 [38.924]] [[25.959]
 [25.959]
 [25.959]
 [25.959]
 [25.959]
 [25.959]
 [25.959]]
Printing some Q and Qe and total Qs values:  [[-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]
 [-0.]] [[35.353]
 [35.353]
 [35.353]
 [35.353]
 [35.353]
 [35.353]
 [35.353]] [[0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]]
siam score:  -0.76331574
using explorer policy with actor:  1
printing an ep nov before normalisation:  22.9025965688906
printing an ep nov before normalisation:  29.4956636428833
line 256 mcts: sample exp_bonus 32.89358192079902
printing an ep nov before normalisation:  35.603569506774456
Printing some Q and Qe and total Qs values:  [[0.159]
 [0.071]
 [0.116]
 [0.116]
 [0.116]
 [0.116]
 [0.001]] [[46.124]
 [45.817]
 [33.351]
 [33.351]
 [33.351]
 [33.351]
 [44.834]] [[1.636]
 [1.531]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [1.408]]
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.076]
 [0.125]
 [0.125]
 [0.391]
 [0.125]
 [0.125]] [[50.377]
 [52.055]
 [51.172]
 [51.172]
 [41.953]
 [51.172]
 [51.172]] [[1.781]
 [1.877]
 [1.879]
 [1.879]
 [1.663]
 [1.879]
 [1.879]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.049339999999999856 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.5315, 0.0139, 0.0902, 0.0918, 0.1136, 0.0765, 0.0824],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0036,     0.9506,     0.0025,     0.0024,     0.0010,     0.0008,
            0.0390], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1586, 0.0334, 0.4590, 0.0726, 0.0724, 0.1223, 0.0817],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1416, 0.0043, 0.0893, 0.4594, 0.0799, 0.1045, 0.1210],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2446, 0.0113, 0.1305, 0.0989, 0.2939, 0.0916, 0.1291],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1080, 0.0072, 0.1107, 0.0680, 0.0888, 0.5520, 0.0653],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1796, 0.0789, 0.1306, 0.0909, 0.0911, 0.0813, 0.3476],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.049339999999999856 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.049339999999999856 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.056]
 [0.023]
 [0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]] [[36.951]
 [34.778]
 [34.811]
 [34.811]
 [34.811]
 [34.811]
 [34.811]] [[1.723]
 [1.509]
 [1.578]
 [1.578]
 [1.578]
 [1.578]
 [1.578]]
maxi score, test score, baseline:  0.049339999999999856 0.6903333333333335 0.6903333333333335
maxi score, test score, baseline:  0.049339999999999856 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.18115375243082
printing an ep nov before normalisation:  33.26457681349346
maxi score, test score, baseline:  0.049339999999999856 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.71736599768683
maxi score, test score, baseline:  0.049339999999999856 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.21808243632624
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.681629749742264
Printing some Q and Qe and total Qs values:  [[-0.097]
 [-0.093]
 [-0.11 ]
 [-0.11 ]
 [-0.111]
 [-0.096]
 [-0.096]] [[25.579]
 [23.203]
 [23.088]
 [24.171]
 [23.203]
 [25.338]
 [24.886]] [[0.969]
 [0.875]
 [0.852]
 [0.898]
 [0.856]
 [0.96 ]
 [0.941]]
actor:  0 policy actor:  0  step number:  58 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.05183333333333319 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.028988830664964
maxi score, test score, baseline:  0.05183333333333319 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.361539331462915
siam score:  -0.76526445
printing an ep nov before normalisation:  32.86892026590913
maxi score, test score, baseline:  0.05183333333333319 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.05183333333333319 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  66 total reward:  0.13999999999999935  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.05183333333333319 0.6903333333333335 0.6903333333333335
maxi score, test score, baseline:  0.05183333333333319 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.05183333333333319 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.176196575164795
maxi score, test score, baseline:  0.05183333333333319 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.05183333333333319 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.3533333333333333  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  31.004493292940953
maxi score, test score, baseline:  0.05183333333333319 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.028229053779388
printing an ep nov before normalisation:  0.0
actor:  1 policy actor:  1  step number:  68 total reward:  0.07333333333333236  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.405]
 [0.348]
 [0.341]
 [0.35 ]
 [0.347]
 [0.351]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.335]
 [0.405]
 [0.348]
 [0.341]
 [0.35 ]
 [0.347]
 [0.351]]
printing an ep nov before normalisation:  31.125056105749433
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.147]
 [0.068]
 [0.068]
 [0.068]
 [0.068]
 [0.068]] [[29.546]
 [34.699]
 [29.546]
 [29.546]
 [29.546]
 [29.546]
 [29.546]] [[1.023]
 [1.402]
 [1.023]
 [1.023]
 [1.023]
 [1.023]
 [1.023]]
maxi score, test score, baseline:  0.05183333333333319 0.6903333333333335 0.6903333333333335
printing an ep nov before normalisation:  43.36011653221041
Printing some Q and Qe and total Qs values:  [[0.746]
 [0.752]
 [0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.746]] [[49.28]
 [46.58]
 [49.28]
 [49.28]
 [49.28]
 [49.28]
 [49.28]] [[0.746]
 [0.752]
 [0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.746]]
printing an ep nov before normalisation:  40.419894006322444
maxi score, test score, baseline:  0.05183333333333319 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.692]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]] [[28.767]
 [31.273]
 [28.767]
 [28.767]
 [28.767]
 [28.767]
 [28.767]] [[1.531]
 [1.645]
 [1.531]
 [1.531]
 [1.531]
 [1.531]
 [1.531]]
maxi score, test score, baseline:  0.05183333333333319 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.55911346911525
printing an ep nov before normalisation:  37.2535915062854
actor:  1 policy actor:  1  step number:  53 total reward:  0.31999999999999984  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.05183333333333319 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.71694354020477
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.538]
 [0.497]
 [0.493]
 [0.493]
 [0.493]
 [0.49 ]] [[31.454]
 [38.991]
 [29.211]
 [29.953]
 [29.661]
 [29.317]
 [30.54 ]] [[1.23 ]
 [1.637]
 [1.135]
 [1.166]
 [1.153]
 [1.137]
 [1.191]]
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.694]
 [0.664]
 [0.627]
 [0.623]
 [0.483]
 [0.664]] [[32.482]
 [35.98 ]
 [34.292]
 [34.234]
 [34.78 ]
 [35.045]
 [34.292]] [[1.711]
 [2.011]
 [1.865]
 [1.825]
 [1.858]
 [1.736]
 [1.865]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.911540495409522
maxi score, test score, baseline:  0.05183333333333319 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.03795289993286
printing an ep nov before normalisation:  65.85768621182453
printing an ep nov before normalisation:  63.678467539399364
actor:  1 policy actor:  1  step number:  75 total reward:  0.21333333333333204  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.04920666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04920666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.577]
 [0.5  ]
 [0.506]
 [0.517]
 [0.504]
 [0.522]] [[32.889]
 [39.639]
 [31.897]
 [32.714]
 [34.159]
 [35.311]
 [33.704]] [[1.549]
 [1.977]
 [1.444]
 [1.497]
 [1.594]
 [1.648]
 [1.572]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.1599999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.04920666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.60260544193051
printing an ep nov before normalisation:  42.322275274919505
printing an ep nov before normalisation:  42.30837313634019
maxi score, test score, baseline:  0.04920666666666652 0.6903333333333335 0.6903333333333335
printing an ep nov before normalisation:  40.68809229512448
maxi score, test score, baseline:  0.04920666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04920666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.613284086051983
line 256 mcts: sample exp_bonus 36.7338418642534
printing an ep nov before normalisation:  31.854045391082764
maxi score, test score, baseline:  0.04920666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]] [[30.017]
 [30.017]
 [30.017]
 [30.017]
 [30.017]
 [30.017]
 [30.017]] [[0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]]
printing an ep nov before normalisation:  22.933778762817383
maxi score, test score, baseline:  0.04620666666666653 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  53 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  35.367928642973446
maxi score, test score, baseline:  0.04589999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04589999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.869415283203125
printing an ep nov before normalisation:  33.34633642518866
maxi score, test score, baseline:  0.04589999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04589999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.15195357270423
maxi score, test score, baseline:  0.04589999999999986 0.6903333333333335 0.6903333333333335
maxi score, test score, baseline:  0.04589999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04589999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  46 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.864520948318596
maxi score, test score, baseline:  0.04569999999999985 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.664]
 [0.578]
 [0.578]
 [0.578]
 [0.42 ]
 [0.578]] [[40.957]
 [37.963]
 [40.957]
 [40.957]
 [40.957]
 [32.796]
 [40.957]] [[1.16 ]
 [1.186]
 [1.16 ]
 [1.16 ]
 [1.16 ]
 [0.839]
 [1.16 ]]
printing an ep nov before normalisation:  25.596078876080743
Printing some Q and Qe and total Qs values:  [[1.008]
 [1.016]
 [1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]] [[29.396]
 [33.677]
 [29.396]
 [29.396]
 [29.396]
 [29.396]
 [29.396]] [[1.008]
 [1.016]
 [1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]]
printing an ep nov before normalisation:  25.122964332115057
actor:  0 policy actor:  0  step number:  48 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.32113927400749
actor:  1 policy actor:  1  step number:  52 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.255543935523576
maxi score, test score, baseline:  0.04556666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.778]
 [0.902]
 [0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]] [[35.822]
 [38.38 ]
 [35.822]
 [35.822]
 [35.822]
 [35.822]
 [35.822]] [[0.778]
 [0.902]
 [0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]]
actor:  0 policy actor:  0  step number:  43 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.04595333333333319 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
actor:  0 policy actor:  0  step number:  36 total reward:  0.5133333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.04608666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.81920875528053
Printing some Q and Qe and total Qs values:  [[0.081]
 [0.026]
 [0.026]
 [0.026]
 [0.026]
 [0.026]
 [0.026]] [[26.239]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[ 0.273]
 [-0.124]
 [-0.124]
 [-0.124]
 [-0.124]
 [-0.124]
 [-0.124]]
Printing some Q and Qe and total Qs values:  [[0.188]
 [0.257]
 [0.188]
 [0.188]
 [0.188]
 [0.188]
 [0.188]] [[38.375]
 [33.463]
 [38.375]
 [38.375]
 [38.375]
 [38.375]
 [38.375]] [[0.521]
 [0.523]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]]
printing an ep nov before normalisation:  45.53125587512069
maxi score, test score, baseline:  0.04608666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.515728664553112
actor:  1 policy actor:  1  step number:  49 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.04608666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  61 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.04608666666666652 0.6903333333333335 0.6903333333333335
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.228]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.069]
 [0.228]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]]
printing an ep nov before normalisation:  33.76062776390135
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.587]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]] [[37.759]
 [45.86 ]
 [37.759]
 [37.759]
 [37.759]
 [37.759]
 [37.759]] [[0.567]
 [0.587]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]]
printing an ep nov before normalisation:  31.237812042236328
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  0.04608666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.008565300919372021
actions average: 
K:  0  action  0 :  tensor([0.4994, 0.0038, 0.0837, 0.0863, 0.1306, 0.0871, 0.1091],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0028, 0.9558, 0.0076, 0.0048, 0.0010, 0.0034, 0.0246],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1316, 0.0028, 0.5193, 0.0742, 0.0788, 0.0935, 0.0998],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1678, 0.0326, 0.1410, 0.2729, 0.0996, 0.1174, 0.1687],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1698, 0.0134, 0.0999, 0.0860, 0.3881, 0.1000, 0.1427],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1529, 0.0006, 0.2161, 0.0875, 0.1113, 0.3602, 0.0715],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1811, 0.0712, 0.2053, 0.0930, 0.0796, 0.0829, 0.2869],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.012]
 [ 0.024]
 [-0.009]
 [-0.006]
 [-0.002]
 [-0.002]
 [ 0.005]] [[37.977]
 [42.618]
 [32.263]
 [37.757]
 [32.703]
 [32.703]
 [39.187]] [[0.224]
 [0.311]
 [0.165]
 [0.228]
 [0.176]
 [0.176]
 [0.254]]
maxi score, test score, baseline:  0.043019999999999864 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  18.37380886077881
actor:  1 policy actor:  1  step number:  54 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  47 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.043019999999999864 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.62236880325257
printing an ep nov before normalisation:  54.25336745153925
printing an ep nov before normalisation:  39.85272671872948
maxi score, test score, baseline:  0.043019999999999864 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.043019999999999864 0.6903333333333335 0.6903333333333335
printing an ep nov before normalisation:  20.854561366914684
printing an ep nov before normalisation:  21.30640215373184
actor:  1 policy actor:  1  step number:  78 total reward:  0.04666666666666597  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  58 total reward:  0.35333333333333283  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  58 total reward:  0.36666666666666603  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.043019999999999864 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.043019999999999864 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.043019999999999864 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.043019999999999864 0.6903333333333335 0.6903333333333335
maxi score, test score, baseline:  0.043019999999999864 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.043019999999999864 0.6903333333333335 0.6903333333333335
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.72 ]
 [0.475]
 [0.528]
 [0.601]
 [0.56 ]
 [0.529]] [[27.108]
 [18.086]
 [28.326]
 [27.956]
 [27.746]
 [28.119]
 [27.528]] [[0.865]
 [0.919]
 [0.786]
 [0.836]
 [0.907]
 [0.869]
 [0.832]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.043019999999999864 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.30666666666666653  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  45 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.043019999999999864 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.043019999999999864 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  35.67140666237161
actions average: 
K:  1  action  0 :  tensor([0.4769, 0.0365, 0.0950, 0.0909, 0.0919, 0.0916, 0.1172],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0138, 0.9279, 0.0050, 0.0143, 0.0020, 0.0057, 0.0314],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1184, 0.0078, 0.3138, 0.0957, 0.1005, 0.1924, 0.1713],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2212, 0.0197, 0.1159, 0.1455, 0.1429, 0.1466, 0.2082],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1503, 0.0105, 0.0822, 0.1054, 0.4641, 0.0725, 0.1150],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1569, 0.0133, 0.0988, 0.0629, 0.0945, 0.4694, 0.1041],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2003, 0.0280, 0.1403, 0.1299, 0.1362, 0.1419, 0.2235],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.043019999999999864 0.6903333333333335 0.6903333333333335
maxi score, test score, baseline:  0.043019999999999864 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77559924
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.74 ]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]] [[38.447]
 [38.849]
 [38.447]
 [38.447]
 [38.447]
 [38.447]
 [38.447]] [[0.817]
 [0.995]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]]
printing an ep nov before normalisation:  31.105937957763672
maxi score, test score, baseline:  0.043019999999999864 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.3266666666666659  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  26.134438514709473
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]] [[40.544]
 [40.544]
 [40.544]
 [40.544]
 [40.544]
 [40.544]
 [40.544]] [[0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.043019999999999864 0.6903333333333335 0.6903333333333335
siam score:  -0.7742822
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  0.043019999999999864 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.043019999999999864 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.69980766303398
printing an ep nov before normalisation:  30.289246012303476
maxi score, test score, baseline:  0.043019999999999864 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.13999999999999924  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.043019999999999864 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.043019999999999864 0.6903333333333335 0.6903333333333335
printing an ep nov before normalisation:  34.46334857985926
maxi score, test score, baseline:  0.040246666666666514 0.6903333333333335 0.6903333333333335
maxi score, test score, baseline:  0.040246666666666514 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.040246666666666514 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  43 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.04020666666666652 0.6903333333333335 0.6903333333333335
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.365]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]] [[40.476]
 [50.208]
 [40.476]
 [40.476]
 [40.476]
 [40.476]
 [40.476]] [[0.951]
 [1.175]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.4866666666666668  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.478]
 [0.318]
 [0.318]
 [0.318]
 [0.318]
 [0.318]] [[29.555]
 [45.545]
 [29.555]
 [29.555]
 [29.555]
 [29.555]
 [29.555]] [[0.61 ]
 [1.146]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]]
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.544]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]] [[34.28 ]
 [47.028]
 [34.28 ]
 [34.28 ]
 [34.28 ]
 [34.28 ]
 [34.28 ]] [[0.836]
 [1.276]
 [0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.836]]
maxi score, test score, baseline:  0.04020666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04020666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  0.10666666666666613  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.04020666666666652 0.6903333333333335 0.6903333333333335
maxi score, test score, baseline:  0.04020666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.84952735900879
actor:  1 policy actor:  1  step number:  45 total reward:  0.4133333333333329  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.04020666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04020666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.167]
 [0.217]
 [0.167]
 [0.167]
 [0.167]
 [0.167]
 [0.167]] [[40.818]
 [44.813]
 [40.818]
 [40.818]
 [40.818]
 [40.818]
 [40.818]] [[1.257]
 [1.515]
 [1.257]
 [1.257]
 [1.257]
 [1.257]
 [1.257]]
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]] [[52.162]
 [52.162]
 [52.162]
 [52.162]
 [52.162]
 [52.162]
 [52.162]] [[1.9]
 [1.9]
 [1.9]
 [1.9]
 [1.9]
 [1.9]
 [1.9]]
maxi score, test score, baseline:  0.04020666666666652 0.6903333333333335 0.6903333333333335
maxi score, test score, baseline:  0.04020666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.463]
 [0.313]
 [0.313]
 [0.336]
 [0.313]
 [0.313]] [[32.807]
 [34.481]
 [32.807]
 [32.807]
 [35.898]
 [32.807]
 [32.807]] [[1.182]
 [1.419]
 [1.182]
 [1.182]
 [1.366]
 [1.182]
 [1.182]]
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.531]
 [0.482]
 [0.443]
 [0.482]
 [0.482]
 [0.482]] [[36.251]
 [40.331]
 [36.251]
 [40.285]
 [36.251]
 [36.251]
 [36.251]] [[1.553]
 [1.826]
 [1.553]
 [1.735]
 [1.553]
 [1.553]
 [1.553]]
maxi score, test score, baseline:  0.04020666666666652 0.6903333333333335 0.6903333333333335
maxi score, test score, baseline:  0.04020666666666652 0.6903333333333335 0.6903333333333335
actor:  1 policy actor:  1  step number:  67 total reward:  0.19999999999999885  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  49.17638093687329
printing an ep nov before normalisation:  31.81584119796753
actor:  1 policy actor:  1  step number:  61 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.04020666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.04020666666666652 0.6903333333333335 0.6903333333333335
Printing some Q and Qe and total Qs values:  [[0.27 ]
 [0.269]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]] [[36.13 ]
 [38.832]
 [36.13 ]
 [36.13 ]
 [36.13 ]
 [36.13 ]
 [36.13 ]] [[2.073]
 [2.269]
 [2.073]
 [2.073]
 [2.073]
 [2.073]
 [2.073]]
maxi score, test score, baseline:  0.04020666666666652 0.6903333333333335 0.6903333333333335
printing an ep nov before normalisation:  42.051416526995865
maxi score, test score, baseline:  0.04020666666666652 0.6903333333333335 0.6903333333333335
printing an ep nov before normalisation:  39.26626592959846
maxi score, test score, baseline:  0.04020666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.291]
 [0.318]
 [0.318]
 [0.318]
 [0.318]
 [0.318]] [[51.381]
 [50.62 ]
 [51.381]
 [51.381]
 [51.381]
 [51.381]
 [51.381]] [[1.931]
 [1.873]
 [1.931]
 [1.931]
 [1.931]
 [1.931]
 [1.931]]
maxi score, test score, baseline:  0.04020666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04020666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.50367102182683
Printing some Q and Qe and total Qs values:  [[0.599]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]] [[46.545]
 [50.548]
 [50.548]
 [50.548]
 [50.548]
 [50.548]
 [50.548]] [[2.354]
 [2.558]
 [2.558]
 [2.558]
 [2.558]
 [2.558]
 [2.558]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.04020666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.735193265363755
printing an ep nov before normalisation:  39.381242058036335
printing an ep nov before normalisation:  37.01694600923326
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.389]
 [0.372]] [[34.297]
 [34.297]
 [34.297]
 [34.297]
 [34.297]
 [39.628]
 [34.297]] [[1.428]
 [1.428]
 [1.428]
 [1.428]
 [1.428]
 [1.796]
 [1.428]]
printing an ep nov before normalisation:  32.017154693603516
maxi score, test score, baseline:  0.04020666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.942284256625257
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04020666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4600000000000001  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.04020666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04020666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.39009422624221
printing an ep nov before normalisation:  32.97475171885202
maxi score, test score, baseline:  0.04020666666666652 0.6903333333333335 0.6903333333333335
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.627]
 [0.673]
 [0.627]
 [0.627]
 [0.58 ]
 [0.627]] [[40.22 ]
 [40.22 ]
 [37.838]
 [40.22 ]
 [40.22 ]
 [44.584]
 [40.22 ]] [[1.941]
 [1.941]
 [1.831]
 [1.941]
 [1.941]
 [2.177]
 [1.941]]
printing an ep nov before normalisation:  46.20882993848829
printing an ep nov before normalisation:  21.539576435559198
Printing some Q and Qe and total Qs values:  [[1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]] [[29.323]
 [29.323]
 [29.323]
 [29.323]
 [29.323]
 [29.323]
 [29.323]] [[1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.44000000000000006  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  54 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  54 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  58.84790828420312
printing an ep nov before normalisation:  22.126278612348766
maxi score, test score, baseline:  0.03964666666666652 0.6903333333333335 0.6903333333333335
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.308]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]] [[33.397]
 [38.551]
 [33.397]
 [33.397]
 [33.397]
 [33.397]
 [33.397]] [[0.312]
 [0.308]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]]
siam score:  -0.7825453
maxi score, test score, baseline:  0.03964666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.01295797433656
maxi score, test score, baseline:  0.03964666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03964666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.219938682613872
printing an ep nov before normalisation:  30.348044841351445
Printing some Q and Qe and total Qs values:  [[0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]] [[42.524]
 [42.524]
 [42.524]
 [42.524]
 [42.524]
 [42.524]
 [42.524]] [[0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]]
actor:  1 policy actor:  1  step number:  41 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  0.667
siam score:  -0.78126085
maxi score, test score, baseline:  0.03964666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 50.43442043948441
Printing some Q and Qe and total Qs values:  [[0.122]
 [0.166]
 [0.071]
 [0.122]
 [0.122]
 [0.122]
 [0.165]] [[40.648]
 [42.659]
 [36.178]
 [40.648]
 [37.572]
 [40.648]
 [36.2  ]] [[1.305]
 [1.455]
 [1.017]
 [1.305]
 [1.142]
 [1.305]
 [1.112]]
maxi score, test score, baseline:  0.03964666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03964666666666652 0.6903333333333335 0.6903333333333335
actions average: 
K:  1  action  0 :  tensor([0.5164, 0.1025, 0.0711, 0.0631, 0.1320, 0.0507, 0.0642],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0047, 0.9333, 0.0079, 0.0176, 0.0024, 0.0069, 0.0273],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0577, 0.0123, 0.6750, 0.0450, 0.0425, 0.1183, 0.0492],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1000, 0.1445, 0.0733, 0.4005, 0.0742, 0.0936, 0.1139],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1795, 0.0020, 0.1617, 0.1352, 0.2257, 0.1939, 0.1021],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1378, 0.0104, 0.1773, 0.0936, 0.0858, 0.4037, 0.0915],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1100, 0.2583, 0.0723, 0.0893, 0.0718, 0.0780, 0.3202],
       grad_fn=<DivBackward0>)
actions average: 
K:  4  action  0 :  tensor([0.4670, 0.0103, 0.1089, 0.0982, 0.1232, 0.1066, 0.0859],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0132, 0.9059, 0.0105, 0.0163, 0.0155, 0.0216, 0.0170],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0820, 0.1039, 0.5209, 0.0411, 0.0382, 0.1839, 0.0301],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1937, 0.1910, 0.1531, 0.0975, 0.1374, 0.1343, 0.0929],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1101, 0.0351, 0.1674, 0.1399, 0.2671, 0.1482, 0.1322],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1531, 0.0022, 0.2232, 0.1300, 0.1275, 0.2480, 0.1160],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1899, 0.2362, 0.1426, 0.0989, 0.1189, 0.1137, 0.0998],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.03964666666666652 0.6903333333333335 0.6903333333333335
line 256 mcts: sample exp_bonus 47.097274770292586
maxi score, test score, baseline:  0.03964666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.17431452940172
actor:  0 policy actor:  0  step number:  67 total reward:  0.10666666666666602  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.123]
 [0.082]
 [0.062]
 [0.063]
 [0.059]
 [0.083]] [[52.833]
 [49.441]
 [50.443]
 [46.739]
 [46.834]
 [46.177]
 [46.848]] [[1.347]
 [1.265]
 [1.264]
 [1.1  ]
 [1.105]
 [1.075]
 [1.125]]
maxi score, test score, baseline:  0.038459999999999855 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  47.95068573907902
maxi score, test score, baseline:  0.038459999999999855 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.719]
 [0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]] [[39.118]
 [41.758]
 [39.118]
 [39.118]
 [39.118]
 [39.118]
 [39.118]] [[0.715]
 [0.719]
 [0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]]
siam score:  -0.77989364
printing an ep nov before normalisation:  23.721086978912354
maxi score, test score, baseline:  0.038459999999999855 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.038459999999999855 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.11335397339675
actor:  1 policy actor:  1  step number:  48 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  3  action  0 :  tensor([0.3444, 0.0129, 0.1100, 0.0947, 0.2156, 0.1075, 0.1149],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0175, 0.9495, 0.0045, 0.0053, 0.0064, 0.0026, 0.0143],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1312, 0.0707, 0.3726, 0.0920, 0.0885, 0.0794, 0.1657],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1827, 0.1069, 0.0847, 0.2940, 0.1314, 0.0843, 0.1159],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2144, 0.1145, 0.1018, 0.0600, 0.3326, 0.1095, 0.0672],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1875, 0.0089, 0.1628, 0.0728, 0.1280, 0.3814, 0.0586],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1081, 0.0771, 0.0975, 0.1409, 0.0949, 0.1013, 0.3802],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.038459999999999855 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.038459999999999855 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.728686739011664
siam score:  -0.7769396
maxi score, test score, baseline:  0.038459999999999855 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  54 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  43.90349899694233
printing an ep nov before normalisation:  33.89184580829865
maxi score, test score, baseline:  0.03757999999999985 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.74653344579869
maxi score, test score, baseline:  0.03757999999999985 0.6903333333333335 0.6903333333333335
maxi score, test score, baseline:  0.03757999999999985 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.174717108115455
maxi score, test score, baseline:  0.03757999999999985 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03757999999999985 0.6903333333333335 0.6903333333333335
siam score:  -0.77499944
maxi score, test score, baseline:  0.03757999999999985 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03757999999999985 0.6903333333333335 0.6903333333333335
actor:  0 policy actor:  0  step number:  44 total reward:  0.37999999999999945  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  53 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.03693999999999985 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.2999999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.337]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]] [[32.766]
 [33.259]
 [32.766]
 [32.766]
 [32.766]
 [32.766]
 [32.766]] [[1.85 ]
 [1.914]
 [1.85 ]
 [1.85 ]
 [1.85 ]
 [1.85 ]
 [1.85 ]]
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.116]
 [0.052]
 [0.047]
 [0.044]
 [0.043]
 [0.045]] [[19.41 ]
 [27.845]
 [18.976]
 [19.117]
 [18.826]
 [18.607]
 [19.039]] [[0.567]
 [1.115]
 [0.551]
 [0.554]
 [0.534]
 [0.522]
 [0.548]]
siam score:  -0.77254605
actions average: 
K:  4  action  0 :  tensor([0.4938, 0.0490, 0.0690, 0.0634, 0.1698, 0.0640, 0.0909],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0031, 0.9585, 0.0019, 0.0105, 0.0015, 0.0013, 0.0232],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1210, 0.0927, 0.3478, 0.0832, 0.1070, 0.1125, 0.1358],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1469, 0.0431, 0.1262, 0.2906, 0.1235, 0.1111, 0.1586],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.3414, 0.0006, 0.0746, 0.0437, 0.3408, 0.1539, 0.0450],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1727, 0.0282, 0.1765, 0.1263, 0.1172, 0.2764, 0.1026],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1611, 0.1242, 0.1454, 0.0998, 0.1244, 0.1007, 0.2444],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.35374155248235
maxi score, test score, baseline:  0.03693999999999985 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  45 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  40 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.03637999999999985 0.6903333333333335 0.6903333333333335
maxi score, test score, baseline:  0.03637999999999985 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03637999999999985 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.84204053878784
actions average: 
K:  4  action  0 :  tensor([0.4347, 0.0714, 0.0780, 0.1073, 0.1408, 0.0846, 0.0832],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0117, 0.8937, 0.0205, 0.0177, 0.0165, 0.0175, 0.0226],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1123, 0.0026, 0.1832, 0.1096, 0.1351, 0.2436, 0.2137],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1733, 0.0425, 0.1075, 0.2050, 0.1499, 0.1814, 0.1405],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1792, 0.0007, 0.1023, 0.0823, 0.5040, 0.0584, 0.0731],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1447, 0.0132, 0.1303, 0.1342, 0.1494, 0.2866, 0.1415],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2127, 0.0337, 0.1539, 0.0983, 0.1944, 0.1608, 0.1462],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.03637999999999985 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03637999999999985 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  46 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.035646666666666514 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.035646666666666514 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.035646666666666514 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.706]
 [0.631]
 [0.601]
 [0.596]
 [0.6  ]
 [0.624]] [[22.062]
 [23.157]
 [22.452]
 [22.565]
 [22.584]
 [23.236]
 [23.131]] [[1.242]
 [1.406]
 [1.289]
 [1.265]
 [1.262]
 [1.304]
 [1.322]]
maxi score, test score, baseline:  0.035646666666666514 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.095886138331224
actor:  1 policy actor:  1  step number:  69 total reward:  0.19999999999999918  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.013]
 [ 0.024]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.013]
 [ 0.024]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]]
Printing some Q and Qe and total Qs values:  [[-0.007]
 [ 0.029]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.005]
 [-0.007]] [[30.714]
 [32.592]
 [30.984]
 [31.88 ]
 [31.88 ]
 [30.976]
 [30.821]] [[0.42 ]
 [0.509]
 [0.428]
 [0.453]
 [0.453]
 [0.429]
 [0.423]]
printing an ep nov before normalisation:  37.06595131237649
maxi score, test score, baseline:  0.035646666666666514 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  48 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.57205498321122
printing an ep nov before normalisation:  34.89121841350493
printing an ep nov before normalisation:  21.88366550926849
printing an ep nov before normalisation:  33.149961674486036
maxi score, test score, baseline:  0.034806666666666514 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.034806666666666514 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  37 total reward:  0.666666666666667  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[ 0.011]
 [ 0.156]
 [-0.   ]
 [ 0.016]
 [-0.009]
 [-0.004]
 [ 0.04 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.011]
 [ 0.156]
 [-0.   ]
 [ 0.016]
 [-0.009]
 [-0.004]
 [ 0.04 ]]
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]
 [0.546]] [[44.492]
 [44.492]
 [44.492]
 [44.492]
 [44.492]
 [44.492]
 [44.492]] [[1.499]
 [1.499]
 [1.499]
 [1.499]
 [1.499]
 [1.499]
 [1.499]]
maxi score, test score, baseline:  0.034766666666666515 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.61493608103994
maxi score, test score, baseline:  0.034766666666666515 0.6903333333333335 0.6903333333333335
actor:  1 policy actor:  1  step number:  72 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.034766666666666515 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.328933238983154
maxi score, test score, baseline:  0.034766666666666515 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  81 total reward:  0.07999999999999896  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.557]
 [0.308]
 [0.516]
 [0.516]
 [0.516]
 [0.516]] [[47.974]
 [34.075]
 [40.144]
 [47.974]
 [47.974]
 [47.974]
 [47.974]] [[1.516]
 [1.152]
 [1.08 ]
 [1.516]
 [1.516]
 [1.516]
 [1.516]]
maxi score, test score, baseline:  0.034766666666666515 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.034766666666666515 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.37517425724313
maxi score, test score, baseline:  0.034766666666666515 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.499642589137647
actor:  1 policy actor:  1  step number:  58 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  49.51486307387118
actor:  1 policy actor:  1  step number:  39 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.034766666666666515 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
actor:  1 policy actor:  1  step number:  61 total reward:  0.15999999999999925  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.034766666666666515 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.691]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]] [[39.079]
 [45.315]
 [39.079]
 [39.079]
 [39.079]
 [39.079]
 [39.079]] [[1.117]
 [1.524]
 [1.117]
 [1.117]
 [1.117]
 [1.117]
 [1.117]]
maxi score, test score, baseline:  0.034766666666666515 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.75787782208289
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.67 ]
 [0.647]
 [0.647]
 [0.647]
 [0.433]
 [0.647]] [[38.296]
 [38.649]
 [38.296]
 [38.296]
 [38.296]
 [43.352]
 [38.296]] [[1.715]
 [1.756]
 [1.715]
 [1.715]
 [1.715]
 [1.766]
 [1.715]]
printing an ep nov before normalisation:  42.94868163753536
maxi score, test score, baseline:  0.034766666666666515 0.6903333333333335 0.6903333333333335
printing an ep nov before normalisation:  29.882173748677747
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.705]
 [0.579]
 [0.282]
 [0.583]
 [0.192]
 [0.602]] [[30.621]
 [25.274]
 [29.089]
 [23.979]
 [28.18 ]
 [21.549]
 [29.03 ]] [[1.538]
 [1.487]
 [1.479]
 [1.024]
 [1.454]
 [0.858]
 [1.5  ]]
maxi score, test score, baseline:  0.034766666666666515 0.6903333333333335 0.6903333333333335
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  61 total reward:  0.22666666666666602  reward:  1.0 rdn_beta:  1.0
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  62 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.03461999999999985 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.01094608047107
maxi score, test score, baseline:  0.03461999999999985 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03461999999999985 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  40 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.035233333333333186 0.6903333333333335 0.6903333333333335
maxi score, test score, baseline:  0.035233333333333186 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.035233333333333186 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  48 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  43.81244308982423
actions average: 
K:  0  action  0 :  tensor([0.5252, 0.0035, 0.1016, 0.0922, 0.0942, 0.0943, 0.0890],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0063, 0.9451, 0.0079, 0.0139, 0.0031, 0.0047, 0.0190],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1561, 0.0032, 0.3750, 0.1102, 0.1082, 0.1433, 0.1041],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1798, 0.0187, 0.1563, 0.1626, 0.1677, 0.1640, 0.1509],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1996, 0.0420, 0.0949, 0.0711, 0.4062, 0.0755, 0.1107],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0602, 0.0019, 0.0633, 0.0493, 0.0530, 0.7120, 0.0602],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1706, 0.1343, 0.1207, 0.1240, 0.1205, 0.1048, 0.2250],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  22.691987900479553
maxi score, test score, baseline:  0.03788666666666651 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.24666666666666648  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.03788666666666651 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03788666666666651 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.6624578812857
siam score:  -0.7713377
Printing some Q and Qe and total Qs values:  [[0.075]
 [0.232]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]] [[37.465]
 [37.233]
 [37.465]
 [37.465]
 [37.465]
 [37.465]
 [37.465]] [[1.609]
 [1.748]
 [1.609]
 [1.609]
 [1.609]
 [1.609]
 [1.609]]
maxi score, test score, baseline:  0.03788666666666651 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.400324821472168
printing an ep nov before normalisation:  11.614253520965576
maxi score, test score, baseline:  0.03788666666666651 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.44556732589603
actor:  1 policy actor:  1  step number:  59 total reward:  0.3733333333333335  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  35.66925654196259
actor:  1 policy actor:  1  step number:  49 total reward:  0.37333333333333285  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  40.23801953576765
UNIT TEST: sample policy line 217 mcts : [0.143 0.367 0.224 0.122 0.02  0.02  0.102]
maxi score, test score, baseline:  0.03788666666666651 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7668921
maxi score, test score, baseline:  0.03788666666666651 0.6903333333333335 0.6903333333333335
printing an ep nov before normalisation:  28.351460519714976
maxi score, test score, baseline:  0.03788666666666651 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03572666666666651 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03572666666666651 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.195]
 [0.114]
 [0.128]
 [0.121]
 [0.08 ]
 [0.266]] [[35.952]
 [36.378]
 [34.409]
 [34.669]
 [34.854]
 [34.463]
 [36.416]] [[1.415]
 [1.541]
 [1.289]
 [1.326]
 [1.335]
 [1.26 ]
 [1.616]]
printing an ep nov before normalisation:  28.44944530031474
actor:  1 policy actor:  1  step number:  51 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.033566666666666516 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.033566666666666516 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.786292615768446
actions average: 
K:  1  action  0 :  tensor([0.5056, 0.0363, 0.1185, 0.0838, 0.1015, 0.0895, 0.0649],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0024, 0.9623, 0.0059, 0.0026, 0.0011, 0.0011, 0.0247],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1778, 0.0229, 0.3147, 0.1244, 0.1271, 0.1254, 0.1076],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1470, 0.0402, 0.1175, 0.3084, 0.1378, 0.0920, 0.1571],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2030, 0.0051, 0.1460, 0.1109, 0.3302, 0.0997, 0.1050],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1190, 0.0065, 0.2202, 0.0877, 0.1110, 0.3727, 0.0829],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1840, 0.1068, 0.2372, 0.0809, 0.1073, 0.1195, 0.1642],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.033566666666666516 0.6903333333333335 0.6903333333333335
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  0.32666666666666677  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.033566666666666516 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.35999999999999943  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  37.215047593182035
printing an ep nov before normalisation:  29.155536219259098
printing an ep nov before normalisation:  18.04676502943039
maxi score, test score, baseline:  0.033566666666666516 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.033566666666666516 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.033566666666666516 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  45 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.03409999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.98720681477083
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.30135154724121
printing an ep nov before normalisation:  38.863004776874824
maxi score, test score, baseline:  0.03409999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03409999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.330372262463964
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  62 total reward:  0.11333333333333229  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.03409999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03409999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03409999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.87926763647979
maxi score, test score, baseline:  0.03409999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.33 ]
 [0.289]
 [0.293]
 [0.297]
 [0.297]
 [0.292]] [[27.023]
 [27.628]
 [26.691]
 [27.492]
 [27.61 ]
 [27.917]
 [27.179]] [[1.663]
 [1.775]
 [1.641]
 [1.724]
 [1.74 ]
 [1.771]
 [1.693]]
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.39 ]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]] [[29.343]
 [28.907]
 [29.343]
 [29.343]
 [29.343]
 [29.343]
 [29.343]] [[1.317]
 [1.326]
 [1.317]
 [1.317]
 [1.317]
 [1.317]
 [1.317]]
Printing some Q and Qe and total Qs values:  [[0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]] [[31.773]
 [31.773]
 [31.773]
 [31.773]
 [31.773]
 [31.773]
 [31.773]] [[0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.422]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.03409999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.487]
 [0.465]
 [0.456]
 [0.457]
 [0.454]
 [0.445]] [[36.488]
 [49.267]
 [45.157]
 [43.629]
 [36.584]
 [36.458]
 [45.397]] [[0.621]
 [0.754]
 [0.698]
 [0.675]
 [0.616]
 [0.612]
 [0.679]]
siam score:  -0.78683454
actor:  1 policy actor:  1  step number:  60 total reward:  0.3266666666666658  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.03409999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03409999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  70 total reward:  0.2199999999999993  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7822922
maxi score, test score, baseline:  0.03409999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.03409999999999986 0.6903333333333335 0.6903333333333335
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.286]
 [0.27 ]
 [0.259]
 [0.259]
 [0.255]
 [0.263]] [[35.795]
 [35.051]
 [35.412]
 [33.44 ]
 [33.44 ]
 [36.334]
 [36.941]] [[0.993]
 [0.983]
 [0.982]
 [0.882]
 [0.882]
 [1.009]
 [1.045]]
maxi score, test score, baseline:  0.03409999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03409999999999986 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 21.85396916085777
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.360539570108635
printing an ep nov before normalisation:  27.538247108459473
siam score:  -0.77485573
printing an ep nov before normalisation:  31.21821771539516
printing an ep nov before normalisation:  5.9982399704949785e-05
actor:  0 policy actor:  0  step number:  68 total reward:  0.019999999999999463  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  50 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.036139999999999846 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.036139999999999846 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.333996030107144
maxi score, test score, baseline:  0.036139999999999846 0.6903333333333335 0.6903333333333335
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  44 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  36.16217803548991
printing an ep nov before normalisation:  30.939080838415162
actor:  1 policy actor:  1  step number:  63 total reward:  0.18666666666666565  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.036139999999999846 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.59583308375921
maxi score, test score, baseline:  0.036139999999999846 0.6903333333333335 0.6903333333333335
maxi score, test score, baseline:  0.036139999999999846 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  0.0999999999999992  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.036139999999999846 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.036139999999999846 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.036139999999999846 0.6903333333333335 0.6903333333333335
UNIT TEST: sample policy line 217 mcts : [0.061 0.571 0.041 0.061 0.041 0.041 0.184]
printing an ep nov before normalisation:  31.66073273840261
maxi score, test score, baseline:  0.036139999999999846 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.036139999999999846 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  38.285850230470615
siam score:  -0.7750679
maxi score, test score, baseline:  0.036139999999999846 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.516]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]] [[41.689]
 [36.337]
 [41.689]
 [41.689]
 [41.689]
 [41.689]
 [41.689]] [[1.797]
 [1.569]
 [1.797]
 [1.797]
 [1.797]
 [1.797]
 [1.797]]
actor:  0 policy actor:  0  step number:  49 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.668]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]] [[29.303]
 [34.459]
 [29.303]
 [29.303]
 [29.303]
 [29.303]
 [29.303]] [[0.516]
 [0.668]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]]
printing an ep nov before normalisation:  50.74480229582781
maxi score, test score, baseline:  0.03888666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.95847110104819
using explorer policy with actor:  1
siam score:  -0.7739612
maxi score, test score, baseline:  0.03888666666666652 0.6903333333333335 0.6903333333333335
printing an ep nov before normalisation:  47.395050852598416
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.191]
 [0.107]
 [0.107]
 [0.107]
 [0.084]
 [0.094]] [[33.094]
 [35.763]
 [33.094]
 [33.094]
 [33.094]
 [34.82 ]
 [34.622]] [[0.893]
 [1.101]
 [0.893]
 [0.893]
 [0.893]
 [0.95 ]
 [0.951]]
maxi score, test score, baseline:  0.03888666666666652 0.6903333333333335 0.6903333333333335
maxi score, test score, baseline:  0.03888666666666652 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  54 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.352]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]] [[26.599]
 [30.785]
 [26.599]
 [26.599]
 [26.599]
 [26.599]
 [26.599]] [[0.59 ]
 [0.685]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]]
Starting evaluation
printing an ep nov before normalisation:  26.054701805114746
Printing some Q and Qe and total Qs values:  [[0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]]
maxi score, test score, baseline:  0.04127333333333318 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  28.72560774248675
Printing some Q and Qe and total Qs values:  [[0.377]
 [0.271]
 [0.271]
 [0.271]
 [0.339]
 [0.271]
 [0.271]] [[43.198]
 [44.958]
 [44.958]
 [44.958]
 [43.966]
 [44.958]
 [44.958]] [[0.947]
 [0.874]
 [0.874]
 [0.874]
 [0.923]
 [0.874]
 [0.874]]
maxi score, test score, baseline:  0.04127333333333318 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.303041246494146
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.361]
 [0.361]
 [0.361]
 [0.492]
 [0.361]
 [0.361]] [[39.25 ]
 [36.293]
 [36.293]
 [36.293]
 [39.065]
 [36.293]
 [36.293]] [[0.552]
 [0.361]
 [0.361]
 [0.361]
 [0.492]
 [0.361]
 [0.361]]
printing an ep nov before normalisation:  25.34106837378608
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.904261764354345
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]] [[29.544]
 [29.544]
 [29.544]
 [29.544]
 [29.544]
 [29.544]
 [29.544]] [[0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  37.67915563079499
maxi score, test score, baseline:  0.04127333333333318 0.6903333333333335 0.6903333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.4800000000000002  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.10482049828906
printing an ep nov before normalisation:  35.98665316821524
Printing some Q and Qe and total Qs values:  [[0.952]
 [0.991]
 [0.877]
 [0.952]
 [0.952]
 [0.891]
 [0.952]] [[31.952]
 [35.842]
 [34.169]
 [31.952]
 [31.952]
 [36.702]
 [31.952]] [[0.952]
 [0.991]
 [0.877]
 [0.952]
 [0.952]
 [0.891]
 [0.952]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
Printing some Q and Qe and total Qs values:  [[0.846]
 [0.868]
 [0.815]
 [0.822]
 [0.818]
 [0.826]
 [0.821]] [[32.694]
 [32.225]
 [32.686]
 [33.465]
 [28.955]
 [32.377]
 [32.172]] [[0.846]
 [0.868]
 [0.815]
 [0.822]
 [0.818]
 [0.826]
 [0.821]]
printing an ep nov before normalisation:  28.49654978946678
printing an ep nov before normalisation:  32.416962276531194
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]] [[43.809]
 [43.809]
 [43.809]
 [43.809]
 [43.809]
 [43.809]
 [43.809]] [[0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  27 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  0.08557999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.358086824417114
maxi score, test score, baseline:  0.08557999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.389]
 [0.381]
 [0.381]
 [0.381]
 [0.381]
 [0.381]] [[37.546]
 [42.402]
 [37.546]
 [37.546]
 [37.546]
 [37.546]
 [37.546]] [[0.661]
 [0.722]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]]
maxi score, test score, baseline:  0.08557999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.6270, 0.0469, 0.0578, 0.0578, 0.0731, 0.0849, 0.0525],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0057, 0.9655, 0.0034, 0.0063, 0.0020, 0.0020, 0.0152],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1874, 0.0274, 0.2571, 0.1183, 0.1168, 0.1811, 0.1119],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0988, 0.1132, 0.0947, 0.4164, 0.0790, 0.0861, 0.1117],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3050, 0.0779, 0.1021, 0.0962, 0.2142, 0.0979, 0.1067],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0675, 0.0011, 0.2262, 0.0210, 0.0202, 0.6496, 0.0143],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1373, 0.1650, 0.1168, 0.1468, 0.0930, 0.0920, 0.2491],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  20.3673007834333
maxi score, test score, baseline:  0.08557999999999986 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  0.08557999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08557999999999986 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  0.08557999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08557999999999986 0.6860000000000002 0.6860000000000002
actor:  1 policy actor:  1  step number:  58 total reward:  0.24666666666666637  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.08557999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.723]
 [0.725]
 [0.725]
 [0.725]
 [0.725]
 [0.696]
 [0.725]] [[24.438]
 [27.236]
 [23.867]
 [21.522]
 [22.998]
 [26.55 ]
 [29.128]] [[0.959]
 [1.019]
 [0.95 ]
 [0.902]
 [0.932]
 [0.976]
 [1.058]]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [ 0.133]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.012]
 [-0.003]] [[37.194]
 [33.595]
 [37.194]
 [37.194]
 [37.194]
 [44.046]
 [37.194]] [[0.847]
 [0.823]
 [0.847]
 [0.847]
 [0.847]
 [1.143]
 [0.847]]
printing an ep nov before normalisation:  36.915154457092285
actor:  1 policy actor:  1  step number:  67 total reward:  0.013333333333332864  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.08557999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08557999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  52 total reward:  0.32666666666666655  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  36.3680362701416
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]] [[33.421]
 [33.421]
 [33.421]
 [33.421]
 [33.421]
 [33.421]
 [33.421]] [[1.596]
 [1.596]
 [1.596]
 [1.596]
 [1.596]
 [1.596]
 [1.596]]
maxi score, test score, baseline:  0.08823333333333319 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.167]
 [0.249]
 [0.167]
 [0.167]
 [0.167]
 [0.153]
 [0.167]] [[37.802]
 [30.237]
 [37.802]
 [37.802]
 [37.802]
 [36.719]
 [37.802]] [[1.431]
 [1.021]
 [1.431]
 [1.431]
 [1.431]
 [1.347]
 [1.431]]
maxi score, test score, baseline:  0.08823333333333319 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08823333333333319 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  32.75362262548802
printing an ep nov before normalisation:  10.738730430603027
maxi score, test score, baseline:  0.08823333333333319 0.6860000000000002 0.6860000000000002
siam score:  -0.7753655
actor:  1 policy actor:  1  step number:  51 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.044311712989824
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.461899018399066
siam score:  -0.7752408
maxi score, test score, baseline:  0.08823333333333319 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  0.08823333333333319 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08823333333333319 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.379489774095056
actor:  1 policy actor:  1  step number:  57 total reward:  0.21333333333333315  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  30.268013739754856
printing an ep nov before normalisation:  36.97991084925328
maxi score, test score, baseline:  0.08823333333333319 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.459]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]] [[34.572]
 [36.072]
 [34.572]
 [34.572]
 [34.572]
 [34.572]
 [34.572]] [[1.411]
 [1.437]
 [1.411]
 [1.411]
 [1.411]
 [1.411]
 [1.411]]
maxi score, test score, baseline:  0.08823333333333319 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.88051927287409
printing an ep nov before normalisation:  39.396383701990445
printing an ep nov before normalisation:  31.76125870373418
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.001]
 [0.   ]
 [0.668]
 [0.668]
 [0.668]
 [0.   ]] [[20.797]
 [ 0.028]
 [ 0.02 ]
 [20.797]
 [20.797]
 [20.797]
 [ 0.019]] [[0.924]
 [0.001]
 [0.   ]
 [0.924]
 [0.924]
 [0.924]
 [0.   ]]
actor:  1 policy actor:  1  step number:  79 total reward:  0.0933333333333326  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.08823333333333319 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08823333333333319 0.6860000000000002 0.6860000000000002
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08823333333333319 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  75 total reward:  0.1333333333333322  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.08823333333333319 0.6860000000000002 0.6860000000000002
printing an ep nov before normalisation:  0.2313854100577828
actor:  1 policy actor:  1  step number:  62 total reward:  0.15333333333333266  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  60.79678949761781
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.57 ]
 [0.511]
 [0.559]
 [0.559]
 [0.502]
 [0.559]] [[39.911]
 [38.544]
 [37.304]
 [39.911]
 [39.911]
 [41.504]
 [39.911]] [[1.885]
 [1.818]
 [1.687]
 [1.885]
 [1.885]
 [1.92 ]
 [1.885]]
printing an ep nov before normalisation:  24.886038303375244
maxi score, test score, baseline:  0.08823333333333319 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08823333333333319 0.6860000000000002 0.6860000000000002
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.34 ]
 [0.294]
 [0.294]
 [0.294]
 [0.263]
 [0.294]] [[30.541]
 [31.631]
 [30.541]
 [30.541]
 [30.541]
 [34.352]
 [30.541]] [[1.046]
 [1.144]
 [1.046]
 [1.046]
 [1.046]
 [1.195]
 [1.046]]
maxi score, test score, baseline:  0.08823333333333319 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.2866666666666666  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.531]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]] [[27.479]
 [32.327]
 [27.479]
 [27.479]
 [27.479]
 [27.479]
 [27.479]] [[0.913]
 [1.253]
 [0.913]
 [0.913]
 [0.913]
 [0.913]
 [0.913]]
printing an ep nov before normalisation:  29.784055510742952
actor:  1 policy actor:  1  step number:  42 total reward:  0.5400000000000001  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.043]
 [0.043]
 [0.043]
 [0.043]
 [0.043]
 [0.043]] [[48.977]
 [37.027]
 [37.027]
 [37.027]
 [37.027]
 [37.027]
 [37.027]] [[1.736]
 [1.157]
 [1.157]
 [1.157]
 [1.157]
 [1.157]
 [1.157]]
printing an ep nov before normalisation:  42.46730654233754
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.205]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]] [[37.524]
 [41.652]
 [37.524]
 [37.524]
 [37.524]
 [37.524]
 [37.524]] [[0.9  ]
 [1.033]
 [0.9  ]
 [0.9  ]
 [0.9  ]
 [0.9  ]
 [0.9  ]]
Printing some Q and Qe and total Qs values:  [[0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.241]] [[46.674]
 [46.674]
 [46.674]
 [46.674]
 [46.674]
 [46.674]
 [46.674]] [[1.232]
 [1.232]
 [1.232]
 [1.232]
 [1.232]
 [1.232]
 [1.232]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.38666666666666616  reward:  1.0 rdn_beta:  1.0
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  43.61728612682494
printing an ep nov before normalisation:  20.668477476307594
maxi score, test score, baseline:  0.08823333333333319 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.04960508437958
Printing some Q and Qe and total Qs values:  [[0.899]
 [0.978]
 [0.899]
 [0.899]
 [0.899]
 [0.779]
 [0.899]] [[34.309]
 [36.268]
 [34.309]
 [34.309]
 [34.309]
 [34.791]
 [34.309]] [[0.899]
 [0.978]
 [0.899]
 [0.899]
 [0.899]
 [0.779]
 [0.899]]
maxi score, test score, baseline:  0.08823333333333319 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.059107780456543
maxi score, test score, baseline:  0.08823333333333319 0.6860000000000002 0.6860000000000002
printing an ep nov before normalisation:  30.636807924744367
maxi score, test score, baseline:  0.08823333333333319 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 38.04800380045879
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.80057380820944
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7814553
printing an ep nov before normalisation:  37.31970897836882
actor:  1 policy actor:  1  step number:  67 total reward:  0.17333333333333256  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  29.596148304625316
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.09999999999999942  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.633]
 [0.542]
 [0.57 ]
 [0.542]
 [0.542]
 [0.542]] [[41.613]
 [40.072]
 [34.203]
 [40.112]
 [34.203]
 [34.203]
 [34.203]] [[1.881]
 [1.84 ]
 [1.438]
 [1.779]
 [1.438]
 [1.438]
 [1.438]]
printing an ep nov before normalisation:  25.77857734895958
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.072]
 [ 0.07 ]
 [ 0.013]
 [-0.007]
 [-0.134]
 [ 0.026]
 [-0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.072]
 [ 0.07 ]
 [ 0.013]
 [-0.007]
 [-0.134]
 [ 0.026]
 [-0.001]]
printing an ep nov before normalisation:  34.13510663168771
siam score:  -0.77940017
printing an ep nov before normalisation:  34.26943389741594
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.4066666666666666  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.78006256
printing an ep nov before normalisation:  31.690996885299683
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.575]
 [0.483]
 [0.483]
 [0.483]
 [0.483]
 [0.483]] [[33.557]
 [34.402]
 [33.557]
 [33.557]
 [33.557]
 [33.557]
 [33.557]] [[1.303]
 [1.435]
 [1.303]
 [1.303]
 [1.303]
 [1.303]
 [1.303]]
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.21089525651702
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7754226
actor:  1 policy actor:  1  step number:  56 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  39 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.491]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]] [[41.981]
 [43.728]
 [41.981]
 [41.981]
 [41.981]
 [41.981]
 [41.981]] [[1.459]
 [1.664]
 [1.459]
 [1.459]
 [1.459]
 [1.459]
 [1.459]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  62 total reward:  0.19333333333333258  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.693]
 [0.651]
 [0.651]
 [0.524]
 [0.532]
 [0.651]] [[38.799]
 [40.892]
 [38.799]
 [38.799]
 [42.023]
 [42.149]
 [38.799]] [[1.549]
 [1.668]
 [1.549]
 [1.549]
 [1.541]
 [1.554]
 [1.549]]
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.63259551780055
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.189504623413086
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.373]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]] [[36.27 ]
 [31.878]
 [36.27 ]
 [36.27 ]
 [36.27 ]
 [36.27 ]
 [36.27 ]] [[1.078]
 [1.002]
 [1.078]
 [1.078]
 [1.078]
 [1.078]
 [1.078]]
printing an ep nov before normalisation:  42.309387437008596
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]] [[22.285]
 [22.285]
 [22.285]
 [22.285]
 [22.285]
 [22.285]
 [22.285]] [[0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]
 [0.58]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  34.915480538509954
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.53 ]
 [0.615]
 [0.406]
 [0.563]
 [0.539]
 [0.399]] [[30.892]
 [34.437]
 [31.318]
 [29.997]
 [31.159]
 [35.9  ]
 [35.899]] [[1.076]
 [1.102]
 [1.086]
 [0.834]
 [1.028]
 [1.158]
 [1.019]]
printing an ep nov before normalisation:  30.25107718110159
printing an ep nov before normalisation:  32.368978963397225
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.98247375558034
printing an ep nov before normalisation:  32.62780983136584
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.936463379924376
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.239217801609037
actor:  0 policy actor:  0  step number:  52 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  46 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  40.01345715441124
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.96746471967467
Printing some Q and Qe and total Qs values:  [[0.276]
 [0.276]
 [0.276]
 [0.276]
 [0.276]
 [0.276]
 [0.276]] [[30.695]
 [30.695]
 [30.695]
 [30.695]
 [30.695]
 [30.695]
 [30.695]] [[0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]]
printing an ep nov before normalisation:  43.12629368265198
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.20895702664062
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
actor:  1 policy actor:  1  step number:  48 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  0.27333333333333243  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.16 ]
 [0.163]
 [0.123]
 [0.127]
 [0.127]
 [0.124]
 [0.127]] [[29.832]
 [32.126]
 [26.209]
 [26.395]
 [26.342]
 [26.2  ]
 [26.179]] [[1.136]
 [1.305]
 [0.835]
 [0.852]
 [0.849]
 [0.835]
 [0.837]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
actor:  1 policy actor:  1  step number:  50 total reward:  0.38  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.302]
 [0.302]
 [0.302]
 [0.257]
 [0.302]
 [0.302]] [[38.291]
 [36.046]
 [36.046]
 [36.046]
 [41.837]
 [36.046]
 [36.046]] [[1.503]
 [1.409]
 [1.409]
 [1.409]
 [1.711]
 [1.409]
 [1.409]]
printing an ep nov before normalisation:  30.325665090023225
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.9729521633216
actor:  1 policy actor:  1  step number:  44 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  60 total reward:  0.23333333333333317  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.21165895462036
actor:  1 policy actor:  1  step number:  50 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.964937210083008
actions average: 
K:  1  action  0 :  tensor([0.4622, 0.0017, 0.0818, 0.1139, 0.1585, 0.0875, 0.0943],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0043, 0.9662, 0.0054, 0.0030, 0.0011, 0.0010, 0.0190],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1284, 0.0040, 0.4298, 0.0911, 0.1031, 0.1716, 0.0720],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0819, 0.1729, 0.0649, 0.4029, 0.0961, 0.0719, 0.1094],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1482, 0.0483, 0.0923, 0.1106, 0.4362, 0.1012, 0.0633],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([    0.0893,     0.0004,     0.1038,     0.0656,     0.0633,     0.6275,
            0.0500], grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1202, 0.1559, 0.0615, 0.1809, 0.0847, 0.0509, 0.3459],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.016]
 [ 0.022]
 [-0.014]
 [-0.005]
 [-0.005]
 [-0.014]
 [-0.005]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.016]
 [ 0.022]
 [-0.014]
 [-0.005]
 [-0.005]
 [-0.014]
 [-0.005]]
printing an ep nov before normalisation:  17.669946257488164
printing an ep nov before normalisation:  22.343933731995847
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  73 total reward:  0.11999999999999922  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.179]
 [0.208]
 [0.179]
 [0.179]
 [0.167]
 [0.179]
 [0.168]] [[33.233]
 [34.57 ]
 [33.233]
 [33.233]
 [26.305]
 [33.233]
 [26.068]] [[0.438]
 [0.487]
 [0.438]
 [0.438]
 [0.322]
 [0.438]
 [0.32 ]]
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.82129236868259
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.444596235121786
maxi score, test score, baseline:  0.08561999999999986 0.6860000000000002 0.6860000000000002
Printing some Q and Qe and total Qs values:  [[0.767]
 [0.772]
 [0.701]
 [0.701]
 [0.701]
 [0.701]
 [0.701]] [[42.237]
 [38.339]
 [39.4  ]
 [39.4  ]
 [39.4  ]
 [39.4  ]
 [39.4  ]] [[2.294]
 [2.072]
 [2.064]
 [2.064]
 [2.064]
 [2.064]
 [2.064]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  48 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  43.04174501197191
maxi score, test score, baseline:  0.08853999999999985 0.6860000000000002 0.6860000000000002
printing an ep nov before normalisation:  34.50098204327459
maxi score, test score, baseline:  0.08853999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08853999999999985 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  0.08853999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08853999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08853999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.5133752822876
siam score:  -0.76130813
maxi score, test score, baseline:  0.08853999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.01623467831851
maxi score, test score, baseline:  0.08853999999999985 0.6860000000000002 0.6860000000000002
actor:  1 policy actor:  1  step number:  40 total reward:  0.6200000000000003  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  43 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08853999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08853999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08853999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.22666666666666602  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08557999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.87385484252325
maxi score, test score, baseline:  0.08557999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08557999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.193594977714554
printing an ep nov before normalisation:  36.613759994506836
maxi score, test score, baseline:  0.08557999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.00205866943122
Printing some Q and Qe and total Qs values:  [[0.747]
 [0.779]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]] [[26.873]
 [26.985]
 [26.873]
 [26.873]
 [26.873]
 [26.873]
 [26.873]] [[0.747]
 [0.779]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]]
printing an ep nov before normalisation:  28.6065411567688
maxi score, test score, baseline:  0.08557999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  39 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  60 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.491]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]] [[29.275]
 [38.43 ]
 [29.275]
 [29.275]
 [29.275]
 [29.275]
 [29.275]] [[0.683]
 [1.005]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]]
maxi score, test score, baseline:  0.08853999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08853999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7638314
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.19]
 [0.19]
 [0.19]
 [0.19]
 [0.19]
 [0.19]
 [0.19]] [[47.478]
 [47.478]
 [47.478]
 [47.478]
 [47.478]
 [47.478]
 [47.478]] [[0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.436]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.08853999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08853999999999985 0.6860000000000002 0.6860000000000002
Printing some Q and Qe and total Qs values:  [[-0.009]
 [-0.002]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]] [[49.74 ]
 [53.763]
 [42.111]
 [42.111]
 [42.111]
 [42.111]
 [42.111]] [[1.27 ]
 [1.442]
 [0.961]
 [0.961]
 [0.961]
 [0.961]
 [0.961]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.08853999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.76001757692572
printing an ep nov before normalisation:  33.80190206590031
printing an ep nov before normalisation:  45.11604752459968
maxi score, test score, baseline:  0.08853999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08853999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08853999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.35018643159227
actor:  0 policy actor:  0  step number:  46 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  34.19514731173859
maxi score, test score, baseline:  0.08828666666666653 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08828666666666653 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.863]
 [0.944]
 [0.863]
 [0.863]
 [0.863]
 [0.863]
 [0.863]] [[31.507]
 [32.212]
 [31.507]
 [31.507]
 [31.507]
 [31.507]
 [31.507]] [[0.863]
 [0.944]
 [0.863]
 [0.863]
 [0.863]
 [0.863]
 [0.863]]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5666666666666668  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.023704949740264
printing an ep nov before normalisation:  29.58402834522605
maxi score, test score, baseline:  0.08828666666666653 0.6860000000000002 0.6860000000000002
actor:  0 policy actor:  0  step number:  56 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0876333333333332 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0876333333333332 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  43 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  24.665099000603863
printing an ep nov before normalisation:  32.90732171036745
actor:  1 policy actor:  1  step number:  48 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  27.676264444986977
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.203]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]] [[23.746]
 [32.932]
 [23.746]
 [23.746]
 [23.746]
 [23.746]
 [23.746]] [[0.35 ]
 [0.677]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.35 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08744666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.1129463515243
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.42295096111504
Printing some Q and Qe and total Qs values:  [[0.149]
 [0.196]
 [0.159]
 [0.146]
 [0.145]
 [0.162]
 [0.132]] [[36.904]
 [36.159]
 [36.603]
 [37.162]
 [37.334]
 [35.875]
 [36.951]] [[0.65 ]
 [0.677]
 [0.652]
 [0.653]
 [0.657]
 [0.636]
 [0.634]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.08744666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08744666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.37655210494995
maxi score, test score, baseline:  0.08744666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  47 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0900333333333332 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  0.05999999999999939  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0900333333333332 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  0.0900333333333332 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5400000000000003  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0900333333333332 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  34.2520420796758
maxi score, test score, baseline:  0.0900333333333332 0.6860000000000002 0.6860000000000002
printing an ep nov before normalisation:  36.56016879031111
printing an ep nov before normalisation:  30.349840192468804
siam score:  -0.7695787
printing an ep nov before normalisation:  25.015183929022022
maxi score, test score, baseline:  0.0900333333333332 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 19.899156807466287
printing an ep nov before normalisation:  0.015315486714939652
maxi score, test score, baseline:  0.0900333333333332 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4600000000000001  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.083]
 [-0.073]
 [-0.083]
 [-0.079]
 [-0.076]
 [-0.075]
 [-0.073]] [[7.556]
 [5.977]
 [4.419]
 [4.591]
 [3.945]
 [5.567]
 [4.029]] [[0.158]
 [0.117]
 [0.058]
 [0.067]
 [0.05 ]
 [0.103]
 [0.056]]
Printing some Q and Qe and total Qs values:  [[0.23 ]
 [0.285]
 [0.229]
 [0.237]
 [0.237]
 [0.257]
 [0.236]] [[22.105]
 [26.893]
 [22.908]
 [22.078]
 [22.377]
 [25.132]
 [22.513]] [[0.362]
 [0.481]
 [0.372]
 [0.369]
 [0.372]
 [0.429]
 [0.373]]
actions average: 
K:  2  action  0 :  tensor([0.5479, 0.0123, 0.0868, 0.0854, 0.1076, 0.0798, 0.0803],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0063, 0.9241, 0.0054, 0.0081, 0.0014, 0.0026, 0.0521],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2252, 0.0084, 0.3582, 0.1035, 0.1071, 0.0876, 0.1100],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1665, 0.0521, 0.1054, 0.2901, 0.1304, 0.1058, 0.1498],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1801, 0.1161, 0.0807, 0.1443, 0.2711, 0.1270, 0.0806],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0945, 0.0026, 0.0916, 0.0768, 0.0763, 0.5827, 0.0754],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1423, 0.0937, 0.1025, 0.1170, 0.1066, 0.0953, 0.3426],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  36.603447183202086
printing an ep nov before normalisation:  29.923765882126467
printing an ep nov before normalisation:  26.24607433523144
printing an ep nov before normalisation:  26.52525184999112
actor:  1 policy actor:  1  step number:  72 total reward:  0.04666666666666597  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 28.722706892354857
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.825]
 [0.416]
 [0.655]
 [0.73 ]
 [0.722]
 [0.656]] [[23.25 ]
 [18.071]
 [23.422]
 [23.202]
 [23.159]
 [23.291]
 [24.227]] [[2.056]
 [1.899]
 [1.807]
 [2.033]
 [2.105]
 [2.105]
 [2.095]]
printing an ep nov before normalisation:  0.0
actor:  1 policy actor:  1  step number:  55 total reward:  0.23999999999999932  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0900333333333332 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.24666666666666648  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0900333333333332 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0900333333333332 0.6860000000000002 0.6860000000000002
printing an ep nov before normalisation:  21.509326220857105
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.519]
 [0.341]
 [0.341]
 [0.341]
 [0.334]
 [0.341]] [[30.669]
 [27.739]
 [29.5  ]
 [29.5  ]
 [29.5  ]
 [32.516]
 [29.5  ]] [[1.06 ]
 [1.101]
 [0.999]
 [0.999]
 [0.999]
 [1.124]
 [0.999]]
maxi score, test score, baseline:  0.0900333333333332 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.149770701016074
printing an ep nov before normalisation:  45.8010584607942
actor:  1 policy actor:  1  step number:  56 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0900333333333332 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0900333333333332 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.057]
 [0.102]
 [0.078]
 [0.077]
 [0.075]
 [0.06 ]
 [0.058]] [[26.273]
 [29.203]
 [26.908]
 [26.098]
 [26.612]
 [25.538]
 [25.482]] [[0.261]
 [0.35 ]
 [0.292]
 [0.279]
 [0.284]
 [0.253]
 [0.25 ]]
actor:  1 policy actor:  1  step number:  78 total reward:  0.16666666666666552  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0900333333333332 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.84891480811878
Printing some Q and Qe and total Qs values:  [[ 0.281]
 [ 0.374]
 [-0.036]
 [ 0.175]
 [ 0.175]
 [ 0.281]
 [ 0.281]] [[29.589]
 [36.604]
 [39.45 ]
 [28.859]
 [31.817]
 [29.589]
 [29.589]] [[0.428]
 [0.598]
 [0.218]
 [0.314]
 [0.346]
 [0.428]
 [0.428]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  44 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0900333333333332 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0900333333333332 0.6860000000000002 0.6860000000000002
actor:  1 policy actor:  1  step number:  34 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0900333333333332 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.535]
 [0.346]
 [0.346]] [[37.987]
 [37.987]
 [37.987]
 [37.987]
 [45.257]
 [37.987]
 [37.987]] [[1.38]
 [1.38]
 [1.38]
 [1.38]
 [1.99]
 [1.38]
 [1.38]]
printing an ep nov before normalisation:  53.708569433322126
maxi score, test score, baseline:  0.0900333333333332 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  0.0900333333333332 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09003333333333319 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.6333333333333337  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09003333333333319 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77058077
actor:  0 policy actor:  0  step number:  37 total reward:  0.52  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09013999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.42010811146639
maxi score, test score, baseline:  0.09013999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09013999999999987 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  0.09013999999999987 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.89124275455583
maxi score, test score, baseline:  0.09013999999999987 0.6860000000000002 0.6860000000000002
actor:  1 policy actor:  1  step number:  59 total reward:  0.11999999999999966  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  34.86810609872435
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09013999999999987 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09013999999999987 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.28866215320137
actor:  1 policy actor:  1  step number:  46 total reward:  0.40666666666666607  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  45 total reward:  0.4666666666666668  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09013999999999987 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.292104328656897
actor:  1 policy actor:  1  step number:  55 total reward:  0.3466666666666661  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  0.0005839454914280395
actor:  0 policy actor:  0  step number:  52 total reward:  0.21999999999999975  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09016666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09016666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.731]
 [0.769]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]] [[29.053]
 [36.828]
 [29.053]
 [29.053]
 [29.053]
 [29.053]
 [29.053]] [[0.731]
 [0.769]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]]
printing an ep nov before normalisation:  50.12913220962012
siam score:  -0.7778024
Printing some Q and Qe and total Qs values:  [[0.145]
 [0.189]
 [0.145]
 [0.145]
 [0.145]
 [0.09 ]
 [0.145]] [[40.654]
 [31.897]
 [40.654]
 [40.654]
 [40.654]
 [31.028]
 [40.654]] [[2.192]
 [1.522]
 [2.192]
 [2.192]
 [2.192]
 [1.352]
 [2.192]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09016666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.469]
 [0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]] [[35.11 ]
 [37.008]
 [35.11 ]
 [35.11 ]
 [35.11 ]
 [35.11 ]
 [35.11 ]] [[1.602]
 [1.802]
 [1.602]
 [1.602]
 [1.602]
 [1.602]
 [1.602]]
printing an ep nov before normalisation:  39.39611419466851
maxi score, test score, baseline:  0.09016666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  57 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0894333333333332 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77747655
maxi score, test score, baseline:  0.0894333333333332 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0894333333333332 0.6860000000000002 0.6860000000000002
actor:  1 policy actor:  1  step number:  45 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  38 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  36.74333754443415
Printing some Q and Qe and total Qs values:  [[0.736]
 [0.759]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]] [[27.933]
 [29.856]
 [27.933]
 [27.933]
 [27.933]
 [27.933]
 [27.933]] [[0.736]
 [0.759]
 [0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]]
printing an ep nov before normalisation:  43.85988237038207
siam score:  -0.77740914
maxi score, test score, baseline:  0.0894333333333332 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0894333333333332 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.359927654266357
printing an ep nov before normalisation:  29.618090572885947
actor:  0 policy actor:  0  step number:  43 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  44 total reward:  0.54  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  29.603204295684275
maxi score, test score, baseline:  0.09217999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.38  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 34.469059109687805
maxi score, test score, baseline:  0.09217999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09217999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.285]
 [0.262]
 [0.262]
 [0.241]
 [0.262]
 [0.262]] [[46.345]
 [43.046]
 [42.121]
 [42.121]
 [47.092]
 [42.121]
 [42.121]] [[1.088]
 [1.001]
 [0.948]
 [0.948]
 [1.085]
 [0.948]
 [0.948]]
printing an ep nov before normalisation:  33.33842992782593
maxi score, test score, baseline:  0.09217999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09217999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.94711293688541
actor:  0 policy actor:  0  step number:  53 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  46 total reward:  0.4333333333333329  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]] [[37.201]
 [37.201]
 [37.201]
 [37.201]
 [37.201]
 [37.201]
 [37.201]] [[0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]]
printing an ep nov before normalisation:  28.474788527347386
maxi score, test score, baseline:  0.09481999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.662019901566026
printing an ep nov before normalisation:  23.915853070952835
siam score:  -0.7729343
maxi score, test score, baseline:  0.09481999999999986 0.6860000000000002 0.6860000000000002
printing an ep nov before normalisation:  44.16297912597656
maxi score, test score, baseline:  0.09481999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.931]
 [0.61 ]
 [0.625]
 [0.586]
 [0.816]
 [0.58 ]] [[40.98 ]
 [37.223]
 [41.916]
 [42.514]
 [43.007]
 [39.593]
 [23.449]] [[1.325]
 [1.497]
 [1.258]
 [1.282]
 [1.252]
 [1.423]
 [0.906]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.36  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09481999999999986 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  0.09481999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.17048190677777
actor:  1 policy actor:  1  step number:  45 total reward:  0.5200000000000001  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.049]
 [0.096]
 [0.07 ]
 [0.049]
 [0.041]
 [0.057]
 [0.067]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.049]
 [0.096]
 [0.07 ]
 [0.049]
 [0.041]
 [0.057]
 [0.067]]
line 256 mcts: sample exp_bonus 18.171594146123873
maxi score, test score, baseline:  0.09481999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.45212333336478
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.67 ]
 [0.567]
 [0.46 ]
 [0.554]
 [0.545]
 [0.581]] [[18.541]
 [16.353]
 [19.686]
 [18.549]
 [19.544]
 [19.059]
 [20.908]] [[1.67 ]
 [1.749]
 [1.867]
 [1.685]
 [1.844]
 [1.803]
 [1.961]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  43.89366185477916
actions average: 
K:  0  action  0 :  tensor([0.5137, 0.0092, 0.0794, 0.0934, 0.1103, 0.0744, 0.1196],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0184, 0.9399, 0.0049, 0.0071, 0.0045, 0.0032, 0.0220],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1432, 0.0022, 0.4474, 0.0959, 0.0894, 0.1073, 0.1145],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1585, 0.0142, 0.1047, 0.3643, 0.1208, 0.1211, 0.1165],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1713, 0.0051, 0.0797, 0.1124, 0.4437, 0.0895, 0.0983],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1013, 0.0059, 0.0892, 0.0936, 0.0645, 0.5690, 0.0764],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1579, 0.1093, 0.1172, 0.1097, 0.0983, 0.0668, 0.3407],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09481999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09481999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.923]
 [0.993]
 [0.989]
 [0.934]
 [0.989]
 [0.989]
 [0.989]] [[31.214]
 [36.577]
 [32.755]
 [31.639]
 [32.755]
 [32.755]
 [32.755]] [[0.923]
 [0.993]
 [0.989]
 [0.934]
 [0.989]
 [0.989]
 [0.989]]
Printing some Q and Qe and total Qs values:  [[0.921]
 [0.94 ]
 [0.919]
 [0.882]
 [0.892]
 [0.905]
 [0.9  ]] [[24.464]
 [25.162]
 [25.028]
 [22.405]
 [22.666]
 [22.84 ]
 [22.745]] [[0.921]
 [0.94 ]
 [0.919]
 [0.882]
 [0.892]
 [0.905]
 [0.9  ]]
actor:  0 policy actor:  0  step number:  67 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09727333333333321 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.684]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]] [[42.396]
 [51.015]
 [42.396]
 [42.396]
 [42.396]
 [42.396]
 [42.396]] [[0.635]
 [0.684]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]]
Printing some Q and Qe and total Qs values:  [[-0.006]
 [ 0.047]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.006]
 [ 0.047]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]]
maxi score, test score, baseline:  0.09727333333333321 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.2999999999999994  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  50 total reward:  0.23333333333333273  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09705999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.132]
 [0.195]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]] [[32.863]
 [26.703]
 [32.863]
 [32.863]
 [32.863]
 [32.863]
 [32.863]] [[0.465]
 [0.445]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]]
maxi score, test score, baseline:  0.09705999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7772194
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.573]
 [0.443]
 [0.487]
 [0.522]
 [0.484]
 [0.52 ]] [[35.199]
 [30.172]
 [33.724]
 [25.602]
 [35.199]
 [27.536]
 [27.336]] [[0.822]
 [0.805]
 [0.722]
 [0.656]
 [0.822]
 [0.68 ]
 [0.713]]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.522]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]] [[23.813]
 [33.286]
 [23.813]
 [23.813]
 [23.813]
 [23.813]
 [23.813]] [[0.629]
 [0.855]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]]
maxi score, test score, baseline:  0.09705999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7767378
maxi score, test score, baseline:  0.09705999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.079763412475586
maxi score, test score, baseline:  0.09705999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.900571307587246
maxi score, test score, baseline:  0.09705999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09705999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09705999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.30666666666666653  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  61 total reward:  0.05333333333333279  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09705999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09705999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09705999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.11132549128098
actor:  1 policy actor:  1  step number:  37 total reward:  0.56  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.05666541729186
maxi score, test score, baseline:  0.09705999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09705999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.03316775938473
maxi score, test score, baseline:  0.09705999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.20667774748879
actor:  1 policy actor:  1  step number:  53 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  41.32811249761832
actor:  1 policy actor:  1  step number:  41 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09705999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09705999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09705999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09705999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09705999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  66 total reward:  0.059999999999999054  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  40 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  23.144735395908356
printing an ep nov before normalisation:  11.831812220840634
printing an ep nov before normalisation:  47.57554564704218
printing an ep nov before normalisation:  14.086340428268368
printing an ep nov before normalisation:  37.624217055635256
actor:  1 policy actor:  1  step number:  66 total reward:  0.22000000000000008  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  51.67762928872976
printing an ep nov before normalisation:  39.50436931110702
printing an ep nov before normalisation:  26.632522492629644
maxi score, test score, baseline:  0.09632666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09632666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09632666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09632666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.177]
 [0.203]
 [0.177]
 [0.177]
 [0.177]
 [0.177]
 [0.177]] [[ 0.   ]
 [28.969]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.979]
 [ 1.227]
 [-0.979]
 [-0.979]
 [-0.979]
 [-0.979]
 [-0.979]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09632666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.13861083984375
actor:  1 policy actor:  1  step number:  49 total reward:  0.40000000000000013  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09632666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.920631880974913
printing an ep nov before normalisation:  39.20042415029943
printing an ep nov before normalisation:  43.969039278343075
actor:  1 policy actor:  1  step number:  62 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09632666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77532923
printing an ep nov before normalisation:  36.88281920057873
printing an ep nov before normalisation:  33.666831689597736
siam score:  -0.7767506
printing an ep nov before normalisation:  37.34729425220151
printing an ep nov before normalisation:  42.74699687957764
actions average: 
K:  2  action  0 :  tensor([0.5263, 0.0059, 0.0683, 0.0899, 0.1154, 0.0945, 0.0997],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0047, 0.9671, 0.0031, 0.0051, 0.0036, 0.0046, 0.0119],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1959, 0.0032, 0.2648, 0.1594, 0.1162, 0.1428, 0.1176],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1534, 0.0669, 0.0856, 0.2413, 0.1115, 0.1874, 0.1538],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1529, 0.0098, 0.1307, 0.1514, 0.2842, 0.1410, 0.1300],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1386, 0.0078, 0.1880, 0.0840, 0.0735, 0.4224, 0.0857],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1807, 0.1088, 0.1196, 0.1477, 0.1090, 0.0955, 0.2388],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  29.09074395022499
maxi score, test score, baseline:  0.09632666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.40065601159905
maxi score, test score, baseline:  0.09632666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09632666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09632666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.299]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]] [[24.498]
 [33.097]
 [24.498]
 [24.498]
 [24.498]
 [24.498]
 [24.498]] [[0.724]
 [0.915]
 [0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]]
printing an ep nov before normalisation:  46.279162013279475
maxi score, test score, baseline:  0.09632666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.23 ]
 [-0.083]
 [ 0.171]
 [ 0.064]
 [ 0.056]
 [ 0.158]
 [-0.01 ]] [[23.453]
 [26.693]
 [26.085]
 [25.373]
 [25.441]
 [34.654]
 [25.586]] [[0.78 ]
 [0.628]
 [0.852]
 [0.71 ]
 [0.705]
 [1.264]
 [0.646]]
maxi score, test score, baseline:  0.09632666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09632666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09632666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.06014528853878
printing an ep nov before normalisation:  38.00840400026564
maxi score, test score, baseline:  0.09632666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09632666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  0.09632666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09632666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09632666666666652 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  0.09632666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  50 total reward:  0.3533333333333327  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  36.49705256096798
printing an ep nov before normalisation:  13.393558774675643
Printing some Q and Qe and total Qs values:  [[0.914]
 [0.957]
 [0.926]
 [0.923]
 [0.946]
 [0.937]
 [0.954]] [[27.496]
 [28.884]
 [26.908]
 [28.631]
 [28.486]
 [28.752]
 [26.863]] [[0.914]
 [0.957]
 [0.926]
 [0.923]
 [0.946]
 [0.937]
 [0.954]]
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  0.09903333333333317 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4333333333333329  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  25.67594647407532
Printing some Q and Qe and total Qs values:  [[0.122]
 [0.101]
 [0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.125]] [[41.449]
 [30.681]
 [31.634]
 [31.634]
 [31.634]
 [31.634]
 [31.634]] [[1.387]
 [0.895]
 [0.961]
 [0.961]
 [0.961]
 [0.961]
 [0.961]]
actor:  0 policy actor:  0  step number:  48 total reward:  0.37999999999999945  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  42.363595962524414
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.769073066813654
maxi score, test score, baseline:  0.1017933333333332 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  0.54  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  31.309447522038827
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.549]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]] [[31.003]
 [37.052]
 [31.003]
 [31.003]
 [31.003]
 [31.003]
 [31.003]] [[0.485]
 [0.549]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]]
maxi score, test score, baseline:  0.1017933333333332 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.01933023449574
actor:  0 policy actor:  0  step number:  44 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.693]
 [0.681]
 [0.681]
 [0.653]
 [0.681]
 [0.681]] [[29.751]
 [31.236]
 [29.751]
 [29.751]
 [31.5  ]
 [29.751]
 [29.751]] [[0.681]
 [0.693]
 [0.681]
 [0.681]
 [0.653]
 [0.681]
 [0.681]]
printing an ep nov before normalisation:  42.43072539708611
printing an ep nov before normalisation:  37.41897032678693
Printing some Q and Qe and total Qs values:  [[0.855]
 [0.903]
 [0.821]
 [0.821]
 [0.824]
 [0.821]
 [0.821]] [[29.215]
 [35.502]
 [27.898]
 [27.898]
 [23.322]
 [27.898]
 [27.898]] [[0.855]
 [0.903]
 [0.821]
 [0.821]
 [0.824]
 [0.821]
 [0.821]]
maxi score, test score, baseline:  0.09877999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09877999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  45 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  60 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  44 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]]
printing an ep nov before normalisation:  47.1178922423525
maxi score, test score, baseline:  0.10121999999999984 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.10121999999999984 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.10121999999999984 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.753483854206415
line 256 mcts: sample exp_bonus 18.427706627085552
printing an ep nov before normalisation:  14.019584971545948
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.10121999999999984 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.10121999999999984 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  69 total reward:  0.17333333333333267  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09867333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7730576
maxi score, test score, baseline:  0.09867333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.12328985167949
maxi score, test score, baseline:  0.09596666666666652 0.6860000000000002 0.6860000000000002
printing an ep nov before normalisation:  31.317858695983887
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.762]
 [0.754]
 [0.74 ]
 [0.707]
 [0.656]
 [0.736]] [[29.684]
 [29.392]
 [31.031]
 [28.81 ]
 [32.121]
 [29.406]
 [28.145]] [[0.678]
 [0.762]
 [0.754]
 [0.74 ]
 [0.707]
 [0.656]
 [0.736]]
siam score:  -0.77104896
maxi score, test score, baseline:  0.09596666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09596666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.613280079308854
actor:  1 policy actor:  1  step number:  47 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.07 ]
 [0.142]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]] [[41.897]
 [48.622]
 [41.897]
 [41.897]
 [41.897]
 [41.897]
 [41.897]] [[1.505]
 [1.999]
 [1.505]
 [1.505]
 [1.505]
 [1.505]
 [1.505]]
maxi score, test score, baseline:  0.09596666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09596666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.16903060336166
maxi score, test score, baseline:  0.09596666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.98888754127381
line 256 mcts: sample exp_bonus 42.44297568553619
using explorer policy with actor:  1
siam score:  -0.7686792
printing an ep nov before normalisation:  34.49976509374605
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  34.031572341918945
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.44 ]
 [0.305]
 [0.352]
 [0.29 ]
 [0.352]
 [0.352]] [[32.921]
 [35.507]
 [37.478]
 [26.651]
 [30.297]
 [26.651]
 [26.651]] [[0.916]
 [1.088]
 [1.018]
 [0.704]
 [0.764]
 [0.704]
 [0.704]]
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.49200797080994
printing an ep nov before normalisation:  34.782371520996094
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.75277108150767
siam score:  -0.7642071
printing an ep nov before normalisation:  35.42351424657745
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.194]
 [0.194]
 [0.194]
 [0.194]
 [0.194]
 [0.194]
 [0.194]] [[43.329]
 [43.329]
 [43.329]
 [43.329]
 [43.329]
 [43.329]
 [43.329]] [[0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]]
siam score:  -0.7645557
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
actor:  1 policy actor:  1  step number:  62 total reward:  0.3666666666666666  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  55.13880638828366
Printing some Q and Qe and total Qs values:  [[0.097]
 [0.097]
 [0.097]
 [0.097]
 [0.097]
 [0.097]
 [0.097]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.097]
 [0.097]
 [0.097]
 [0.097]
 [0.097]
 [0.097]
 [0.097]]
printing an ep nov before normalisation:  32.92419065866606
actor:  1 policy actor:  1  step number:  45 total reward:  0.5200000000000001  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  51.79992762942828
printing an ep nov before normalisation:  33.61831172108641
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.79393763409677
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.013888250634863653
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.90191352252217
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.25355965400136
actor:  1 policy actor:  1  step number:  60 total reward:  0.2999999999999994  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
printing an ep nov before normalisation:  32.314281921556706
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.42000000000000004  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.508625507354736
printing an ep nov before normalisation:  45.5012710303833
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09623333333333318 0.6860000000000002 0.6860000000000002
actor:  0 policy actor:  0  step number:  50 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  20.760769852535255
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.134450771779218
actor:  1 policy actor:  1  step number:  54 total reward:  0.3666666666666666  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.108]
 [0.19 ]
 [0.19 ]] [[34.078]
 [36.601]
 [36.601]
 [36.601]
 [35.174]
 [36.601]
 [36.601]] [[1.612]
 [1.718]
 [1.718]
 [1.718]
 [1.522]
 [1.718]
 [1.718]]
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.383]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]] [[32.767]
 [35.832]
 [32.767]
 [32.767]
 [32.767]
 [32.767]
 [32.767]] [[0.889]
 [1.14 ]
 [0.889]
 [0.889]
 [0.889]
 [0.889]
 [0.889]]
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.10716273324907
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.436]
 [0.415]
 [0.415]
 [0.415]
 [0.415]
 [0.415]] [[30.93 ]
 [29.152]
 [30.93 ]
 [30.93 ]
 [30.93 ]
 [30.93 ]
 [30.93 ]] [[1.922]
 [1.773]
 [1.922]
 [1.922]
 [1.922]
 [1.922]
 [1.922]]
printing an ep nov before normalisation:  28.836664980266935
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  45.05705914529361
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.661]
 [0.459]
 [0.503]
 [0.501]
 [0.559]
 [0.489]] [[28.574]
 [25.632]
 [26.314]
 [26.514]
 [28.098]
 [28.623]
 [27.449]] [[2.265]
 [2.207]
 [2.046]
 [2.102]
 [2.195]
 [2.284]
 [2.143]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  49 total reward:  0.41333333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.049783322349374
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
actor:  1 policy actor:  1  step number:  48 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  34.01973198797432
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.39312498266994
siam score:  -0.78210986
printing an ep nov before normalisation:  38.32571682785621
printing an ep nov before normalisation:  39.467601516366905
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.914631274316974
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.564]
 [0.503]
 [0.492]
 [0.492]
 [0.515]
 [0.499]] [[26.167]
 [28.402]
 [25.53 ]
 [25.499]
 [25.444]
 [29.744]
 [25.371]] [[0.681]
 [0.752]
 [0.646]
 [0.633]
 [0.633]
 [0.726]
 [0.639]]
siam score:  -0.7817785
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.18305784480014
printing an ep nov before normalisation:  19.205094575881958
printing an ep nov before normalisation:  34.31834936141968
actor:  1 policy actor:  1  step number:  65 total reward:  0.11999999999999922  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  52 total reward:  0.3400000000000001  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  61 total reward:  0.3199999999999993  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.21 ]
 [0.178]
 [0.178]
 [0.178]
 [0.222]
 [0.178]] [[30.663]
 [43.58 ]
 [30.663]
 [30.663]
 [30.663]
 [40.698]
 [30.663]] [[0.696]
 [1.402]
 [0.696]
 [0.696]
 [0.696]
 [1.263]
 [0.696]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.233]
 [0.261]
 [0.233]
 [0.233]
 [0.233]
 [0.233]
 [0.233]] [[28.764]
 [35.231]
 [28.764]
 [28.764]
 [28.764]
 [28.764]
 [28.764]] [[0.713]
 [0.952]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]]
printing an ep nov before normalisation:  40.309861074258436
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.527]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]] [[28.859]
 [33.72 ]
 [28.859]
 [28.859]
 [28.859]
 [28.859]
 [28.859]] [[1.461]
 [1.798]
 [1.461]
 [1.461]
 [1.461]
 [1.461]
 [1.461]]
printing an ep nov before normalisation:  42.21459018472268
printing an ep nov before normalisation:  33.539273388367924
printing an ep nov before normalisation:  39.77634298033941
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.265]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]] [[39.884]
 [36.573]
 [39.884]
 [39.884]
 [39.884]
 [39.884]
 [39.884]] [[1.832]
 [1.65 ]
 [1.832]
 [1.832]
 [1.832]
 [1.832]
 [1.832]]
actor:  1 policy actor:  1  step number:  57 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.790771384578015
actor:  1 policy actor:  1  step number:  61 total reward:  0.14666666666666606  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  33.69037866592407
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09599333333333318 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.117469142052926
actor:  0 policy actor:  0  step number:  58 total reward:  0.48666666666666714  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09629999999999984 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  0.09629999999999984 0.6860000000000002 0.6860000000000002
printing an ep nov before normalisation:  25.433745164206883
maxi score, test score, baseline:  0.09629999999999984 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.662]
 [0.664]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]] [[38.712]
 [38.584]
 [38.712]
 [38.712]
 [38.712]
 [38.712]
 [38.712]] [[0.662]
 [0.664]
 [0.662]
 [0.662]
 [0.662]
 [0.662]
 [0.662]]
printing an ep nov before normalisation:  21.017122268676758
maxi score, test score, baseline:  0.09404666666666651 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.20204545492567
actor:  1 policy actor:  1  step number:  69 total reward:  0.10666666666666558  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09404666666666651 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09404666666666651 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09404666666666651 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.755817072787806
printing an ep nov before normalisation:  44.325157113153175
Printing some Q and Qe and total Qs values:  [[ 0.63 ]
 [ 0.664]
 [ 0.558]
 [ 0.65 ]
 [-0.037]
 [ 0.596]
 [ 0.538]] [[37.472]
 [31.753]
 [35.421]
 [34.657]
 [32.013]
 [31.425]
 [28.159]] [[1.41 ]
 [1.248]
 [1.267]
 [1.333]
 [0.555]
 [1.168]
 [0.998]]
siam score:  -0.77365595
actor:  1 policy actor:  1  step number:  44 total reward:  0.5400000000000001  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  2  action  0 :  tensor([0.5199, 0.1027, 0.0618, 0.0598, 0.1417, 0.0456, 0.0686],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0096, 0.9457, 0.0055, 0.0148, 0.0033, 0.0042, 0.0170],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0983, 0.0190, 0.4081, 0.1174, 0.1002, 0.1756, 0.0813],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1362, 0.1637, 0.1229, 0.2622, 0.1046, 0.1041, 0.1064],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1645, 0.0018, 0.0521, 0.0619, 0.6222, 0.0589, 0.0386],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1408, 0.0010, 0.1161, 0.0668, 0.0632, 0.5536, 0.0584],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0930, 0.2874, 0.0931, 0.1280, 0.1059, 0.0830, 0.2097],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  0.38  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 29.594838619232178
actions average: 
K:  4  action  0 :  tensor([0.4573, 0.0100, 0.1059, 0.1014, 0.1212, 0.0988, 0.1054],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0148, 0.9221, 0.0107, 0.0095, 0.0105, 0.0084, 0.0239],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1645, 0.0290, 0.3440, 0.1158, 0.1058, 0.1061, 0.1347],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2583, 0.0097, 0.1761, 0.1146, 0.1444, 0.1184, 0.1785],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1395, 0.0143, 0.0880, 0.0912, 0.5041, 0.0826, 0.0803],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0737, 0.0694, 0.1763, 0.1166, 0.1603, 0.3244, 0.0793],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1658, 0.2635, 0.1362, 0.0958, 0.1012, 0.0835, 0.1539],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.2614317480345
actor:  1 policy actor:  1  step number:  59 total reward:  0.17333333333333312  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09404666666666651 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09404666666666651 0.6860000000000002 0.6860000000000002
maxi score, test score, baseline:  0.09404666666666651 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.001978489754116
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.496]
 [0.504]
 [0.464]
 [0.46 ]
 [0.536]
 [0.464]] [[37.434]
 [36.588]
 [37.6  ]
 [37.434]
 [35.482]
 [38.618]
 [37.434]] [[1.636]
 [1.615]
 [1.686]
 [1.636]
 [1.511]
 [1.78 ]
 [1.636]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09404666666666651 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  52 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09325999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09325999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09325999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  0.6266666666666669  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09325999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09325999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.239951764572695
Printing some Q and Qe and total Qs values:  [[0.724]
 [0.724]
 [0.724]
 [0.762]
 [0.724]
 [0.724]
 [0.72 ]] [[32.721]
 [32.721]
 [32.721]
 [34.44 ]
 [32.721]
 [32.721]
 [37.463]] [[1.586]
 [1.586]
 [1.586]
 [1.715]
 [1.586]
 [1.586]
 [1.834]]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.80154360822769
printing an ep nov before normalisation:  31.117100054689605
maxi score, test score, baseline:  0.09325999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.36  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.607]
 [1.049]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.607]
 [1.049]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]]
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.053]
 [0.07 ]
 [0.014]
 [0.019]
 [0.018]
 [0.028]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.023]
 [0.053]
 [0.07 ]
 [0.014]
 [0.019]
 [0.018]
 [0.028]]
actor:  0 policy actor:  0  step number:  54 total reward:  0.1933333333333327  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09225999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09225999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7675839
maxi score, test score, baseline:  0.09225999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.18679841423915
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.484]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]] [[34.754]
 [48.623]
 [34.754]
 [34.754]
 [34.754]
 [34.754]
 [34.754]] [[0.729]
 [1.037]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]]
maxi score, test score, baseline:  0.09225999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09225999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.779]
 [0.765]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]] [[25.182]
 [26.609]
 [25.182]
 [25.182]
 [25.182]
 [25.182]
 [25.182]] [[0.779]
 [0.765]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]]
printing an ep nov before normalisation:  22.25480079650879
maxi score, test score, baseline:  0.09225999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.00717545863671
maxi score, test score, baseline:  0.09225999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09225999999999986 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  67 total reward:  0.21333333333333304  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  29.853027810904923
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.157]
 [0.229]
 [0.148]
 [0.157]
 [0.157]
 [0.157]
 [0.153]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.157]
 [0.229]
 [0.148]
 [0.157]
 [0.157]
 [0.157]
 [0.153]]
maxi score, test score, baseline:  0.09133999999999985 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.26731581345529
printing an ep nov before normalisation:  50.14431280103481
printing an ep nov before normalisation:  45.811991789054176
printing an ep nov before normalisation:  47.63197688431479
actor:  0 policy actor:  0  step number:  52 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  47.184387061010156
printing an ep nov before normalisation:  42.06735976906371
maxi score, test score, baseline:  0.09048666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.582359503087865
printing an ep nov before normalisation:  48.245399646133414
printing an ep nov before normalisation:  47.583855611231726
line 256 mcts: sample exp_bonus 47.362658495519085
maxi score, test score, baseline:  0.09048666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.002981134251065
printing an ep nov before normalisation:  38.36575228405176
maxi score, test score, baseline:  0.09048666666666652 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09048666666666652 0.6860000000000002 0.6860000000000002
printing an ep nov before normalisation:  35.78132901477954
printing an ep nov before normalisation:  25.883764587395497
Printing some Q and Qe and total Qs values:  [[0.968]
 [0.968]
 [0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]] [[13.227]
 [ 4.516]
 [13.309]
 [13.424]
 [13.993]
 [13.995]
 [26.613]] [[0.968]
 [0.968]
 [0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.925]
 [0.964]
 [0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]] [[49.103]
 [31.458]
 [49.103]
 [49.103]
 [49.103]
 [49.103]
 [49.103]] [[0.925]
 [0.964]
 [0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]]
printing an ep nov before normalisation:  37.75637750187557
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.886]
 [0.95 ]
 [0.919]
 [0.921]
 [0.865]
 [0.887]
 [0.946]] [[29.181]
 [35.487]
 [32.546]
 [31.135]
 [33.131]
 [31.634]
 [33.874]] [[0.886]
 [0.95 ]
 [0.919]
 [0.921]
 [0.865]
 [0.887]
 [0.946]]
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  0.09095333333333319 0.6860000000000002 0.6860000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.51874695346946
actor:  0 policy actor:  0  step number:  29 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  29 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  29 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  30 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  30 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  30 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  34.92203608931508
actor:  0 policy actor:  0  step number:  31 total reward:  0.56  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  31 total reward:  0.56  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  33 total reward:  0.5333333333333333  reward:  1.0 rdn_beta:  2
actor:  1 policy actor:  1  step number:  48 total reward:  0.4600000000000002  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.66 ]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]] [[33.529]
 [33.614]
 [33.529]
 [33.529]
 [33.529]
 [33.529]
 [33.529]] [[0.592]
 [0.66 ]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]]
printing an ep nov before normalisation:  33.45719019300661
maxi score, test score, baseline:  0.09584666666666652 0.6373333333333335 0.6373333333333335
printing an ep nov before normalisation:  35.25532008963436
actor:  0 policy actor:  0  step number:  55 total reward:  0.10666666666666602  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09521999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
maxi score, test score, baseline:  0.09521999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
actor:  0 policy actor:  0  step number:  44 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]] [[44.004]
 [44.004]
 [44.004]
 [44.004]
 [44.004]
 [44.004]
 [44.004]] [[0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]
 [0.693]]
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.707]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]] [[36.116]
 [42.806]
 [36.116]
 [36.116]
 [36.116]
 [36.116]
 [36.116]] [[0.654]
 [0.707]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]]
actor:  0 policy actor:  0  step number:  47 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  38.12766893613642
actor:  0 policy actor:  0  step number:  44 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  2.0
siam score:  -0.7756726
actor:  0 policy actor:  1  step number:  49 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 26.931653579567485
printing an ep nov before normalisation:  64.30618427604071
maxi score, test score, baseline:  0.09839333333333318 0.6373333333333335 0.6373333333333335
maxi score, test score, baseline:  0.09839333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
maxi score, test score, baseline:  0.09839333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
printing an ep nov before normalisation:  43.52379923174871
maxi score, test score, baseline:  0.09839333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
maxi score, test score, baseline:  0.09839333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.413]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]] [[30.644]
 [37.872]
 [30.644]
 [30.644]
 [30.644]
 [30.644]
 [30.644]] [[1.491]
 [1.973]
 [1.491]
 [1.491]
 [1.491]
 [1.491]
 [1.491]]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.532]
 [0.499]
 [0.509]
 [0.502]
 [0.502]
 [0.492]] [[49.835]
 [50.549]
 [52.246]
 [52.246]
 [53.901]
 [51.261]
 [52.184]] [[2.063]
 [2.127]
 [2.189]
 [2.199]
 [2.285]
 [2.138]
 [2.179]]
printing an ep nov before normalisation:  30.161805152893066
actor:  1 policy actor:  1  step number:  55 total reward:  0.38666666666666605  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.267]
 [0.186]
 [0.186]
 [0.186]
 [0.18 ]
 [0.186]] [[38.849]
 [32.532]
 [38.849]
 [38.849]
 [38.849]
 [39.887]
 [38.849]] [[1.198]
 [1.008]
 [1.198]
 [1.198]
 [1.198]
 [1.237]
 [1.198]]
printing an ep nov before normalisation:  27.57292070385354
actor:  1 policy actor:  1  step number:  72 total reward:  0.1133333333333324  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.093]
 [0.066]
 [0.051]
 [0.05 ]
 [0.051]
 [0.067]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.063]
 [0.093]
 [0.066]
 [0.051]
 [0.05 ]
 [0.051]
 [0.067]]
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.273]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.19 ]
 [0.273]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]]
maxi score, test score, baseline:  0.09568666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
maxi score, test score, baseline:  0.09568666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
printing an ep nov before normalisation:  26.38277530670166
maxi score, test score, baseline:  0.09568666666666652 0.6373333333333335 0.6373333333333335
actor:  1 policy actor:  1  step number:  56 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  1  action  0 :  tensor([0.4804, 0.0236, 0.1029, 0.1010, 0.1243, 0.0979, 0.0700],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0069, 0.9258, 0.0060, 0.0118, 0.0039, 0.0051, 0.0406],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0349, 0.0278, 0.6783, 0.0453, 0.0509, 0.1390, 0.0239],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1725, 0.0808, 0.1431, 0.1660, 0.1707, 0.1640, 0.1029],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0986, 0.0405, 0.0902, 0.1022, 0.5379, 0.0629, 0.0678],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1425, 0.0035, 0.1298, 0.0989, 0.1102, 0.4311, 0.0839],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1490, 0.0095, 0.1479, 0.1492, 0.1166, 0.1483, 0.2795],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  53 total reward:  0.2666666666666664  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  38 total reward:  0.6066666666666669  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09568666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  54.975929583685655
maxi score, test score, baseline:  0.09568666666666652 0.6373333333333335 0.6373333333333335
printing an ep nov before normalisation:  30.037252568093834
from probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
maxi score, test score, baseline:  0.09568666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
siam score:  -0.77685905
printing an ep nov before normalisation:  42.157050635554725
maxi score, test score, baseline:  0.09568666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
printing an ep nov before normalisation:  46.00264380312286
Printing some Q and Qe and total Qs values:  [[0.744]
 [0.787]
 [0.342]
 [0.735]
 [0.72 ]
 [0.724]
 [0.737]] [[33.045]
 [36.354]
 [32.023]
 [30.15 ]
 [30.76 ]
 [35.548]
 [28.932]] [[0.744]
 [0.787]
 [0.342]
 [0.735]
 [0.72 ]
 [0.724]
 [0.737]]
printing an ep nov before normalisation:  26.384277953950335
maxi score, test score, baseline:  0.09568666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
maxi score, test score, baseline:  0.09568666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
Printing some Q and Qe and total Qs values:  [[0.848]
 [0.848]
 [0.848]
 [0.8  ]
 [0.848]
 [0.848]
 [0.796]] [[34.531]
 [46.098]
 [34.531]
 [28.907]
 [34.531]
 [34.531]
 [29.477]] [[0.848]
 [0.848]
 [0.848]
 [0.8  ]
 [0.848]
 [0.848]
 [0.796]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  40 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09596666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
maxi score, test score, baseline:  0.09596666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.453]
 [0.428]
 [0.428]
 [0.428]
 [0.428]
 [0.428]] [[29.244]
 [37.887]
 [29.244]
 [29.244]
 [29.244]
 [29.244]
 [29.244]] [[1.198]
 [1.586]
 [1.198]
 [1.198]
 [1.198]
 [1.198]
 [1.198]]
printing an ep nov before normalisation:  43.27819011569934
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.468]
 [0.531]
 [0.468]
 [0.468]
 [0.506]
 [0.468]] [[43.001]
 [43.001]
 [53.434]
 [43.001]
 [43.001]
 [46.517]
 [43.001]] [[1.292]
 [1.292]
 [1.745]
 [1.292]
 [1.292]
 [1.462]
 [1.292]]
maxi score, test score, baseline:  0.09596666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
maxi score, test score, baseline:  0.09596666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
maxi score, test score, baseline:  0.09596666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
Printing some Q and Qe and total Qs values:  [[0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]]
siam score:  -0.7750251
maxi score, test score, baseline:  0.09596666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
printing an ep nov before normalisation:  10.540847830564271
maxi score, test score, baseline:  0.09596666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
actor:  1 policy actor:  1  step number:  57 total reward:  0.23999999999999988  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  0.09596666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
line 256 mcts: sample exp_bonus 43.183756454908384
maxi score, test score, baseline:  0.09596666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5800000000000003  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  51.96037389237975
maxi score, test score, baseline:  0.09596666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
maxi score, test score, baseline:  0.09596666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
siam score:  -0.77358377
maxi score, test score, baseline:  0.09596666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
maxi score, test score, baseline:  0.09596666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
siam score:  -0.7737874
actor:  1 policy actor:  1  step number:  52 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  43.552617002144736
actor:  1 policy actor:  1  step number:  40 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  47.269728441345094
Printing some Q and Qe and total Qs values:  [[0.71 ]
 [0.861]
 [0.71 ]
 [0.71 ]
 [0.71 ]
 [0.68 ]
 [0.71 ]] [[33.446]
 [43.419]
 [33.446]
 [33.446]
 [33.446]
 [36.219]
 [33.446]] [[1.399]
 [1.905]
 [1.399]
 [1.399]
 [1.399]
 [1.467]
 [1.399]]
maxi score, test score, baseline:  0.09596666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
maxi score, test score, baseline:  0.09596666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
actor:  1 policy actor:  1  step number:  39 total reward:  0.56  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09596666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
printing an ep nov before normalisation:  40.000625407997
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.798]
 [0.46 ]
 [0.642]
 [0.642]
 [0.498]
 [0.642]] [[43.164]
 [41.544]
 [40.495]
 [43.164]
 [43.164]
 [39.872]
 [43.164]] [[1.734]
 [1.82 ]
 [1.437]
 [1.734]
 [1.734]
 [1.448]
 [1.734]]
maxi score, test score, baseline:  0.09596666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
actor:  0 policy actor:  1  step number:  29 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  37 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  28.287885189056396
UNIT TEST: sample policy line 217 mcts : [0.02  0.694 0.041 0.061 0.041 0.102 0.041]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09929999999999985 0.6373333333333335 0.6373333333333335
printing an ep nov before normalisation:  55.84084943590829
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.575]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.459]] [[43.725]
 [44.581]
 [43.725]
 [43.725]
 [43.725]
 [43.725]
 [43.725]] [[2.078]
 [2.242]
 [2.078]
 [2.078]
 [2.078]
 [2.078]
 [2.078]]
printing an ep nov before normalisation:  38.264047674148244
printing an ep nov before normalisation:  58.381447075407856
maxi score, test score, baseline:  0.09929999999999985 0.6373333333333335 0.6373333333333335
actor:  1 policy actor:  1  step number:  56 total reward:  0.2733333333333332  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  33.64001644982232
actor:  1 policy actor:  1  step number:  54 total reward:  0.2599999999999999  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09929999999999985 0.6373333333333335 0.6373333333333335
maxi score, test score, baseline:  0.09929999999999985 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]] [[34.873]
 [34.873]
 [34.873]
 [34.873]
 [34.873]
 [34.873]
 [34.873]] [[0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]]
maxi score, test score, baseline:  0.09929999999999985 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
maxi score, test score, baseline:  0.09929999999999985 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
actions average: 
K:  4  action  0 :  tensor([0.4008, 0.0163, 0.1084, 0.0936, 0.2376, 0.0737, 0.0695],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0120, 0.9288, 0.0171, 0.0056, 0.0037, 0.0055, 0.0273],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1325, 0.2989, 0.1370, 0.0918, 0.1278, 0.1161, 0.0959],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0927, 0.1958, 0.0841, 0.1457, 0.2065, 0.1869, 0.0883],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1599, 0.0094, 0.1064, 0.0790, 0.4773, 0.0914, 0.0767],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0837, 0.0039, 0.1560, 0.0515, 0.0783, 0.5984, 0.0280],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1473, 0.0869, 0.1058, 0.1844, 0.1446, 0.1027, 0.2284],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.09929999999999985 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
printing an ep nov before normalisation:  29.974269136015828
actor:  1 policy actor:  1  step number:  40 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09929999999999985 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
maxi score, test score, baseline:  0.09929999999999985 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.218]
 [0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]] [[33.876]
 [42.711]
 [33.876]
 [33.876]
 [33.876]
 [33.876]
 [33.876]] [[0.877]
 [1.368]
 [0.877]
 [0.877]
 [0.877]
 [0.877]
 [0.877]]
actor:  0 policy actor:  1  step number:  43 total reward:  0.4266666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09955333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
printing an ep nov before normalisation:  50.77456463084746
maxi score, test score, baseline:  0.09955333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
maxi score, test score, baseline:  0.09955333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
printing an ep nov before normalisation:  23.228747117454926
maxi score, test score, baseline:  0.09955333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
maxi score, test score, baseline:  0.09955333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
maxi score, test score, baseline:  0.09955333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
maxi score, test score, baseline:  0.09955333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
printing an ep nov before normalisation:  55.20663794192774
actor:  1 policy actor:  1  step number:  44 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  29.88046492059274
Printing some Q and Qe and total Qs values:  [[0.841]
 [0.892]
 [0.837]
 [0.837]
 [0.839]
 [0.837]
 [0.837]] [[20.767]
 [25.167]
 [19.029]
 [19.029]
 [19.389]
 [19.029]
 [19.029]] [[0.841]
 [0.892]
 [0.837]
 [0.837]
 [0.839]
 [0.837]
 [0.837]]
actor:  1 policy actor:  1  step number:  39 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  54 total reward:  0.3266666666666661  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0993533333333332 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
maxi score, test score, baseline:  0.0993533333333332 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
siam score:  -0.77417773
maxi score, test score, baseline:  0.0993533333333332 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
printing an ep nov before normalisation:  39.993508368951254
maxi score, test score, baseline:  0.0993533333333332 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
maxi score, test score, baseline:  0.0993533333333332 0.6373333333333335 0.6373333333333335
maxi score, test score, baseline:  0.0993533333333332 0.6373333333333335 0.6373333333333335
probs:  [0.06918745147053827, 0.06918745147053827, 0.06918745147053827, 0.44515824129117676, 0.06918745147053827, 0.27809195282667015]
actor:  1 policy actor:  1  step number:  35 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  43.4936773284916
maxi score, test score, baseline:  0.0993533333333332 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
maxi score, test score, baseline:  0.0993533333333332 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
actor:  1 policy actor:  1  step number:  33 total reward:  0.5866666666666668  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.704568164927046
printing an ep nov before normalisation:  48.79112021183686
printing an ep nov before normalisation:  0.0061964808310222
actor:  1 policy actor:  1  step number:  46 total reward:  0.3933333333333334  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0993533333333332 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
line 256 mcts: sample exp_bonus 35.718599464269204
maxi score, test score, baseline:  0.0993533333333332 0.6373333333333335 0.6373333333333335
printing an ep nov before normalisation:  37.87101536785193
maxi score, test score, baseline:  0.0993533333333332 0.6373333333333335 0.6373333333333335
maxi score, test score, baseline:  0.0993533333333332 0.6373333333333335 0.6373333333333335
printing an ep nov before normalisation:  25.274289790061157
actor:  1 policy actor:  1  step number:  46 total reward:  0.4733333333333335  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  19.47499021737707
actor:  1 policy actor:  1  step number:  61 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  43 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  53.48233125865944
maxi score, test score, baseline:  0.09971333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  33.78549101966806
printing an ep nov before normalisation:  30.050181415729988
printing an ep nov before normalisation:  30.11016810327503
printing an ep nov before normalisation:  52.3447722564799
printing an ep nov before normalisation:  41.75311127066225
printing an ep nov before normalisation:  28.587018462669516
maxi score, test score, baseline:  0.09971333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  25.65079700782442
printing an ep nov before normalisation:  25.662124518494423
maxi score, test score, baseline:  0.09971333333333318 0.6373333333333335 0.6373333333333335
actor:  0 policy actor:  0  step number:  48 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  1.667
siam score:  -0.76410186
actor:  1 policy actor:  1  step number:  55 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.27 ]
 [0.349]
 [0.272]
 [0.273]
 [0.27 ]
 [0.272]
 [0.268]] [[23.164]
 [27.898]
 [16.447]
 [16.558]
 [16.611]
 [16.724]
 [16.901]] [[0.27 ]
 [0.349]
 [0.272]
 [0.273]
 [0.27 ]
 [0.272]
 [0.268]]
maxi score, test score, baseline:  0.09933999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09933999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
maxi score, test score, baseline:  0.09933999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
actor:  1 policy actor:  1  step number:  38 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  45.28726443300942
maxi score, test score, baseline:  0.09933999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
maxi score, test score, baseline:  0.09933999999999986 0.6373333333333335 0.6373333333333335
maxi score, test score, baseline:  0.09933999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  37.0862164508924
actor:  1 policy actor:  1  step number:  43 total reward:  0.5333333333333335  reward:  1.0 rdn_beta:  1.667
using another actor
maxi score, test score, baseline:  0.09933999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
actor:  0 policy actor:  0  step number:  57 total reward:  0.13333333333333286  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7746519
actor:  1 policy actor:  1  step number:  35 total reward:  0.56  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.174]
 [0.11 ]
 [0.114]
 [0.044]
 [0.033]
 [0.049]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.022]
 [0.174]
 [0.11 ]
 [0.114]
 [0.044]
 [0.033]
 [0.049]]
actions average: 
K:  0  action  0 :  tensor([0.5264, 0.0079, 0.0899, 0.0746, 0.1305, 0.0750, 0.0957],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0010,     0.9796,     0.0008,     0.0012,     0.0002,     0.0002,
            0.0171], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1105, 0.0200, 0.4742, 0.0959, 0.0885, 0.1217, 0.0892],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1460, 0.0190, 0.1119, 0.3720, 0.0955, 0.1162, 0.1394],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1222, 0.0089, 0.0988, 0.1520, 0.4036, 0.1145, 0.1001],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0550, 0.0018, 0.0769, 0.0383, 0.0312, 0.7546, 0.0423],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1105, 0.0212, 0.1766, 0.0916, 0.1496, 0.1242, 0.3263],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  37.04158041631127
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.637]
 [0.556]
 [0.564]
 [0.556]
 [0.572]
 [0.554]] [[31.849]
 [33.439]
 [28.345]
 [31.945]
 [28.483]
 [31.425]
 [28.639]] [[0.565]
 [0.637]
 [0.556]
 [0.564]
 [0.556]
 [0.572]
 [0.554]]
maxi score, test score, baseline:  0.09905999999999986 0.6373333333333335 0.6373333333333335
Printing some Q and Qe and total Qs values:  [[0.154]
 [0.256]
 [0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.185]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.154]
 [0.256]
 [0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.185]]
printing an ep nov before normalisation:  32.39549967388602
printing an ep nov before normalisation:  26.30565643310547
actor:  0 policy actor:  1  step number:  45 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]] [[42.874]
 [42.874]
 [42.874]
 [42.874]
 [42.874]
 [42.874]
 [42.874]] [[2.591]
 [2.591]
 [2.591]
 [2.591]
 [2.591]
 [2.591]
 [2.591]]
maxi score, test score, baseline:  0.10204666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
maxi score, test score, baseline:  0.10204666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
maxi score, test score, baseline:  0.10204666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
maxi score, test score, baseline:  0.10204666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  18.987317085266113
maxi score, test score, baseline:  0.10204666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  14.861019849777222
siam score:  -0.78513956
printing an ep nov before normalisation:  21.242052423847397
maxi score, test score, baseline:  0.10204666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
actor:  1 policy actor:  1  step number:  66 total reward:  0.20666666666666633  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  46 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10480666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
line 256 mcts: sample exp_bonus 0.0
line 256 mcts: sample exp_bonus 39.32301203409243
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]] [[36.414]
 [36.414]
 [36.414]
 [36.414]
 [36.414]
 [36.414]
 [36.414]] [[2.029]
 [2.029]
 [2.029]
 [2.029]
 [2.029]
 [2.029]
 [2.029]]
line 256 mcts: sample exp_bonus 50.0918017329035
actor:  1 policy actor:  1  step number:  36 total reward:  0.54  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10480666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5933333333333335  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10480666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  54.54201627309833
maxi score, test score, baseline:  0.10480666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  27.170313873106036
siam score:  -0.7811405
actions average: 
K:  1  action  0 :  tensor([0.5704, 0.0028, 0.0887, 0.0901, 0.0881, 0.0821, 0.0778],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0027,     0.9467,     0.0041,     0.0048,     0.0004,     0.0016,
            0.0397], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1496, 0.0084, 0.3791, 0.1382, 0.1061, 0.1189, 0.0997],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1594, 0.0239, 0.1358, 0.2416, 0.1688, 0.1364, 0.1340],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2116, 0.0035, 0.1109, 0.1039, 0.3507, 0.1028, 0.1166],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1016, 0.0070, 0.0862, 0.0875, 0.0701, 0.5992, 0.0484],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0938, 0.0960, 0.1161, 0.1268, 0.1164, 0.0768, 0.3741],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.10229999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  30.74437540886135
printing an ep nov before normalisation:  40.31461356606759
printing an ep nov before normalisation:  45.70028846247659
printing an ep nov before normalisation:  35.46744935595506
maxi score, test score, baseline:  0.10229999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
maxi score, test score, baseline:  0.10229999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
maxi score, test score, baseline:  0.10229999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]] [[6.814]
 [6.799]
 [8.873]
 [7.065]
 [8.648]
 [8.829]
 [8.605]] [[0.814]
 [0.813]
 [0.872]
 [0.82 ]
 [0.865]
 [0.87 ]
 [0.864]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.10229999999999986 0.6373333333333335 0.6373333333333335
maxi score, test score, baseline:  0.10229999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
actor:  0 policy actor:  0  step number:  38 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  53 total reward:  0.3466666666666667  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.21077835341555
maxi score, test score, baseline:  0.10524666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  32.519991397857666
maxi score, test score, baseline:  0.10524666666666652 0.6373333333333335 0.6373333333333335
maxi score, test score, baseline:  0.10524666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  35.54766915179411
maxi score, test score, baseline:  0.10524666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  21.92610297843177
printing an ep nov before normalisation:  35.39179149573305
printing an ep nov before normalisation:  0.13180618528650712
actor:  1 policy actor:  1  step number:  39 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  42 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10535333333333319 0.6373333333333335 0.6373333333333335
maxi score, test score, baseline:  0.10535333333333319 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
actor:  1 policy actor:  1  step number:  43 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10535333333333319 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
actions average: 
K:  0  action  0 :  tensor([0.5652, 0.0102, 0.0780, 0.0602, 0.1620, 0.0645, 0.0598],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0081, 0.9681, 0.0036, 0.0050, 0.0026, 0.0036, 0.0089],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1640, 0.0093, 0.4544, 0.0817, 0.1019, 0.1088, 0.0799],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1673, 0.0100, 0.1108, 0.3658, 0.1261, 0.0924, 0.1275],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1708, 0.0048, 0.0815, 0.0711, 0.5319, 0.0833, 0.0566],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1512, 0.0020, 0.1004, 0.0613, 0.0887, 0.5286, 0.0677],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0869, 0.0714, 0.1538, 0.0977, 0.0673, 0.1025, 0.4204],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.10535333333333319 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
maxi score, test score, baseline:  0.10535333333333319 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10535333333333319 0.6373333333333335 0.6373333333333335
siam score:  -0.78225845
actor:  1 policy actor:  1  step number:  38 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.27 ]
 [0.308]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.27 ]
 [0.308]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]]
printing an ep nov before normalisation:  0.005441291118302161
actor:  1 policy actor:  1  step number:  52 total reward:  0.4733333333333336  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10535333333333319 0.6373333333333335 0.6373333333333335
maxi score, test score, baseline:  0.10535333333333319 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]] [[24.054]
 [24.054]
 [24.054]
 [24.054]
 [24.054]
 [24.054]
 [24.054]] [[0.904]
 [0.904]
 [0.904]
 [0.904]
 [0.904]
 [0.904]
 [0.904]]
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.481]
 [0.313]
 [0.313]
 [0.313]
 [0.313]
 [0.313]] [[34.217]
 [47.822]
 [34.217]
 [34.217]
 [34.217]
 [34.217]
 [34.217]] [[0.986]
 [1.676]
 [0.986]
 [0.986]
 [0.986]
 [0.986]
 [0.986]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.11999999999999944  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  52.09081480045057
using another actor
printing an ep nov before normalisation:  42.74434346628998
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  42 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  38 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.72721670063957
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.543]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.48 ]
 [0.543]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]
 [0.48 ]]
maxi score, test score, baseline:  0.10535333333333319 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
actions average: 
K:  2  action  0 :  tensor([0.5886, 0.0070, 0.0809, 0.0544, 0.1311, 0.0675, 0.0706],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0028, 0.9563, 0.0032, 0.0066, 0.0023, 0.0022, 0.0266],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1630, 0.0153, 0.3929, 0.0582, 0.0768, 0.1144, 0.1794],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2113, 0.0025, 0.0779, 0.2250, 0.1375, 0.1845, 0.1613],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1385, 0.0052, 0.0366, 0.0530, 0.6579, 0.0649, 0.0439],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1285, 0.0050, 0.1057, 0.0752, 0.1012, 0.4910, 0.0935],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1539, 0.0834, 0.1341, 0.0821, 0.0671, 0.0823, 0.3970],
       grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  28.751073943244084
maxi score, test score, baseline:  0.10535333333333319 0.6373333333333335 0.6373333333333335
Printing some Q and Qe and total Qs values:  [[0.846]
 [0.891]
 [0.846]
 [0.846]
 [0.846]
 [0.846]
 [0.846]] [[30.858]
 [35.818]
 [30.858]
 [30.858]
 [30.858]
 [30.858]
 [30.858]] [[0.846]
 [0.891]
 [0.846]
 [0.846]
 [0.846]
 [0.846]
 [0.846]]
maxi score, test score, baseline:  0.10236666666666651 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  42.000396667692144
maxi score, test score, baseline:  0.10236666666666651 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
maxi score, test score, baseline:  0.10236666666666651 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  43 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  1.333
siam score:  -0.77885085
maxi score, test score, baseline:  0.10245999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
actor:  0 policy actor:  1  step number:  46 total reward:  0.40666666666666607  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  1  action  0 :  tensor([0.3431, 0.0082, 0.1303, 0.1364, 0.1553, 0.1202, 0.1066],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0094, 0.9459, 0.0052, 0.0171, 0.0041, 0.0044, 0.0140],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1289, 0.0055, 0.4279, 0.1118, 0.1227, 0.1002, 0.1029],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1689, 0.0571, 0.0958, 0.3164, 0.1096, 0.1274, 0.1248],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1888, 0.0046, 0.1057, 0.0983, 0.4195, 0.1005, 0.0826],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([    0.0627,     0.0004,     0.0800,     0.0436,     0.0515,     0.7235,
            0.0384], grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1071, 0.1137, 0.0831, 0.1893, 0.1068, 0.0516, 0.3485],
       grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 43.09558997750915
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.345]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]
 [0.31 ]] [[49.253]
 [35.551]
 [49.253]
 [49.253]
 [49.253]
 [49.253]
 [49.253]] [[1.643]
 [1.088]
 [1.643]
 [1.643]
 [1.643]
 [1.643]
 [1.643]]
maxi score, test score, baseline:  0.10527333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10527333333333318 0.6373333333333335 0.6373333333333335
maxi score, test score, baseline:  0.10527333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
maxi score, test score, baseline:  0.10527333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  32.39995411464147
maxi score, test score, baseline:  0.10527333333333318 0.6373333333333335 0.6373333333333335
actor:  1 policy actor:  1  step number:  42 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  59 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
from probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.207]
 [0.244]
 [0.244]
 [0.227]
 [0.244]
 [0.244]] [[29.754]
 [45.72 ]
 [29.754]
 [29.754]
 [45.759]
 [29.754]
 [29.754]] [[0.883]
 [1.516]
 [0.883]
 [0.883]
 [1.538]
 [0.883]
 [0.883]]
printing an ep nov before normalisation:  61.21121431424801
maxi score, test score, baseline:  0.10527333333333318 0.6373333333333335 0.6373333333333335
maxi score, test score, baseline:  0.10527333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
maxi score, test score, baseline:  0.10527333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
maxi score, test score, baseline:  0.10527333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.418]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.357]
 [0.33 ]] [[29.908]
 [26.548]
 [29.908]
 [29.908]
 [29.908]
 [36.219]
 [29.908]] [[1.353]
 [1.227]
 [1.353]
 [1.353]
 [1.353]
 [1.782]
 [1.353]]
printing an ep nov before normalisation:  62.92331465012274
maxi score, test score, baseline:  0.10527333333333318 0.6373333333333335 0.6373333333333335
maxi score, test score, baseline:  0.10527333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.453]
 [0.449]
 [0.392]
 [0.354]
 [0.322]
 [0.446]] [[29.108]
 [39.604]
 [39.78 ]
 [36.73 ]
 [36.376]
 [34.437]
 [39.164]] [[0.609]
 [0.911]
 [0.91 ]
 [0.794]
 [0.75 ]
 [0.679]
 [0.895]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  24.904068913977795
actor:  1 policy actor:  1  step number:  33 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  35.922333255977996
Printing some Q and Qe and total Qs values:  [[0.77]
 [0.81]
 [0.77]
 [0.77]
 [0.77]
 [0.77]
 [0.77]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.77]
 [0.81]
 [0.77]
 [0.77]
 [0.77]
 [0.77]
 [0.77]]
maxi score, test score, baseline:  0.10239333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
maxi score, test score, baseline:  0.10239333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.683]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]] [[34.25 ]
 [33.985]
 [34.25 ]
 [34.25 ]
 [34.25 ]
 [34.25 ]
 [34.25 ]] [[1.266]
 [1.28 ]
 [1.266]
 [1.266]
 [1.266]
 [1.266]
 [1.266]]
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.882]
 [0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.758]] [[28.822]
 [32.554]
 [28.822]
 [28.822]
 [28.822]
 [28.822]
 [28.822]] [[0.758]
 [0.882]
 [0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.758]]
maxi score, test score, baseline:  0.10239333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
actor:  1 policy actor:  1  step number:  51 total reward:  0.42666666666666675  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  39 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10297999999999984 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
Printing some Q and Qe and total Qs values:  [[ 0.037]
 [-0.121]
 [ 0.037]
 [ 0.037]
 [ 0.037]
 [-0.137]
 [ 0.037]] [[50.318]
 [45.069]
 [50.318]
 [50.318]
 [50.318]
 [45.933]
 [50.318]] [[1.929]
 [1.421]
 [1.929]
 [1.929]
 [1.929]
 [1.463]
 [1.929]]
maxi score, test score, baseline:  0.10297999999999984 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  25.790047645568848
printing an ep nov before normalisation:  25.335892519929494
printing an ep nov before normalisation:  48.9891260108773
using explorer policy with actor:  0
maxi score, test score, baseline:  0.10297999999999984 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
actor:  0 policy actor:  1  step number:  75 total reward:  0.09333333333333216  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  51.212689632478124
maxi score, test score, baseline:  0.10516666666666652 0.6373333333333335 0.6373333333333335
maxi score, test score, baseline:  0.10516666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
maxi score, test score, baseline:  0.10516666666666652 0.6373333333333335 0.6373333333333335
maxi score, test score, baseline:  0.10516666666666652 0.6373333333333335 0.6373333333333335
maxi score, test score, baseline:  0.10516666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  43.89159932464727
maxi score, test score, baseline:  0.10516666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
Printing some Q and Qe and total Qs values:  [[0.569]
 [0.653]
 [0.569]
 [0.569]
 [0.569]
 [0.569]
 [0.569]] [[42.312]
 [46.394]
 [42.312]
 [42.312]
 [42.312]
 [42.312]
 [42.312]] [[2.087]
 [2.394]
 [2.087]
 [2.087]
 [2.087]
 [2.087]
 [2.087]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.557]] [[44.395]
 [44.395]
 [44.395]
 [44.395]
 [44.395]
 [44.395]
 [45.526]] [[2.502]
 [2.502]
 [2.502]
 [2.502]
 [2.502]
 [2.502]
 [2.557]]
printing an ep nov before normalisation:  33.64566933324904
actor:  1 policy actor:  1  step number:  74 total reward:  0.033333333333333104  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10516666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
siam score:  -0.783248
actor:  1 policy actor:  1  step number:  48 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10516666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
line 256 mcts: sample exp_bonus 38.18232774734497
maxi score, test score, baseline:  0.10516666666666652 0.6373333333333335 0.6373333333333335
actor:  1 policy actor:  1  step number:  76 total reward:  0.11333333333333229  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10516666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
maxi score, test score, baseline:  0.10516666666666652 0.6373333333333335 0.6373333333333335
maxi score, test score, baseline:  0.10516666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
maxi score, test score, baseline:  0.10516666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
maxi score, test score, baseline:  0.10516666666666652 0.6373333333333335 0.6373333333333335
actor:  1 policy actor:  1  step number:  70 total reward:  0.08666666666666623  reward:  1.0 rdn_beta:  1.0
using another actor
maxi score, test score, baseline:  0.10516666666666652 0.6373333333333335 0.6373333333333335
actor:  0 policy actor:  0  step number:  48 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1051933333333332 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
Printing some Q and Qe and total Qs values:  [[0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]] [[30.031]
 [30.031]
 [30.031]
 [30.031]
 [30.031]
 [30.031]
 [30.031]] [[0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]
 [0.329]]
printing an ep nov before normalisation:  44.587127343414366
actor:  1 policy actor:  1  step number:  63 total reward:  0.25333333333333263  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  33.1123032975249
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1051933333333332 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  30.444192652141936
actor:  1 policy actor:  1  step number:  41 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1051933333333332 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
maxi score, test score, baseline:  0.1051933333333332 0.6373333333333335 0.6373333333333335
line 256 mcts: sample exp_bonus 31.4613163524895
printing an ep nov before normalisation:  24.262773604714134
maxi score, test score, baseline:  0.1051933333333332 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.752]
 [0.623]
 [0.623]
 [0.623]
 [0.623]
 [0.623]] [[30.179]
 [27.527]
 [30.179]
 [30.179]
 [30.179]
 [30.179]
 [30.179]] [[1.863]
 [1.824]
 [1.863]
 [1.863]
 [1.863]
 [1.863]
 [1.863]]
actor:  1 policy actor:  1  step number:  57 total reward:  0.23999999999999977  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  44 total reward:  0.5133333333333333  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[ 0.061]
 [ 0.061]
 [ 0.061]
 [-0.005]
 [ 0.061]
 [ 0.061]
 [ 0.061]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.061]
 [ 0.061]
 [ 0.061]
 [-0.005]
 [ 0.061]
 [ 0.061]
 [ 0.061]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.51470076346538
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.485]
 [0.551]
 [0.551]
 [0.551]
 [0.54 ]
 [0.551]] [[43.235]
 [43.392]
 [43.235]
 [43.235]
 [43.235]
 [51.01 ]
 [43.235]] [[1.815]
 [1.756]
 [1.815]
 [1.815]
 [1.815]
 [2.154]
 [1.815]]
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.602]
 [0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]] [[38.743]
 [41.168]
 [38.743]
 [38.743]
 [38.743]
 [38.743]
 [38.743]] [[1.781]
 [2.181]
 [1.781]
 [1.781]
 [1.781]
 [1.781]
 [1.781]]
actions average: 
K:  3  action  0 :  tensor([0.5131, 0.0514, 0.0908, 0.0843, 0.0968, 0.0801, 0.0836],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0049, 0.9492, 0.0043, 0.0117, 0.0075, 0.0107, 0.0116],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0921, 0.0162, 0.5481, 0.0677, 0.0697, 0.0906, 0.1157],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0866, 0.0661, 0.1044, 0.4211, 0.0868, 0.1337, 0.1014],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1440, 0.0288, 0.0893, 0.0853, 0.4521, 0.1101, 0.0904],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0367, 0.0285, 0.1417, 0.0609, 0.0395, 0.6501, 0.0426],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0761, 0.3236, 0.0405, 0.0829, 0.0668, 0.0873, 0.3228],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1058866666666665 0.6373333333333335 0.6373333333333335
actor:  1 policy actor:  1  step number:  43 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]] [[44.371]
 [44.371]
 [44.371]
 [44.371]
 [44.371]
 [44.371]
 [44.371]] [[1.651]
 [1.651]
 [1.651]
 [1.651]
 [1.651]
 [1.651]
 [1.651]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.98156337836877
maxi score, test score, baseline:  0.1058866666666665 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
actions average: 
K:  2  action  0 :  tensor([0.3569, 0.0164, 0.1170, 0.1204, 0.2026, 0.0937, 0.0929],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0097, 0.9380, 0.0058, 0.0106, 0.0046, 0.0051, 0.0262],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1669, 0.0266, 0.5040, 0.0686, 0.0799, 0.0625, 0.0916],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0810, 0.0749, 0.0685, 0.4991, 0.1303, 0.0757, 0.0705],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1813, 0.0114, 0.1154, 0.1080, 0.3877, 0.0834, 0.1129],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([    0.0609,     0.0006,     0.0960,     0.0300,     0.0627,     0.7315,
            0.0183], grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0755, 0.0281, 0.2293, 0.2237, 0.0833, 0.0856, 0.2744],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  51.02437184827944
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.339]
 [0.286]
 [0.291]
 [0.289]
 [0.288]
 [0.289]] [[24.879]
 [33.934]
 [23.809]
 [23.987]
 [24.042]
 [23.823]
 [23.053]] [[1.003]
 [1.547]
 [0.946]
 [0.961]
 [0.962]
 [0.949]
 [0.908]]
printing an ep nov before normalisation:  23.000636919232594
actor:  1 policy actor:  1  step number:  67 total reward:  0.13333333333333264  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1058866666666665 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  38.74744802460584
maxi score, test score, baseline:  0.1058866666666665 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]] [[30.649]
 [30.649]
 [30.649]
 [30.649]
 [30.649]
 [30.649]
 [30.649]] [[2.081]
 [2.081]
 [2.081]
 [2.081]
 [2.081]
 [2.081]
 [2.081]]
printing an ep nov before normalisation:  36.003922660098176
maxi score, test score, baseline:  0.1058866666666665 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
maxi score, test score, baseline:  0.1058866666666665 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  24.1743116442521
maxi score, test score, baseline:  0.1058866666666665 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
actor:  1 policy actor:  1  step number:  65 total reward:  0.11999999999999955  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  4  action  0 :  tensor([0.5160, 0.0097, 0.0807, 0.0720, 0.1411, 0.1124, 0.0683],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0117, 0.9223, 0.0082, 0.0105, 0.0095, 0.0102, 0.0275],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0676, 0.0849, 0.4834, 0.0487, 0.0814, 0.1896, 0.0444],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0935, 0.0403, 0.0792, 0.4323, 0.0368, 0.2147, 0.1032],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1381, 0.0134, 0.0741, 0.1020, 0.4948, 0.1149, 0.0627],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2168, 0.0053, 0.1371, 0.1533, 0.1901, 0.1738, 0.1235],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1163, 0.2989, 0.0922, 0.1010, 0.0927, 0.0866, 0.2123],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.10588666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
maxi score, test score, baseline:  0.10588666666666652 0.6373333333333335 0.6373333333333335
actor:  1 policy actor:  1  step number:  51 total reward:  0.31999999999999984  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10588666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  30.17749786376953
maxi score, test score, baseline:  0.10588666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.54574632644653
maxi score, test score, baseline:  0.10588666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
actor:  1 policy actor:  1  step number:  46 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.151]
 [0.09 ]
 [0.119]
 [0.119]
 [0.063]
 [0.067]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.058]
 [0.151]
 [0.09 ]
 [0.119]
 [0.119]
 [0.063]
 [0.067]]
printing an ep nov before normalisation:  21.900721391042076
printing an ep nov before normalisation:  28.620606660842896
printing an ep nov before normalisation:  33.53952884674072
maxi score, test score, baseline:  0.10588666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  41.36121417836665
printing an ep nov before normalisation:  40.294942390101205
maxi score, test score, baseline:  0.10588666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  31.87765598297119
siam score:  -0.776526
printing an ep nov before normalisation:  48.99340630313843
maxi score, test score, baseline:  0.10588666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  28.378384113311768
maxi score, test score, baseline:  0.10588666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3866666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10588666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  29.08048296218498
actor:  1 policy actor:  1  step number:  61 total reward:  0.38666666666666694  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  34.19591684574359
maxi score, test score, baseline:  0.10588666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  41.01942221130163
actor:  1 policy actor:  1  step number:  70 total reward:  0.17999999999999883  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10588666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
UNIT TEST: sample policy line 217 mcts : [0.122 0.245 0.122 0.163 0.122 0.122 0.102]
printing an ep nov before normalisation:  40.16384659139478
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10385999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10385999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
printing an ep nov before normalisation:  54.42398173354923
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.085]
 [ 0.   ]
 [-0.085]
 [-0.085]
 [-0.085]
 [-0.085]
 [-0.085]] [[27.699]
 [ 0.   ]
 [27.699]
 [27.699]
 [27.699]
 [27.699]
 [27.699]] [[1.248]
 [0.   ]
 [1.248]
 [1.248]
 [1.248]
 [1.248]
 [1.248]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10385999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
maxi score, test score, baseline:  0.10385999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.617766558786535
maxi score, test score, baseline:  0.10385999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.575]
 [0.602]
 [0.519]
 [0.512]
 [0.617]
 [0.536]] [[28.617]
 [28.637]
 [34.667]
 [29.331]
 [32.555]
 [39.299]
 [29.939]] [[1.182]
 [1.271]
 [1.613]
 [1.252]
 [1.413]
 [1.869]
 [1.301]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  39.170625527693126
printing an ep nov before normalisation:  54.55832364715667
line 256 mcts: sample exp_bonus 51.639942696742004
maxi score, test score, baseline:  0.10385999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
Printing some Q and Qe and total Qs values:  [[0.537]
 [0.637]
 [0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]] [[48.624]
 [46.97 ]
 [48.624]
 [48.624]
 [48.624]
 [48.624]
 [48.624]] [[2.52 ]
 [2.515]
 [2.52 ]
 [2.52 ]
 [2.52 ]
 [2.52 ]
 [2.52 ]]
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.521]
 [0.442]
 [0.432]
 [0.432]
 [0.445]
 [0.439]] [[22.026]
 [27.283]
 [22.395]
 [22.357]
 [22.241]
 [21.993]
 [21.631]] [[1.137]
 [1.627]
 [1.172]
 [1.159]
 [1.15 ]
 [1.144]
 [1.111]]
maxi score, test score, baseline:  0.10385999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
maxi score, test score, baseline:  0.10385999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06358155287011798, 0.06358155287011798, 0.06358155287011798, 0.32834044872865975, 0.06358155287011798, 0.4173333397908683]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  47 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  2.0
siam score:  -0.77421796
printing an ep nov before normalisation:  30.98532199859619
printing an ep nov before normalisation:  36.38710604276961
actor:  0 policy actor:  0  step number:  57 total reward:  0.18666666666666576  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10356666666666653 0.6373333333333335 0.6373333333333335
actor:  1 policy actor:  1  step number:  31 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10356666666666653 0.6373333333333335 0.6373333333333335
maxi score, test score, baseline:  0.10356666666666653 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.10356666666666653 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5066666666666669  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]
 [-0.031]] [[48.962]
 [48.962]
 [48.962]
 [48.962]
 [48.962]
 [48.962]
 [48.962]] [[1.224]
 [1.224]
 [1.224]
 [1.224]
 [1.224]
 [1.224]
 [1.224]]
printing an ep nov before normalisation:  22.622379990156386
maxi score, test score, baseline:  0.10356666666666653 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
actor:  1 policy actor:  1  step number:  35 total reward:  0.52  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  55 total reward:  0.22666666666666646  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10356666666666653 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.10356666666666653 0.6373333333333335 0.6373333333333335
maxi score, test score, baseline:  0.10356666666666653 0.6373333333333335 0.6373333333333335
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.709]
 [0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]] [[28.851]
 [37.558]
 [28.851]
 [28.851]
 [28.851]
 [28.851]
 [28.851]] [[0.977]
 [1.599]
 [0.977]
 [0.977]
 [0.977]
 [0.977]
 [0.977]]
actions average: 
K:  4  action  0 :  tensor([0.5401, 0.1477, 0.0623, 0.0658, 0.0610, 0.0412, 0.0820],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0219, 0.9020, 0.0119, 0.0119, 0.0132, 0.0140, 0.0250],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0832, 0.0238, 0.4166, 0.0819, 0.0894, 0.2339, 0.0712],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2759, 0.0009, 0.0986, 0.3523, 0.0927, 0.0782, 0.1014],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1389, 0.0029, 0.0603, 0.1057, 0.5532, 0.0819, 0.0572],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1328, 0.0054, 0.1540, 0.0715, 0.0853, 0.4775, 0.0734],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1229, 0.2973, 0.0866, 0.1011, 0.1230, 0.1142, 0.1550],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  49.4653479290081
line 256 mcts: sample exp_bonus 32.75948822714796
maxi score, test score, baseline:  0.10356666666666653 0.6373333333333335 0.6373333333333335
actor:  1 policy actor:  1  step number:  47 total reward:  0.3733333333333333  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  46 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10356666666666653 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.10356666666666653 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10356666666666653 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.10356666666666653 0.6373333333333335 0.6373333333333335
actor:  0 policy actor:  0  step number:  45 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  47.66184521529222
maxi score, test score, baseline:  0.10379333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
printing an ep nov before normalisation:  33.494527167534265
maxi score, test score, baseline:  0.10379333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.10379333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.10379333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10379333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.10379333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
printing an ep nov before normalisation:  28.04044625587511
maxi score, test score, baseline:  0.10379333333333318 0.6373333333333335 0.6373333333333335
printing an ep nov before normalisation:  32.84991264343262
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.42 ]
 [0.275]
 [0.275]
 [0.275]
 [0.275]
 [0.275]] [[33.639]
 [33.196]
 [33.639]
 [33.639]
 [33.639]
 [33.639]
 [33.639]] [[0.521]
 [0.66 ]
 [0.521]
 [0.521]
 [0.521]
 [0.521]
 [0.521]]
printing an ep nov before normalisation:  48.6849904994109
printing an ep nov before normalisation:  47.06516257960726
actor:  1 policy actor:  1  step number:  35 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.145]
 [0.146]
 [0.096]
 [0.091]
 [0.091]
 [0.088]
 [0.091]] [[36.729]
 [37.398]
 [27.452]
 [30.934]
 [30.934]
 [27.009]
 [30.934]] [[0.558]
 [0.571]
 [0.347]
 [0.403]
 [0.403]
 [0.331]
 [0.403]]
printing an ep nov before normalisation:  30.709105380777313
maxi score, test score, baseline:  0.10379333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
printing an ep nov before normalisation:  33.950605392456055
siam score:  -0.7749202
Printing some Q and Qe and total Qs values:  [[0.746]
 [0.784]
 [0.734]
 [0.734]
 [0.734]
 [0.734]
 [0.736]] [[16.67 ]
 [20.345]
 [16.583]
 [16.583]
 [16.583]
 [16.583]
 [17.024]] [[0.999]
 [1.096]
 [0.986]
 [0.986]
 [0.986]
 [0.986]
 [0.995]]
printing an ep nov before normalisation:  34.16598360614808
maxi score, test score, baseline:  0.10379333333333318 0.6373333333333335 0.6373333333333335
printing an ep nov before normalisation:  16.250824013089527
maxi score, test score, baseline:  0.10379333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
actor:  1 policy actor:  1  step number:  68 total reward:  0.3399999999999993  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7751625
maxi score, test score, baseline:  0.10112666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2866666666666665  reward:  1.0 rdn_beta:  1.333
siam score:  -0.77481353
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  34.31594931939548
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  45 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  1.333
siam score:  -0.7761784
printing an ep nov before normalisation:  47.47801296803543
actor:  1 policy actor:  1  step number:  52 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  40 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10144666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.10144666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
siam score:  -0.77386045
maxi score, test score, baseline:  0.10144666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.10144666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
printing an ep nov before normalisation:  31.511847134640917
printing an ep nov before normalisation:  41.69549200632624
maxi score, test score, baseline:  0.10144666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
actor:  1 policy actor:  1  step number:  61 total reward:  0.4533333333333335  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  46 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.497]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.463]
 [0.497]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]]
printing an ep nov before normalisation:  49.42102222329709
maxi score, test score, baseline:  0.10144666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.10144666666666652 0.6373333333333335 0.6373333333333335
actions average: 
K:  4  action  0 :  tensor([0.3624, 0.0741, 0.0899, 0.1406, 0.1372, 0.0883, 0.1075],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0184, 0.8633, 0.0241, 0.0169, 0.0072, 0.0134, 0.0567],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1574, 0.0262, 0.4313, 0.0866, 0.0907, 0.1184, 0.0894],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1328, 0.0436, 0.1237, 0.2113, 0.1750, 0.2027, 0.1110],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0558, 0.0098, 0.0240, 0.0408, 0.7987, 0.0466, 0.0243],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1579, 0.0095, 0.1316, 0.1031, 0.1190, 0.3252, 0.1538],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0990, 0.1732, 0.0659, 0.1951, 0.1383, 0.1122, 0.2163],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.10144666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.10144666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
printing an ep nov before normalisation:  30.970758918779932
actor:  1 policy actor:  1  step number:  51 total reward:  0.346666666666666  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  52 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  25.939674810554948
maxi score, test score, baseline:  0.10404666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.10404666666666652 0.6373333333333335 0.6373333333333335
actor:  1 policy actor:  1  step number:  45 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.605]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]] [[16.421]
 [22.816]
 [16.421]
 [16.421]
 [16.421]
 [16.421]
 [16.421]] [[2.031]
 [2.605]
 [2.031]
 [2.031]
 [2.031]
 [2.031]
 [2.031]]
actor:  1 policy actor:  1  step number:  66 total reward:  0.05999999999999972  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  42.746784181842585
maxi score, test score, baseline:  0.10404666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.10404666666666652 0.6373333333333335 0.6373333333333335
printing an ep nov before normalisation:  35.96948571260991
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.547]
 [0.262]
 [0.262]
 [0.262]
 [0.262]
 [0.262]] [[48.595]
 [46.686]
 [48.595]
 [48.595]
 [48.595]
 [48.595]
 [48.595]] [[1.929]
 [2.122]
 [1.929]
 [1.929]
 [1.929]
 [1.929]
 [1.929]]
maxi score, test score, baseline:  0.10404666666666652 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
actor:  0 policy actor:  0  step number:  71 total reward:  0.06666666666666565  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10617999999999984 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.10617999999999984 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.10617999999999984 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
printing an ep nov before normalisation:  0.00011724796763701306
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
actions average: 
K:  2  action  0 :  tensor([0.3434, 0.0776, 0.0788, 0.1106, 0.2136, 0.1084, 0.0676],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0243, 0.9443, 0.0046, 0.0040, 0.0026, 0.0021, 0.0181],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1366, 0.0690, 0.3227, 0.1117, 0.0641, 0.1615, 0.1344],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1333, 0.0132, 0.1167, 0.3874, 0.1413, 0.1021, 0.1060],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2123, 0.0038, 0.0819, 0.1117, 0.3938, 0.1203, 0.0763],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([    0.0788,     0.0005,     0.0853,     0.0579,     0.0776,     0.6470,
            0.0529], grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1021, 0.3104, 0.0705, 0.1053, 0.1112, 0.0908, 0.2097],
       grad_fn=<DivBackward0>)
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  36.21005385064206
actor:  1 policy actor:  1  step number:  60 total reward:  0.3266666666666669  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]] [[27.591]
 [27.591]
 [27.591]
 [27.591]
 [27.591]
 [27.591]
 [27.591]] [[0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]
 [0.753]]
siam score:  -0.78829575
maxi score, test score, baseline:  0.10617999999999984 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
actions average: 
K:  4  action  0 :  tensor([0.4739, 0.0067, 0.0994, 0.0926, 0.1545, 0.1022, 0.0707],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0186, 0.8939, 0.0304, 0.0113, 0.0083, 0.0078, 0.0298],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0584, 0.0004, 0.2728, 0.1235, 0.2075, 0.2992, 0.0382],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([nan, nan, nan, nan, nan, nan, nan], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2020, 0.0242, 0.1339, 0.1524, 0.2070, 0.1591, 0.1213],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1561, 0.1970, 0.1981, 0.0623, 0.0753, 0.2333, 0.0780],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2209, 0.0073, 0.1179, 0.1350, 0.2281, 0.1953, 0.0956],
       grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  38 total reward:  0.48666666666666647  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  46 total reward:  0.44666666666666677  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.68 ]
 [0.491]
 [0.491]
 [0.421]
 [0.491]
 [0.491]] [[38.386]
 [37.839]
 [38.386]
 [38.386]
 [45.236]
 [38.386]
 [38.386]] [[1.791]
 [1.951]
 [1.791]
 [1.791]
 [2.093]
 [1.791]
 [1.791]]
printing an ep nov before normalisation:  37.86316838005547
maxi score, test score, baseline:  0.10597999999999984 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.10597999999999984 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.10597999999999984 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.10597999999999984 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.10597999999999984 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]] [[23.271]
 [23.271]
 [23.271]
 [23.271]
 [23.271]
 [23.271]
 [23.271]] [[0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]]
maxi score, test score, baseline:  0.10597999999999984 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10597999999999984 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10597999999999984 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
Printing some Q and Qe and total Qs values:  [[0.188]
 [0.188]
 [0.188]
 [0.188]
 [0.188]
 [0.188]
 [0.188]] [[45.355]
 [45.355]
 [45.355]
 [45.355]
 [45.355]
 [45.355]
 [45.355]] [[1.521]
 [1.521]
 [1.521]
 [1.521]
 [1.521]
 [1.521]
 [1.521]]
UNIT TEST: sample policy line 217 mcts : [0.02  0.714 0.02  0.122 0.061 0.02  0.041]
maxi score, test score, baseline:  0.10597999999999984 0.6373333333333335 0.6373333333333335
maxi score, test score, baseline:  0.10597999999999984 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
actor:  0 policy actor:  0  step number:  67 total reward:  0.07999999999999907  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10813999999999985 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.10813999999999985 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
actor:  1 policy actor:  1  step number:  43 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.92091608047485
Printing some Q and Qe and total Qs values:  [[0.931]
 [0.957]
 [0.9  ]
 [0.902]
 [0.903]
 [0.911]
 [0.9  ]] [[32.236]
 [30.572]
 [30.993]
 [30.885]
 [31.334]
 [30.704]
 [31.582]] [[0.931]
 [0.957]
 [0.9  ]
 [0.902]
 [0.903]
 [0.911]
 [0.9  ]]
actor:  0 policy actor:  0  step number:  60 total reward:  0.05999999999999939  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11025999999999984 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
printing an ep nov before normalisation:  29.673054218292236
printing an ep nov before normalisation:  33.646689302060054
printing an ep nov before normalisation:  37.60178022343102
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.446]
 [0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]] [[29.362]
 [31.915]
 [29.362]
 [29.362]
 [29.362]
 [29.362]
 [29.362]] [[1.691]
 [1.958]
 [1.691]
 [1.691]
 [1.691]
 [1.691]
 [1.691]]
maxi score, test score, baseline:  0.11025999999999984 0.6373333333333335 0.6373333333333335
printing an ep nov before normalisation:  38.50080925607479
actor:  1 policy actor:  1  step number:  59 total reward:  0.22666666666666646  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  31.020140647888184
maxi score, test score, baseline:  0.11025999999999984 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11025999999999984 0.6373333333333335 0.6373333333333335
printing an ep nov before normalisation:  40.31991427954448
actor:  1 policy actor:  1  step number:  50 total reward:  0.31333333333333324  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.611]
 [0.558]
 [0.492]
 [0.51 ]
 [0.558]
 [0.583]] [[43.867]
 [43.147]
 [48.892]
 [47.26 ]
 [42.395]
 [48.892]
 [48.416]] [[1.552]
 [1.632]
 [1.811]
 [1.679]
 [1.5  ]
 [1.811]
 [1.817]]
from probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
printing an ep nov before normalisation:  0.0
using another actor
from probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  46 total reward:  0.4600000000000002  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.11025999999999984 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.11025999999999984 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
printing an ep nov before normalisation:  43.49825165708972
printing an ep nov before normalisation:  27.79176036990127
actor:  1 policy actor:  1  step number:  30 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  1.333
using another actor
maxi score, test score, baseline:  0.11025999999999984 0.6373333333333335 0.6373333333333335
maxi score, test score, baseline:  0.11025999999999984 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
actor:  0 policy actor:  0  step number:  57 total reward:  0.2933333333333328  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  21.12106276712629
maxi score, test score, baseline:  0.1128466666666665 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.1128466666666665 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
using explorer policy with actor:  0
printing an ep nov before normalisation:  53.12480233919366
printing an ep nov before normalisation:  49.31238257068332
printing an ep nov before normalisation:  13.693705513374645
maxi score, test score, baseline:  0.11015333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
line 256 mcts: sample exp_bonus 16.437611961213285
maxi score, test score, baseline:  0.11015333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
Printing some Q and Qe and total Qs values:  [[0.737]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]] [[13.039]
 [13.53 ]
 [13.53 ]
 [13.53 ]
 [13.53 ]
 [13.53 ]
 [13.53 ]] [[0.737]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]]
maxi score, test score, baseline:  0.11015333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
printing an ep nov before normalisation:  43.14424642799381
maxi score, test score, baseline:  0.11015333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.11015333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
printing an ep nov before normalisation:  46.90575379434348
actor:  1 policy actor:  1  step number:  56 total reward:  0.33999999999999997  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11015333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5933333333333335  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.344]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.261]
 [0.344]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]]
maxi score, test score, baseline:  0.11015333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
printing an ep nov before normalisation:  45.31520436486316
printing an ep nov before normalisation:  0.0041130975790792945
actor:  1 policy actor:  1  step number:  52 total reward:  0.43333333333333357  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11015333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11015333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
line 256 mcts: sample exp_bonus 48.37354875780579
actor:  1 policy actor:  1  step number:  54 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.11015333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
actor:  1 policy actor:  1  step number:  48 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.11015333333333317 0.6373333333333335 0.6373333333333335
maxi score, test score, baseline:  0.11015333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([0.6212, 0.0129, 0.0848, 0.0702, 0.0948, 0.0606, 0.0554],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0040, 0.9792, 0.0021, 0.0026, 0.0016, 0.0014, 0.0092],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1228, 0.0090, 0.4635, 0.1010, 0.1083, 0.1217, 0.0737],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1599, 0.0669, 0.0840, 0.3321, 0.1314, 0.1109, 0.1147],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1847, 0.0221, 0.0496, 0.1204, 0.4947, 0.0559, 0.0727],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0320, 0.0044, 0.0578, 0.0716, 0.0558, 0.7538, 0.0245],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2249, 0.0039, 0.1374, 0.1408, 0.1851, 0.1643, 0.1436],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.628]
 [0.596]
 [0.596]
 [0.656]
 [0.583]
 [0.596]] [[28.152]
 [38.042]
 [30.828]
 [30.828]
 [38.066]
 [31.157]
 [30.828]] [[1.255]
 [1.632]
 [1.324]
 [1.324]
 [1.661]
 [1.325]
 [1.324]]
actions average: 
K:  4  action  0 :  tensor([0.5276, 0.0014, 0.0740, 0.0795, 0.1458, 0.0724, 0.0993],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0208, 0.9116, 0.0150, 0.0094, 0.0138, 0.0102, 0.0192],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1262, 0.0151, 0.4297, 0.0710, 0.0712, 0.1557, 0.1311],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2501, 0.0711, 0.1265, 0.1276, 0.1443, 0.1568, 0.1236],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1565, 0.0031, 0.0760, 0.0706, 0.5670, 0.0728, 0.0540],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0319, 0.0576, 0.1519, 0.0391, 0.0528, 0.6076, 0.0591],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1502, 0.0080, 0.1604, 0.1749, 0.2724, 0.1139, 0.1202],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.269]
 [0.305]
 [0.269]
 [0.269]
 [0.269]
 [0.269]
 [0.269]] [[27.583]
 [34.395]
 [27.583]
 [27.583]
 [27.583]
 [27.583]
 [27.583]] [[0.441]
 [0.569]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]]
actor:  1 policy actor:  1  step number:  57 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11015333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.11015333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
printing an ep nov before normalisation:  30.86539371993439
maxi score, test score, baseline:  0.11015333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.333
using another actor
from probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.11015333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
actor:  1 policy actor:  1  step number:  42 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  1.0
using another actor
printing an ep nov before normalisation:  9.033231426137792
printing an ep nov before normalisation:  34.51311549006187
Printing some Q and Qe and total Qs values:  [[0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]] [[48.775]
 [48.775]
 [48.775]
 [48.775]
 [48.775]
 [48.775]
 [48.775]] [[0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]]
maxi score, test score, baseline:  0.11015333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.11015333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
actor:  0 policy actor:  1  step number:  44 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  47 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11579333333333319 0.6373333333333335 0.6373333333333335
maxi score, test score, baseline:  0.11579333333333319 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.11579333333333319 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.369]
 [0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]] [[27.37 ]
 [34.817]
 [27.37 ]
 [27.37 ]
 [27.37 ]
 [27.37 ]
 [27.37 ]] [[1.062]
 [1.474]
 [1.062]
 [1.062]
 [1.062]
 [1.062]
 [1.062]]
maxi score, test score, baseline:  0.11579333333333319 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
printing an ep nov before normalisation:  47.14565483001555
siam score:  -0.76433945
printing an ep nov before normalisation:  28.398335386102605
maxi score, test score, baseline:  0.11579333333333319 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
printing an ep nov before normalisation:  55.67881741338939
actions average: 
K:  4  action  0 :  tensor([0.5547, 0.0281, 0.0610, 0.0628, 0.1566, 0.0692, 0.0676],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0134, 0.9333, 0.0040, 0.0082, 0.0063, 0.0047, 0.0301],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0523, 0.0346, 0.7325, 0.0211, 0.0701, 0.0638, 0.0257],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1106, 0.0682, 0.0950, 0.2004, 0.1721, 0.2050, 0.1487],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1991, 0.0972, 0.1309, 0.0964, 0.2301, 0.1764, 0.0698],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1672, 0.0173, 0.2258, 0.0859, 0.1367, 0.2909, 0.0763],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1617, 0.1900, 0.1376, 0.1022, 0.1866, 0.1083, 0.1137],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  41.071815490722656
maxi score, test score, baseline:  0.11053999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
maxi score, test score, baseline:  0.11053999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
Printing some Q and Qe and total Qs values:  [[0.129]
 [0.328]
 [0.367]
 [0.233]
 [0.328]
 [0.429]
 [0.328]] [[25.171]
 [27.242]
 [25.288]
 [25.733]
 [27.242]
 [31.868]
 [27.242]] [[1.006]
 [1.348]
 [1.252]
 [1.149]
 [1.348]
 [1.771]
 [1.348]]
maxi score, test score, baseline:  0.11053999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11053999999999986 0.6373333333333335 0.6373333333333335
maxi score, test score, baseline:  0.11053999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.06353033830543829, 0.06353033830543829, 0.06353033830543829, 0.3288820648418537, 0.06353033830543829, 0.4169965819363931]
actor:  1 policy actor:  1  step number:  37 total reward:  0.56  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  30 total reward:  0.7000000000000001  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.09 ]
 [0.187]
 [0.09 ]
 [0.09 ]
 [0.09 ]
 [0.036]
 [0.09 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.09 ]
 [0.187]
 [0.09 ]
 [0.09 ]
 [0.09 ]
 [0.036]
 [0.09 ]]
printing an ep nov before normalisation:  56.39521969136858
actor:  1 policy actor:  1  step number:  46 total reward:  0.5533333333333337  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.11053999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
printing an ep nov before normalisation:  36.19102074051121
Printing some Q and Qe and total Qs values:  [[ 0.246]
 [ 0.415]
 [-0.064]
 [ 0.198]
 [ 0.253]
 [-0.074]
 [ 0.191]] [[32.434]
 [40.161]
 [31.883]
 [29.859]
 [30.659]
 [31.09 ]
 [29.858]] [[1.123]
 [1.704]
 [0.783]
 [0.938]
 [1.036]
 [0.731]
 [0.93 ]]
maxi score, test score, baseline:  0.11053999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]] [[50.234]
 [50.234]
 [50.234]
 [50.234]
 [50.234]
 [50.234]
 [50.234]] [[1.665]
 [1.665]
 [1.665]
 [1.665]
 [1.665]
 [1.665]
 [1.665]]
maxi score, test score, baseline:  0.11053999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
actions average: 
K:  4  action  0 :  tensor([0.3982, 0.0171, 0.0879, 0.1287, 0.1865, 0.0732, 0.1084],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0135, 0.9196, 0.0128, 0.0162, 0.0095, 0.0076, 0.0208],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0806, 0.1420, 0.3787, 0.1085, 0.0790, 0.1267, 0.0845],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1153, 0.0511, 0.1099, 0.2432, 0.1987, 0.1148, 0.1670],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2071, 0.0028, 0.1114, 0.1341, 0.3662, 0.0803, 0.0982],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1358, 0.0473, 0.1456, 0.1125, 0.1280, 0.1642, 0.2667],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1051, 0.0005, 0.1428, 0.1149, 0.3033, 0.1130, 0.2204],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  0
printing an ep nov before normalisation:  39.09302294142243
printing an ep nov before normalisation:  0.01380582066929037
actor:  1 policy actor:  1  step number:  44 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.11053999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.247]
 [0.19 ]
 [0.186]
 [0.179]
 [0.183]
 [0.187]] [[41.979]
 [37.176]
 [44.44 ]
 [46.746]
 [45.213]
 [46.382]
 [43.226]] [[1.657]
 [1.456]
 [1.79 ]
 [1.909]
 [1.82 ]
 [1.887]
 [1.721]]
printing an ep nov before normalisation:  30.463407151312374
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.118]
 [0.126]
 [0.115]
 [0.128]
 [0.117]
 [0.112]
 [0.106]] [[52.763]
 [49.636]
 [51.994]
 [52.116]
 [52.22 ]
 [51.814]
 [51.494]] [[1.25 ]
 [1.141]
 [1.218]
 [1.235]
 [1.229]
 [1.209]
 [1.191]]
siam score:  -0.7634188
actor:  0 policy actor:  1  step number:  40 total reward:  0.5400000000000001  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]] [[38.265]
 [38.265]
 [38.265]
 [38.265]
 [38.265]
 [38.265]
 [38.265]] [[1.335]
 [1.335]
 [1.335]
 [1.335]
 [1.335]
 [1.335]
 [1.335]]
printing an ep nov before normalisation:  35.80315564759591
maxi score, test score, baseline:  0.11361999999999985 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
maxi score, test score, baseline:  0.11361999999999985 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
actor:  1 policy actor:  1  step number:  35 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  2.0
siam score:  -0.7673062
printing an ep nov before normalisation:  39.240777843790255
printing an ep nov before normalisation:  33.269167896141376
actor:  1 policy actor:  1  step number:  49 total reward:  0.36  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  41.217588747745154
maxi score, test score, baseline:  0.11361999999999985 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
printing an ep nov before normalisation:  29.499783876610355
actor:  0 policy actor:  0  step number:  51 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1160733333333332 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
actions average: 
K:  3  action  0 :  tensor([0.4357, 0.0036, 0.1411, 0.1096, 0.0840, 0.0998, 0.1261],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0235, 0.9213, 0.0228, 0.0060, 0.0035, 0.0038, 0.0192],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0729, 0.0824, 0.4041, 0.1276, 0.0995, 0.0868, 0.1267],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1772, 0.0212, 0.1664, 0.1351, 0.1812, 0.1571, 0.1617],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1091, 0.0691, 0.0501, 0.0803, 0.5887, 0.0623, 0.0403],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1291, 0.0252, 0.0957, 0.1529, 0.1429, 0.3576, 0.0965],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1414, 0.1876, 0.1034, 0.1238, 0.0895, 0.0724, 0.2819],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.11267333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
printing an ep nov before normalisation:  24.939820995304775
actor:  1 policy actor:  1  step number:  57 total reward:  0.10666666666666635  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  30.908358097076416
printing an ep nov before normalisation:  48.56374982401888
Printing some Q and Qe and total Qs values:  [[0.07 ]
 [0.103]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]
 [0.07 ]] [[40.161]
 [40.662]
 [40.161]
 [40.161]
 [40.161]
 [40.161]
 [40.161]] [[0.327]
 [0.366]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]]
printing an ep nov before normalisation:  39.02608871459961
maxi score, test score, baseline:  0.11267333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
printing an ep nov before normalisation:  47.07899909991048
actor:  1 policy actor:  1  step number:  48 total reward:  0.2999999999999998  reward:  1.0 rdn_beta:  1.333
from probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
maxi score, test score, baseline:  0.11267333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
maxi score, test score, baseline:  0.11267333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.23999999999999988  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11267333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
maxi score, test score, baseline:  0.11267333333333318 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  49 total reward:  0.4533333333333335  reward:  1.0 rdn_beta:  2.0
siam score:  -0.76870847
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.508]
 [0.426]
 [0.426]
 [0.426]
 [0.426]
 [0.426]] [[33.445]
 [38.199]
 [33.445]
 [33.445]
 [33.445]
 [33.445]
 [33.445]] [[1.678]
 [2.069]
 [1.678]
 [1.678]
 [1.678]
 [1.678]
 [1.678]]
Printing some Q and Qe and total Qs values:  [[0.248]
 [0.273]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]] [[50.852]
 [48.99 ]
 [50.852]
 [50.852]
 [50.852]
 [50.852]
 [50.852]] [[1.581]
 [1.532]
 [1.581]
 [1.581]
 [1.581]
 [1.581]
 [1.581]]
maxi score, test score, baseline:  0.11217999999999985 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
printing an ep nov before normalisation:  42.76241884588026
printing an ep nov before normalisation:  21.56423330307007
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11217999999999985 0.6373333333333335 0.6373333333333335
actor:  1 policy actor:  1  step number:  55 total reward:  0.2533333333333332  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]]
maxi score, test score, baseline:  0.11217999999999985 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.514]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]] [[35.708]
 [46.598]
 [35.708]
 [35.708]
 [35.708]
 [35.708]
 [35.708]] [[1.25 ]
 [1.588]
 [1.25 ]
 [1.25 ]
 [1.25 ]
 [1.25 ]
 [1.25 ]]
maxi score, test score, baseline:  0.11217999999999985 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
actor:  1 policy actor:  1  step number:  61 total reward:  0.3066666666666661  reward:  1.0 rdn_beta:  1.333
from probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
UNIT TEST: sample policy line 217 mcts : [0.02  0.796 0.02  0.02  0.082 0.02  0.041]
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.599]
 [0.046]
 [0.372]
 [0.47 ]
 [0.007]
 [0.454]] [[25.973]
 [38.203]
 [25.668]
 [24.007]
 [27.772]
 [28.553]
 [22.106]] [[0.492]
 [0.599]
 [0.046]
 [0.372]
 [0.47 ]
 [0.007]
 [0.454]]
siam score:  -0.76663613
maxi score, test score, baseline:  0.10877999999999984 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
printing an ep nov before normalisation:  31.646273094731654
actor:  1 policy actor:  1  step number:  45 total reward:  0.5466666666666671  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10877999999999984 0.6373333333333335 0.6373333333333335
actions average: 
K:  0  action  0 :  tensor([0.5691, 0.0041, 0.0747, 0.0899, 0.0897, 0.0779, 0.0947],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0022,     0.9448,     0.0026,     0.0291,     0.0006,     0.0006,
            0.0203], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1444, 0.0049, 0.5394, 0.0595, 0.0924, 0.0915, 0.0679],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1358, 0.0045, 0.0570, 0.3439, 0.1971, 0.1611, 0.1006],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1834, 0.0054, 0.0846, 0.0921, 0.4833, 0.0680, 0.0833],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1146, 0.0024, 0.1359, 0.0937, 0.0912, 0.4863, 0.0759],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2310, 0.1082, 0.1080, 0.1099, 0.1303, 0.1349, 0.1777],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  36.19333056092369
maxi score, test score, baseline:  0.10877999999999984 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.2405278430377
printing an ep nov before normalisation:  47.58761201962911
actor:  1 policy actor:  1  step number:  42 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10877999999999984 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
maxi score, test score, baseline:  0.10537999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
printing an ep nov before normalisation:  49.669515761903234
Printing some Q and Qe and total Qs values:  [[ 0.231]
 [ 0.413]
 [-0.09 ]
 [ 0.17 ]
 [ 0.195]
 [-0.031]
 [ 0.187]] [[30.987]
 [38.372]
 [25.603]
 [23.848]
 [26.756]
 [27.87 ]
 [26.204]] [[0.754]
 [1.148]
 [0.278]
 [0.488]
 [0.596]
 [0.402]
 [0.572]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  27.46443748474121
maxi score, test score, baseline:  0.10537999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
actions average: 
K:  1  action  0 :  tensor([0.4904, 0.0063, 0.0853, 0.0939, 0.1324, 0.0933, 0.0983],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0050, 0.9488, 0.0059, 0.0055, 0.0034, 0.0020, 0.0295],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1436, 0.0191, 0.4579, 0.0747, 0.1166, 0.1121, 0.0760],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1353, 0.0992, 0.0808, 0.3436, 0.1649, 0.0799, 0.0961],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2126, 0.0117, 0.1079, 0.1011, 0.3343, 0.1147, 0.1176],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0707, 0.0581, 0.0660, 0.0473, 0.0610, 0.6457, 0.0511],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2027, 0.1542, 0.1012, 0.0986, 0.1189, 0.0912, 0.2331],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  32.74082660675049
maxi score, test score, baseline:  0.10537999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
maxi score, test score, baseline:  0.10537999999999986 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.2999999999999998  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  43 total reward:  0.38666666666666616  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10475333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
maxi score, test score, baseline:  0.10475333333333317 0.6373333333333335 0.6373333333333335
printing an ep nov before normalisation:  38.1480725996586
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10475333333333317 0.6373333333333335 0.6373333333333335
maxi score, test score, baseline:  0.10475333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
from probs:  [0.07287779706201067, 0.07287779706201067, 0.07287779706201067, 0.21318793620531626, 0.07287779706201067, 0.4953008755466411]
actor:  1 policy actor:  1  step number:  33 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  21.19638227581383
printing an ep nov before normalisation:  34.763221740722656
maxi score, test score, baseline:  0.10475333333333317 0.6373333333333335 0.6373333333333335
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  0.10135333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.07288673947142219, 0.07288673947142219, 0.07288673947142219, 0.2130772553997927, 0.07288673947142219, 0.4953757867145186]
printing an ep nov before normalisation:  37.27419841689443
printing an ep nov before normalisation:  37.593715336401814
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.636]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]] [[35.647]
 [41.757]
 [35.647]
 [35.647]
 [35.647]
 [35.647]
 [35.647]] [[1.742]
 [2.303]
 [1.742]
 [1.742]
 [1.742]
 [1.742]
 [1.742]]
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.359]
 [0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.284]] [[44.832]
 [45.39 ]
 [44.832]
 [44.832]
 [44.832]
 [44.832]
 [44.832]] [[0.584]
 [0.665]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]]
maxi score, test score, baseline:  0.10135333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.07288673947142219, 0.07288673947142219, 0.07288673947142219, 0.2130772553997927, 0.07288673947142219, 0.4953757867145186]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.10135333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.07288673947142219, 0.07288673947142219, 0.07288673947142219, 0.2130772553997927, 0.07288673947142219, 0.4953757867145186]
actor:  1 policy actor:  1  step number:  40 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.667
siam score:  -0.76727796
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.438]
 [0.37 ]
 [0.389]
 [0.396]
 [0.396]
 [0.459]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.396]
 [0.438]
 [0.37 ]
 [0.389]
 [0.396]
 [0.396]
 [0.459]]
maxi score, test score, baseline:  0.10135333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.07288673947142219, 0.07288673947142219, 0.07288673947142219, 0.2130772553997927, 0.07288673947142219, 0.4953757867145186]
maxi score, test score, baseline:  0.10135333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.07288673947142219, 0.07288673947142219, 0.07288673947142219, 0.2130772553997927, 0.07288673947142219, 0.4953757867145186]
Printing some Q and Qe and total Qs values:  [[0.141]
 [0.141]
 [0.141]
 [0.141]
 [0.111]
 [0.141]
 [0.141]] [[43.796]
 [43.796]
 [43.796]
 [43.796]
 [48.655]
 [43.796]
 [43.796]] [[1.551]
 [1.551]
 [1.551]
 [1.551]
 [1.796]
 [1.551]
 [1.551]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3333333333333328  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  48.020585465176055
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.716]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]] [[35.857]
 [34.59 ]
 [35.857]
 [35.857]
 [35.857]
 [35.857]
 [35.857]] [[0.695]
 [0.716]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]]
printing an ep nov before normalisation:  36.82941436767578
printing an ep nov before normalisation:  46.75124732877137
printing an ep nov before normalisation:  42.737884832727616
maxi score, test score, baseline:  0.10135333333333317 0.6373333333333335 0.6373333333333335
Printing some Q and Qe and total Qs values:  [[0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]] [[37.266]
 [37.266]
 [37.266]
 [37.266]
 [37.266]
 [37.266]
 [37.266]] [[0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]]
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.39 ]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]] [[31.954]
 [37.226]
 [31.954]
 [31.954]
 [31.954]
 [31.954]
 [31.954]] [[1.399]
 [1.829]
 [1.399]
 [1.399]
 [1.399]
 [1.399]
 [1.399]]
maxi score, test score, baseline:  0.10135333333333317 0.6373333333333335 0.6373333333333335
probs:  [0.07288673947142219, 0.07288673947142219, 0.07288673947142219, 0.2130772553997927, 0.07288673947142219, 0.4953757867145186]
Printing some Q and Qe and total Qs values:  [[0.839]
 [0.839]
 [0.839]
 [0.85 ]
 [0.839]
 [0.839]
 [0.839]] [[38.847]
 [38.847]
 [38.847]
 [35.751]
 [38.847]
 [38.847]
 [38.847]] [[0.839]
 [0.839]
 [0.839]
 [0.85 ]
 [0.839]
 [0.839]
 [0.839]]
printing an ep nov before normalisation:  32.33329814702324
printing an ep nov before normalisation:  29.70988314082639
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  27.247674425856996
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  0.10545999999999985 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  33.29486714975245
printing an ep nov before normalisation:  35.43872332368601
actor:  1 policy actor:  1  step number:  51 total reward:  0.3866666666666667  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10545999999999985 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.73 ]
 [0.581]
 [0.73 ]
 [0.73 ]
 [0.456]
 [0.73 ]] [[51.762]
 [51.762]
 [45.91 ]
 [51.762]
 [51.762]
 [46.543]
 [51.762]] [[1.906]
 [1.906]
 [1.589]
 [1.906]
 [1.906]
 [1.482]
 [1.906]]
Printing some Q and Qe and total Qs values:  [[0.066]
 [0.132]
 [0.064]
 [0.061]
 [0.064]
 [0.064]
 [0.064]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.066]
 [0.132]
 [0.064]
 [0.061]
 [0.064]
 [0.064]
 [0.064]]
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.122]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]] [[ 0.   ]
 [38.375]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.394]
 [ 1.445]
 [-0.394]
 [-0.394]
 [-0.394]
 [-0.394]
 [-0.394]]
actions average: 
K:  4  action  0 :  tensor([0.3743, 0.1070, 0.1003, 0.0811, 0.1610, 0.0876, 0.0887],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0078, 0.9161, 0.0058, 0.0230, 0.0013, 0.0017, 0.0443],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0693, 0.0068, 0.5274, 0.0859, 0.1130, 0.1330, 0.0646],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1199, 0.0234, 0.1063, 0.3440, 0.1360, 0.1463, 0.1241],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1796, 0.0646, 0.1140, 0.0818, 0.3617, 0.0871, 0.1112],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1595, 0.0967, 0.0837, 0.0894, 0.1317, 0.3505, 0.0884],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1348, 0.0982, 0.1315, 0.2185, 0.1263, 0.0758, 0.2148],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.10545999999999985 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3533333333333334  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  51 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10545999999999985 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  41.20206654573037
printing an ep nov before normalisation:  36.208001943913445
maxi score, test score, baseline:  0.10313999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10313999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  35.33974180355369
Printing some Q and Qe and total Qs values:  [[ 0.043]
 [ 0.036]
 [-0.   ]
 [ 0.003]
 [-0.   ]
 [ 0.009]
 [-0.002]] [[32.632]
 [27.142]
 [18.4  ]
 [18.146]
 [18.059]
 [23.285]
 [19.259]] [[0.47 ]
 [0.338]
 [0.103]
 [0.101]
 [0.095]
 [0.224]
 [0.121]]
Printing some Q and Qe and total Qs values:  [[ 0.051]
 [ 0.036]
 [-0.   ]
 [ 0.003]
 [-0.   ]
 [ 0.009]
 [-0.002]] [[34.816]
 [27.142]
 [18.4  ]
 [18.146]
 [18.059]
 [23.285]
 [19.259]] [[0.479]
 [0.307]
 [0.092]
 [0.091]
 [0.086]
 [0.202]
 [0.109]]
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.386]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]] [[32.626]
 [44.579]
 [32.626]
 [32.626]
 [32.626]
 [32.626]
 [32.626]] [[1.076]
 [1.432]
 [1.076]
 [1.076]
 [1.076]
 [1.076]
 [1.076]]
maxi score, test score, baseline:  0.10313999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  50 total reward:  0.3666666666666667  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10313999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.10313999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.10313999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.5  ]
 [0.566]
 [0.5  ]
 [0.5  ]
 [0.486]
 [0.5  ]] [[32.841]
 [32.841]
 [42.892]
 [32.841]
 [32.841]
 [42.565]
 [32.841]] [[1.025]
 [1.025]
 [1.403]
 [1.025]
 [1.025]
 [1.314]
 [1.025]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.10076666666666652 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.10076666666666652 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  37 total reward:  0.52  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10076666666666652 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  59.557076055400636
maxi score, test score, baseline:  0.10076666666666652 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  32.96285670402101
Printing some Q and Qe and total Qs values:  [[0.844]
 [0.836]
 [0.703]
 [0.825]
 [0.826]
 [0.798]
 [0.815]] [[32.689]
 [37.348]
 [34.815]
 [32.7  ]
 [34.072]
 [32.601]
 [28.629]] [[0.844]
 [0.836]
 [0.703]
 [0.825]
 [0.826]
 [0.798]
 [0.815]]
printing an ep nov before normalisation:  0.0003580973725547665
Printing some Q and Qe and total Qs values:  [[0.659]
 [0.845]
 [0.659]
 [0.659]
 [0.659]
 [0.485]
 [0.659]] [[45.882]
 [41.463]
 [45.882]
 [45.882]
 [45.882]
 [44.865]
 [45.882]] [[2.655]
 [2.532]
 [2.655]
 [2.655]
 [2.655]
 [2.409]
 [2.655]]
actor:  1 policy actor:  1  step number:  41 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10076666666666652 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  45.166491942566914
printing an ep nov before normalisation:  38.16041949310037
maxi score, test score, baseline:  0.09816666666666651 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09816666666666651 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
siam score:  -0.7615695
maxi score, test score, baseline:  0.09816666666666651 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]] [[42.61]
 [42.61]
 [42.61]
 [42.61]
 [42.61]
 [42.61]
 [42.61]] [[2.469]
 [2.469]
 [2.469]
 [2.469]
 [2.469]
 [2.469]
 [2.469]]
line 256 mcts: sample exp_bonus 42.881403936758076
printing an ep nov before normalisation:  38.962618117546924
actor:  1 policy actor:  1  step number:  54 total reward:  0.2866666666666666  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09816666666666651 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09816666666666651 0.6886666666666669 0.6886666666666669
printing an ep nov before normalisation:  35.052232132521766
printing an ep nov before normalisation:  44.838957694836274
actor:  1 policy actor:  1  step number:  53 total reward:  0.5466666666666671  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09816666666666651 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  51.399918287549404
maxi score, test score, baseline:  0.09816666666666651 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  60 total reward:  0.4733333333333336  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]] [[16.677]
 [19.977]
 [18.635]
 [33.382]
 [34.374]
 [18.08 ]
 [38.471]] [[0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.824]]
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.116]
 [0.093]
 [0.093]
 [0.035]
 [0.021]
 [0.057]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.022]
 [0.116]
 [0.093]
 [0.093]
 [0.035]
 [0.021]
 [0.057]]
actor:  0 policy actor:  0  step number:  44 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09800666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09800666666666653 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.09800666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  42.339749612992144
printing an ep nov before normalisation:  44.995812905250276
actor:  1 policy actor:  1  step number:  39 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  34.873104042521405
maxi score, test score, baseline:  0.09800666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  55 total reward:  0.39999999999999936  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  39.20424869886089
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.261]
 [0.195]
 [0.195]
 [0.266]
 [0.227]
 [0.195]] [[35.285]
 [37.648]
 [32.39 ]
 [32.39 ]
 [35.948]
 [31.429]
 [32.39 ]] [[1.35 ]
 [1.48 ]
 [1.131]
 [1.131]
 [1.394]
 [1.111]
 [1.131]]
printing an ep nov before normalisation:  40.86212273096667
printing an ep nov before normalisation:  40.921760463350715
maxi score, test score, baseline:  0.09805999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  31.18927502499648
actor:  1 policy actor:  1  step number:  45 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09805999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09805999999999986 0.6886666666666669 0.6886666666666669
actor:  1 policy actor:  1  step number:  44 total reward:  0.5133333333333335  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  54 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.09805999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  0.09805999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09805999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  39.668258031209305
using explorer policy with actor:  0
printing an ep nov before normalisation:  28.695473074913025
maxi score, test score, baseline:  0.09537999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  55 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.474]
 [0.351]
 [0.319]
 [0.315]
 [0.312]
 [0.351]] [[50.015]
 [45.468]
 [45.389]
 [49.295]
 [50.584]
 [50.11 ]
 [45.389]] [[1.448]
 [1.461]
 [1.336]
 [1.436]
 [1.476]
 [1.457]
 [1.336]]
printing an ep nov before normalisation:  43.822285269765715
actor:  0 policy actor:  1  step number:  47 total reward:  0.45333333333333303  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09828666666666651 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  33.80267301661852
siam score:  -0.7561476
actor:  1 policy actor:  1  step number:  38 total reward:  0.5  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  55.98898678117702
maxi score, test score, baseline:  0.09828666666666651 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]] [[40.157]
 [40.157]
 [40.157]
 [40.157]
 [40.157]
 [40.157]
 [40.157]] [[0.989]
 [0.989]
 [0.989]
 [0.989]
 [0.989]
 [0.989]
 [0.989]]
printing an ep nov before normalisation:  30.23601271587544
maxi score, test score, baseline:  0.09828666666666651 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09828666666666651 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  60.45057954292215
maxi score, test score, baseline:  0.09828666666666651 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  63.68118165234376
maxi score, test score, baseline:  0.09828666666666651 0.6886666666666669 0.6886666666666669
actor:  1 policy actor:  1  step number:  47 total reward:  0.4666666666666668  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  34.80055664857471
Printing some Q and Qe and total Qs values:  [[0.175]
 [0.233]
 [0.128]
 [0.123]
 [0.221]
 [0.127]
 [0.128]] [[38.91 ]
 [39.047]
 [25.481]
 [25.331]
 [36.659]
 [24.771]
 [24.803]] [[0.866]
 [0.927]
 [0.45 ]
 [0.441]
 [0.85 ]
 [0.43 ]
 [0.432]]
actions average: 
K:  1  action  0 :  tensor([0.6840, 0.0048, 0.0552, 0.0558, 0.0919, 0.0588, 0.0495],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0060, 0.9501, 0.0039, 0.0041, 0.0013, 0.0042, 0.0305],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1043, 0.0105, 0.5161, 0.0822, 0.0729, 0.1266, 0.0874],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0525, 0.3074, 0.0301, 0.3915, 0.0584, 0.0396, 0.1205],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2032, 0.0313, 0.1272, 0.1327, 0.2875, 0.0807, 0.1374],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1684, 0.0616, 0.1251, 0.0836, 0.1024, 0.3675, 0.0916],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1474, 0.2281, 0.0881, 0.0912, 0.0876, 0.0775, 0.2801],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.09828666666666651 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09828666666666651 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  44 total reward:  0.3666666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09828666666666651 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  42.72566199973754
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  39 total reward:  0.52  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[ 0.115]
 [ 0.294]
 [-0.011]
 [ 0.115]
 [ 0.115]
 [-0.005]
 [ 0.115]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.115]
 [ 0.294]
 [-0.011]
 [ 0.115]
 [ 0.115]
 [-0.005]
 [ 0.115]]
printing an ep nov before normalisation:  38.55514709310994
printing an ep nov before normalisation:  25.641790530324965
printing an ep nov before normalisation:  45.738461746113224
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.771]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]] [[11.506]
 [23.399]
 [11.506]
 [11.506]
 [11.506]
 [11.506]
 [11.506]] [[0.665]
 [0.771]
 [0.665]
 [0.665]
 [0.665]
 [0.665]
 [0.665]]
Printing some Q and Qe and total Qs values:  [[0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]] [[25.95]
 [25.95]
 [25.95]
 [25.95]
 [25.95]
 [25.95]
 [25.95]] [[0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]]
printing an ep nov before normalisation:  31.982446927351024
siam score:  -0.7624561
actor:  1 policy actor:  1  step number:  35 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  48.99024677248806
maxi score, test score, baseline:  0.09828666666666651 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  32.66741733499815
actor:  0 policy actor:  1  step number:  69 total reward:  0.053333333333332233  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.84 ]
 [0.67 ]
 [0.488]
 [0.701]
 [0.811]
 [0.607]
 [0.492]] [[39.482]
 [28.82 ]
 [33.73 ]
 [30.913]
 [38.152]
 [34.819]
 [29.066]] [[0.84 ]
 [0.67 ]
 [0.488]
 [0.701]
 [0.811]
 [0.607]
 [0.492]]
maxi score, test score, baseline:  0.09791333333333319 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09791333333333319 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.841]
 [0.831]
 [0.804]
 [0.805]
 [0.785]
 [0.827]] [[27.399]
 [31.517]
 [37.944]
 [23.937]
 [23.649]
 [28.145]
 [29.308]] [[0.815]
 [0.841]
 [0.831]
 [0.804]
 [0.805]
 [0.785]
 [0.827]]
printing an ep nov before normalisation:  0.0050389467737943505
actor:  0 policy actor:  1  step number:  37 total reward:  0.4933333333333332  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09808666666666654 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.577]
 [0.556]
 [0.56 ]
 [0.551]
 [0.56 ]
 [0.547]] [[45.774]
 [44.865]
 [48.849]
 [49.037]
 [49.923]
 [49.037]
 [50.548]] [[2.291]
 [2.229]
 [2.452]
 [2.467]
 [2.512]
 [2.467]
 [2.547]]
siam score:  -0.76302916
actor:  1 policy actor:  1  step number:  33 total reward:  0.666666666666667  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  50 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  30.636460781097412
printing an ep nov before normalisation:  30.49272612762646
maxi score, test score, baseline:  0.09808666666666654 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09808666666666654 0.6886666666666669 0.6886666666666669
printing an ep nov before normalisation:  39.47283830332538
printing an ep nov before normalisation:  37.57918058488893
maxi score, test score, baseline:  0.09808666666666654 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  18.968885578502718
actor:  0 policy actor:  1  step number:  69 total reward:  0.29333333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10067333333333318 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  53 total reward:  0.506666666666667  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.072]
 [0.054]
 [0.077]
 [0.077]
 [0.07 ]
 [0.077]
 [0.053]] [[35.345]
 [42.096]
 [36.54 ]
 [36.54 ]
 [43.124]
 [36.54 ]
 [34.114]] [[0.8  ]
 [1.13 ]
 [0.866]
 [0.866]
 [1.2  ]
 [0.866]
 [0.717]]
maxi score, test score, baseline:  0.10067333333333318 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.10067333333333318 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  36.032296135721225
printing an ep nov before normalisation:  20.63305377960205
maxi score, test score, baseline:  0.10067333333333318 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4533333333333335  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10067333333333318 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.10067333333333318 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  44.205192907580184
maxi score, test score, baseline:  0.10067333333333318 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.10067333333333318 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  61 total reward:  0.1333333333333332  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  50 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.61 ]
 [0.667]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]] [[48.703]
 [48.156]
 [40.962]
 [40.962]
 [40.962]
 [40.962]
 [40.962]] [[2.005]
 [2.036]
 [1.596]
 [1.596]
 [1.596]
 [1.596]
 [1.596]]
maxi score, test score, baseline:  0.10067333333333318 0.6886666666666669 0.6886666666666669
actor:  1 policy actor:  1  step number:  42 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  40.86317981939019
printing an ep nov before normalisation:  36.38845199988504
maxi score, test score, baseline:  0.10067333333333318 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.10067333333333318 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.10067333333333318 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
siam score:  -0.7595773
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10067333333333318 0.6886666666666669 0.6886666666666669
actions average: 
K:  3  action  0 :  tensor([0.4084, 0.1196, 0.0836, 0.0660, 0.1424, 0.0635, 0.1163],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0116, 0.9094, 0.0177, 0.0131, 0.0044, 0.0053, 0.0384],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1123, 0.0346, 0.5108, 0.0754, 0.0801, 0.0695, 0.1173],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1224, 0.1075, 0.1298, 0.1937, 0.1345, 0.1165, 0.1958],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2171, 0.0088, 0.0945, 0.0915, 0.4051, 0.0802, 0.1028],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1036, 0.1129, 0.1083, 0.1299, 0.2252, 0.1956, 0.1245],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1413, 0.1419, 0.0604, 0.2565, 0.0624, 0.0522, 0.2853],
       grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 35.350860498234006
printing an ep nov before normalisation:  49.74716097739589
maxi score, test score, baseline:  0.10067333333333318 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.10067333333333318 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  29.86152885594523
printing an ep nov before normalisation:  36.323873692710364
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.653]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]] [[53.413]
 [45.523]
 [53.413]
 [53.413]
 [53.413]
 [53.413]
 [53.413]] [[1.2  ]
 [1.183]
 [1.2  ]
 [1.2  ]
 [1.2  ]
 [1.2  ]
 [1.2  ]]
maxi score, test score, baseline:  0.10067333333333318 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  57 total reward:  0.14666666666666606  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  44.13889189825568
maxi score, test score, baseline:  0.10067333333333318 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  44.063725930748916
actor:  1 policy actor:  1  step number:  43 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10067333333333318 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.10067333333333318 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  64 total reward:  0.15333333333333254  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10067333333333318 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10067333333333318 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  32.10521697998047
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.531]
 [0.41 ]
 [0.413]
 [0.413]
 [0.415]
 [0.414]] [[28.736]
 [29.878]
 [28.443]
 [30.541]
 [30.5  ]
 [29.134]
 [28.815]] [[1.764]
 [1.993]
 [1.746]
 [1.933]
 [1.93 ]
 [1.811]
 [1.783]]
printing an ep nov before normalisation:  35.884197170328655
printing an ep nov before normalisation:  32.19351777012818
actor:  1 policy actor:  1  step number:  45 total reward:  0.6000000000000003  reward:  1.0 rdn_beta:  2.0
using another actor
maxi score, test score, baseline:  0.10067333333333318 0.6886666666666669 0.6886666666666669
actor:  1 policy actor:  1  step number:  38 total reward:  0.5933333333333335  reward:  1.0 rdn_beta:  1.667
siam score:  -0.74891096
Printing some Q and Qe and total Qs values:  [[0.792]
 [0.862]
 [0.783]
 [0.793]
 [0.793]
 [0.783]
 [0.861]] [[21.982]
 [22.627]
 [18.219]
 [21.828]
 [21.932]
 [18.275]
 [23.063]] [[0.792]
 [0.862]
 [0.783]
 [0.793]
 [0.793]
 [0.783]
 [0.861]]
printing an ep nov before normalisation:  20.698127567807397
printing an ep nov before normalisation:  26.156296730041504
printing an ep nov before normalisation:  26.967881481540395
printing an ep nov before normalisation:  30.470291685619774
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.75117831122018
actor:  0 policy actor:  0  step number:  35 total reward:  0.4933333333333332  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  34.82045135114889
maxi score, test score, baseline:  0.10091333333333319 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  38.899717138710066
Printing some Q and Qe and total Qs values:  [[0.89 ]
 [0.911]
 [0.875]
 [0.883]
 [0.866]
 [0.882]
 [0.859]] [[28.726]
 [30.944]
 [27.109]
 [27.041]
 [26.251]
 [26.918]
 [26.111]] [[0.89 ]
 [0.911]
 [0.875]
 [0.883]
 [0.866]
 [0.882]
 [0.859]]
maxi score, test score, baseline:  0.10091333333333319 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  53 total reward:  0.39999999999999925  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  43 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10371333333333317 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.10371333333333317 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.10371333333333317 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]] [[32.733]
 [32.733]
 [32.733]
 [32.733]
 [32.733]
 [32.733]
 [32.733]] [[21.867]
 [21.867]
 [21.867]
 [21.867]
 [21.867]
 [21.867]
 [21.867]]
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]] [[28.529]
 [28.529]
 [28.529]
 [28.529]
 [28.529]
 [28.529]
 [28.529]] [[0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]]
maxi score, test score, baseline:  0.10371333333333317 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.10371333333333317 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  54.886063157486134
printing an ep nov before normalisation:  35.10876607529987
actor:  1 policy actor:  1  step number:  59 total reward:  0.09333333333333282  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  32.54192935095893
Printing some Q and Qe and total Qs values:  [[-0.011]
 [ 0.068]
 [-0.012]
 [-0.01 ]
 [-0.01 ]
 [-0.011]
 [-0.011]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.011]
 [ 0.068]
 [-0.012]
 [-0.01 ]
 [-0.01 ]
 [-0.011]
 [-0.011]]
actor:  1 policy actor:  1  step number:  41 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10371333333333317 0.6886666666666669 0.6886666666666669
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10371333333333317 0.6886666666666669 0.6886666666666669
line 256 mcts: sample exp_bonus 41.76137821443367
printing an ep nov before normalisation:  39.16489405689265
maxi score, test score, baseline:  0.10371333333333317 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  42.62495571051802
maxi score, test score, baseline:  0.10371333333333317 0.6886666666666669 0.6886666666666669
actor:  0 policy actor:  0  step number:  37 total reward:  0.52  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1067533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  38.892896050164815
maxi score, test score, baseline:  0.1067533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  0.31333333333333324  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  34.02082920074463
actor:  1 policy actor:  1  step number:  53 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  50.02068478203958
printing an ep nov before normalisation:  49.29291653996909
maxi score, test score, baseline:  0.1067533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  50.30302681023808
actor:  1 policy actor:  1  step number:  50 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1067533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  50.86192982171021
printing an ep nov before normalisation:  31.94961311255228
printing an ep nov before normalisation:  34.00942221260509
maxi score, test score, baseline:  0.1067533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.747]
 [0.7  ]
 [0.725]
 [0.649]
 [0.743]
 [0.725]
 [0.666]] [[28.392]
 [29.893]
 [34.472]
 [29.755]
 [33.62 ]
 [34.472]
 [27.985]] [[1.496]
 [1.547]
 [1.868]
 [1.487]
 [1.831]
 [1.868]
 [1.39 ]]
maxi score, test score, baseline:  0.1067533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  39.96561727571475
actor:  1 policy actor:  1  step number:  55 total reward:  0.23999999999999988  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1067533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
using explorer policy with actor:  1
siam score:  -0.7633435
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  39 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1067533333333332 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.1067533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.1067533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.1067533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.571]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]] [[36.462]
 [41.49 ]
 [36.462]
 [36.462]
 [36.462]
 [36.462]
 [36.462]] [[1.827]
 [2.234]
 [1.827]
 [1.827]
 [1.827]
 [1.827]
 [1.827]]
maxi score, test score, baseline:  0.1067533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  36.88099615398818
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  47 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  53 total reward:  0.4666666666666669  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10404666666666652 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
siam score:  -0.76828784
printing an ep nov before normalisation:  54.455726376746355
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.591]
 [0.492]
 [0.523]
 [0.523]
 [0.489]
 [0.523]] [[33.967]
 [29.128]
 [45.762]
 [33.967]
 [33.967]
 [43.848]
 [33.967]] [[1.283]
 [1.162]
 [1.71 ]
 [1.283]
 [1.283]
 [1.633]
 [1.283]]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.10404666666666652 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]] [[35.94]
 [35.94]
 [35.94]
 [35.94]
 [35.94]
 [35.94]
 [35.94]] [[0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]]
maxi score, test score, baseline:  0.10404666666666652 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  41 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.10404666666666652 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  23.80469457852964
siam score:  -0.76853514
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.331]
 [0.331]
 [0.331]
 [0.542]
 [0.331]
 [0.331]] [[58.074]
 [58.492]
 [58.492]
 [58.492]
 [55.768]
 [58.492]
 [58.492]] [[1.757]
 [1.587]
 [1.587]
 [1.587]
 [1.715]
 [1.587]
 [1.587]]
actor:  0 policy actor:  0  step number:  45 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10388666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
line 256 mcts: sample exp_bonus 39.71823624518059
actor:  1 policy actor:  1  step number:  61 total reward:  0.1599999999999996  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  41.022150293257724
maxi score, test score, baseline:  0.10388666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  61 total reward:  0.23999999999999988  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  28.257922974446934
maxi score, test score, baseline:  0.10388666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  0.10388666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  37.92291411420301
actor:  1 policy actor:  1  step number:  39 total reward:  0.52  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.633]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]] [[34.396]
 [38.74 ]
 [34.396]
 [34.396]
 [34.396]
 [34.396]
 [34.396]] [[1.556]
 [1.834]
 [1.556]
 [1.556]
 [1.556]
 [1.556]
 [1.556]]
maxi score, test score, baseline:  0.10388666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  36.71127253688789
actor:  1 policy actor:  1  step number:  53 total reward:  0.2933333333333331  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  42.11901193963864
actor:  1 policy actor:  1  step number:  59 total reward:  0.2133333333333327  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.10388666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  29.159183502197266
printing an ep nov before normalisation:  41.524828832944145
actor:  1 policy actor:  1  step number:  40 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  39.36389343553423
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  42 total reward:  0.5  reward:  1.0 rdn_beta:  1.333
from probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  29.612217531836347
maxi score, test score, baseline:  0.10388666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  48.556334936506296
printing an ep nov before normalisation:  46.308577876411015
actor:  1 policy actor:  1  step number:  51 total reward:  0.2799999999999999  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  44 total reward:  0.4600000000000001  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.482057461739
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.356]
 [0.251]
 [0.251]
 [0.27 ]
 [0.251]
 [0.251]] [[38.222]
 [39.293]
 [38.222]
 [38.222]
 [41.652]
 [38.222]
 [38.222]] [[1.864]
 [2.046]
 [1.864]
 [1.864]
 [2.127]
 [1.864]
 [1.864]]
maxi score, test score, baseline:  0.10680666666666652 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.10680666666666652 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  0 policy actor:  0  step number:  53 total reward:  0.1999999999999993  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  21.94654941558838
printing an ep nov before normalisation:  0.25665344962703784
maxi score, test score, baseline:  0.10671333333333317 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.10671333333333317 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3933333333333333  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  53.144544260737575
actor:  1 policy actor:  1  step number:  39 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  50.896461539143175
maxi score, test score, baseline:  0.10671333333333317 0.6886666666666669 0.6886666666666669
printing an ep nov before normalisation:  36.065276045329405
maxi score, test score, baseline:  0.10671333333333317 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  57 total reward:  0.41333333333333355  reward:  1.0 rdn_beta:  2.0
from probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  39.5180951448318
printing an ep nov before normalisation:  37.94328689575195
printing an ep nov before normalisation:  25.93645899611419
maxi score, test score, baseline:  0.10671333333333317 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.609]
 [0.56 ]
 [0.604]
 [0.601]
 [0.599]
 [0.599]
 [0.597]] [[27.897]
 [30.87 ]
 [29.748]
 [31.078]
 [30.655]
 [29.122]
 [30.238]] [[1.735]
 [1.923]
 [1.877]
 [1.981]
 [1.945]
 [1.823]
 [1.91 ]]
maxi score, test score, baseline:  0.10671333333333317 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  44.700042088419345
maxi score, test score, baseline:  0.10671333333333317 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5866666666666669  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10671333333333317 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
line 256 mcts: sample exp_bonus 31.322178799495177
siam score:  -0.76740944
printing an ep nov before normalisation:  22.38326072692871
maxi score, test score, baseline:  0.10671333333333317 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.10671333333333317 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  0 policy actor:  0  step number:  78 total reward:  0.08666666666666634  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.10629999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  21.458840427256415
line 256 mcts: sample exp_bonus 35.1982124245681
actions average: 
K:  4  action  0 :  tensor([0.5324, 0.0104, 0.1097, 0.0679, 0.1343, 0.0837, 0.0616],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0094, 0.9262, 0.0131, 0.0077, 0.0047, 0.0143, 0.0246],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0331, 0.2244, 0.5405, 0.0080, 0.0083, 0.1249, 0.0609],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.3338, 0.1225, 0.0717, 0.1723, 0.1075, 0.1210, 0.0713],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2683, 0.1179, 0.0936, 0.0756, 0.3169, 0.0714, 0.0563],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1534, 0.0105, 0.0927, 0.0678, 0.0664, 0.5486, 0.0607],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2744, 0.0061, 0.1420, 0.1353, 0.1650, 0.1253, 0.1519],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.10415333333333317 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.10415333333333317 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.10415333333333317 0.6886666666666669 0.6886666666666669
actions average: 
K:  0  action  0 :  tensor([0.5017, 0.0083, 0.0852, 0.1140, 0.1159, 0.0822, 0.0927],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0101, 0.9493, 0.0044, 0.0091, 0.0042, 0.0027, 0.0200],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0723, 0.0058, 0.6459, 0.0395, 0.0292, 0.1626, 0.0447],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2322, 0.0058, 0.1305, 0.2402, 0.1262, 0.1161, 0.1490],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2046, 0.0073, 0.0748, 0.0809, 0.4460, 0.0798, 0.1067],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1382, 0.0095, 0.1404, 0.0871, 0.0793, 0.4590, 0.0865],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1423, 0.2053, 0.0921, 0.1015, 0.1036, 0.0821, 0.2732],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  54 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  24.367887434760366
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]] [[28.891]
 [28.891]
 [28.891]
 [28.891]
 [28.891]
 [28.891]
 [28.891]] [[0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]
 [0.727]]
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  41 total reward:  0.48  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10663333333333319 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5666666666666668  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  35.540120272077964
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.564]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]] [[27.608]
 [40.307]
 [23.982]
 [23.982]
 [23.982]
 [23.982]
 [23.982]] [[0.565]
 [0.564]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]]
maxi score, test score, baseline:  0.10663333333333319 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  28.889563405248502
printing an ep nov before normalisation:  27.10757847808638
actor:  1 policy actor:  1  step number:  64 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10663333333333319 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.10663333333333319 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.10663333333333319 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3533333333333327  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  44.674229316445626
printing an ep nov before normalisation:  38.53918768502692
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.511970822924166
actor:  1 policy actor:  1  step number:  38 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  37.200821003146274
printing an ep nov before normalisation:  33.92332469741836
actor:  1 policy actor:  1  step number:  52 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
Printing some Q and Qe and total Qs values:  [[0.31]
 [0.31]
 [0.31]
 [0.31]
 [0.31]
 [0.31]
 [0.31]] [[37.992]
 [37.992]
 [37.992]
 [37.992]
 [37.992]
 [37.992]
 [37.992]] [[0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  34.93340687858047
siam score:  -0.76521194
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
actor:  1 policy actor:  1  step number:  44 total reward:  0.43333333333333335  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]
 [0.672]] [[48.532]
 [48.532]
 [48.532]
 [48.532]
 [48.532]
 [48.532]
 [48.532]] [[1.277]
 [1.277]
 [1.277]
 [1.277]
 [1.277]
 [1.277]
 [1.277]]
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
Printing some Q and Qe and total Qs values:  [[-0.004]
 [ 0.13 ]
 [-0.004]
 [-0.004]
 [-0.002]
 [-0.004]
 [-0.004]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.004]
 [ 0.13 ]
 [-0.004]
 [-0.004]
 [-0.002]
 [-0.004]
 [-0.004]]
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  37.09918708888162
actor:  1 policy actor:  1  step number:  42 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  25.831632766387187
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  50 total reward:  0.44666666666666677  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  54 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actions average: 
K:  2  action  0 :  tensor([0.6078, 0.0138, 0.0628, 0.0747, 0.1056, 0.0725, 0.0628],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0086, 0.9524, 0.0031, 0.0116, 0.0037, 0.0043, 0.0164],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0632, 0.0297, 0.6320, 0.0302, 0.0484, 0.1447, 0.0518],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([nan, nan, nan, nan, nan, nan, nan], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1706, 0.0022, 0.0821, 0.0843, 0.4770, 0.0817, 0.1022],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0785, 0.0022, 0.0943, 0.0605, 0.0631, 0.6441, 0.0573],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1124, 0.2163, 0.0812, 0.1166, 0.1336, 0.0585, 0.2814],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.196]
 [0.29 ]
 [0.213]
 [0.175]
 [0.253]
 [0.15 ]
 [0.222]] [[35.031]
 [38.982]
 [36.468]
 [30.818]
 [38.319]
 [31.288]
 [36.079]] [[0.981]
 [1.237]
 [1.057]
 [0.786]
 [1.172]
 [0.78 ]
 [1.05 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]
 [0.677]] [[43.27]
 [43.27]
 [43.27]
 [43.27]
 [43.27]
 [43.27]
 [43.27]] [[1.88]
 [1.88]
 [1.88]
 [1.88]
 [1.88]
 [1.88]
 [1.88]]
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  0.4066666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  57 total reward:  0.2666666666666666  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  28.218260937963915
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]
 [0.229]] [[50.54]
 [50.54]
 [50.54]
 [50.54]
 [50.54]
 [50.54]
 [50.54]] [[0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.896]]
printing an ep nov before normalisation:  40.91135499006337
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5466666666666667  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.155]
 [0.214]
 [0.155]
 [0.155]
 [0.155]
 [0.155]
 [0.155]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.155]
 [0.214]
 [0.155]
 [0.155]
 [0.155]
 [0.155]
 [0.155]]
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  55 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  42 total reward:  0.5133333333333335  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  33.98455560814749
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  33.295810507831284
printing an ep nov before normalisation:  41.48594816619891
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.481]
 [0.336]
 [0.396]
 [0.342]
 [0.346]
 [0.333]] [[32.934]
 [35.112]
 [32.476]
 [34.382]
 [31.62 ]
 [31.693]
 [34.281]] [[0.951]
 [1.185]
 [0.944]
 [1.074]
 [0.917]
 [0.924]
 [1.007]]
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  35.47309131795171
printing an ep nov before normalisation:  37.33691523359839
actor:  1 policy actor:  1  step number:  52 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  51 total reward:  0.42666666666666697  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1041533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  25.68196660127305
printing an ep nov before normalisation:  37.62889557640777
maxi score, test score, baseline:  0.1017533333333332 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.1017533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.145]
 [0.241]
 [0.235]
 [0.164]
 [0.189]
 [0.189]
 [0.159]] [[32.528]
 [35.158]
 [36.173]
 [32.73 ]
 [36.597]
 [36.597]
 [33.935]] [[1.354]
 [1.663]
 [1.74 ]
 [1.39 ]
 [1.729]
 [1.729]
 [1.482]]
maxi score, test score, baseline:  0.1017533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  38 total reward:  0.54  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1017533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  43 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1017533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.162]
 [0.201]
 [0.162]
 [0.162]
 [0.196]
 [0.162]
 [0.162]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.162]
 [0.201]
 [0.162]
 [0.162]
 [0.196]
 [0.162]
 [0.162]]
maxi score, test score, baseline:  0.1017533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  43.055962155710084
maxi score, test score, baseline:  0.1017533333333332 0.6886666666666669 0.6886666666666669
printing an ep nov before normalisation:  31.97439950937146
maxi score, test score, baseline:  0.1017533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.1017533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  18.913698196411133
maxi score, test score, baseline:  0.1017533333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  43 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  4  action  0 :  tensor([0.4439, 0.0542, 0.0754, 0.0890, 0.1371, 0.1106, 0.0899],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0162, 0.9073, 0.0093, 0.0200, 0.0081, 0.0085, 0.0306],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1046, 0.0778, 0.4795, 0.0409, 0.0454, 0.1550, 0.0968],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1671, 0.0108, 0.0889, 0.3799, 0.1495, 0.0915, 0.1123],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2643, 0.0044, 0.1296, 0.0785, 0.3100, 0.0964, 0.1169],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1220, 0.0059, 0.1903, 0.0478, 0.0753, 0.4990, 0.0597],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1747, 0.1871, 0.1170, 0.0875, 0.1286, 0.1035, 0.2016],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.09917999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09917999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]] [[37.314]
 [37.314]
 [37.314]
 [37.314]
 [37.314]
 [37.314]
 [37.314]] [[2.24]
 [2.24]
 [2.24]
 [2.24]
 [2.24]
 [2.24]
 [2.24]]
maxi score, test score, baseline:  0.09917999999999987 0.6886666666666669 0.6886666666666669
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[23.319]
 [23.319]
 [23.319]
 [23.319]
 [23.319]
 [23.319]
 [23.319]] [[1.239]
 [1.239]
 [1.239]
 [1.239]
 [1.239]
 [1.239]
 [1.239]]
maxi score, test score, baseline:  0.09917999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  26.757207008882766
printing an ep nov before normalisation:  16.256267762962224
actions average: 
K:  0  action  0 :  tensor([0.5631, 0.0178, 0.0950, 0.0645, 0.0987, 0.0806, 0.0804],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0035, 0.9631, 0.0029, 0.0037, 0.0012, 0.0014, 0.0242],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0976, 0.0022, 0.5540, 0.0642, 0.0637, 0.1558, 0.0626],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2477, 0.0068, 0.1390, 0.1908, 0.1384, 0.1299, 0.1475],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1859, 0.0022, 0.1012, 0.0829, 0.4640, 0.0715, 0.0922],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0564, 0.0101, 0.0957, 0.0371, 0.0498, 0.7186, 0.0322],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2005, 0.0522, 0.1394, 0.1098, 0.1003, 0.0914, 0.3064],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.09917999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  65 total reward:  0.19999999999999984  reward:  1.0 rdn_beta:  2.0
from probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09917999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.85 ]
 [0.941]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]] [[32.003]
 [35.614]
 [32.003]
 [32.003]
 [32.003]
 [32.003]
 [32.003]] [[0.85 ]
 [0.941]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]]
printing an ep nov before normalisation:  40.93877028438463
actor:  1 policy actor:  1  step number:  33 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.803]
 [0.878]
 [0.775]
 [0.74 ]
 [0.863]
 [0.58 ]
 [0.681]] [[37.109]
 [43.371]
 [36.32 ]
 [36.898]
 [39.068]
 [38.742]
 [31.911]] [[0.803]
 [0.878]
 [0.775]
 [0.74 ]
 [0.863]
 [0.58 ]
 [0.681]]
siam score:  -0.77352107
printing an ep nov before normalisation:  37.44149312784324
actor:  0 policy actor:  0  step number:  40 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  39.55591715452756
maxi score, test score, baseline:  0.09953999999999988 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.482]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]] [[32.283]
 [37.635]
 [32.283]
 [32.283]
 [32.283]
 [32.283]
 [32.283]] [[1.614]
 [2.034]
 [1.614]
 [1.614]
 [1.614]
 [1.614]
 [1.614]]
maxi score, test score, baseline:  0.09953999999999988 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.09953999999999988 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  0.8066078711010505
actor:  0 policy actor:  0  step number:  47 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  46 total reward:  0.526666666666667  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0996333333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]] [[30.44]
 [30.44]
 [30.44]
 [30.44]
 [30.44]
 [30.44]
 [30.44]] [[1.636]
 [1.636]
 [1.636]
 [1.636]
 [1.636]
 [1.636]
 [1.636]]
Printing some Q and Qe and total Qs values:  [[-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]
 [-0.002]] [[33.236]
 [25.025]
 [25.025]
 [25.025]
 [25.025]
 [25.025]
 [25.025]] [[1.6  ]
 [1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]]
printing an ep nov before normalisation:  38.620205731456714
printing an ep nov before normalisation:  43.806011324273804
actor:  1 policy actor:  1  step number:  42 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  40.068619252966776
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]
 [0.543]] [[26.42]
 [26.42]
 [26.42]
 [26.42]
 [26.42]
 [26.42]
 [26.42]] [[0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]]
maxi score, test score, baseline:  0.0996333333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5333333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0996333333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  43.09042858295895
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.607841670871778
actor:  1 policy actor:  1  step number:  73 total reward:  0.2533333333333331  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  49 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
siam score:  -0.77081496
maxi score, test score, baseline:  0.0996333333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.0996333333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.0996333333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.569]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]] [[31.278]
 [36.512]
 [31.278]
 [31.278]
 [31.278]
 [31.278]
 [31.278]] [[0.998]
 [1.264]
 [0.998]
 [0.998]
 [0.998]
 [0.998]
 [0.998]]
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.577]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]] [[32.373]
 [40.489]
 [32.373]
 [32.373]
 [32.373]
 [32.373]
 [32.373]] [[1.73 ]
 [2.265]
 [1.73 ]
 [1.73 ]
 [1.73 ]
 [1.73 ]
 [1.73 ]]
maxi score, test score, baseline:  0.0996333333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.0996333333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  43 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  3  action  0 :  tensor([0.4282, 0.0216, 0.0912, 0.1018, 0.1380, 0.1168, 0.1023],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0072, 0.9620, 0.0040, 0.0076, 0.0024, 0.0022, 0.0146],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0008,     0.0010,     0.8533,     0.0008,     0.0001,     0.1380,
            0.0060], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2520, 0.1392, 0.0849, 0.1835, 0.1295, 0.1044, 0.1064],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1521, 0.0746, 0.0745, 0.0695, 0.5108, 0.0715, 0.0471],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1456, 0.0559, 0.0746, 0.0979, 0.0809, 0.4792, 0.0659],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1124, 0.1827, 0.1006, 0.1324, 0.0784, 0.0650, 0.3286],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0996333333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
siam score:  -0.777831
actions average: 
K:  1  action  0 :  tensor([0.5770, 0.0128, 0.0699, 0.0743, 0.1165, 0.0735, 0.0762],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0216, 0.9105, 0.0096, 0.0104, 0.0042, 0.0029, 0.0408],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1080, 0.0030, 0.6539, 0.0401, 0.0382, 0.1157, 0.0409],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1671, 0.0367, 0.0973, 0.2494, 0.1811, 0.1045, 0.1638],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0887, 0.2100, 0.0960, 0.0634, 0.4344, 0.0422, 0.0652],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([    0.1085,     0.0004,     0.0588,     0.0445,     0.0573,     0.6783,
            0.0522], grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1413, 0.1993, 0.0879, 0.1156, 0.1144, 0.0852, 0.2562],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  44 total reward:  0.44666666666666677  reward:  1.0 rdn_beta:  2.0
from probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.0996333333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.156]
 [0.219]
 [0.219]
 [0.333]
 [0.219]
 [0.219]] [[34.168]
 [32.309]
 [34.161]
 [34.161]
 [32.8  ]
 [34.161]
 [34.161]] [[1.767]
 [1.461]
 [1.674]
 [1.674]
 [1.677]
 [1.674]
 [1.674]]
printing an ep nov before normalisation:  29.070355323200605
maxi score, test score, baseline:  0.0996333333333332 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.0965933333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
using another actor
maxi score, test score, baseline:  0.0965933333333332 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.0965933333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.0965933333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.0965933333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  47.948724653429615
printing an ep nov before normalisation:  46.37911858935817
printing an ep nov before normalisation:  0.013573568688229898
actor:  0 policy actor:  0  step number:  44 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09943333333333318 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09943333333333321 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09943333333333321 0.6886666666666669 0.6886666666666669
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]] [[35.554]
 [35.554]
 [35.554]
 [35.554]
 [35.554]
 [35.554]
 [35.554]] [[1.377]
 [1.377]
 [1.377]
 [1.377]
 [1.377]
 [1.377]
 [1.377]]
maxi score, test score, baseline:  0.09943333333333321 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  0 policy actor:  1  step number:  54 total reward:  0.5533333333333337  reward:  1.0 rdn_beta:  2.0
from probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  55.34065370287052
maxi score, test score, baseline:  0.0970733333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  69.0869430890807
actor:  1 policy actor:  1  step number:  43 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  44 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  56.20636406326343
Printing some Q and Qe and total Qs values:  [[0.083]
 [0.228]
 [0.065]
 [0.083]
 [0.084]
 [0.08 ]
 [0.157]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.083]
 [0.228]
 [0.065]
 [0.083]
 [0.084]
 [0.08 ]
 [0.157]]
maxi score, test score, baseline:  0.0970733333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.48 ]
 [0.411]
 [0.411]
 [0.411]
 [0.411]
 [0.411]] [[33.049]
 [34.499]
 [33.049]
 [33.049]
 [33.049]
 [33.049]
 [33.049]] [[0.605]
 [0.69 ]
 [0.605]
 [0.605]
 [0.605]
 [0.605]
 [0.605]]
maxi score, test score, baseline:  0.09707333333333319 0.6886666666666669 0.6886666666666669
printing an ep nov before normalisation:  30.593390464782715
printing an ep nov before normalisation:  34.72954769920721
maxi score, test score, baseline:  0.09707333333333319 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  0 policy actor:  0  step number:  36 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10012666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  0.0015177256045717513
actor:  1 policy actor:  1  step number:  61 total reward:  0.19999999999999907  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
Printing some Q and Qe and total Qs values:  [[0.057]
 [0.088]
 [0.042]
 [0.057]
 [0.058]
 [0.057]
 [0.057]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.057]
 [0.088]
 [0.042]
 [0.057]
 [0.058]
 [0.057]
 [0.057]]
printing an ep nov before normalisation:  32.80690670013428
maxi score, test score, baseline:  0.10012666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7828927
maxi score, test score, baseline:  0.10012666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.10012666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  50 total reward:  0.3533333333333333  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.083]
 [0.085]
 [0.085]
 [0.085]
 [0.078]
 [0.085]] [[52.895]
 [54.893]
 [35.669]
 [35.669]
 [35.669]
 [40.916]
 [35.669]] [[1.247]
 [1.284]
 [0.615]
 [0.615]
 [0.615]
 [0.791]
 [0.615]]
maxi score, test score, baseline:  0.10012666666666653 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.10012666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  32.68380880355835
printing an ep nov before normalisation:  31.133337020874023
maxi score, test score, baseline:  0.10012666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  24.340111929569254
maxi score, test score, baseline:  0.10012666666666653 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.10012666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  20.664236862654832
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.787]
 [0.63 ]
 [0.571]
 [0.655]
 [0.641]
 [0.702]] [[13.753]
 [12.923]
 [13.556]
 [13.587]
 [14.012]
 [14.568]
 [13.365]] [[2.066]
 [2.134]
 [2.043]
 [1.987]
 [2.115]
 [2.159]
 [2.094]]
maxi score, test score, baseline:  0.10012666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.10012666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.10012666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  35.40379444964836
maxi score, test score, baseline:  0.10012666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  26.948394775390625
line 256 mcts: sample exp_bonus 54.65171215955079
maxi score, test score, baseline:  0.10012666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  45.86176746184524
maxi score, test score, baseline:  0.10012666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  38.28928846327337
maxi score, test score, baseline:  0.10012666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.459058160809157
maxi score, test score, baseline:  0.10012666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  58 total reward:  0.32666666666666655  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.10012666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.98]
 [0.98]
 [0.98]
 [0.98]
 [0.98]
 [0.98]
 [0.98]] [[32.308]
 [32.308]
 [32.308]
 [32.308]
 [32.308]
 [32.308]
 [32.308]] [[0.98]
 [0.98]
 [0.98]
 [0.98]
 [0.98]
 [0.98]
 [0.98]]
actor:  1 policy actor:  1  step number:  67 total reward:  0.5200000000000005  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.39]
 [0.39]
 [0.39]
 [0.39]
 [0.39]
 [0.39]
 [0.39]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.39]
 [0.39]
 [0.39]
 [0.39]
 [0.39]
 [0.39]
 [0.39]]
maxi score, test score, baseline:  0.10012666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
from probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.10012666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]] [[34.212]
 [34.212]
 [34.212]
 [34.212]
 [34.212]
 [34.212]
 [34.212]] [[0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]]
actor:  0 policy actor:  0  step number:  47 total reward:  0.41333333333333266  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  0
maxi score, test score, baseline:  0.1000733333333332 0.6886666666666669 0.6886666666666669
siam score:  -0.77911896
maxi score, test score, baseline:  0.1000733333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.042]
 [0.139]
 [0.048]
 [0.059]
 [0.051]
 [0.096]
 [0.096]] [[45.127]
 [44.513]
 [44.163]
 [44.71 ]
 [45.177]
 [53.065]
 [53.065]] [[1.04 ]
 [1.111]
 [1.005]
 [1.039]
 [1.051]
 [1.432]
 [1.432]]
Printing some Q and Qe and total Qs values:  [[0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]] [[53.01]
 [53.01]
 [53.01]
 [53.01]
 [53.01]
 [53.01]
 [53.01]] [[1.098]
 [1.098]
 [1.098]
 [1.098]
 [1.098]
 [1.098]
 [1.098]]
maxi score, test score, baseline:  0.1000733333333332 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09704666666666652 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  32.16589689254761
maxi score, test score, baseline:  0.09704666666666652 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09704666666666652 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  0.0027111118512834764
actor:  0 policy actor:  1  step number:  51 total reward:  0.34666666666666657  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  30.635910888315017
maxi score, test score, baseline:  0.09973999999999984 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.09973999999999984 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]
 [0.745]] [[35.698]
 [35.698]
 [35.698]
 [35.698]
 [35.698]
 [35.698]
 [35.698]] [[1.768]
 [1.768]
 [1.768]
 [1.768]
 [1.768]
 [1.768]
 [1.768]]
printing an ep nov before normalisation:  22.348801848113986
printing an ep nov before normalisation:  21.60046757407204
printing an ep nov before normalisation:  44.56810348364497
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  49 total reward:  0.506666666666667  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  23.449034391112438
maxi score, test score, baseline:  0.09973999999999984 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09973999999999984 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.23 ]
 [0.238]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]] [[31.835]
 [43.4  ]
 [31.835]
 [31.835]
 [31.835]
 [31.835]
 [31.835]] [[0.663]
 [0.935]
 [0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]]
printing an ep nov before normalisation:  46.72778070618262
maxi score, test score, baseline:  0.09973999999999984 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.09973999999999984 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  65 total reward:  0.22666666666666668  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09973999999999984 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.26617066915464
printing an ep nov before normalisation:  40.38411578779688
maxi score, test score, baseline:  0.09973999999999984 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4866666666666668  reward:  1.0 rdn_beta:  1.667
using another actor
maxi score, test score, baseline:  0.09973999999999984 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[ 0.46 ]
 [ 0.508]
 [-0.004]
 [ 0.449]
 [ 0.437]
 [ 0.467]
 [ 0.469]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.46 ]
 [ 0.508]
 [-0.004]
 [ 0.449]
 [ 0.437]
 [ 0.467]
 [ 0.469]]
maxi score, test score, baseline:  0.09973999999999984 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  57 total reward:  0.14666666666666606  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  43 total reward:  0.5733333333333336  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09973999999999984 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  41.013791256128954
Printing some Q and Qe and total Qs values:  [[0.832]
 [0.904]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]] [[46.975]
 [44.899]
 [46.975]
 [46.975]
 [46.975]
 [46.975]
 [46.975]] [[0.832]
 [0.904]
 [0.832]
 [0.832]
 [0.832]
 [0.832]
 [0.832]]
actions average: 
K:  2  action  0 :  tensor([0.5293, 0.0065, 0.0809, 0.0799, 0.1561, 0.0700, 0.0774],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0194, 0.9340, 0.0070, 0.0099, 0.0043, 0.0087, 0.0168],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1049, 0.0212, 0.5044, 0.0951, 0.0687, 0.0905, 0.1153],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0614, 0.2472, 0.0376, 0.3233, 0.0994, 0.0647, 0.1664],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2390, 0.0031, 0.0641, 0.0912, 0.4548, 0.0841, 0.0637],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1435, 0.0076, 0.1632, 0.0770, 0.0748, 0.4375, 0.0964],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1245, 0.3375, 0.0726, 0.0811, 0.1264, 0.0459, 0.2120],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.09973999999999984 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09697999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09697999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.521]
 [0.396]
 [0.393]
 [0.392]
 [0.393]
 [0.4  ]] [[28.812]
 [44.534]
 [29.211]
 [30.236]
 [30.952]
 [29.734]
 [29.405]] [[0.868]
 [1.471]
 [0.89 ]
 [0.918]
 [0.938]
 [0.903]
 [0.9  ]]
Printing some Q and Qe and total Qs values:  [[0.054]
 [0.075]
 [0.079]
 [0.078]
 [0.067]
 [0.077]
 [0.067]] [[32.68 ]
 [30.881]
 [27.863]
 [28.141]
 [30.321]
 [37.444]
 [31.582]] [[0.593]
 [0.564]
 [0.482]
 [0.489]
 [0.539]
 [0.752]
 [0.575]]
maxi score, test score, baseline:  0.09697999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09697999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09697999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09697999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09697999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  28.81911618615802
printing an ep nov before normalisation:  60.94889641997788
printing an ep nov before normalisation:  28.682663440704346
actor:  1 policy actor:  1  step number:  72 total reward:  0.2733333333333321  reward:  1.0 rdn_beta:  1.333
siam score:  -0.7852081
maxi score, test score, baseline:  0.09697999999999986 0.6886666666666669 0.6886666666666669
printing an ep nov before normalisation:  30.523861779106987
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  0.09697999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.679]
 [0.735]
 [0.659]
 [0.663]
 [0.665]
 [0.706]
 [0.679]] [[38.684]
 [44.445]
 [32.509]
 [33.053]
 [32.595]
 [39.535]
 [34.819]] [[0.679]
 [0.735]
 [0.659]
 [0.663]
 [0.665]
 [0.706]
 [0.679]]
printing an ep nov before normalisation:  44.64230094492442
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.79 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]] [[32.397]
 [37.424]
 [23.65 ]
 [23.65 ]
 [23.65 ]
 [23.65 ]
 [23.65 ]] [[0.692]
 [0.79 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]
 [0.69 ]]
maxi score, test score, baseline:  0.09697999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  48.597092548449545
actor:  0 policy actor:  0  step number:  50 total reward:  0.3666666666666666  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  45 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.438]
 [0.214]
 [0.257]
 [0.294]
 [0.257]
 [0.257]] [[58.14 ]
 [59.684]
 [64.945]
 [59.973]
 [58.771]
 [59.973]
 [59.973]] [[1.621]
 [1.881]
 [1.865]
 [1.711]
 [1.7  ]
 [1.711]
 [1.711]]
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  56 total reward:  0.32666666666666677  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  52.241825449673456
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  52.69614826936171
printing an ep nov before normalisation:  34.771000879221965
siam score:  -0.7743896
actor:  1 policy actor:  1  step number:  34 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  65 total reward:  0.013333333333332975  reward:  1.0 rdn_beta:  1.333
siam score:  -0.76734525
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  15.082790851593018
actor:  1 policy actor:  1  step number:  43 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
actor:  1 policy actor:  1  step number:  57 total reward:  0.23999999999999977  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.21 ]
 [0.251]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.21 ]] [[48.884]
 [52.024]
 [48.884]
 [48.884]
 [52.784]
 [48.884]
 [48.884]] [[1.855]
 [2.038]
 [1.855]
 [1.855]
 [2.031]
 [1.855]
 [1.855]]
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[-0.079]
 [-0.088]
 [-0.055]
 [-0.079]
 [-0.079]
 [-0.043]
 [-0.079]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.079]
 [-0.088]
 [-0.055]
 [-0.079]
 [-0.079]
 [-0.043]
 [-0.079]]
Printing some Q and Qe and total Qs values:  [[-0.053]
 [ 0.063]
 [ 0.09 ]
 [-0.001]
 [-0.033]
 [ 0.085]
 [-0.053]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.053]
 [ 0.063]
 [ 0.09 ]
 [-0.001]
 [-0.033]
 [ 0.085]
 [-0.053]]
Printing some Q and Qe and total Qs values:  [[1.132]
 [1.489]
 [1.132]
 [1.132]
 [1.132]
 [1.484]
 [1.132]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.132]
 [1.489]
 [1.132]
 [1.132]
 [1.132]
 [1.484]
 [1.132]]
printing an ep nov before normalisation:  30.716293988885468
siam score:  -0.7672757
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.436]
 [0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]] [[48.043]
 [50.303]
 [48.043]
 [48.043]
 [48.043]
 [48.043]
 [48.043]] [[1.87]
 [2.1 ]
 [1.87]
 [1.87]
 [1.87]
 [1.87]
 [1.87]]
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  46 total reward:  0.40666666666666673  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  33.80889892578125
actor:  1 policy actor:  1  step number:  45 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  34.77681561689315
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.733]
 [0.767]
 [0.732]
 [0.733]
 [0.732]
 [0.75 ]
 [0.733]] [[33.316]
 [37.951]
 [29.656]
 [29.467]
 [29.434]
 [34.126]
 [30.078]] [[0.733]
 [0.767]
 [0.732]
 [0.733]
 [0.732]
 [0.75 ]
 [0.733]]
printing an ep nov before normalisation:  36.536215918689486
actor:  1 policy actor:  1  step number:  40 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  48 total reward:  0.42000000000000004  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
siam score:  -0.7693474
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.09761999999999987 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  0 policy actor:  0  step number:  53 total reward:  0.3066666666666661  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  33 total reward:  0.5866666666666668  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.10023333333333319 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]] [[34.372]
 [34.372]
 [34.372]
 [34.372]
 [34.372]
 [34.372]
 [34.372]] [[1.56]
 [1.56]
 [1.56]
 [1.56]
 [1.56]
 [1.56]
 [1.56]]
actions average: 
K:  1  action  0 :  tensor([0.2972, 0.0557, 0.1329, 0.1285, 0.1532, 0.1356, 0.0968],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0101, 0.9537, 0.0047, 0.0039, 0.0020, 0.0036, 0.0219],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1294, 0.0025, 0.5412, 0.0676, 0.1158, 0.0932, 0.0503],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1006, 0.1325, 0.0849, 0.3998, 0.0774, 0.0689, 0.1360],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2066, 0.0395, 0.1396, 0.1268, 0.2245, 0.1039, 0.1591],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0392, 0.0013, 0.1198, 0.0516, 0.0597, 0.6588, 0.0696],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1507, 0.0181, 0.1508, 0.0947, 0.1048, 0.0913, 0.3896],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.245]
 [0.338]
 [0.245]
 [0.245]
 [0.245]
 [0.245]
 [0.245]] [[30.076]
 [45.359]
 [30.076]
 [30.076]
 [30.076]
 [30.076]
 [30.076]] [[0.956]
 [1.755]
 [0.956]
 [0.956]
 [0.956]
 [0.956]
 [0.956]]
maxi score, test score, baseline:  0.10023333333333319 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.10023333333333319 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.592]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]] [[44.897]
 [45.951]
 [44.897]
 [44.897]
 [44.897]
 [44.897]
 [44.897]] [[2.224]
 [2.458]
 [2.224]
 [2.224]
 [2.224]
 [2.224]
 [2.224]]
Printing some Q and Qe and total Qs values:  [[0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]]
printing an ep nov before normalisation:  39.382542972681854
actor:  1 policy actor:  1  step number:  60 total reward:  0.25999999999999923  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  29.485755154152773
printing an ep nov before normalisation:  26.044622014915042
actor:  1 policy actor:  1  step number:  37 total reward:  0.52  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10023333333333319 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.10023333333333319 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  41.70411026521009
maxi score, test score, baseline:  0.10023333333333319 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.537]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]] [[35.02]
 [37.93]
 [35.02]
 [35.02]
 [35.02]
 [35.02]
 [35.02]] [[0.831]
 [1.055]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.831]]
maxi score, test score, baseline:  0.10023333333333319 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  35.38148844251358
maxi score, test score, baseline:  0.10023333333333319 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  39 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  0.1176838786724943
printing an ep nov before normalisation:  29.791561654328415
actor:  0 policy actor:  0  step number:  52 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  0.333
using another actor
actor:  1 policy actor:  1  step number:  41 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  17.926086698259628
maxi score, test score, baseline:  0.09997999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  28.13825078253013
Printing some Q and Qe and total Qs values:  [[0.206]
 [0.244]
 [0.206]
 [0.206]
 [0.206]
 [0.196]
 [0.206]] [[28.582]
 [31.032]
 [28.582]
 [28.582]
 [28.582]
 [28.647]
 [28.582]] [[1.141]
 [1.336]
 [1.141]
 [1.141]
 [1.141]
 [1.135]
 [1.141]]
Printing some Q and Qe and total Qs values:  [[0.802]
 [0.825]
 [0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.802]] [[27.398]
 [34.957]
 [27.398]
 [27.398]
 [27.398]
 [27.398]
 [27.398]] [[0.802]
 [0.825]
 [0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.802]]
printing an ep nov before normalisation:  40.03245178494445
maxi score, test score, baseline:  0.09997999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  38.38531305905921
actor:  0 policy actor:  1  step number:  35 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.10304666666666652 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.876]
 [0.894]
 [0.858]
 [0.858]
 [0.885]
 [0.85 ]
 [0.851]] [[32.277]
 [33.521]
 [29.746]
 [29.955]
 [28.822]
 [29.3  ]
 [31.617]] [[0.876]
 [0.894]
 [0.858]
 [0.858]
 [0.885]
 [0.85 ]
 [0.851]]
actor:  1 policy actor:  1  step number:  38 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  0.10304666666666652 0.6886666666666669 0.6886666666666669
actor:  0 policy actor:  1  step number:  52 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10276666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  42.761016786518724
maxi score, test score, baseline:  0.10276666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.10276666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  36.154658279238994
maxi score, test score, baseline:  0.10276666666666653 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.10276666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  1 policy actor:  1  step number:  46 total reward:  0.44666666666666677  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  34 total reward:  0.5666666666666668  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  31.909667264065178
actor:  1 policy actor:  1  step number:  34 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.98795472458642
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  38.02980938080522
printing an ep nov before normalisation:  38.19125859952588
printing an ep nov before normalisation:  42.23886992357596
printing an ep nov before normalisation:  24.627280698186194
printing an ep nov before normalisation:  37.252277843778224
maxi score, test score, baseline:  0.10276666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  27.280665858837576
actor:  1 policy actor:  1  step number:  47 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  23.67681108673953
printing an ep nov before normalisation:  43.45382834552291
printing an ep nov before normalisation:  42.076464764561884
actor:  1 policy actor:  1  step number:  35 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  2.0
siam score:  -0.76827073
maxi score, test score, baseline:  0.10276666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  25.529484928262892
maxi score, test score, baseline:  0.10276666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  39.17971991031956
maxi score, test score, baseline:  0.10276666666666653 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.10276666666666653 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
actor:  0 policy actor:  0  step number:  52 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10517999999999986 0.6886666666666669 0.6886666666666669
maxi score, test score, baseline:  0.10517999999999986 0.6886666666666669 0.6886666666666669
actor:  1 policy actor:  1  step number:  59 total reward:  0.13333333333333308  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10517999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  30.704914247422717
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]] [[48.47]
 [48.47]
 [48.47]
 [48.47]
 [48.47]
 [48.47]
 [48.47]] [[0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]]
printing an ep nov before normalisation:  45.17274081225388
Printing some Q and Qe and total Qs values:  [[0.555]
 [0.558]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]] [[53.719]
 [51.662]
 [48.007]
 [48.007]
 [48.007]
 [48.007]
 [48.007]] [[2.308]
 [2.218]
 [2.026]
 [2.026]
 [2.026]
 [2.026]
 [2.026]]
printing an ep nov before normalisation:  21.964618212426217
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  0.09091803877254279
maxi score, test score, baseline:  0.10517999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
maxi score, test score, baseline:  0.10517999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
printing an ep nov before normalisation:  12.706283754828718
Starting evaluation
printing an ep nov before normalisation:  42.350767944757685
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10517999999999986 0.6886666666666669 0.6886666666666669
probs:  [0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.12359268816681084, 0.38203655916594575]
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]]
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.352]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.334]
 [0.352]
 [0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]]
printing an ep nov before normalisation:  31.086459755897522
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.77121148932126
Printing some Q and Qe and total Qs values:  [[0.81 ]
 [0.918]
 [0.81 ]
 [0.81 ]
 [0.81 ]
 [0.81 ]
 [0.81 ]] [[48.029]
 [44.299]
 [48.029]
 [48.029]
 [48.029]
 [48.029]
 [48.029]] [[0.81 ]
 [0.918]
 [0.81 ]
 [0.81 ]
 [0.81 ]
 [0.81 ]
 [0.81 ]]
printing an ep nov before normalisation:  42.211620989270656
printing an ep nov before normalisation:  46.28124034866007
line 256 mcts: sample exp_bonus 30.401408672332764
printing an ep nov before normalisation:  36.370525128999944
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  32.3720026825639
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]] [[54.259]
 [54.259]
 [54.259]
 [54.259]
 [54.259]
 [54.259]
 [54.259]] [[2.101]
 [2.101]
 [2.101]
 [2.101]
 [2.101]
 [2.101]
 [2.101]]
printing an ep nov before normalisation:  44.248281443133635
printing an ep nov before normalisation:  50.26129419245742
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.01474348896997
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.583]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]] [[34.326]
 [41.663]
 [34.326]
 [34.326]
 [34.326]
 [34.326]
 [34.326]] [[1.074]
 [1.542]
 [1.074]
 [1.074]
 [1.074]
 [1.074]
 [1.074]]
printing an ep nov before normalisation:  27.840490341186523
printing an ep nov before normalisation:  37.36710272316453
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  24.48799292920098
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  0.12104857731173979
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.4666666666666669  reward:  1.0 rdn_beta:  1.333
from probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  52 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  45 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.11871333333333318 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  38.84217724566127
maxi score, test score, baseline:  0.11871333333333318 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.11871333333333318 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  34.89614367734739
printing an ep nov before normalisation:  30.231049967254847
printing an ep nov before normalisation:  40.39444225205904
actor:  0 policy actor:  1  step number:  34 total reward:  0.6733333333333336  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.12205999999999988 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.12205999999999988 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.148]
 [0.309]] [[34.672]
 [34.672]
 [34.672]
 [34.672]
 [34.672]
 [37.187]
 [34.672]] [[0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.899]
 [0.973]]
maxi score, test score, baseline:  0.12205999999999988 0.6933333333333334 0.6933333333333334
actor:  0 policy actor:  1  step number:  53 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  51 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  33.45883775294547
maxi score, test score, baseline:  0.1224333333333332 0.6933333333333334 0.6933333333333334
Printing some Q and Qe and total Qs values:  [[0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]] [[42.049]
 [42.049]
 [42.049]
 [42.049]
 [42.049]
 [42.049]
 [42.049]] [[1.098]
 [1.098]
 [1.098]
 [1.098]
 [1.098]
 [1.098]
 [1.098]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1224333333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1224333333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  50 total reward:  0.4333333333333329  reward:  1.0 rdn_beta:  1.333
siam score:  -0.77474266
printing an ep nov before normalisation:  40.932247334735756
printing an ep nov before normalisation:  37.65129650857284
printing an ep nov before normalisation:  40.416849547623585
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.805]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]] [[19.492]
 [25.832]
 [19.603]
 [19.603]
 [19.603]
 [19.603]
 [19.603]] [[1.316]
 [1.701]
 [1.315]
 [1.315]
 [1.315]
 [1.315]
 [1.315]]
actor:  0 policy actor:  1  step number:  63 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  60 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.12275333333333321 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  41 total reward:  0.52  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12275333333333321 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.248]
 [0.044]
 [0.044]
 [0.044]
 [0.044]
 [0.044]] [[ 0.   ]
 [32.754]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.214]
 [ 0.675]
 [-0.214]
 [-0.214]
 [-0.214]
 [-0.214]
 [-0.214]]
maxi score, test score, baseline:  0.12275333333333321 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.12275333333333321 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
line 256 mcts: sample exp_bonus 32.16890561505958
maxi score, test score, baseline:  0.12275333333333321 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.12275333333333321 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.12275333333333321 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  41.07818804184942
maxi score, test score, baseline:  0.12275333333333321 0.6933333333333334 0.6933333333333334
actor:  1 policy actor:  1  step number:  38 total reward:  0.5933333333333335  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  46 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  38 total reward:  0.5400000000000001  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  0.6333333333333334  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12275333333333321 0.6933333333333334 0.6933333333333334
printing an ep nov before normalisation:  55.08642318566182
maxi score, test score, baseline:  0.12275333333333321 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  43 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  53 total reward:  0.2933333333333331  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  31.534543968689423
maxi score, test score, baseline:  0.12275333333333321 0.6933333333333334 0.6933333333333334
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.28 ]
 [0.247]
 [0.247]
 [0.247]
 [0.247]
 [0.247]] [[44.766]
 [48.272]
 [44.766]
 [44.766]
 [44.766]
 [44.766]
 [44.766]] [[1.541]
 [1.758]
 [1.541]
 [1.541]
 [1.541]
 [1.541]
 [1.541]]
printing an ep nov before normalisation:  46.24384072655043
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.586]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]] [[49.191]
 [45.665]
 [50.115]
 [50.115]
 [50.115]
 [50.115]
 [50.115]] [[2.115]
 [2.044]
 [2.154]
 [2.154]
 [2.154]
 [2.154]
 [2.154]]
maxi score, test score, baseline:  0.1206733333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1206733333333332 0.6933333333333334 0.6933333333333334
printing an ep nov before normalisation:  47.58919077550367
printing an ep nov before normalisation:  42.9093395557748
printing an ep nov before normalisation:  39.52554968548816
maxi score, test score, baseline:  0.1206733333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1206733333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  43.01778033609154
maxi score, test score, baseline:  0.1206733333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1206733333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  48 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1206733333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  34.770282109578446
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.725]
 [0.54 ]
 [0.686]
 [0.686]
 [0.449]
 [0.686]] [[42.987]
 [41.1  ]
 [44.967]
 [42.987]
 [42.987]
 [42.458]
 [42.987]] [[1.76 ]
 [1.726]
 [1.691]
 [1.76 ]
 [1.76 ]
 [1.503]
 [1.76 ]]
printing an ep nov before normalisation:  59.40865505429898
maxi score, test score, baseline:  0.1206733333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  35 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  15.881786346435547
maxi score, test score, baseline:  0.1206733333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  56 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.482]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]] [[37.58 ]
 [49.014]
 [37.58 ]
 [37.58 ]
 [37.58 ]
 [37.58 ]
 [37.58 ]] [[1.065]
 [1.578]
 [1.065]
 [1.065]
 [1.065]
 [1.065]
 [1.065]]
maxi score, test score, baseline:  0.1206733333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  55 total reward:  0.2799999999999999  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  58.52572710032265
actor:  1 policy actor:  1  step number:  32 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.661]
 [0.588]
 [0.594]
 [0.6  ]
 [0.614]
 [0.62 ]] [[30.734]
 [35.842]
 [27.103]
 [26.012]
 [26.184]
 [29.518]
 [24.747]] [[0.839]
 [0.928]
 [0.783]
 [0.78 ]
 [0.787]
 [0.829]
 [0.796]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1206733333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  44 total reward:  0.4866666666666668  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1206733333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  38.96693388151825
printing an ep nov before normalisation:  47.68166601435908
printing an ep nov before normalisation:  52.87230409484704
actor:  0 policy actor:  0  step number:  53 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  43.072127323497966
printing an ep nov before normalisation:  48.54426799612364
actor:  1 policy actor:  1  step number:  46 total reward:  0.42000000000000004  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]]
maxi score, test score, baseline:  0.1229933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1229933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  27.19688047271735
actor:  1 policy actor:  1  step number:  57 total reward:  0.3866666666666667  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.095]
 [0.135]
 [0.095]
 [0.095]
 [0.095]
 [0.095]
 [0.095]] [[36.306]
 [49.097]
 [36.306]
 [36.306]
 [36.306]
 [36.306]
 [36.306]] [[0.391]
 [0.631]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]]
maxi score, test score, baseline:  0.1229933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
using explorer policy with actor:  0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1229933333333332 0.6933333333333334 0.6933333333333334
Printing some Q and Qe and total Qs values:  [[0.127]
 [0.289]
 [0.127]
 [0.127]
 [0.127]
 [0.127]
 [0.127]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.127]
 [0.289]
 [0.127]
 [0.127]
 [0.127]
 [0.127]
 [0.127]]
printing an ep nov before normalisation:  45.10951012588011
maxi score, test score, baseline:  0.1229933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  45.547293065575936
maxi score, test score, baseline:  0.1229933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  0 policy actor:  1  step number:  51 total reward:  0.506666666666667  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12600666666666652 0.6933333333333334 0.6933333333333334
printing an ep nov before normalisation:  36.16738895122405
printing an ep nov before normalisation:  36.24257897231085
maxi score, test score, baseline:  0.12600666666666652 0.6933333333333334 0.6933333333333334
actor:  1 policy actor:  1  step number:  34 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12600666666666652 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.12600666666666652 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  37.04503867109563
printing an ep nov before normalisation:  38.66474636214699
maxi score, test score, baseline:  0.12600666666666652 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4266666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.12600666666666652 0.6933333333333334 0.6933333333333334
actor:  1 policy actor:  1  step number:  49 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12600666666666652 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.12600666666666652 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.12600666666666652 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.12600666666666652 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.12600666666666652 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  24.394890862396323
maxi score, test score, baseline:  0.12600666666666652 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.12600666666666652 0.6933333333333334 0.6933333333333334
Printing some Q and Qe and total Qs values:  [[0.767]
 [0.767]
 [0.766]
 [0.767]
 [0.767]
 [0.76 ]
 [0.767]] [[34.571]
 [34.571]
 [36.929]
 [34.571]
 [34.571]
 [34.297]
 [34.571]] [[0.767]
 [0.767]
 [0.766]
 [0.767]
 [0.767]
 [0.76 ]
 [0.767]]
maxi score, test score, baseline:  0.12396666666666653 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.316488913586575
actor:  0 policy actor:  1  step number:  50 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12691333333333318 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.12691333333333318 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  41.39927255471265
printing an ep nov before normalisation:  36.580440321946234
using explorer policy with actor:  0
maxi score, test score, baseline:  0.12691333333333318 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  32.93579816818237
actor:  0 policy actor:  1  step number:  54 total reward:  0.3533333333333324  reward:  1.0 rdn_beta:  2.0
siam score:  -0.7640147
actor:  1 policy actor:  1  step number:  38 total reward:  0.6066666666666668  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1265933333333332 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.1265933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1265933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  32.639644691933064
actor:  1 policy actor:  1  step number:  35 total reward:  0.56  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1265933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  41.39657550387912
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1265933333333332 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.1265933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1265933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  33.33760738372803
maxi score, test score, baseline:  0.1265933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1265933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1265933333333332 0.6933333333333334 0.6933333333333334
actor:  1 policy actor:  1  step number:  51 total reward:  0.4266666666666662  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  54 total reward:  0.2733333333333331  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  39.86142575552974
maxi score, test score, baseline:  0.1265933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.123]
 [0.158]
 [0.044]
 [0.034]
 [0.041]
 [0.034]
 [0.037]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.123]
 [0.158]
 [0.044]
 [0.034]
 [0.041]
 [0.034]
 [0.037]]
Printing some Q and Qe and total Qs values:  [[0.094]
 [0.182]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]] [[35.229]
 [52.56 ]
 [35.229]
 [35.229]
 [35.229]
 [35.229]
 [35.229]] [[0.792]
 [1.79 ]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]]
maxi score, test score, baseline:  0.1265933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1265933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  46.97514045783765
actor:  1 policy actor:  1  step number:  48 total reward:  0.40666666666666673  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.76615405
maxi score, test score, baseline:  0.1265933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  50.56744001638376
printing an ep nov before normalisation:  33.52185997626167
maxi score, test score, baseline:  0.1265933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  38.6452937237089
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.65 ]
 [0.604]
 [0.528]
 [0.589]
 [0.581]
 [0.513]] [[31.266]
 [31.071]
 [32.818]
 [29.131]
 [32.86 ]
 [50.232]
 [29.524]] [[1.148]
 [1.206]
 [1.223]
 [1.014]
 [1.209]
 [1.828]
 [1.013]]
siam score:  -0.76695144
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  44 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.12672666666666654 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.12672666666666654 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.12672666666666654 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  63 total reward:  0.33333333333333337  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12672666666666654 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  28.871651437633393
actor:  0 policy actor:  0  step number:  47 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  0.1273133333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.361]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.326]
 [0.361]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]]
maxi score, test score, baseline:  0.1273133333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1273133333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1273133333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  47.327990631347966
printing an ep nov before normalisation:  41.84964167680243
maxi score, test score, baseline:  0.1273133333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1273133333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.013]
 [0.737]
 [0.737]
 [0.737]
 [0.737]
 [0.737]] [[ 0.307]
 [ 0.301]
 [27.569]
 [27.569]
 [27.569]
 [27.569]
 [27.569]] [[0.019]
 [0.019]
 [1.333]
 [1.333]
 [1.333]
 [1.333]
 [1.333]]
maxi score, test score, baseline:  0.1273133333333332 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.1273133333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  40 total reward:  0.5933333333333336  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  42.907561150964085
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.067]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.016]
 [0.067]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]]
Printing some Q and Qe and total Qs values:  [[0.187]
 [0.224]
 [0.187]
 [0.126]
 [0.187]
 [0.187]
 [0.187]] [[35.989]
 [40.457]
 [35.989]
 [38.228]
 [35.989]
 [35.989]
 [35.989]] [[0.898]
 [1.161]
 [0.898]
 [0.95 ]
 [0.898]
 [0.898]
 [0.898]]
maxi score, test score, baseline:  0.1273133333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.095]
 [0.116]
 [0.031]
 [0.076]
 [0.057]
 [0.029]
 [0.078]] [[33.055]
 [30.1  ]
 [41.442]
 [30.115]
 [34.091]
 [42.181]
 [29.629]] [[1.08 ]
 [0.944]
 [1.459]
 [0.905]
 [1.096]
 [1.497]
 [0.882]]
printing an ep nov before normalisation:  61.03021142877826
maxi score, test score, baseline:  0.1273133333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  38.670041782373715
siam score:  -0.76774925
maxi score, test score, baseline:  0.1273133333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1273133333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1273133333333332 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.1273133333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.238]
 [0.282]
 [0.014]
 [0.152]
 [0.242]
 [0.099]
 [0.212]] [[31.277]
 [32.532]
 [34.187]
 [31.342]
 [34.188]
 [34.757]
 [31.687]] [[1.373]
 [1.531]
 [1.415]
 [1.293]
 [1.643]
 [1.551]
 [1.383]]
maxi score, test score, baseline:  0.1273133333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1273133333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  49 total reward:  0.31999999999999984  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.659]
 [0.517]
 [0.514]
 [0.51 ]
 [0.535]
 [0.52 ]] [[33.818]
 [28.559]
 [33.254]
 [29.528]
 [33.357]
 [32.726]
 [35.836]] [[2.341]
 [2.184]
 [2.293]
 [2.091]
 [2.292]
 [2.283]
 [2.434]]
maxi score, test score, baseline:  0.1273133333333332 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.1273133333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  48.85786038542195
actor:  1 policy actor:  1  step number:  68 total reward:  0.01999999999999935  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  29.56746946141738
actor:  1 policy actor:  1  step number:  49 total reward:  0.31999999999999984  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12465999999999988 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  37.64037107428114
actions average: 
K:  3  action  0 :  tensor([0.4938, 0.0137, 0.0788, 0.1316, 0.1300, 0.0776, 0.0745],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0081, 0.9664, 0.0045, 0.0034, 0.0027, 0.0035, 0.0114],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1179, 0.0392, 0.3716, 0.0808, 0.0887, 0.1091, 0.1927],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.3314, 0.0973, 0.1052, 0.1007, 0.1366, 0.0912, 0.1376],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2556, 0.0354, 0.0736, 0.0595, 0.3434, 0.1425, 0.0900],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0672, 0.0227, 0.0887, 0.0628, 0.0780, 0.6279, 0.0527],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1746, 0.1895, 0.0960, 0.1154, 0.1384, 0.1323, 0.1537],
       grad_fn=<DivBackward0>)
siam score:  -0.772889
maxi score, test score, baseline:  0.12465999999999988 0.6933333333333334 0.6933333333333334
actor:  0 policy actor:  1  step number:  37 total reward:  0.52  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1254466666666665 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  33.629685731515494
actor:  1 policy actor:  1  step number:  50 total reward:  0.38  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  39.211147338817305
printing an ep nov before normalisation:  51.98239477812186
maxi score, test score, baseline:  0.1254466666666665 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.474]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]] [[50.997]
 [49.535]
 [50.997]
 [50.997]
 [50.997]
 [50.997]
 [50.997]] [[2.096]
 [2.074]
 [2.096]
 [2.096]
 [2.096]
 [2.096]
 [2.096]]
maxi score, test score, baseline:  0.1254466666666665 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  36.19826730646183
printing an ep nov before normalisation:  40.74454112789959
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.46789619215792
Printing some Q and Qe and total Qs values:  [[0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]] [[8.083]
 [5.354]
 [7.944]
 [6.087]
 [6.307]
 [6.547]
 [6.53 ]] [[1.156]
 [1.017]
 [1.149]
 [1.054]
 [1.065]
 [1.078]
 [1.077]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5066666666666669  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1254466666666665 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  38.691328791351815
maxi score, test score, baseline:  0.1254466666666665 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.602]
 [0.539]
 [0.539]
 [0.539]
 [0.591]
 [0.455]
 [0.539]] [[30.166]
 [30.057]
 [30.057]
 [30.057]
 [31.114]
 [37.318]
 [30.057]] [[1.693]
 [1.623]
 [1.623]
 [1.623]
 [1.749]
 [2.049]
 [1.623]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 43.49747474336757
printing an ep nov before normalisation:  39.806358430667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1254466666666665 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1254466666666665 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
siam score:  -0.7681269
printing an ep nov before normalisation:  0.00019564015133255452
printing an ep nov before normalisation:  37.542677909405036
actor:  1 policy actor:  1  step number:  39 total reward:  0.52  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  54 total reward:  0.32666666666666666  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1254466666666665 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  25.75371530320909
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.317]
 [0.25 ]
 [0.249]
 [0.25 ]
 [0.25 ]
 [0.25 ]] [[34.12 ]
 [37.551]
 [32.725]
 [36.714]
 [32.725]
 [32.725]
 [32.725]] [[1.104]
 [1.387]
 [1.015]
 [1.266]
 [1.015]
 [1.015]
 [1.015]]
maxi score, test score, baseline:  0.1254466666666665 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1254466666666665 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  37.73914001120785
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.6140560903487
maxi score, test score, baseline:  0.1254466666666665 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1254466666666665 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
line 256 mcts: sample exp_bonus 31.6587300576179
printing an ep nov before normalisation:  33.67410559329295
actor:  1 policy actor:  1  step number:  46 total reward:  0.43333333333333335  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1254466666666665 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  63 total reward:  0.239999999999999  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  61 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  2  action  0 :  tensor([0.4994, 0.0698, 0.1231, 0.0671, 0.0967, 0.0625, 0.0815],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0070, 0.9404, 0.0061, 0.0209, 0.0050, 0.0045, 0.0160],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0817, 0.0031, 0.6420, 0.0426, 0.0382, 0.0767, 0.1156],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2250, 0.0916, 0.1117, 0.1366, 0.1527, 0.1426, 0.1398],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1554, 0.0179, 0.0647, 0.0857, 0.5132, 0.0680, 0.0951],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1575, 0.0167, 0.2055, 0.1052, 0.0870, 0.1470, 0.2811],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0873, 0.2498, 0.0689, 0.1991, 0.0682, 0.0539, 0.2727],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  0
using explorer policy with actor:  0
siam score:  -0.7664455
Printing some Q and Qe and total Qs values:  [[0.098]
 [0.198]
 [0.098]
 [0.098]
 [0.098]
 [0.098]
 [0.098]] [[46.381]
 [44.551]
 [46.381]
 [46.381]
 [46.381]
 [46.381]
 [46.381]] [[0.386]
 [0.467]
 [0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]]
actor:  0 policy actor:  0  step number:  55 total reward:  0.17333333333333267  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.459]
 [0.421]
 [0.421]
 [0.475]
 [0.426]
 [0.475]] [[34.658]
 [31.34 ]
 [25.98 ]
 [26.146]
 [34.658]
 [26.602]
 [34.658]] [[0.867]
 [0.792]
 [0.658]
 [0.661]
 [0.867]
 [0.674]
 [0.867]]
maxi score, test score, baseline:  0.12479333333333321 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.12479333333333321 0.6933333333333334 0.6933333333333334
actor:  1 policy actor:  1  step number:  48 total reward:  0.2999999999999998  reward:  1.0 rdn_beta:  1.333
siam score:  -0.77220035
actor:  1 policy actor:  1  step number:  50 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7698177
maxi score, test score, baseline:  0.12479333333333321 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.12479333333333321 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1247933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  0 policy actor:  1  step number:  32 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.12808666666666652 0.6933333333333334 0.6933333333333334
printing an ep nov before normalisation:  36.5850897911777
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12808666666666652 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  38.89581424907263
maxi score, test score, baseline:  0.12808666666666652 0.6933333333333334 0.6933333333333334
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.645]
 [0.676]
 [0.638]
 [0.55 ]
 [0.664]
 [0.664]] [[39.87 ]
 [44.018]
 [41.518]
 [37.553]
 [40.426]
 [41.71 ]
 [43.742]] [[2.072]
 [2.182]
 [2.077]
 [1.825]
 [1.893]
 [2.076]
 [2.186]]
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.776]
 [0.686]
 [0.706]
 [0.686]
 [0.686]
 [0.686]] [[11.494]
 [17.226]
 [11.494]
 [22.411]
 [11.494]
 [11.494]
 [11.494]] [[0.877]
 [1.2  ]
 [0.877]
 [1.342]
 [0.877]
 [0.877]
 [0.877]]
Printing some Q and Qe and total Qs values:  [[0.63 ]
 [0.63 ]
 [0.696]
 [0.63 ]
 [0.693]
 [0.686]
 [0.63 ]] [[44.636]
 [44.636]
 [47.432]
 [44.636]
 [41.406]
 [45.964]
 [44.636]] [[2.078]
 [2.078]
 [2.324]
 [2.078]
 [1.934]
 [2.219]
 [2.078]]
printing an ep nov before normalisation:  51.301392989951026
maxi score, test score, baseline:  0.12808666666666652 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3266666666666661  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12808666666666652 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.12808666666666652 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  69 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12808666666666652 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  30.67347526550293
maxi score, test score, baseline:  0.12808666666666652 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.12808666666666652 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.12808666666666652 0.6933333333333334 0.6933333333333334
printing an ep nov before normalisation:  25.763988910490625
maxi score, test score, baseline:  0.12808666666666652 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.12808666666666652 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.731]
 [0.777]
 [0.724]
 [0.731]
 [0.731]
 [0.725]
 [0.731]] [[34.009]
 [31.247]
 [35.762]
 [34.009]
 [34.009]
 [36.905]
 [34.009]] [[0.731]
 [0.777]
 [0.724]
 [0.731]
 [0.731]
 [0.725]
 [0.731]]
maxi score, test score, baseline:  0.12808666666666652 0.6933333333333334 0.6933333333333334
printing an ep nov before normalisation:  38.04771205360512
printing an ep nov before normalisation:  25.806579617552266
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.564]
 [0.558]
 [0.564]
 [0.564]
 [0.561]
 [0.564]] [[37.034]
 [37.034]
 [34.234]
 [37.034]
 [37.034]
 [37.927]
 [37.034]] [[1.451]
 [1.451]
 [1.333]
 [1.451]
 [1.451]
 [1.483]
 [1.451]]
printing an ep nov before normalisation:  48.606780913150516
actor:  1 policy actor:  1  step number:  61 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  57 total reward:  0.39999999999999947  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  1  action  0 :  tensor([0.4537, 0.0460, 0.0779, 0.1147, 0.1372, 0.0957, 0.0749],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0061, 0.9236, 0.0047, 0.0080, 0.0023, 0.0018, 0.0535],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0710, 0.0068, 0.7178, 0.0364, 0.0393, 0.0621, 0.0665],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1480, 0.0293, 0.1262, 0.3120, 0.1290, 0.1321, 0.1233],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1767, 0.0017, 0.0721, 0.0763, 0.5364, 0.0868, 0.0500],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1289, 0.0031, 0.1317, 0.1022, 0.1187, 0.3929, 0.1224],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1803, 0.0021, 0.1618, 0.1327, 0.1554, 0.1265, 0.2411],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  1  step number:  50 total reward:  0.5400000000000003  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  30.1737642288208
maxi score, test score, baseline:  0.13116666666666654 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.13116666666666654 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  33.356920902040265
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.457]
 [0.356]
 [0.311]
 [0.356]
 [0.356]
 [0.269]] [[36.313]
 [38.369]
 [36.313]
 [37.104]
 [36.313]
 [36.313]
 [39.684]] [[0.555]
 [0.681]
 [0.555]
 [0.519]
 [0.555]
 [0.555]
 [0.509]]
printing an ep nov before normalisation:  36.42349197431764
printing an ep nov before normalisation:  35.531542521526035
maxi score, test score, baseline:  0.13116666666666654 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  46 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.13116666666666654 0.6933333333333334 0.6933333333333334
printing an ep nov before normalisation:  41.07390652044916
actor:  1 policy actor:  1  step number:  51 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.287]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]] [[42.611]
 [41.77 ]
 [42.611]
 [42.611]
 [42.611]
 [42.611]
 [42.611]] [[0.773]
 [0.792]
 [0.773]
 [0.773]
 [0.773]
 [0.773]
 [0.773]]
maxi score, test score, baseline:  0.13116666666666654 0.6933333333333334 0.6933333333333334
actor:  1 policy actor:  1  step number:  62 total reward:  0.39333333333333365  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.13116666666666654 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.13116666666666654 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  47.24200779191471
maxi score, test score, baseline:  0.13116666666666654 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.13116666666666654 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
from probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.13116666666666654 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.13116666666666654 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.13116666666666654 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  36.766471333472985
maxi score, test score, baseline:  0.13116666666666654 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.13116666666666654 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.13116666666666654 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.13116666666666654 0.6933333333333334 0.6933333333333334
actor:  0 policy actor:  0  step number:  60 total reward:  0.12666666666666626  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  32.216501235961914
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  0.13112666666666653 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.13112666666666653 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  0 policy actor:  0  step number:  47 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 51.90911605852527
maxi score, test score, baseline:  0.13091333333333321 0.6933333333333334 0.6933333333333334
actor:  1 policy actor:  1  step number:  46 total reward:  0.4866666666666669  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.13091333333333321 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.13091333333333321 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  27.3765669716399
maxi score, test score, baseline:  0.1249533333333332 0.6933333333333334 0.6933333333333334
printing an ep nov before normalisation:  30.74406980601644
maxi score, test score, baseline:  0.1249533333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1249533333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.685]
 [0.715]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]] [[24.021]
 [25.657]
 [24.021]
 [24.021]
 [24.021]
 [24.021]
 [24.021]] [[2.156]
 [2.416]
 [2.156]
 [2.156]
 [2.156]
 [2.156]
 [2.156]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.38  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1249533333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1249533333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  40 total reward:  0.6333333333333337  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.511]
 [0.423]
 [0.42 ]
 [0.421]
 [0.433]
 [0.492]] [[28.371]
 [27.494]
 [20.47 ]
 [20.353]
 [20.396]
 [23.103]
 [28.818]] [[0.672]
 [0.692]
 [0.521]
 [0.516]
 [0.518]
 [0.562]
 [0.688]]
Printing some Q and Qe and total Qs values:  [[0.714]
 [0.668]
 [0.55 ]
 [0.55 ]
 [0.599]
 [0.55 ]
 [0.55 ]] [[40.35 ]
 [38.507]
 [32.196]
 [32.196]
 [31.69 ]
 [32.196]
 [32.196]] [[0.988]
 [0.923]
 [0.742]
 [0.742]
 [0.787]
 [0.742]
 [0.742]]
maxi score, test score, baseline:  0.1249533333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1249533333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.101]
 [0.116]
 [0.101]
 [0.101]
 [0.101]
 [0.101]
 [0.101]] [[44.97]
 [50.51]
 [44.97]
 [44.97]
 [44.97]
 [44.97]
 [44.97]] [[1.641]
 [1.999]
 [1.641]
 [1.641]
 [1.641]
 [1.641]
 [1.641]]
printing an ep nov before normalisation:  43.97698513053499
printing an ep nov before normalisation:  41.99376331159853
actor:  1 policy actor:  1  step number:  71 total reward:  0.06666666666666565  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1249533333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
using another actor
maxi score, test score, baseline:  0.1249533333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5933333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1249533333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1249533333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1249533333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  42.481389107801924
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  36.63115978240967
actor:  1 policy actor:  1  step number:  54 total reward:  0.33999999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
printing an ep nov before normalisation:  33.954399824142456
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  56 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  45 total reward:  0.626666666666667  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  35.198430341653136
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.418]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]] [[45.025]
 [42.822]
 [45.025]
 [45.025]
 [45.025]
 [45.025]
 [45.025]] [[0.943]
 [0.999]
 [0.943]
 [0.943]
 [0.943]
 [0.943]
 [0.943]]
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  40.57275177736003
actor:  1 policy actor:  1  step number:  53 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.263]
 [0.328]
 [0.105]
 [0.259]
 [0.216]
 [0.259]
 [0.259]] [[37.761]
 [38.581]
 [38.739]
 [35.415]
 [41.689]
 [35.415]
 [35.415]] [[1.295]
 [1.397]
 [1.181]
 [1.186]
 [1.425]
 [1.186]
 [1.186]]
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.593]
 [0.591]] [[16.166]
 [16.166]
 [16.166]
 [16.166]
 [16.166]
 [14.273]
 [16.166]] [[3.109]
 [3.109]
 [3.109]
 [3.109]
 [3.109]
 [2.593]
 [3.109]]
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.553]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]] [[30.285]
 [38.782]
 [30.285]
 [30.285]
 [30.285]
 [30.285]
 [30.285]] [[1.252]
 [1.718]
 [1.252]
 [1.252]
 [1.252]
 [1.252]
 [1.252]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.2466666666666666  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5133333333333335  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  47 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.565]
 [0.583]
 [0.497]
 [0.416]
 [0.509]
 [0.573]] [[30.523]
 [34.608]
 [37.483]
 [30.523]
 [30.345]
 [36.211]
 [34.988]] [[0.628]
 [0.74 ]
 [0.788]
 [0.628]
 [0.546]
 [0.701]
 [0.751]]
printing an ep nov before normalisation:  42.8454314781831
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  46 total reward:  0.5  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  29.117231843795718
actor:  1 policy actor:  1  step number:  44 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  60.11062096121407
actor:  1 policy actor:  1  step number:  41 total reward:  0.5200000000000001  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  31.177211942367347
printing an ep nov before normalisation:  26.44631862640381
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  33.385398031211196
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  35.81296552185886
printing an ep nov before normalisation:  24.958718481499886
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  46.66874344183102
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.80637936041207
printing an ep nov before normalisation:  24.239601667594876
actor:  1 policy actor:  1  step number:  65 total reward:  0.18666666666666587  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actions average: 
K:  1  action  0 :  tensor([0.4986, 0.0048, 0.1088, 0.0909, 0.1147, 0.0891, 0.0930],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0082, 0.9399, 0.0074, 0.0047, 0.0032, 0.0038, 0.0328],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0466, 0.0031, 0.6847, 0.0387, 0.0327, 0.1603, 0.0339],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1557, 0.1119, 0.0917, 0.2661, 0.1287, 0.1192, 0.1267],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2072, 0.0094, 0.0972, 0.0647, 0.4309, 0.0664, 0.1241],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1303, 0.0122, 0.0993, 0.1006, 0.0823, 0.5087, 0.0666],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1174, 0.1790, 0.1181, 0.0931, 0.0880, 0.1131, 0.2914],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  71 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  41.33220025940498
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  45 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  42 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  30.166056539244952
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
actor:  1 policy actor:  1  step number:  35 total reward:  0.6400000000000002  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  25.45860767364502
printing an ep nov before normalisation:  39.883064855645216
actor:  1 policy actor:  1  step number:  39 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  33.880040677777416
printing an ep nov before normalisation:  44.525142508133534
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.517]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]] [[35.315]
 [34.428]
 [28.714]
 [28.714]
 [28.714]
 [28.714]
 [28.714]] [[1.31 ]
 [1.261]
 [0.99 ]
 [0.99 ]
 [0.99 ]
 [0.99 ]
 [0.99 ]]
actor:  1 policy actor:  1  step number:  74 total reward:  0.15333333333333254  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  38.9227337203535
printing an ep nov before normalisation:  32.16535895765571
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  31.101689338684082
maxi score, test score, baseline:  0.12249999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  0 policy actor:  0  step number:  52 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12237999999999986 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.12237999999999986 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  56 total reward:  0.25999999999999923  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  44 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  40.951196494097275
Printing some Q and Qe and total Qs values:  [[ 0.066]
 [ 0.126]
 [-0.003]
 [ 0.045]
 [ 0.037]
 [ 0.039]
 [ 0.094]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.066]
 [ 0.126]
 [-0.003]
 [ 0.045]
 [ 0.037]
 [ 0.039]
 [ 0.094]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3733333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12521999999999986 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[ 0.175]
 [ 0.128]
 [-0.033]
 [ 0.174]
 [ 0.161]
 [ 0.175]
 [ 0.175]] [[42.562]
 [37.229]
 [34.824]
 [34.892]
 [37.903]
 [42.562]
 [42.562]] [[2.646]
 [2.06 ]
 [1.655]
 [1.869]
 [2.161]
 [2.646]
 [2.646]]
maxi score, test score, baseline:  0.12521999999999986 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  53 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.12221999999999988 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.12221999999999988 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.12221999999999988 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.12221999999999988 0.6933333333333334 0.6933333333333334
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.658]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]] [[28.716]
 [38.35 ]
 [28.716]
 [28.716]
 [28.716]
 [28.716]
 [28.716]] [[1.469]
 [2.113]
 [1.469]
 [1.469]
 [1.469]
 [1.469]
 [1.469]]
printing an ep nov before normalisation:  34.947375350637024
maxi score, test score, baseline:  0.12221999999999988 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.685]
 [0.911]
 [0.782]
 [0.668]
 [0.711]
 [0.676]
 [0.669]] [[20.832]
 [21.605]
 [20.817]
 [22.153]
 [21.883]
 [21.97 ]
 [21.612]] [[2.188]
 [2.531]
 [2.283]
 [2.372]
 [2.374]
 [2.351]
 [2.29 ]]
maxi score, test score, baseline:  0.12221999999999988 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  0 policy actor:  1  step number:  41 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  42 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  24.0097713470459
actor:  1 policy actor:  1  step number:  44 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.7538948059082
maxi score, test score, baseline:  0.1222333333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
siam score:  -0.7733765
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.57908998189364
printing an ep nov before normalisation:  28.612721926902893
maxi score, test score, baseline:  0.1222333333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[ 0.531]
 [ 0.403]
 [-0.012]
 [ 0.403]
 [ 0.448]
 [ 0.245]
 [ 0.403]] [[26.125]
 [34.771]
 [36.338]
 [34.771]
 [26.727]
 [31.978]
 [34.771]] [[1.423]
 [1.591]
 [1.23 ]
 [1.591]
 [1.361]
 [1.337]
 [1.591]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.44000000000000017  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  25.506697098414104
actor:  1 policy actor:  1  step number:  66 total reward:  0.2333333333333324  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  31.56645956413492
printing an ep nov before normalisation:  38.774963154556765
printing an ep nov before normalisation:  28.48243381244934
maxi score, test score, baseline:  0.1222333333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1222333333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1222333333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1222333333333332 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.1222333333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1222333333333332 0.6933333333333334 0.6933333333333334
actor:  0 policy actor:  0  step number:  42 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  50 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1220333333333332 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.1220333333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1220333333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1220333333333332 0.6933333333333334 0.6933333333333334
printing an ep nov before normalisation:  39.96169567108154
printing an ep nov before normalisation:  30.969252808377572
printing an ep nov before normalisation:  41.7000317517677
actor:  1 policy actor:  1  step number:  52 total reward:  0.32666666666666655  reward:  1.0 rdn_beta:  2.0
from probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1220333333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.948]
 [0.948]
 [0.948]
 [0.948]
 [0.948]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.948]
 [0.948]
 [0.948]
 [0.948]
 [0.948]]
maxi score, test score, baseline:  0.1220333333333332 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.1220333333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  23.01449998441308
maxi score, test score, baseline:  0.1220333333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  29.283217918306303
using explorer policy with actor:  1
printing an ep nov before normalisation:  27.531266498285657
printing an ep nov before normalisation:  40.063265818852756
maxi score, test score, baseline:  0.1220333333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1220333333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  37.01003844023318
maxi score, test score, baseline:  0.1220333333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  0 policy actor:  1  step number:  47 total reward:  0.48  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.52776581929259
printing an ep nov before normalisation:  32.66509005844575
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  61 total reward:  0.36  reward:  1.0 rdn_beta:  1.667
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.77983963
maxi score, test score, baseline:  0.12217999999999986 0.6933333333333334 0.6933333333333334
actor:  0 policy actor:  0  step number:  41 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12261999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actions average: 
K:  2  action  0 :  tensor([0.4665, 0.0040, 0.0998, 0.0876, 0.2037, 0.0696, 0.0688],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0235, 0.9050, 0.0129, 0.0148, 0.0123, 0.0081, 0.0234],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1094, 0.0028, 0.4381, 0.1079, 0.1100, 0.1645, 0.0673],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1666, 0.0148, 0.1022, 0.4075, 0.1058, 0.0732, 0.1299],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0891, 0.0007, 0.0670, 0.0534, 0.6947, 0.0489, 0.0461],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0450, 0.0013, 0.2329, 0.0520, 0.0488, 0.5739, 0.0459],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1291, 0.1874, 0.0855, 0.1277, 0.0672, 0.0773, 0.3259],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.12261999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  0 policy actor:  1  step number:  46 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.368]
 [0.331]
 [0.225]
 [0.329]
 [0.331]
 [0.331]] [[24.045]
 [25.988]
 [26.938]
 [26.37 ]
 [24.903]
 [26.938]
 [26.938]] [[1.303]
 [1.462]
 [1.506]
 [1.351]
 [1.331]
 [1.506]
 [1.506]]
printing an ep nov before normalisation:  24.846483778788723
maxi score, test score, baseline:  0.1255933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1255933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[ 0.   ]
 [-0.   ]
 [-0.126]
 [-0.126]
 [-0.   ]
 [-0.126]
 [-0.   ]] [[ 0.001]
 [ 0.001]
 [19.88 ]
 [19.88 ]
 [ 0.001]
 [19.88 ]
 [ 0.001]] [[0.   ]
 [0.   ]
 [0.157]
 [0.157]
 [0.   ]
 [0.157]
 [0.   ]]
Printing some Q and Qe and total Qs values:  [[0.761]
 [0.895]
 [0.761]
 [0.761]
 [0.721]
 [0.761]
 [0.761]] [[14.197]
 [17.283]
 [14.197]
 [14.197]
 [16.966]
 [14.197]
 [14.197]] [[1.182]
 [1.612]
 [1.182]
 [1.182]
 [1.408]
 [1.182]
 [1.182]]
maxi score, test score, baseline:  0.1255933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3866666666666667  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1255933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  33.05426724682347
maxi score, test score, baseline:  0.1255933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  61 total reward:  0.30666666666666675  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1255933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  53.877310941640786
Printing some Q and Qe and total Qs values:  [[-0.093]
 [-0.093]
 [-0.093]
 [-0.093]
 [-0.093]
 [-0.093]
 [-0.093]] [[46.288]
 [46.288]
 [46.288]
 [46.288]
 [46.288]
 [46.288]
 [46.288]] [[1.825]
 [1.825]
 [1.825]
 [1.825]
 [1.825]
 [1.825]
 [1.825]]
Printing some Q and Qe and total Qs values:  [[-0.1  ]
 [-0.097]
 [-0.097]
 [-0.097]
 [-0.102]
 [-0.097]
 [-0.097]] [[36.182]
 [30.404]
 [30.404]
 [30.404]
 [34.662]
 [30.404]
 [30.404]] [[1.659]
 [1.102]
 [1.102]
 [1.102]
 [1.51 ]
 [1.102]
 [1.102]]
maxi score, test score, baseline:  0.1255933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  37.702002869072
actor:  1 policy actor:  1  step number:  49 total reward:  0.31999999999999984  reward:  1.0 rdn_beta:  2.0
from probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.49 ]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]] [[30.298]
 [29.516]
 [30.298]
 [30.298]
 [30.298]
 [30.298]
 [30.298]] [[1.103]
 [1.198]
 [1.103]
 [1.103]
 [1.103]
 [1.103]
 [1.103]]
actor:  0 policy actor:  1  step number:  51 total reward:  0.4533333333333336  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12323333333333321 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  30.09715522126287
printing an ep nov before normalisation:  47.2512776584074
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
actions average: 
K:  0  action  0 :  tensor([0.4326, 0.0311, 0.1132, 0.0993, 0.1356, 0.0907, 0.0975],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0025, 0.9832, 0.0028, 0.0024, 0.0021, 0.0016, 0.0053],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0751, 0.0137, 0.7052, 0.0223, 0.0332, 0.1108, 0.0397],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2343, 0.1053, 0.1085, 0.1334, 0.1138, 0.0954, 0.2093],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2204, 0.0048, 0.1002, 0.0766, 0.4142, 0.0838, 0.1000],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1370, 0.0169, 0.1193, 0.0831, 0.1104, 0.4631, 0.0702],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1867, 0.0544, 0.1907, 0.0806, 0.0864, 0.1086, 0.2926],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  39 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.12323333333333321 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.12323333333333321 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.297]
 [0.263]
 [0.269]
 [0.26 ]
 [0.245]
 [0.252]] [[25.359]
 [25.342]
 [24.826]
 [24.748]
 [24.915]
 [24.849]
 [24.932]] [[0.796]
 [0.817]
 [0.759]
 [0.762]
 [0.761]
 [0.742]
 [0.753]]
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.285]
 [0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.268]] [[22.308]
 [25.195]
 [22.308]
 [22.308]
 [22.308]
 [22.308]
 [22.308]] [[0.607]
 [0.71 ]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]]
printing an ep nov before normalisation:  31.97318815235593
maxi score, test score, baseline:  0.12323333333333321 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.12323333333333321 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  58 total reward:  0.17999999999999972  reward:  1.0 rdn_beta:  2.0
from probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]] [[37.003]
 [37.003]
 [37.003]
 [37.003]
 [37.003]
 [37.003]
 [37.003]] [[1.557]
 [1.557]
 [1.557]
 [1.557]
 [1.557]
 [1.557]
 [1.557]]
printing an ep nov before normalisation:  34.70163822174072
actor:  1 policy actor:  1  step number:  69 total reward:  0.05333333333333268  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  51 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  1.333
from probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  33.05756659895085
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.728]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]] [[33.477]
 [33.413]
 [33.477]
 [33.477]
 [33.477]
 [33.477]
 [33.477]] [[0.678]
 [0.728]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.678]]
maxi score, test score, baseline:  0.12323333333333321 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.12323333333333321 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.12323333333333321 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.208]
 [0.324]
 [0.208]
 [0.208]
 [0.208]
 [0.208]
 [0.208]] [[37.24 ]
 [36.886]
 [37.24 ]
 [37.24 ]
 [37.24 ]
 [37.24 ]
 [37.24 ]] [[0.976]
 [1.077]
 [0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]]
printing an ep nov before normalisation:  42.375406437353654
actor:  0 policy actor:  0  step number:  44 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12348666666666654 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  46.43024975477799
printing an ep nov before normalisation:  29.971320217280876
maxi score, test score, baseline:  0.12348666666666654 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  30.943949222564697
maxi score, test score, baseline:  0.12348666666666654 0.6933333333333334 0.6933333333333334
actor:  0 policy actor:  1  step number:  45 total reward:  0.4933333333333335  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.275]
 [0.289]
 [0.253]
 [0.254]
 [0.256]
 [0.258]] [[21.486]
 [24.75 ]
 [26.15 ]
 [20.25 ]
 [20.308]
 [20.457]
 [20.335]] [[1.1  ]
 [1.417]
 [1.558]
 [0.989]
 [0.995]
 [1.01 ]
 [1.002]]
Printing some Q and Qe and total Qs values:  [[ 0.075]
 [ 0.08 ]
 [-0.   ]
 [ 0.009]
 [ 0.013]
 [ 0.01 ]
 [ 0.029]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.075]
 [ 0.08 ]
 [-0.   ]
 [ 0.009]
 [ 0.013]
 [ 0.01 ]
 [ 0.029]]
Printing some Q and Qe and total Qs values:  [[0.133]
 [0.216]
 [0.26 ]
 [0.199]
 [0.256]
 [0.199]
 [0.261]] [[36.992]
 [38.231]
 [37.154]
 [36.09 ]
 [36.442]
 [36.09 ]
 [36.152]] [[1.058]
 [1.211]
 [1.194]
 [1.073]
 [1.15 ]
 [1.073]
 [1.138]]
maxi score, test score, baseline:  0.1264733333333332 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.1264733333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  61 total reward:  0.22666666666666602  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  27.498873973183944
printing an ep nov before normalisation:  43.019514317022534
maxi score, test score, baseline:  0.1264733333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.581]
 [0.664]
 [0.643]
 [0.608]
 [0.226]
 [0.597]
 [0.618]] [[34.861]
 [39.301]
 [38.94 ]
 [34.534]
 [37.04 ]
 [37.979]
 [36.76 ]] [[1.711]
 [2.117]
 [2.07 ]
 [1.714]
 [1.514]
 [1.954]
 [1.886]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5600000000000003  reward:  1.0 rdn_beta:  2.0
siam score:  -0.7816412
actor:  1 policy actor:  1  step number:  49 total reward:  0.5600000000000004  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12337999999999986 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  43.34982453546526
maxi score, test score, baseline:  0.12337999999999986 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  39.943803610237396
maxi score, test score, baseline:  0.12337999999999986 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  19.467380046844482
maxi score, test score, baseline:  0.12337999999999986 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.12337999999999986 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  31.297364114266117
printing an ep nov before normalisation:  30.166839629585123
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.504]
 [0.385]
 [0.461]
 [0.385]
 [0.434]
 [0.387]] [[28.867]
 [30.333]
 [28.453]
 [29.401]
 [28.401]
 [29.883]
 [28.276]] [[0.828]
 [0.988]
 [0.814]
 [0.918]
 [0.813]
 [0.905]
 [0.811]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  0.42000000000000004  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  3  action  0 :  tensor([0.5148, 0.0067, 0.0626, 0.0883, 0.1451, 0.0804, 0.1022],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0136, 0.8853, 0.0309, 0.0134, 0.0108, 0.0280, 0.0180],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1521, 0.0118, 0.4858, 0.0875, 0.0749, 0.0909, 0.0970],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1963, 0.2234, 0.0807, 0.1544, 0.0945, 0.1255, 0.1251],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1813, 0.0016, 0.0500, 0.0834, 0.5363, 0.0650, 0.0824],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1450, 0.0138, 0.1169, 0.1470, 0.0977, 0.3584, 0.1212],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1460, 0.1264, 0.1065, 0.0984, 0.1126, 0.1035, 0.3067],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  52 total reward:  0.24666666666666648  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  53 total reward:  0.30666666666666664  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  2  action  0 :  tensor([0.5357, 0.0148, 0.0723, 0.0742, 0.1330, 0.0781, 0.0919],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0042,     0.9488,     0.0141,     0.0065,     0.0008,     0.0052,
            0.0204], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0231, 0.1175, 0.6417, 0.0329, 0.0250, 0.1346, 0.0252],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1672, 0.0297, 0.0848, 0.3169, 0.0785, 0.0974, 0.2254],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1825, 0.0634, 0.0760, 0.0651, 0.4602, 0.0735, 0.0791],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.2471, 0.0183, 0.1769, 0.1046, 0.1021, 0.2438, 0.1073],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2439, 0.0822, 0.0852, 0.0969, 0.1138, 0.0781, 0.3001],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.192]
 [0.037]
 [0.037]
 [0.037]
 [0.037]
 [0.037]] [[37.222]
 [40.262]
 [37.222]
 [37.222]
 [37.222]
 [37.222]
 [37.222]] [[1.561]
 [1.903]
 [1.561]
 [1.561]
 [1.561]
 [1.561]
 [1.561]]
maxi score, test score, baseline:  0.12337999999999986 0.6933333333333334 0.6933333333333334
Printing some Q and Qe and total Qs values:  [[0.17 ]
 [0.229]
 [0.051]
 [0.165]
 [0.173]
 [0.055]
 [0.2  ]] [[29.927]
 [35.98 ]
 [34.594]
 [36.769]
 [41.377]
 [46.309]
 [34.697]] [[0.821]
 [1.149]
 [0.909]
 [1.12 ]
 [1.332]
 [1.434]
 [1.063]]
actor:  1 policy actor:  1  step number:  33 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12337999999999986 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.12337999999999986 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  29.173123397879486
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]] [[21.059]
 [21.059]
 [21.059]
 [21.059]
 [21.059]
 [21.059]
 [21.059]] [[1.125]
 [1.125]
 [1.125]
 [1.125]
 [1.125]
 [1.125]
 [1.125]]
printing an ep nov before normalisation:  35.60593128204346
actor:  1 policy actor:  1  step number:  47 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.12337999999999986 0.6933333333333334 0.6933333333333334
printing an ep nov before normalisation:  33.89836298746281
maxi score, test score, baseline:  0.12337999999999986 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.12337999999999986 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.12337999999999986 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.771]
 [0.906]
 [0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]] [[27.846]
 [37.825]
 [24.602]
 [24.602]
 [24.602]
 [24.602]
 [24.602]] [[0.771]
 [0.906]
 [0.778]
 [0.778]
 [0.778]
 [0.778]
 [0.778]]
actor:  0 policy actor:  1  step number:  42 total reward:  0.5400000000000003  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1239933333333332 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.1239933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.1239933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[-0.089]
 [-0.085]
 [-0.09 ]
 [-0.085]
 [-0.105]
 [-0.099]
 [-0.085]] [[27.627]
 [25.466]
 [26.415]
 [25.466]
 [26.284]
 [12.643]
 [25.466]] [[0.911]
 [0.836]
 [0.866]
 [0.836]
 [0.847]
 [0.359]
 [0.836]]
maxi score, test score, baseline:  0.1239933333333332 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  0 policy actor:  0  step number:  65 total reward:  0.09333333333333238  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  66 total reward:  0.16666666666666607  reward:  1.0 rdn_beta:  0.333
from probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]
 [0.015]]
actions average: 
K:  4  action  0 :  tensor([0.3920, 0.0086, 0.1034, 0.1274, 0.1456, 0.1185, 0.1045],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0084, 0.9263, 0.0094, 0.0120, 0.0033, 0.0021, 0.0384],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0042, 0.0214, 0.7518, 0.0856, 0.0081, 0.0962, 0.0327],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2020, 0.0188, 0.1045, 0.2452, 0.1527, 0.1524, 0.1245],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1828, 0.2064, 0.1221, 0.1158, 0.1317, 0.1095, 0.1318],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0659, 0.0022, 0.3156, 0.0638, 0.0768, 0.4249, 0.0508],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2220, 0.0157, 0.1394, 0.1274, 0.1094, 0.1472, 0.2389],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  37.73616307204775
maxi score, test score, baseline:  0.12276666666666655 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.11960666666666654 0.6933333333333334 0.6933333333333334
from probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.11680666666666654 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.11680666666666654 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.193]
 [0.192]
 [0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]] [[31.838]
 [30.443]
 [31.838]
 [31.838]
 [31.838]
 [31.838]
 [31.838]] [[1.293]
 [1.2  ]
 [1.293]
 [1.293]
 [1.293]
 [1.293]
 [1.293]]
Printing some Q and Qe and total Qs values:  [[0.088]
 [0.217]
 [0.196]
 [0.085]
 [0.179]
 [0.089]
 [0.088]] [[23.582]
 [31.451]
 [31.185]
 [23.914]
 [30.422]
 [22.928]
 [22.692]] [[0.222]
 [0.438]
 [0.414]
 [0.223]
 [0.389]
 [0.216]
 [0.212]]
printing an ep nov before normalisation:  37.374928964494615
printing an ep nov before normalisation:  49.099012679869205
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11680666666666654 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.699]
 [0.554]
 [0.61 ]
 [0.566]
 [0.587]
 [0.631]] [[24.969]
 [28.528]
 [26.893]
 [28.295]
 [29.086]
 [27.076]
 [26.357]] [[1.606]
 [1.924]
 [1.695]
 [1.823]
 [1.819]
 [1.738]
 [1.745]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  36.84212514155903
maxi score, test score, baseline:  0.11680666666666654 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.11680666666666654 0.6933333333333334 0.6933333333333334
actor:  1 policy actor:  1  step number:  55 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  1.667
from probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  0 policy actor:  0  step number:  59 total reward:  0.013333333333332642  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.11597999999999986 0.6933333333333334 0.6933333333333334
Printing some Q and Qe and total Qs values:  [[ 0.06 ]
 [ 0.289]
 [-0.004]
 [ 0.062]
 [ 0.025]
 [-0.003]
 [ 0.106]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.06 ]
 [ 0.289]
 [-0.004]
 [ 0.062]
 [ 0.025]
 [-0.003]
 [ 0.106]]
maxi score, test score, baseline:  0.11597999999999986 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.11597999999999986 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.11597999999999986 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  47.142207174551125
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]] [[44.641]
 [44.641]
 [44.641]
 [44.641]
 [44.641]
 [44.641]
 [44.641]] [[1.865]
 [1.865]
 [1.865]
 [1.865]
 [1.865]
 [1.865]
 [1.865]]
maxi score, test score, baseline:  0.11597999999999986 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11597999999999986 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  34 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  52 total reward:  0.2733333333333331  reward:  1.0 rdn_beta:  2.0
from probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.11563333333333321 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11563333333333321 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  42.79749593490653
printing an ep nov before normalisation:  36.92962249576904
maxi score, test score, baseline:  0.11563333333333321 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
siam score:  -0.77098405
printing an ep nov before normalisation:  24.830529304312137
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  24.973977379728428
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  34.456068144572185
maxi score, test score, baseline:  0.11563333333333319 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  27.046289024301057
maxi score, test score, baseline:  0.11563333333333319 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.11563333333333319 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11563333333333319 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  38.075275729402165
actor:  1 policy actor:  1  step number:  57 total reward:  0.23999999999999921  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.11563333333333319 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  29 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  3  action  0 :  tensor([0.4102, 0.1060, 0.0810, 0.0902, 0.1075, 0.0969, 0.1082],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0163, 0.9224, 0.0087, 0.0111, 0.0071, 0.0070, 0.0274],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0950, 0.0052, 0.6325, 0.0764, 0.0535, 0.0501, 0.0873],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1184, 0.0882, 0.0569, 0.4127, 0.0752, 0.0688, 0.1798],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2228, 0.0177, 0.0828, 0.1497, 0.2974, 0.1127, 0.1168],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1391, 0.0369, 0.0849, 0.0887, 0.0919, 0.4651, 0.0934],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1326, 0.1308, 0.0845, 0.1278, 0.1121, 0.0687, 0.3435],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  0.07770978740495593
actor:  1 policy actor:  1  step number:  38 total reward:  0.54  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.11563333333333319 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  41 total reward:  0.48  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.11563333333333319 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  45 total reward:  0.4266666666666661  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  34.25914764404297
printing an ep nov before normalisation:  0.007882548429449798
actor:  1 policy actor:  1  step number:  50 total reward:  0.3533333333333327  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 34.81679652695415
maxi score, test score, baseline:  0.11563333333333319 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  47.72099591096073
maxi score, test score, baseline:  0.11563333333333319 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  1 policy actor:  1  step number:  45 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  2.0
from probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  26.240961741554184
actor:  1 policy actor:  1  step number:  45 total reward:  0.41333333333333333  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  56.47930470370658
Printing some Q and Qe and total Qs values:  [[0.118]
 [0.151]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]] [[ 9.018]
 [20.92 ]
 [ 9.018]
 [ 9.018]
 [ 9.018]
 [ 9.018]
 [ 9.018]] [[0.118]
 [0.151]
 [0.118]
 [0.118]
 [0.118]
 [0.118]
 [0.118]]
maxi score, test score, baseline:  0.11563333333333319 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.11563333333333319 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.11563333333333319 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  35.63874289760153
printing an ep nov before normalisation:  41.0832985855589
maxi score, test score, baseline:  0.11253999999999988 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  27.75589620525862
Printing some Q and Qe and total Qs values:  [[1.013]
 [1.013]
 [1.013]
 [1.013]
 [1.013]
 [1.013]
 [1.013]] [[27.351]
 [27.351]
 [27.351]
 [27.351]
 [27.351]
 [27.351]
 [27.351]] [[1.013]
 [1.013]
 [1.013]
 [1.013]
 [1.013]
 [1.013]
 [1.013]]
Printing some Q and Qe and total Qs values:  [[0.865]
 [0.865]
 [0.865]
 [0.865]
 [0.865]
 [0.865]
 [0.865]] [[26.989]
 [26.989]
 [26.989]
 [26.989]
 [26.989]
 [26.989]
 [26.989]] [[0.865]
 [0.865]
 [0.865]
 [0.865]
 [0.865]
 [0.865]
 [0.865]]
actor:  0 policy actor:  1  step number:  35 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  48 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  0.11333999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.11333999999999987 0.6933333333333334 0.6933333333333334
printing an ep nov before normalisation:  34.79109942694197
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.55 ]
 [0.469]
 [0.424]
 [0.424]
 [0.42 ]
 [0.429]] [[26.797]
 [32.411]
 [31.43 ]
 [28.348]
 [29.618]
 [29.659]
 [28.199]] [[1.11 ]
 [1.515]
 [1.386]
 [1.189]
 [1.252]
 [1.25 ]
 [1.187]]
maxi score, test score, baseline:  0.11333999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  32.69273996524399
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.523]
 [0.567]
 [0.523]
 [0.523]
 [0.556]
 [0.523]] [[36.039]
 [36.039]
 [40.277]
 [36.039]
 [36.039]
 [38.991]
 [36.039]] [[1.337]
 [1.337]
 [1.569]
 [1.337]
 [1.337]
 [1.501]
 [1.337]]
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.547]
 [0.542]
 [0.526]
 [0.525]
 [0.529]
 [0.528]] [[29.099]
 [28.14 ]
 [35.153]
 [31.139]
 [30.117]
 [29.065]
 [29.545]] [[1.709]
 [1.63 ]
 [2.211]
 [1.859]
 [1.773]
 [1.688]
 [1.727]]
printing an ep nov before normalisation:  23.745367527008057
printing an ep nov before normalisation:  40.733981019590885
actor:  1 policy actor:  1  step number:  49 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.348]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.197]
 [0.348]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]]
printing an ep nov before normalisation:  37.30519338900978
actor:  1 policy actor:  1  step number:  55 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.168]
 [0.178]
 [0.168]
 [0.114]
 [0.168]
 [0.114]
 [0.168]] [[28.401]
 [35.475]
 [28.401]
 [29.017]
 [28.401]
 [28.55 ]
 [28.401]] [[0.782]
 [1.206]
 [0.782]
 [0.765]
 [0.782]
 [0.737]
 [0.782]]
maxi score, test score, baseline:  0.11333999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
actor:  0 policy actor:  1  step number:  62 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.183]
 [0.288]
 [0.084]
 [0.183]
 [0.247]
 [0.183]
 [0.26 ]] [[33.418]
 [36.079]
 [38.956]
 [33.418]
 [38.417]
 [33.418]
 [30.835]] [[1.279]
 [1.537]
 [1.497]
 [1.279]
 [1.63 ]
 [1.279]
 [1.208]]
maxi score, test score, baseline:  0.11609999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  31.9789050502681
maxi score, test score, baseline:  0.11609999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  32.16537033428909
maxi score, test score, baseline:  0.11609999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  38.44097938295178
maxi score, test score, baseline:  0.11609999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.11609999999999987 0.6933333333333334 0.6933333333333334
printing an ep nov before normalisation:  25.143046379089355
maxi score, test score, baseline:  0.11609999999999987 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.11609999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.76 ]
 [0.619]
 [0.619]
 [0.619]
 [0.619]
 [0.619]] [[39.837]
 [43.985]
 [39.837]
 [39.837]
 [39.837]
 [39.837]
 [39.837]] [[1.03 ]
 [1.254]
 [1.03 ]
 [1.03 ]
 [1.03 ]
 [1.03 ]
 [1.03 ]]
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]] [[38.26]
 [38.26]
 [38.26]
 [38.26]
 [38.26]
 [38.26]
 [38.26]] [[1.209]
 [1.209]
 [1.209]
 [1.209]
 [1.209]
 [1.209]
 [1.209]]
maxi score, test score, baseline:  0.11609999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.11609999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  35.29754477657479
actor:  1 policy actor:  1  step number:  47 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.044]
 [0.065]
 [0.047]
 [0.029]
 [0.029]
 [0.037]
 [0.047]] [[26.63 ]
 [31.065]
 [28.434]
 [21.396]
 [20.449]
 [19.964]
 [28.876]] [[0.437]
 [0.565]
 [0.484]
 [0.296]
 [0.273]
 [0.27 ]
 [0.495]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.545]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.571]
 [0.545]
 [0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]]
Printing some Q and Qe and total Qs values:  [[0.831]
 [0.889]
 [0.845]
 [0.834]
 [0.826]
 [0.835]
 [0.837]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.831]
 [0.889]
 [0.845]
 [0.834]
 [0.826]
 [0.835]
 [0.837]]
maxi score, test score, baseline:  0.11609999999999987 0.6933333333333334 0.6933333333333334
maxi score, test score, baseline:  0.11609999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
maxi score, test score, baseline:  0.11609999999999987 0.6933333333333334 0.6933333333333334
probs:  [0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.14414657874824321, 0.27926710625878404]
printing an ep nov before normalisation:  49.54830305973337
printing an ep nov before normalisation:  40.95856189727783
Printing some Q and Qe and total Qs values:  [[-0.088]
 [-0.091]
 [-0.091]
 [-0.087]
 [-0.087]
 [-0.089]
 [-0.086]] [[15.11 ]
 [18.861]
 [15.756]
 [15.748]
 [15.735]
 [15.533]
 [15.294]] [[0.918]
 [1.407]
 [0.999]
 [1.003]
 [1.001]
 [0.972]
 [0.944]]
printing an ep nov before normalisation:  36.2996724746387
printing an ep nov before normalisation:  38.21094186733797
printing an ep nov before normalisation:  32.92460918426514
using another actor
printing an ep nov before normalisation:  28.969041645704007
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  8.303650247398764e-05
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  0.0002642501974037259
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  34.96495157779184
printing an ep nov before normalisation:  32.870059811575345
actor:  1 policy actor:  1  step number:  67 total reward:  0.0399999999999997  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  0
printing an ep nov before normalisation:  45.339270962582354
actor:  0 policy actor:  1  step number:  40 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.12519333333333318 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.12519333333333318 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actor:  0 policy actor:  1  step number:  48 total reward:  0.3666666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.12476666666666657 0.6983333333333335 0.6983333333333335
printing an ep nov before normalisation:  37.16461885484567
maxi score, test score, baseline:  0.12476666666666657 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.12476666666666657 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.12476666666666657 0.6983333333333335 0.6983333333333335
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  55 total reward:  0.10666666666666613  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  25.212637917827536
actor:  0 policy actor:  0  step number:  47 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12340666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actor:  1 policy actor:  1  step number:  66 total reward:  0.09999999999999953  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]] [[47.741]
 [47.741]
 [47.741]
 [47.741]
 [47.741]
 [47.741]
 [47.741]] [[0.982]
 [0.982]
 [0.982]
 [0.982]
 [0.982]
 [0.982]
 [0.982]]
printing an ep nov before normalisation:  35.556130939059784
maxi score, test score, baseline:  0.12340666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actions average: 
K:  4  action  0 :  tensor([0.3376, 0.0467, 0.1472, 0.1007, 0.1300, 0.1132, 0.1246],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0186, 0.9039, 0.0178, 0.0099, 0.0123, 0.0094, 0.0281],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1050, 0.0195, 0.5521, 0.0478, 0.0971, 0.1362, 0.0424],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.3237, 0.0170, 0.1334, 0.1201, 0.1292, 0.1466, 0.1301],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1094, 0.0245, 0.0841, 0.0806, 0.5373, 0.1192, 0.0449],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2132, 0.0181, 0.0924, 0.1276, 0.1234, 0.3074, 0.1179],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0631, 0.5023, 0.0615, 0.1200, 0.0466, 0.0391, 0.1672],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.489]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]] [[25.559]
 [27.818]
 [25.559]
 [25.559]
 [25.559]
 [25.559]
 [25.559]] [[0.635]
 [0.764]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]]
printing an ep nov before normalisation:  33.19006908772914
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.626]
 [0.577]
 [0.573]
 [0.568]
 [0.564]
 [0.566]] [[15.354]
 [13.327]
 [13.277]
 [15.903]
 [13.434]
 [13.353]
 [13.253]] [[1.146]
 [1.118]
 [1.067]
 [1.16 ]
 [1.065]
 [1.058]
 [1.056]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  59 total reward:  0.18666666666666598  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  48 total reward:  0.4066666666666666  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  50.55206546161184
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.114]
 [0.114]
 [0.114]
 [0.143]
 [0.114]
 [0.114]] [[40.349]
 [33.939]
 [33.939]
 [33.939]
 [31.581]
 [33.939]
 [33.939]] [[0.933]
 [0.569]
 [0.569]
 [0.569]
 [0.537]
 [0.569]
 [0.569]]
maxi score, test score, baseline:  0.12045999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12045999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.12045999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.12045999999999987 0.6983333333333335 0.6983333333333335
actions average: 
K:  1  action  0 :  tensor([0.5554, 0.0032, 0.0705, 0.0751, 0.1028, 0.1004, 0.0926],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0050, 0.9344, 0.0200, 0.0158, 0.0028, 0.0067, 0.0153],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1362, 0.0210, 0.3403, 0.0834, 0.1095, 0.1640, 0.1457],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1839, 0.1548, 0.0890, 0.2578, 0.1035, 0.0998, 0.1111],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1857, 0.0134, 0.0907, 0.0733, 0.4157, 0.1111, 0.1102],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0403, 0.0106, 0.1373, 0.0395, 0.0893, 0.6437, 0.0393],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1471, 0.1115, 0.1256, 0.0881, 0.1254, 0.1108, 0.2914],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12045999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actions average: 
K:  0  action  0 :  tensor([0.5161, 0.0152, 0.1028, 0.0681, 0.1084, 0.0853, 0.1041],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0032,     0.9815,     0.0028,     0.0040,     0.0004,     0.0004,
            0.0077], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1247, 0.0099, 0.4869, 0.0624, 0.0855, 0.0866, 0.1439],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1443, 0.0151, 0.1580, 0.2283, 0.1443, 0.1893, 0.1206],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1716, 0.0035, 0.0819, 0.0636, 0.4876, 0.0836, 0.1083],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1169, 0.0474, 0.1942, 0.0607, 0.0842, 0.3596, 0.1369],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1891, 0.3312, 0.0915, 0.0581, 0.0718, 0.0751, 0.1833],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  51 total reward:  0.21333333333333304  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.067]
 [0.01 ]
 [0.01 ]
 [0.01 ]
 [0.01 ]
 [0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.01 ]
 [0.067]
 [0.01 ]
 [0.01 ]
 [0.01 ]
 [0.01 ]
 [0.01 ]]
printing an ep nov before normalisation:  31.40797122118865
using another actor
printing an ep nov before normalisation:  28.212559549236506
Printing some Q and Qe and total Qs values:  [[0.051]
 [0.105]
 [0.293]
 [0.293]
 [0.069]
 [0.07 ]
 [0.293]] [[35.51 ]
 [38.365]
 [39.004]
 [39.004]
 [34.791]
 [35.069]
 [39.004]] [[0.628]
 [0.772]
 [0.98 ]
 [0.98 ]
 [0.624]
 [0.634]
 [0.98 ]]
printing an ep nov before normalisation:  25.35670280456543
maxi score, test score, baseline:  0.12045999999999987 0.6983333333333335 0.6983333333333335
printing an ep nov before normalisation:  47.31058724962961
maxi score, test score, baseline:  0.12045999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.12045999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actor:  1 policy actor:  1  step number:  34 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.806]
 [0.762]
 [0.762]
 [0.762]
 [0.762]
 [0.762]] [[36.51]
 [34.56]
 [36.51]
 [36.51]
 [36.51]
 [36.51]
 [36.51]] [[0.762]
 [0.806]
 [0.762]
 [0.762]
 [0.762]
 [0.762]
 [0.762]]
printing an ep nov before normalisation:  30.763284686748477
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12045999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  26.73598428911287
maxi score, test score, baseline:  0.12045999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
from probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.12045999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  38.12700480054334
printing an ep nov before normalisation:  27.148768636915417
printing an ep nov before normalisation:  48.64611241410551
actor:  0 policy actor:  1  step number:  61 total reward:  0.27999999999999914  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12041999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  31.348413611220035
printing an ep nov before normalisation:  28.865831124702922
from probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11771333333333321 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.11771333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11561999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  15.20263671875
maxi score, test score, baseline:  0.11561999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actor:  0 policy actor:  1  step number:  59 total reward:  0.23999999999999977  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  47 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.319]
 [0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.048]
 [0.319]
 [0.048]
 [0.048]
 [0.048]
 [0.048]
 [0.048]]
printing an ep nov before normalisation:  30.921290459607803
maxi score, test score, baseline:  0.11493999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
line 256 mcts: sample exp_bonus 39.47103810683671
printing an ep nov before normalisation:  49.593888436288516
maxi score, test score, baseline:  0.11493999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  37.754250627079244
maxi score, test score, baseline:  0.11493999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  10.390016751862277
maxi score, test score, baseline:  0.11493999999999989 0.6983333333333335 0.6983333333333335
printing an ep nov before normalisation:  33.30281572504797
maxi score, test score, baseline:  0.11493999999999989 0.6983333333333335 0.6983333333333335
line 256 mcts: sample exp_bonus 24.054950298193166
actor:  1 policy actor:  1  step number:  54 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.545]
 [0.617]
 [0.545]
 [0.545]
 [0.545]
 [0.545]
 [0.545]] [[28.96 ]
 [33.592]
 [28.96 ]
 [28.96 ]
 [28.96 ]
 [28.96 ]
 [28.96 ]] [[1.163]
 [1.415]
 [1.163]
 [1.163]
 [1.163]
 [1.163]
 [1.163]]
siam score:  -0.7654132
maxi score, test score, baseline:  0.11493999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11493999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  19.832850138780298
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]
 [0.544]] [[35.461]
 [35.461]
 [35.461]
 [35.461]
 [35.461]
 [35.461]
 [35.461]] [[2.136]
 [2.136]
 [2.136]
 [2.136]
 [2.136]
 [2.136]
 [2.136]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.33999999999999986  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  37 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.11493999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11493999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.379]
 [0.122]
 [0.131]
 [0.125]
 [0.132]
 [0.135]] [[21.721]
 [39.519]
 [21.112]
 [21.251]
 [21.963]
 [22.159]
 [21.108]] [[0.545]
 [1.655]
 [0.519]
 [0.534]
 [0.563]
 [0.579]
 [0.531]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.40666666666666673  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.11493999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11493999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11493999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11493999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actions average: 
K:  0  action  0 :  tensor([0.4146, 0.0116, 0.0964, 0.1021, 0.1679, 0.1082, 0.0993],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0091, 0.9588, 0.0024, 0.0080, 0.0019, 0.0013, 0.0185],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1230, 0.0036, 0.4745, 0.0821, 0.0954, 0.1107, 0.1108],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2067, 0.0424, 0.0791, 0.2254, 0.0803, 0.0736, 0.2925],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2306, 0.0169, 0.1203, 0.1265, 0.2615, 0.1179, 0.1263],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1403, 0.0115, 0.1803, 0.0678, 0.0822, 0.4175, 0.1004],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1897, 0.0995, 0.0934, 0.1131, 0.1049, 0.1017, 0.2977],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.11493999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11493999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actor:  1 policy actor:  1  step number:  60 total reward:  0.38000000000000034  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.841]] [[12.856]
 [12.856]
 [12.856]
 [12.856]
 [12.856]
 [12.856]
 [12.405]] [[1.345]
 [1.345]
 [1.345]
 [1.345]
 [1.345]
 [1.345]
 [1.314]]
maxi score, test score, baseline:  0.11493999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actions average: 
K:  2  action  0 :  tensor([0.4183, 0.0477, 0.0890, 0.1309, 0.1185, 0.1149, 0.0807],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0148, 0.9249, 0.0056, 0.0135, 0.0049, 0.0062, 0.0302],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0494, 0.0256, 0.6212, 0.0285, 0.0154, 0.2213, 0.0385],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1231, 0.0997, 0.2723, 0.1454, 0.0877, 0.1425, 0.1294],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1595, 0.0855, 0.0795, 0.0887, 0.3449, 0.1538, 0.0881],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1267, 0.0068, 0.1011, 0.0630, 0.0636, 0.5898, 0.0490],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1622, 0.0273, 0.1171, 0.1137, 0.1347, 0.1168, 0.3282],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  16.601245403289795
actor:  1 policy actor:  1  step number:  49 total reward:  0.2666666666666665  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.11493999999999989 0.6983333333333335 0.6983333333333335
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.423]
 [0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]] [[ 0.081]
 [ 0.083]
 [23.452]
 [23.452]
 [23.452]
 [23.452]
 [23.452]] [[0.423]
 [0.423]
 [0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]]
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]] [[0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.055]] [[0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]]
actor:  0 policy actor:  0  step number:  57 total reward:  0.1599999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11725999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  28.736765384674072
maxi score, test score, baseline:  0.11725999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11725999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actor:  1 policy actor:  1  step number:  32 total reward:  0.6066666666666668  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.11725999999999989 0.6983333333333335 0.6983333333333335
actor:  1 policy actor:  1  step number:  44 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.783091417294834
printing an ep nov before normalisation:  28.568189010522463
printing an ep nov before normalisation:  26.995057459128876
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  40.11498084927457
maxi score, test score, baseline:  0.11725999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actor:  1 policy actor:  1  step number:  43 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11725999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  30.213615255253455
actions average: 
K:  0  action  0 :  tensor([0.5806, 0.0064, 0.0765, 0.0624, 0.0968, 0.0834, 0.0939],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0054, 0.9588, 0.0036, 0.0049, 0.0026, 0.0026, 0.0221],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0766, 0.0076, 0.7090, 0.0307, 0.0394, 0.0773, 0.0593],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1854, 0.0274, 0.0921, 0.3248, 0.1097, 0.1180, 0.1426],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2012, 0.0020, 0.0783, 0.0492, 0.4916, 0.0830, 0.0947],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.2000, 0.0163, 0.1523, 0.0742, 0.1037, 0.2905, 0.1631],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1964, 0.1228, 0.0845, 0.1360, 0.1196, 0.0773, 0.2634],
       grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 31.71679675506705
actor:  0 policy actor:  0  step number:  55 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.16489399044223
actor:  1 policy actor:  1  step number:  51 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.19376573410514
maxi score, test score, baseline:  0.11707333333333324 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11707333333333324 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11707333333333324 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11707333333333324 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actions average: 
K:  1  action  0 :  tensor([0.4595, 0.0400, 0.0746, 0.1010, 0.1385, 0.1069, 0.0796],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0035, 0.9328, 0.0142, 0.0266, 0.0017, 0.0025, 0.0187],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0790, 0.0540, 0.6702, 0.0408, 0.0462, 0.0696, 0.0402],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1097, 0.3850, 0.0530, 0.2326, 0.0662, 0.0480, 0.1055],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2500, 0.0412, 0.1278, 0.1072, 0.2691, 0.0917, 0.1128],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1685, 0.0005, 0.1714, 0.1199, 0.1121, 0.3400, 0.0876],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2238, 0.0629, 0.0834, 0.1627, 0.1192, 0.0791, 0.2689],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  1  step number:  40 total reward:  0.5400000000000001  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  41.81679936686292
Printing some Q and Qe and total Qs values:  [[0.47]
 [0.39]
 [0.39]
 [0.39]
 [0.31]
 [0.39]
 [0.39]] [[38.036]
 [55.408]
 [55.408]
 [55.408]
 [41.242]
 [55.408]
 [55.408]] [[0.837]
 [1.057]
 [1.057]
 [1.057]
 [0.733]
 [1.057]
 [1.057]]
printing an ep nov before normalisation:  28.055913132448612
maxi score, test score, baseline:  0.12015333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5933333333333335  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.15333333333333288  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12015333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  36.09003926540949
printing an ep nov before normalisation:  41.9907805176119
maxi score, test score, baseline:  0.12015333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.12015333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  28.898069442181885
maxi score, test score, baseline:  0.12015333333333322 0.6983333333333335 0.6983333333333335
actor:  1 policy actor:  1  step number:  40 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  1.0
siam score:  -0.7722142
printing an ep nov before normalisation:  0.0
Printing some Q and Qe and total Qs values:  [[0.872]
 [0.942]
 [0.872]
 [0.872]
 [0.872]
 [0.872]
 [0.872]] [[47.4  ]
 [49.768]
 [47.4  ]
 [47.4  ]
 [47.4  ]
 [47.4  ]
 [47.4  ]] [[0.872]
 [0.942]
 [0.872]
 [0.872]
 [0.872]
 [0.872]
 [0.872]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.18666666666666643  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 35.70510846324049
maxi score, test score, baseline:  0.12015333333333322 0.6983333333333335 0.6983333333333335
actor:  0 policy actor:  0  step number:  37 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1202999999999999 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.1202999999999999 0.6983333333333335 0.6983333333333335
printing an ep nov before normalisation:  54.18500690701116
siam score:  -0.7748234
actor:  1 policy actor:  1  step number:  36 total reward:  0.6066666666666669  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.422]
 [0.432]] [[ 0.  ]
 [ 0.  ]
 [ 0.  ]
 [ 0.  ]
 [ 0.  ]
 [32.64]
 [ 0.  ]] [[0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.422]
 [0.432]]
maxi score, test score, baseline:  0.1202999999999999 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.1202999999999999 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.1202999999999999 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  33.84285694660611
actor:  1 policy actor:  1  step number:  36 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  41 total reward:  0.5866666666666669  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1175533333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.290306448936462
printing an ep nov before normalisation:  42.630488874604254
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  49.92864600839347
actor:  1 policy actor:  1  step number:  44 total reward:  0.526666666666667  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.073272673785496
printing an ep nov before normalisation:  15.636716568242797
using explorer policy with actor:  1
printing an ep nov before normalisation:  14.125638906818969
actor:  1 policy actor:  1  step number:  61 total reward:  0.06666666666666621  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]] [[19.967]
 [17.164]
 [17.164]
 [17.164]
 [17.164]
 [17.164]
 [17.164]] [[1.175]
 [0.983]
 [0.983]
 [0.983]
 [0.983]
 [0.983]
 [0.983]]
printing an ep nov before normalisation:  30.13710178985084
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  0.1175533333333332 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.1175533333333332 0.6983333333333335 0.6983333333333335
printing an ep nov before normalisation:  43.65407401941393
printing an ep nov before normalisation:  46.00500257578286
printing an ep nov before normalisation:  29.959371707379038
printing an ep nov before normalisation:  47.30747868588308
actor:  1 policy actor:  1  step number:  46 total reward:  0.40666666666666673  reward:  1.0 rdn_beta:  1.0
using another actor
maxi score, test score, baseline:  0.1175533333333332 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.1175533333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  40.83569811678717
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  40 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.15159644164392
printing an ep nov before normalisation:  35.85023874916418
maxi score, test score, baseline:  0.1175533333333332 0.6983333333333335 0.6983333333333335
actor:  1 policy actor:  1  step number:  43 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.11489999999999989 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.11489999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
Printing some Q and Qe and total Qs values:  [[ 0.218]
 [ 0.024]
 [-0.088]
 [ 0.024]
 [ 0.024]
 [ 0.024]
 [ 0.024]] [[36.655]
 [38.66 ]
 [45.022]
 [38.66 ]
 [38.66 ]
 [38.66 ]
 [38.66 ]] [[1.399]
 [1.343]
 [1.667]
 [1.343]
 [1.343]
 [1.343]
 [1.343]]
Printing some Q and Qe and total Qs values:  [[ 0.289]
 [-0.085]
 [-0.1  ]
 [ 0.138]
 [ 0.22 ]
 [-0.066]
 [ 0.024]] [[36.708]
 [40.468]
 [48.302]
 [34.762]
 [37.05 ]
 [35.928]
 [38.66 ]] [[1.008]
 [0.791]
 [1.102]
 [0.777]
 [0.954]
 [0.621]
 [0.825]]
siam score:  -0.77047944
maxi score, test score, baseline:  0.11489999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  34.17158434937172
maxi score, test score, baseline:  0.11489999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  44.85558664824465
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  46.93542692678899
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.355]
 [0.382]
 [0.382]
 [0.254]
 [0.382]
 [0.385]] [[49.026]
 [47.654]
 [48.794]
 [48.794]
 [42.502]
 [48.794]
 [46.591]] [[0.785]
 [0.84 ]
 [0.884]
 [0.884]
 [0.664]
 [0.884]
 [0.854]]
printing an ep nov before normalisation:  0.030070607659666848
actor:  1 policy actor:  1  step number:  41 total reward:  0.6000000000000003  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  22.43201971054077
maxi score, test score, baseline:  0.11489999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  42.52051353871951
printing an ep nov before normalisation:  40.61505063026171
printing an ep nov before normalisation:  40.790432393590315
maxi score, test score, baseline:  0.11489999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actor:  0 policy actor:  1  step number:  50 total reward:  0.5400000000000004  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.809]
 [0.552]
 [0.563]
 [0.56 ]
 [0.557]
 [0.552]] [[36.556]
 [37.511]
 [36.556]
 [39.581]
 [41.351]
 [40.724]
 [36.556]] [[0.552]
 [0.809]
 [0.552]
 [0.563]
 [0.56 ]
 [0.557]
 [0.552]]
maxi score, test score, baseline:  0.11797999999999989 0.6983333333333335 0.6983333333333335
actor:  1 policy actor:  1  step number:  44 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  0  action  0 :  tensor([0.4019, 0.0103, 0.1145, 0.1177, 0.1315, 0.1157, 0.1085],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0032, 0.9739, 0.0023, 0.0059, 0.0020, 0.0018, 0.0109],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0645, 0.0037, 0.7178, 0.0438, 0.0520, 0.0698, 0.0484],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1951, 0.0240, 0.0960, 0.3725, 0.0958, 0.0928, 0.1238],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1924, 0.0169, 0.1155, 0.1493, 0.2838, 0.1252, 0.1169],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1582, 0.0054, 0.1665, 0.0909, 0.1128, 0.3935, 0.0728],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1274, 0.1425, 0.1156, 0.0688, 0.0607, 0.0389, 0.4462],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  37.26974307455945
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  36 total reward:  0.54  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1210599999999999 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]] [[36.949]
 [36.949]
 [36.949]
 [36.949]
 [36.949]
 [36.949]
 [36.949]] [[1.933]
 [1.933]
 [1.933]
 [1.933]
 [1.933]
 [1.933]
 [1.933]]
maxi score, test score, baseline:  0.1210599999999999 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  38.84878767354657
maxi score, test score, baseline:  0.1210599999999999 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  42.36181787690581
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  39 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  34.143426939497104
printing an ep nov before normalisation:  28.228290289987942
maxi score, test score, baseline:  0.1210599999999999 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
using explorer policy with actor:  0
printing an ep nov before normalisation:  0.005111835175739543
actor:  0 policy actor:  1  step number:  31 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  43.79941881140216
actions average: 
K:  4  action  0 :  tensor([0.3380, 0.0247, 0.0962, 0.1007, 0.2074, 0.1068, 0.1263],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0364, 0.9131, 0.0047, 0.0103, 0.0080, 0.0042, 0.0233],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1935, 0.0359, 0.2544, 0.1004, 0.1469, 0.1354, 0.1336],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2127, 0.0048, 0.1259, 0.1928, 0.1444, 0.1113, 0.2082],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2164, 0.0407, 0.0934, 0.1264, 0.2589, 0.0932, 0.1710],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0999, 0.0857, 0.2225, 0.1016, 0.0997, 0.3334, 0.0573],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1607, 0.0243, 0.1364, 0.1244, 0.1681, 0.1354, 0.2506],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  48 total reward:  0.3533333333333333  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.12140666666666657 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.12140666666666657 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
Printing some Q and Qe and total Qs values:  [[ 0.17 ]
 [ 0.377]
 [-0.074]
 [ 0.136]
 [ 0.158]
 [-0.057]
 [ 0.202]] [[31.558]
 [35.528]
 [29.595]
 [28.192]
 [30.21 ]
 [28.947]
 [31.665]] [[0.627]
 [0.949]
 [0.325]
 [0.494]
 [0.574]
 [0.323]
 [0.661]]
maxi score, test score, baseline:  0.12140666666666657 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.12140666666666657 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  38.98864709444956
maxi score, test score, baseline:  0.12140666666666657 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.12140666666666657 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.12140666666666657 0.6983333333333335 0.6983333333333335
actor:  1 policy actor:  1  step number:  47 total reward:  0.2933333333333331  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  26.48685227350472
maxi score, test score, baseline:  0.12140666666666657 0.6983333333333335 0.6983333333333335
actor:  0 policy actor:  0  step number:  46 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.12169999999999988 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  0.0
printing an ep nov before normalisation:  47.6725107040274
printing an ep nov before normalisation:  42.16761332487843
maxi score, test score, baseline:  0.11624666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11624666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  53.66808340361678
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.61 ]
 [0.561]
 [0.561]
 [0.561]
 [0.561]
 [0.561]] [[23.176]
 [37.811]
 [23.176]
 [23.176]
 [23.176]
 [23.176]
 [23.176]] [[1.014]
 [1.542]
 [1.014]
 [1.014]
 [1.014]
 [1.014]
 [1.014]]
actor:  1 policy actor:  1  step number:  41 total reward:  0.4266666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.11624666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11624666666666655 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.11624666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  56.29749104962665
maxi score, test score, baseline:  0.11624666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  61.746800838640006
maxi score, test score, baseline:  0.11624666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]] [[33.006]
 [33.006]
 [33.006]
 [33.006]
 [33.006]
 [33.006]
 [33.006]] [[0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]
 [0.923]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.33333333333333326  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.11624666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11624666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actor:  1 policy actor:  1  step number:  42 total reward:  0.4066666666666666  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11624666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  42.13596947148556
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.55 ]
 [0.466]
 [0.466]
 [0.466]
 [0.466]
 [0.466]] [[39.022]
 [42.567]
 [39.022]
 [39.022]
 [39.022]
 [39.022]
 [39.022]] [[1.201]
 [1.418]
 [1.201]
 [1.201]
 [1.201]
 [1.201]
 [1.201]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.13333333333333297  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  56.39251630852564
maxi score, test score, baseline:  0.11624666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actor:  1 policy actor:  1  step number:  45 total reward:  0.52  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.11624666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actor:  1 policy actor:  1  step number:  31 total reward:  0.6666666666666669  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11624666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actions average: 
K:  4  action  0 :  tensor([0.4546, 0.0479, 0.0779, 0.1218, 0.1024, 0.0911, 0.1043],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0074, 0.9583, 0.0047, 0.0099, 0.0033, 0.0025, 0.0139],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1062, 0.0006, 0.3866, 0.1012, 0.1362, 0.1213, 0.1479],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1629, 0.0061, 0.1241, 0.1593, 0.1700, 0.1831, 0.1946],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2224, 0.0271, 0.1083, 0.0885, 0.3580, 0.0964, 0.0994],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0918, 0.0484, 0.1342, 0.0666, 0.0890, 0.4888, 0.0811],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0814, 0.3044, 0.1212, 0.0836, 0.0830, 0.0926, 0.2338],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  28.16789150238037
maxi score, test score, baseline:  0.11624666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  31.74345802247362
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11343333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11343333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actor:  1 policy actor:  1  step number:  52 total reward:  0.32666666666666666  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  35 total reward:  0.52  reward:  1.0 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  0.11343333333333322 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.11343333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  39.01547790036496
maxi score, test score, baseline:  0.11343333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.7  ]
 [0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]] [[38.88 ]
 [36.239]
 [38.88 ]
 [38.88 ]
 [38.88 ]
 [38.88 ]
 [38.88 ]] [[0.663]
 [0.7  ]
 [0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]]
Printing some Q and Qe and total Qs values:  [[0.303]
 [0.438]
 [0.303]
 [0.303]
 [0.303]
 [0.303]
 [0.303]] [[30.569]
 [29.887]
 [30.569]
 [30.569]
 [30.569]
 [30.569]
 [30.569]] [[0.803]
 [0.919]
 [0.803]
 [0.803]
 [0.803]
 [0.803]
 [0.803]]
printing an ep nov before normalisation:  25.184210229646705
maxi score, test score, baseline:  0.11343333333333322 0.6983333333333335 0.6983333333333335
line 256 mcts: sample exp_bonus 35.82606979128219
printing an ep nov before normalisation:  32.72582863393873
printing an ep nov before normalisation:  31.11833834500633
maxi score, test score, baseline:  0.11343333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11343333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  37.05698361839713
actor:  0 policy actor:  0  step number:  56 total reward:  0.19333333333333258  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.11320666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.457]
 [0.348]
 [0.343]
 [0.358]
 [0.348]
 [0.302]] [[38.241]
 [36.081]
 [33.36 ]
 [35.881]
 [36.294]
 [33.36 ]
 [40.189]] [[0.85 ]
 [0.906]
 [0.734]
 [0.787]
 [0.812]
 [0.734]
 [0.844]]
printing an ep nov before normalisation:  40.92710421579267
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.505]
 [0.397]
 [0.397]
 [0.397]
 [0.397]
 [0.397]] [[30.598]
 [32.749]
 [30.598]
 [30.598]
 [30.598]
 [30.598]
 [30.598]] [[0.772]
 [0.934]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]]
Printing some Q and Qe and total Qs values:  [[ 0.277]
 [-0.071]
 [ 0.117]
 [ 0.026]
 [ 0.283]
 [ 0.117]
 [ 0.038]] [[39.984]
 [41.012]
 [37.954]
 [35.893]
 [39.798]
 [37.954]
 [46.036]] [[0.506]
 [0.168]
 [0.325]
 [0.213]
 [0.51 ]
 [0.325]
 [0.328]]
actor:  0 policy actor:  1  step number:  58 total reward:  0.326666666666666  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.5 ]
 [0.57]
 [0.5 ]
 [0.5 ]
 [0.5 ]
 [0.5 ]
 [0.5 ]] [[36.022]
 [38.477]
 [36.022]
 [36.022]
 [36.022]
 [36.022]
 [36.022]] [[0.712]
 [0.811]
 [0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.712]]
maxi score, test score, baseline:  0.11307333333333322 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.11307333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11307333333333322 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.11307333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11307333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  40.086458320333804
actor:  1 policy actor:  1  step number:  51 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  1.667
using another actor
maxi score, test score, baseline:  0.11307333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.411]
 [0.311]
 [0.42 ]
 [0.308]
 [0.312]
 [0.311]] [[31.973]
 [33.623]
 [29.386]
 [33.023]
 [29.545]
 [29.051]
 [28.992]] [[0.95 ]
 [1.074]
 [0.826]
 [1.062]
 [0.829]
 [0.816]
 [0.812]]
maxi score, test score, baseline:  0.11307333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  20.87125100555074
Printing some Q and Qe and total Qs values:  [[ 0.557]
 [ 0.594]
 [ 0.557]
 [ 0.557]
 [ 0.431]
 [-0.043]
 [ 0.449]] [[43.452]
 [44.009]
 [43.452]
 [43.452]
 [39.964]
 [43.028]
 [34.246]] [[1.247]
 [1.299]
 [1.247]
 [1.247]
 [1.027]
 [0.636]
 [0.89 ]]
printing an ep nov before normalisation:  16.84013900335609
printing an ep nov before normalisation:  30.384035512922782
printing an ep nov before normalisation:  29.465115771177167
actor:  0 policy actor:  0  step number:  50 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.11283333333333323 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
from probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11283333333333323 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actor:  0 policy actor:  1  step number:  50 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  23.859185381026126
maxi score, test score, baseline:  0.11537999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  37.76345700400844
printing an ep nov before normalisation:  40.97453284510732
maxi score, test score, baseline:  0.11537999999999989 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.11537999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  40.62385321487058
printing an ep nov before normalisation:  27.396693229675293
maxi score, test score, baseline:  0.11537999999999989 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.11537999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  29.251767832788037
maxi score, test score, baseline:  0.11271333333333323 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actor:  1 policy actor:  1  step number:  49 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  75 total reward:  0.07999999999999907  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11271333333333323 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.551]
 [0.572]] [[32.596]
 [32.596]
 [32.596]
 [32.596]
 [32.596]
 [34.594]
 [32.596]] [[1.763]
 [1.763]
 [1.763]
 [1.763]
 [1.763]
 [1.884]
 [1.763]]
maxi score, test score, baseline:  0.11271333333333323 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  37.623564744160134
printing an ep nov before normalisation:  47.940478416837486
maxi score, test score, baseline:  0.11271333333333323 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  19.721708297729492
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.42666666666666675  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  56 total reward:  0.2733333333333333  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  34.87362370999717
actions average: 
K:  0  action  0 :  tensor([0.6291, 0.0145, 0.0425, 0.0606, 0.1346, 0.0534, 0.0654],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0058, 0.9569, 0.0054, 0.0055, 0.0030, 0.0031, 0.0203],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1494, 0.0785, 0.2422, 0.1065, 0.1450, 0.1142, 0.1642],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1419, 0.0066, 0.1223, 0.2877, 0.1679, 0.1165, 0.1571],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.2102,     0.0002,     0.0537,     0.0899,     0.4719,     0.0715,
            0.1025], grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1339, 0.0027, 0.1082, 0.0951, 0.1326, 0.4381, 0.0894],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0843, 0.0695, 0.2058, 0.0806, 0.1068, 0.0911, 0.3620],
       grad_fn=<DivBackward0>)
from probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  36.239612130486265
maxi score, test score, baseline:  0.11271333333333323 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11271333333333323 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.11271333333333323 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  45.93223816671374
printing an ep nov before normalisation:  54.382500063494405
maxi score, test score, baseline:  0.11271333333333323 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.648]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]] [[24.481]
 [30.041]
 [24.481]
 [24.481]
 [24.481]
 [24.481]
 [24.481]] [[1.014]
 [1.245]
 [1.014]
 [1.014]
 [1.014]
 [1.014]
 [1.014]]
actor:  0 policy actor:  1  step number:  57 total reward:  0.21333333333333304  reward:  1.0 rdn_beta:  1.0
using another actor
from probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  0.09987531703643526
actor:  0 policy actor:  0  step number:  43 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  59 total reward:  0.23999999999999988  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[ 0.074]
 [ 0.259]
 [-0.001]
 [ 0.074]
 [ 0.074]
 [ 0.023]
 [ 0.121]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.074]
 [ 0.259]
 [-0.001]
 [ 0.074]
 [ 0.074]
 [ 0.023]
 [ 0.121]]
maxi score, test score, baseline:  0.11528666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  32.44236469268799
printing an ep nov before normalisation:  40.0747481526113
maxi score, test score, baseline:  0.11528666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  34.737512723287246
actor:  1 policy actor:  1  step number:  32 total reward:  0.5666666666666668  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7686428
maxi score, test score, baseline:  0.11528666666666655 0.6983333333333335 0.6983333333333335
actor:  0 policy actor:  1  step number:  36 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  22.44654475858889
Printing some Q and Qe and total Qs values:  [[0.987]
 [0.987]
 [1.006]
 [0.98 ]
 [0.987]
 [0.975]
 [0.987]] [[25.745]
 [25.745]
 [22.03 ]
 [25.26 ]
 [25.745]
 [24.024]
 [25.745]] [[0.987]
 [0.987]
 [1.006]
 [0.98 ]
 [0.987]
 [0.975]
 [0.987]]
Printing some Q and Qe and total Qs values:  [[0.949]
 [0.965]
 [0.949]
 [0.949]
 [0.949]
 [0.949]
 [0.896]] [[38.755]
 [38.326]
 [38.755]
 [38.755]
 [38.755]
 [38.755]
 [36.747]] [[0.949]
 [0.965]
 [0.949]
 [0.949]
 [0.949]
 [0.949]
 [0.896]]
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.747]
 [0.693]
 [0.717]
 [0.713]
 [0.715]
 [0.717]] [[19.917]
 [20.44 ]
 [18.371]
 [19.044]
 [18.881]
 [18.997]
 [18.665]] [[0.721]
 [0.747]
 [0.693]
 [0.717]
 [0.713]
 [0.715]
 [0.717]]
actor:  0 policy actor:  1  step number:  68 total reward:  0.03333333333333255  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.11812666666666656 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  45.6691151032116
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.525]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]] [[26.701]
 [27.585]
 [25.942]
 [25.942]
 [25.942]
 [25.942]
 [25.942]] [[1.321]
 [1.39 ]
 [1.267]
 [1.267]
 [1.267]
 [1.267]
 [1.267]]
actor:  0 policy actor:  0  step number:  46 total reward:  0.2999999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.11825999999999988 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actions average: 
K:  1  action  0 :  tensor([0.4947, 0.0022, 0.0976, 0.0871, 0.1015, 0.0822, 0.1347],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0164, 0.9186, 0.0046, 0.0084, 0.0071, 0.0059, 0.0391],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0931, 0.0045, 0.5817, 0.0605, 0.0813, 0.0993, 0.0796],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1401, 0.1835, 0.0857, 0.2741, 0.0866, 0.0718, 0.1583],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2226, 0.0084, 0.0713, 0.0827, 0.4354, 0.0911, 0.0886],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1280, 0.0011, 0.0832, 0.0450, 0.1041, 0.5752, 0.0634],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2360, 0.0112, 0.0474, 0.0543, 0.1404, 0.0632, 0.4475],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  25.312282216258605
printing an ep nov before normalisation:  38.90809846493621
maxi score, test score, baseline:  0.11825999999999988 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
siam score:  -0.77057517
printing an ep nov before normalisation:  36.05509281158447
actor:  0 policy actor:  0  step number:  42 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.12101999999999988 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
using explorer policy with actor:  1
siam score:  -0.76800543
actions average: 
K:  0  action  0 :  tensor([0.4165, 0.0159, 0.1020, 0.1094, 0.1334, 0.1030, 0.1200],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0021, 0.9578, 0.0021, 0.0111, 0.0019, 0.0013, 0.0238],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0918, 0.0490, 0.4734, 0.0735, 0.0799, 0.1550, 0.0776],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2232, 0.0069, 0.0923, 0.3069, 0.1191, 0.1081, 0.1435],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1702, 0.0152, 0.0811, 0.0836, 0.4831, 0.0884, 0.0783],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1328, 0.0210, 0.1281, 0.0630, 0.0824, 0.4971, 0.0756],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0995, 0.2965, 0.0841, 0.0881, 0.1066, 0.0769, 0.2482],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  0.4466666666666669  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  0  action  0 :  tensor([0.4428, 0.0386, 0.0809, 0.0763, 0.1813, 0.0890, 0.0911],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0011,     0.9787,     0.0009,     0.0020,     0.0006,     0.0005,
            0.0162], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1087, 0.0059, 0.5384, 0.0794, 0.0766, 0.1096, 0.0815],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1342, 0.0333, 0.0848, 0.3357, 0.1118, 0.0950, 0.2051],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1635, 0.0083, 0.0546, 0.0946, 0.5140, 0.0825, 0.0826],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1564, 0.0013, 0.1008, 0.0843, 0.1136, 0.4469, 0.0967],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1603, 0.1738, 0.0839, 0.1436, 0.1008, 0.0893, 0.2483],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  0.003517464617175392
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.267]
 [0.106]
 [0.267]
 [0.335]
 [0.31 ]
 [0.267]] [[30.093]
 [32.524]
 [32.402]
 [32.524]
 [28.811]
 [32.319]
 [32.524]] [[1.865]
 [1.989]
 [1.817]
 [1.989]
 [1.714]
 [2.013]
 [1.989]]
Printing some Q and Qe and total Qs values:  [[0.334]
 [0.249]
 [0.295]
 [0.298]
 [0.321]
 [0.295]
 [0.286]] [[27.587]
 [30.314]
 [28.5  ]
 [28.213]
 [28.38 ]
 [28.5  ]
 [28.62 ]] [[1.577]
 [1.739]
 [1.62 ]
 [1.598]
 [1.635]
 [1.62 ]
 [1.623]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.4066666666666666  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.12101999999999988 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.12101999999999988 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actions average: 
K:  0  action  0 :  tensor([0.7003, 0.0103, 0.0479, 0.0442, 0.0996, 0.0462, 0.0515],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0090, 0.9539, 0.0082, 0.0050, 0.0045, 0.0048, 0.0145],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1060, 0.0139, 0.5018, 0.0621, 0.0621, 0.1803, 0.0739],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1419, 0.0599, 0.1100, 0.2792, 0.1289, 0.1180, 0.1622],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1808, 0.0022, 0.0789, 0.1023, 0.4711, 0.0895, 0.0751],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1247, 0.0039, 0.1048, 0.0821, 0.0686, 0.5197, 0.0963],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1616, 0.0171, 0.1240, 0.1018, 0.0962, 0.1410, 0.3582],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.12101999999999988 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actor:  0 policy actor:  0  step number:  50 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12071333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  23.78546953201294
maxi score, test score, baseline:  0.12071333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
Printing some Q and Qe and total Qs values:  [[-0.001]
 [ 0.34 ]
 [ 0.239]
 [ 0.24 ]
 [ 0.169]
 [ 0.218]
 [ 0.279]] [[27.554]
 [33.177]
 [26.724]
 [23.652]
 [26.747]
 [22.739]
 [30.169]] [[0.489]
 [1.031]
 [0.699]
 [0.591]
 [0.63 ]
 [0.536]
 [0.863]]
maxi score, test score, baseline:  0.12071333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.12071333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.12071333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  55.19559308164981
actor:  0 policy actor:  1  step number:  55 total reward:  0.18666666666666631  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  34.86338780698185
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12009999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.564]
 [0.437]
 [0.421]
 [0.444]
 [0.434]
 [0.445]] [[42.636]
 [41.897]
 [43.49 ]
 [45.426]
 [44.925]
 [43.998]
 [41.797]] [[0.648]
 [0.808]
 [0.695]
 [0.697]
 [0.715]
 [0.697]
 [0.688]]
line 256 mcts: sample exp_bonus 33.09790008871275
actor:  1 policy actor:  1  step number:  43 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]]
Printing some Q and Qe and total Qs values:  [[0.31]
 [0.31]
 [0.31]
 [0.31]
 [0.31]
 [0.31]
 [0.31]] [[40.869]
 [40.869]
 [40.869]
 [40.869]
 [40.869]
 [40.869]
 [40.869]] [[0.977]
 [0.977]
 [0.977]
 [0.977]
 [0.977]
 [0.977]
 [0.977]]
printing an ep nov before normalisation:  46.943268585438126
printing an ep nov before normalisation:  37.037298414442276
printing an ep nov before normalisation:  34.34721038775812
maxi score, test score, baseline:  0.12009999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actor:  1 policy actor:  1  step number:  54 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.187]
 [0.269]
 [0.23 ]
 [0.23 ]
 [0.215]
 [0.23 ]
 [0.23 ]] [[30.613]
 [42.654]
 [31.135]
 [31.135]
 [33.022]
 [31.135]
 [31.135]] [[0.737]
 [1.248]
 [0.798]
 [0.798]
 [0.851]
 [0.798]
 [0.798]]
printing an ep nov before normalisation:  44.88001467788358
maxi score, test score, baseline:  0.12009999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.12009999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.12009999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.36 ]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]] [[30.928]
 [30.835]
 [33.107]
 [33.107]
 [33.107]
 [33.107]
 [33.107]] [[0.829]
 [0.937]
 [0.971]
 [0.971]
 [0.971]
 [0.971]
 [0.971]]
printing an ep nov before normalisation:  47.879988496464335
siam score:  -0.7661847
maxi score, test score, baseline:  0.12009999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12009999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  35.34976705317725
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.337]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.169]
 [0.337]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]]
using another actor
from probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.12009999999999987 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  24.853792168552587
siam score:  -0.7590963
actor:  1 policy actor:  1  step number:  46 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1174733333333332 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.1174733333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.1174733333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  29.587178575696353
printing an ep nov before normalisation:  21.63105754536423
maxi score, test score, baseline:  0.1174733333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  0.0006326992001959297
actor:  1 policy actor:  1  step number:  66 total reward:  0.27333333333333254  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1174733333333332 0.6983333333333335 0.6983333333333335
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]] [[23.728]
 [23.728]
 [23.728]
 [23.728]
 [23.728]
 [23.728]
 [23.728]] [[2.44]
 [2.44]
 [2.44]
 [2.44]
 [2.44]
 [2.44]
 [2.44]]
printing an ep nov before normalisation:  30.28324310405723
actor:  1 policy actor:  1  step number:  41 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1174733333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.1174733333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  44.9378370058294
maxi score, test score, baseline:  0.1174733333333332 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.1174733333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  37.79107119318192
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.699]
 [0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]] [[27.309]
 [25.146]
 [26.343]
 [26.343]
 [26.343]
 [26.343]
 [26.343]] [[1.83 ]
 [1.861]
 [1.773]
 [1.773]
 [1.773]
 [1.773]
 [1.773]]
maxi score, test score, baseline:  0.1174733333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  19.55009698867798
siam score:  -0.76170486
actions average: 
K:  3  action  0 :  tensor([0.4104, 0.0221, 0.1172, 0.0997, 0.1453, 0.0959, 0.1093],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0043, 0.9588, 0.0042, 0.0137, 0.0020, 0.0014, 0.0156],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0066, 0.1563, 0.5423, 0.0094, 0.0039, 0.2693, 0.0124],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1264, 0.1069, 0.0660, 0.4329, 0.0624, 0.0750, 0.1304],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1970, 0.0193, 0.0881, 0.0732, 0.4728, 0.0661, 0.0834],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.2111, 0.0028, 0.1309, 0.0931, 0.0787, 0.4007, 0.0828],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1572, 0.0674, 0.1089, 0.1609, 0.0899, 0.0741, 0.3416],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  65 total reward:  0.1599999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  35.29075384140015
maxi score, test score, baseline:  0.1174733333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1174733333333332 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.1174733333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1174733333333332 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.637]
 [0.545]
 [0.609]
 [0.592]
 [0.545]
 [0.547]] [[17.328]
 [22.534]
 [15.883]
 [22.148]
 [22.329]
 [16.172]
 [16.658]] [[1.32 ]
 [1.639]
 [1.251]
 [1.594]
 [1.585]
 [1.265]
 [1.288]]
printing an ep nov before normalisation:  30.729979572181946
printing an ep nov before normalisation:  36.97001761083972
actor:  1 policy actor:  1  step number:  56 total reward:  0.3933333333333334  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11195333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  36.42313912347819
printing an ep nov before normalisation:  41.723682221868316
maxi score, test score, baseline:  0.11195333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11195333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11195333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]
 [0.558]] [[35.131]
 [35.131]
 [35.131]
 [35.131]
 [35.131]
 [35.131]
 [35.131]] [[70.82]
 [70.82]
 [70.82]
 [70.82]
 [70.82]
 [70.82]
 [70.82]]
printing an ep nov before normalisation:  32.928669948324455
printing an ep nov before normalisation:  34.33896261903945
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  58 total reward:  0.4733333333333337  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  51 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 33.80019805906184
printing an ep nov before normalisation:  36.12501072326682
siam score:  -0.78224456
printing an ep nov before normalisation:  35.43553352355957
actor:  1 policy actor:  1  step number:  56 total reward:  0.3666666666666666  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.11195333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.499]
 [0.349]
 [0.349]
 [0.363]
 [0.349]
 [0.325]] [[39.562]
 [35.835]
 [44.373]
 [44.373]
 [39.015]
 [44.373]
 [38.852]] [[0.93 ]
 [0.944]
 [1.016]
 [1.016]
 [0.89 ]
 [1.016]
 [0.848]]
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.589]
 [0.567]
 [0.567]
 [0.567]
 [0.567]
 [0.567]] [[32.852]
 [38.558]
 [32.852]
 [32.852]
 [32.852]
 [32.852]
 [32.852]] [[1.138]
 [1.35 ]
 [1.138]
 [1.138]
 [1.138]
 [1.138]
 [1.138]]
printing an ep nov before normalisation:  58.35737924995045
maxi score, test score, baseline:  0.11195333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  29.691895821734647
Printing some Q and Qe and total Qs values:  [[-0.065]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.065]
 [-0.065]] [[42.689]
 [42.689]
 [42.689]
 [42.689]
 [42.689]
 [42.689]
 [42.689]] [[1.158]
 [1.158]
 [1.158]
 [1.158]
 [1.158]
 [1.158]
 [1.158]]
printing an ep nov before normalisation:  36.69683749129116
actor:  1 policy actor:  1  step number:  47 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  50 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11195333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11195333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  34.62304575044896
printing an ep nov before normalisation:  23.3233211938252
printing an ep nov before normalisation:  40.04020770251983
actor:  1 policy actor:  1  step number:  50 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.11195333333333321 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.11195333333333321 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
Printing some Q and Qe and total Qs values:  [[0.868]
 [0.922]
 [0.899]
 [0.921]
 [0.741]
 [0.868]
 [0.889]] [[25.658]
 [28.255]
 [27.479]
 [27.579]
 [26.589]
 [25.658]
 [25.608]] [[0.868]
 [0.922]
 [0.899]
 [0.921]
 [0.741]
 [0.868]
 [0.889]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  14.684449715574626
actor:  0 policy actor:  1  step number:  63 total reward:  0.1733333333333329  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.11161999999999989 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.11161999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  51.67976403409552
maxi score, test score, baseline:  0.11161999999999989 0.6983333333333335 0.6983333333333335
printing an ep nov before normalisation:  33.6931579265337
maxi score, test score, baseline:  0.11161999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.11161999999999989 0.6983333333333335 0.6983333333333335
from probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
maxi score, test score, baseline:  0.10904666666666654 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actor:  1 policy actor:  1  step number:  59 total reward:  0.13333333333333308  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  47.34663088238122
printing an ep nov before normalisation:  24.960414267618447
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.318]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]] [[32.873]
 [36.986]
 [32.873]
 [32.873]
 [32.873]
 [32.873]
 [32.873]] [[0.759]
 [0.889]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]]
maxi score, test score, baseline:  0.10904666666666654 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  33.448008455259675
maxi score, test score, baseline:  0.10695333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  30.91657935282647
actor:  1 policy actor:  1  step number:  35 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10695333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.392]
 [0.392]
 [0.392]
 [0.388]
 [0.392]
 [0.392]] [[37.598]
 [38.79 ]
 [38.79 ]
 [38.79 ]
 [41.217]
 [38.79 ]
 [38.79 ]] [[0.632]
 [0.615]
 [0.615]
 [0.615]
 [0.638]
 [0.615]
 [0.615]]
line 256 mcts: sample exp_bonus 41.83022633193963
maxi score, test score, baseline:  0.10695333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
line 256 mcts: sample exp_bonus 38.458054300135046
printing an ep nov before normalisation:  38.34170761589867
printing an ep nov before normalisation:  23.769161354659524
maxi score, test score, baseline:  0.10695333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  30.78599720417034
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.342]
 [0.322]
 [0.363]
 [0.356]
 [0.356]
 [0.357]] [[27.693]
 [27.765]
 [27.77 ]
 [27.013]
 [27.638]
 [27.558]
 [27.532]] [[0.938]
 [0.929]
 [0.908]
 [0.911]
 [0.936]
 [0.932]
 [0.932]]
maxi score, test score, baseline:  0.10695333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
printing an ep nov before normalisation:  42.15064334617001
actor:  1 policy actor:  1  step number:  64 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  21.63550531511837
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.373]
 [0.702]
 [0.734]
 [0.322]
 [0.508]
 [0.742]] [[33.431]
 [36.249]
 [31.265]
 [33.833]
 [33.622]
 [31.945]
 [29.194]] [[1.418]
 [1.363]
 [1.556]
 [1.658]
 [1.241]
 [1.381]
 [1.54 ]]
maxi score, test score, baseline:  0.10695333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.16478131419625575, 0.17609342901872135]
actor:  1 policy actor:  1  step number:  42 total reward:  0.7133333333333336  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  71 total reward:  0.17333333333333267  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  0
maxi score, test score, baseline:  0.10695333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.717]
 [0.568]
 [0.568]
 [0.53 ]
 [0.568]
 [0.568]] [[38.519]
 [36.483]
 [38.519]
 [38.519]
 [33.747]
 [38.519]
 [38.519]] [[0.568]
 [0.717]
 [0.568]
 [0.568]
 [0.53 ]
 [0.568]
 [0.568]]
printing an ep nov before normalisation:  27.956080436706543
maxi score, test score, baseline:  0.10695333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
actions average: 
K:  2  action  0 :  tensor([0.5834, 0.0176, 0.0683, 0.0655, 0.1061, 0.0841, 0.0749],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0167, 0.8923, 0.0149, 0.0137, 0.0048, 0.0070, 0.0507],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0414, 0.0044, 0.6776, 0.0192, 0.0149, 0.2037, 0.0388],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.3592, 0.0028, 0.1106, 0.1218, 0.1449, 0.1470, 0.1136],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0930, 0.0007, 0.0501, 0.0488, 0.6915, 0.0729, 0.0430],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1889, 0.0163, 0.0970, 0.0955, 0.1006, 0.4189, 0.0828],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0959, 0.2114, 0.0890, 0.1596, 0.1026, 0.1003, 0.2412],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.10425999999999985 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
printing an ep nov before normalisation:  28.912286914543408
actor:  1 policy actor:  1  step number:  49 total reward:  0.2933333333333328  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.766]
 [0.766]
 [0.766]
 [0.767]
 [0.767]
 [0.766]
 [0.767]] [[10.243]
 [10.322]
 [21.27 ]
 [18.887]
 [19.819]
 [10.117]
 [ 9.639]] [[0.927]
 [0.928]
 [1.099]
 [1.062]
 [1.077]
 [0.925]
 [0.917]]
maxi score, test score, baseline:  0.10425999999999985 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
actor:  1 policy actor:  1  step number:  83 total reward:  0.05333333333333201  reward:  1.0 rdn_beta:  0.333
from probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
actor:  1 policy actor:  1  step number:  52 total reward:  0.42000000000000004  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.10136666666666654 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
maxi score, test score, baseline:  0.10136666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
maxi score, test score, baseline:  0.10136666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
actor:  0 policy actor:  1  step number:  39 total reward:  0.52  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  29.810776952678985
maxi score, test score, baseline:  0.10440666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
using another actor
maxi score, test score, baseline:  0.10440666666666656 0.6983333333333335 0.6983333333333335
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10440666666666656 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.10440666666666656 0.6983333333333335 0.6983333333333335
using explorer policy with actor:  0
printing an ep nov before normalisation:  35.541845478504655
actor:  1 policy actor:  1  step number:  60 total reward:  0.20666666666666644  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10440666666666656 0.6983333333333335 0.6983333333333335
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]] [[32.907]
 [32.907]
 [32.907]
 [32.907]
 [32.907]
 [32.907]
 [32.907]] [[0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]]
actor:  0 policy actor:  0  step number:  53 total reward:  0.3333333333333326  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10440666666666656 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
maxi score, test score, baseline:  0.10440666666666656 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.10440666666666656 0.6983333333333335 0.6983333333333335
printing an ep nov before normalisation:  41.85493202040451
Printing some Q and Qe and total Qs values:  [[0.436]
 [0.458]
 [0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.436]] [[33.449]
 [32.405]
 [33.449]
 [33.449]
 [33.449]
 [33.449]
 [33.449]] [[1.175]
 [1.152]
 [1.175]
 [1.175]
 [1.175]
 [1.175]
 [1.175]]
actions average: 
K:  0  action  0 :  tensor([0.4044, 0.0201, 0.0970, 0.1006, 0.1505, 0.1208, 0.1067],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0037, 0.9761, 0.0021, 0.0031, 0.0014, 0.0012, 0.0125],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0364, 0.0020, 0.5356, 0.0279, 0.0498, 0.3030, 0.0454],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2264, 0.0275, 0.1299, 0.1336, 0.1963, 0.1787, 0.1076],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1678, 0.0017, 0.0715, 0.0639, 0.5293, 0.0994, 0.0664],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1062, 0.0141, 0.1466, 0.0664, 0.1045, 0.4712, 0.0908],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1962, 0.0590, 0.1419, 0.1190, 0.1756, 0.1463, 0.1620],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  49 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.144]
 [0.236]
 [0.135]
 [0.169]
 [0.132]
 [0.144]
 [0.141]] [[31.282]
 [38.527]
 [31.16 ]
 [38.718]
 [31.333]
 [31.282]
 [30.906]] [[0.292]
 [0.452]
 [0.282]
 [0.387]
 [0.281]
 [0.292]
 [0.285]]
maxi score, test score, baseline:  0.10440666666666656 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
actor:  0 policy actor:  0  step number:  51 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  36.21999674035895
printing an ep nov before normalisation:  48.1289644261557
maxi score, test score, baseline:  0.10685999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
line 256 mcts: sample exp_bonus 28.198107184485032
maxi score, test score, baseline:  0.10685999999999989 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.10685999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
printing an ep nov before normalisation:  27.799696174203746
printing an ep nov before normalisation:  29.35440263761837
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.634]
 [0.419]
 [0.419]
 [0.419]
 [0.419]
 [0.419]] [[39.504]
 [44.975]
 [39.504]
 [39.504]
 [39.504]
 [39.504]
 [39.504]] [[0.695]
 [0.967]
 [0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.695]]
maxi score, test score, baseline:  0.10685999999999989 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.10685999999999989 0.6983333333333335 0.6983333333333335
actor:  1 policy actor:  1  step number:  49 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10685999999999989 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.10685999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
using another actor
actor:  1 policy actor:  1  step number:  68 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  35.61753943970976
maxi score, test score, baseline:  0.10685999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
siam score:  -0.767592
printing an ep nov before normalisation:  27.910137304964593
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
printing an ep nov before normalisation:  35.30102234200168
maxi score, test score, baseline:  0.10685999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.282]
 [0.116]
 [0.272]
 [0.257]
 [0.169]
 [0.272]] [[29.011]
 [32.006]
 [31.643]
 [33.978]
 [33.446]
 [33.357]
 [33.978]] [[0.949]
 [1.171]
 [0.984]
 [1.275]
 [1.229]
 [1.136]
 [1.275]]
maxi score, test score, baseline:  0.10685999999999989 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.10465999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
printing an ep nov before normalisation:  30.664369049586508
line 256 mcts: sample exp_bonus 28.056567435262956
maxi score, test score, baseline:  0.10465999999999989 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
printing an ep nov before normalisation:  25.686126751037737
Printing some Q and Qe and total Qs values:  [[0.711]
 [0.711]
 [0.711]
 [0.711]
 [0.711]
 [0.711]
 [0.711]] [[44.073]
 [44.073]
 [44.073]
 [44.073]
 [44.073]
 [44.073]
 [44.073]] [[1.913]
 [1.913]
 [1.913]
 [1.913]
 [1.913]
 [1.913]
 [1.913]]
actor:  0 policy actor:  1  step number:  70 total reward:  0.07333333333333247  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10425999999999988 0.6983333333333335 0.6983333333333335
actor:  1 policy actor:  1  step number:  40 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10425999999999988 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
printing an ep nov before normalisation:  30.005067188040048
printing an ep nov before normalisation:  32.682829479610646
printing an ep nov before normalisation:  22.687012028824046
printing an ep nov before normalisation:  24.648498269147794
printing an ep nov before normalisation:  30.04224981921974
maxi score, test score, baseline:  0.10425999999999988 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
siam score:  -0.7740841
actor:  0 policy actor:  0  step number:  41 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7732535
actor:  1 policy actor:  1  step number:  65 total reward:  0.10666666666666591  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10732666666666656 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
Printing some Q and Qe and total Qs values:  [[0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]
 [0.579]] [[36.81]
 [36.81]
 [36.81]
 [36.81]
 [36.81]
 [36.81]
 [36.81]] [[0.823]
 [0.823]
 [0.823]
 [0.823]
 [0.823]
 [0.823]
 [0.823]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.35333333333333283  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10732666666666656 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
maxi score, test score, baseline:  0.10732666666666656 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.10732666666666656 0.6983333333333335 0.6983333333333335
printing an ep nov before normalisation:  37.42746533730655
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.554]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]] [[37.728]
 [37.626]
 [37.728]
 [37.728]
 [37.728]
 [37.728]
 [37.728]] [[0.64 ]
 [0.799]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]
 [0.64 ]]
siam score:  -0.7696692
printing an ep nov before normalisation:  27.445855140686035
siam score:  -0.76936615
maxi score, test score, baseline:  0.10732666666666656 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
printing an ep nov before normalisation:  32.94606979336249
actions average: 
K:  3  action  0 :  tensor([0.5749, 0.0202, 0.0718, 0.0776, 0.0920, 0.0851, 0.0783],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0067, 0.9068, 0.0239, 0.0123, 0.0074, 0.0118, 0.0310],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0905, 0.0765, 0.4509, 0.0642, 0.0583, 0.1559, 0.1037],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2226, 0.0059, 0.1711, 0.1633, 0.1574, 0.1475, 0.1323],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1405, 0.0198, 0.1235, 0.1590, 0.3163, 0.1280, 0.1128],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1246, 0.0116, 0.1227, 0.0961, 0.0907, 0.4866, 0.0677],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1748, 0.0163, 0.1592, 0.1570, 0.1377, 0.1273, 0.2277],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10732666666666656 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
printing an ep nov before normalisation:  19.693255479054272
actor:  1 policy actor:  1  step number:  39 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.0018385931343800621
maxi score, test score, baseline:  0.10732666666666656 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
actor:  1 policy actor:  1  step number:  58 total reward:  0.28666666666666596  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10732666666666656 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
maxi score, test score, baseline:  0.10732666666666656 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
actor:  1 policy actor:  1  step number:  79 total reward:  0.10666666666666558  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  48.75960713927386
maxi score, test score, baseline:  0.10732666666666656 0.6983333333333335 0.6983333333333335
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
maxi score, test score, baseline:  0.10732666666666656 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
maxi score, test score, baseline:  0.10732666666666656 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
printing an ep nov before normalisation:  51.4052815939958
printing an ep nov before normalisation:  30.124141692517615
maxi score, test score, baseline:  0.10732666666666656 0.6983333333333335 0.6983333333333335
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 37.990601218418306
printing an ep nov before normalisation:  24.213988417087283
maxi score, test score, baseline:  0.10732666666666656 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
actor:  1 policy actor:  1  step number:  41 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  24.803750011781588
actor:  1 policy actor:  1  step number:  60 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  26.207417060997116
maxi score, test score, baseline:  0.10732666666666656 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4066666666666662  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  33.18728446960449
maxi score, test score, baseline:  0.10732666666666656 0.6983333333333335 0.6983333333333335
actions average: 
K:  0  action  0 :  tensor([0.4218, 0.0181, 0.0870, 0.0971, 0.1897, 0.0833, 0.1030],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0050, 0.9600, 0.0043, 0.0053, 0.0016, 0.0018, 0.0219],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0890, 0.0104, 0.6463, 0.0449, 0.0584, 0.0808, 0.0702],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1141, 0.0629, 0.0927, 0.3779, 0.0878, 0.0676, 0.1970],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2173, 0.0090, 0.1365, 0.0947, 0.3267, 0.0908, 0.1249],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0872, 0.0066, 0.0686, 0.0304, 0.0357, 0.7357, 0.0358],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2251, 0.0186, 0.1272, 0.1514, 0.1334, 0.1099, 0.2344],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.10732666666666656 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.10732666666666656 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
printing an ep nov before normalisation:  20.4266095161438
maxi score, test score, baseline:  0.10732666666666656 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
actor:  0 policy actor:  1  step number:  58 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  50.99642259534957
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.471]
 [0.385]
 [0.386]
 [0.393]
 [0.382]
 [0.349]] [[44.491]
 [44.285]
 [43.581]
 [45.939]
 [45.069]
 [46.334]
 [36.454]] [[0.776]
 [0.709]
 [0.618]
 [0.639]
 [0.638]
 [0.639]
 [0.518]]
actions average: 
K:  0  action  0 :  tensor([0.6039, 0.0512, 0.0641, 0.0615, 0.0862, 0.0590, 0.0742],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0112,     0.9676,     0.0021,     0.0055,     0.0007,     0.0004,
            0.0125], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1024, 0.0110, 0.4302, 0.0575, 0.0757, 0.2458, 0.0774],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2518, 0.0108, 0.0795, 0.3424, 0.1162, 0.0909, 0.1083],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1937, 0.0112, 0.1148, 0.1019, 0.3528, 0.1036, 0.1219],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1379, 0.0039, 0.1256, 0.0702, 0.0764, 0.5115, 0.0745],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2247, 0.0174, 0.1171, 0.1064, 0.1158, 0.1161, 0.3026],
       grad_fn=<DivBackward0>)
siam score:  -0.762486
maxi score, test score, baseline:  0.10739333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10739333333333322 0.6983333333333335 0.6983333333333335
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.159]
 [0.038]
 [0.038]
 [0.038]
 [0.038]
 [0.026]] [[18.344]
 [26.777]
 [20.758]
 [20.758]
 [20.758]
 [20.758]
 [24.22 ]] [[0.133]
 [0.342]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.187]]
siam score:  -0.76241326
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10739333333333322 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.10739333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
maxi score, test score, baseline:  0.10739333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
printing an ep nov before normalisation:  27.159499561254236
actor:  1 policy actor:  1  step number:  47 total reward:  0.4933333333333335  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10739333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
maxi score, test score, baseline:  0.10739333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
Printing some Q and Qe and total Qs values:  [[-0.002]
 [ 0.03 ]
 [ 0.02 ]
 [ 0.113]
 [ 0.113]
 [ 0.113]
 [ 0.113]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.002]
 [ 0.03 ]
 [ 0.02 ]
 [ 0.113]
 [ 0.113]
 [ 0.113]
 [ 0.113]]
Printing some Q and Qe and total Qs values:  [[0.135]
 [0.286]
 [0.135]
 [0.135]
 [0.135]
 [0.135]
 [0.135]] [[28.152]
 [23.723]
 [28.152]
 [28.152]
 [28.152]
 [28.152]
 [28.152]] [[0.364]
 [0.46 ]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]]
maxi score, test score, baseline:  0.10739333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
printing an ep nov before normalisation:  31.95920467376709
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.15836621628641
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.741]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]] [[41.59 ]
 [44.302]
 [41.59 ]
 [41.59 ]
 [41.59 ]
 [41.59 ]
 [41.59 ]] [[0.666]
 [0.993]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]]
actor:  1 policy actor:  1  step number:  40 total reward:  0.6200000000000003  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  28.36505259695566
siam score:  -0.76628953
printing an ep nov before normalisation:  29.579035739218156
printing an ep nov before normalisation:  72.19906381248762
maxi score, test score, baseline:  0.10739333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
actor:  1 policy actor:  1  step number:  50 total reward:  0.3266666666666661  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10739333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
printing an ep nov before normalisation:  11.199152000027276
printing an ep nov before normalisation:  33.72824850700471
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.158]
 [0.158]
 [0.116]
 [0.116]
 [0.12 ]
 [0.164]] [[32.193]
 [30.892]
 [30.124]
 [28.615]
 [28.609]
 [29.059]
 [29.546]] [[1.325]
 [1.244]
 [1.189]
 [1.04 ]
 [1.04 ]
 [1.076]
 [1.154]]
printing an ep nov before normalisation:  40.00506085761809
actor:  1 policy actor:  1  step number:  41 total reward:  0.52  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10468666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
maxi score, test score, baseline:  0.10468666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
maxi score, test score, baseline:  0.10468666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6133333333333336  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10468666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.166]
 [0.06 ]
 [0.06 ]
 [0.042]
 [0.06 ]
 [0.06 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.166]
 [0.06 ]
 [0.06 ]
 [0.042]
 [0.06 ]
 [0.06 ]]
Printing some Q and Qe and total Qs values:  [[0.755]
 [0.758]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.755]
 [0.758]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10128666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
actor:  1 policy actor:  1  step number:  46 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.667
from probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.095]
 [0.026]
 [0.081]
 [0.039]
 [0.078]
 [0.088]] [[31.269]
 [31.773]
 [24.636]
 [30.524]
 [31.218]
 [31.183]
 [30.65 ]] [[0.274]
 [0.343]
 [0.183]
 [0.313]
 [0.279]
 [0.318]
 [0.321]]
maxi score, test score, baseline:  0.10128666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
printing an ep nov before normalisation:  30.819647351934467
siam score:  -0.7697617
maxi score, test score, baseline:  0.10128666666666655 0.6983333333333335 0.6983333333333335
printing an ep nov before normalisation:  22.215700864848802
maxi score, test score, baseline:  0.10128666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
actions average: 
K:  2  action  0 :  tensor([0.4915, 0.0200, 0.1222, 0.0771, 0.1150, 0.0899, 0.0842],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0027, 0.9686, 0.0019, 0.0027, 0.0012, 0.0011, 0.0218],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0661, 0.0024, 0.6557, 0.0524, 0.0602, 0.1081, 0.0551],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1546, 0.0624, 0.1401, 0.2027, 0.1485, 0.1236, 0.1682],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1382, 0.0065, 0.1038, 0.1065, 0.3972, 0.1464, 0.1015],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1349, 0.0082, 0.1784, 0.1158, 0.1434, 0.3359, 0.0835],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1497, 0.2869, 0.1124, 0.1118, 0.1258, 0.0916, 0.1218],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  31.381594464745827
maxi score, test score, baseline:  0.10128666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
maxi score, test score, baseline:  0.10128666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
actions average: 
K:  0  action  0 :  tensor([0.4687, 0.0114, 0.1039, 0.0984, 0.1257, 0.0977, 0.0942],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0030,     0.9641,     0.0021,     0.0026,     0.0007,     0.0006,
            0.0269], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1108, 0.0064, 0.5961, 0.0567, 0.0863, 0.0778, 0.0660],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1421, 0.0140, 0.1014, 0.4136, 0.1353, 0.1000, 0.0936],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1780, 0.0095, 0.0881, 0.0899, 0.4676, 0.0803, 0.0866],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([    0.0778,     0.0002,     0.1282,     0.0490,     0.0602,     0.6487,
            0.0358], grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1317, 0.1031, 0.1641, 0.0615, 0.0531, 0.0515, 0.4351],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.10128666666666655 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.10128666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
printing an ep nov before normalisation:  31.54787711313462
printing an ep nov before normalisation:  33.779358656833395
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10128666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
actor:  0 policy actor:  1  step number:  51 total reward:  0.4533333333333335  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  66 total reward:  0.24666666666666637  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10079333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
printing an ep nov before normalisation:  35.62157928880351
printing an ep nov before normalisation:  37.504383487149305
printing an ep nov before normalisation:  29.211688550305812
printing an ep nov before normalisation:  31.855372512202283
maxi score, test score, baseline:  0.10079333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
printing an ep nov before normalisation:  33.29115044532209
actor:  1 policy actor:  1  step number:  62 total reward:  0.15333333333333288  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10079333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
maxi score, test score, baseline:  0.10079333333333322 0.6983333333333335 0.6983333333333335
maxi score, test score, baseline:  0.10079333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
maxi score, test score, baseline:  0.10079333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
maxi score, test score, baseline:  0.10079333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
printing an ep nov before normalisation:  27.82807065655362
actor:  1 policy actor:  1  step number:  58 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7655971
printing an ep nov before normalisation:  30.520169441069843
maxi score, test score, baseline:  0.10079333333333322 0.6983333333333335 0.6983333333333335
Starting evaluation
maxi score, test score, baseline:  0.10079333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  29.200174521136788
Printing some Q and Qe and total Qs values:  [[0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]] [[31.844]
 [31.844]
 [31.844]
 [31.844]
 [31.844]
 [31.844]
 [31.844]] [[0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]]
maxi score, test score, baseline:  0.10079333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
printing an ep nov before normalisation:  31.470186561413644
printing an ep nov before normalisation:  28.724927657030502
printing an ep nov before normalisation:  47.484512196249526
printing an ep nov before normalisation:  39.694813861241414
printing an ep nov before normalisation:  35.94083793019577
Printing some Q and Qe and total Qs values:  [[0.839]
 [0.866]
 [0.843]
 [0.831]
 [0.831]
 [0.83 ]
 [0.842]] [[34.66 ]
 [33.439]
 [35.933]
 [35.207]
 [35.336]
 [35.261]
 [35.577]] [[0.839]
 [0.866]
 [0.843]
 [0.831]
 [0.831]
 [0.83 ]
 [0.842]]
printing an ep nov before normalisation:  35.48985949484641
Printing some Q and Qe and total Qs values:  [[0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.809]] [[33.63]
 [33.63]
 [33.63]
 [33.63]
 [33.63]
 [33.63]
 [33.63]] [[0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.809]]
printing an ep nov before normalisation:  35.22394864386396
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.542]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]] [[30.116]
 [32.63 ]
 [30.116]
 [30.116]
 [30.116]
 [30.116]
 [30.116]] [[0.73 ]
 [0.875]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]
 [0.73 ]]
printing an ep nov before normalisation:  32.64547824859619
printing an ep nov before normalisation:  33.25531252554664
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.10079333333333322 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
printing an ep nov before normalisation:  37.25062608718872
printing an ep nov before normalisation:  38.53107076786764
printing an ep nov before normalisation:  27.126032843876065
printing an ep nov before normalisation:  27.326574325561523
actor:  0 policy actor:  0  step number:  23 total reward:  0.7333333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7266666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  24 total reward:  0.7000000000000002  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  0.11528666666666655 0.6983333333333335 0.6983333333333335
probs:  [0.4365769602435644, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11188289420798919, 0.11589146292447884]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  0.30818843649313976
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  34.50732917685528
actor:  1 policy actor:  1  step number:  60 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12664666666666655 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  0.12664666666666655 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.039]
 [0.011]
 [0.016]
 [0.016]
 [0.011]
 [0.011]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.039]
 [0.011]
 [0.016]
 [0.016]
 [0.011]
 [0.011]]
actor:  0 policy actor:  1  step number:  47 total reward:  0.4666666666666668  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  42.624456759286296
actor:  1 policy actor:  1  step number:  51 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  38.442455212560795
printing an ep nov before normalisation:  40.48983369714948
maxi score, test score, baseline:  0.1267133333333332 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  45.04182601699687
maxi score, test score, baseline:  0.1267133333333332 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1267133333333332 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  0.1267133333333332 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  31.823208332061768
maxi score, test score, baseline:  0.1267133333333332 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  50.28037832550856
maxi score, test score, baseline:  0.1267133333333332 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  43.00471180075373
maxi score, test score, baseline:  0.1267133333333332 0.6970000000000002 0.6970000000000002
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.704]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]] [[32.672]
 [31.502]
 [32.672]
 [32.672]
 [32.672]
 [32.672]
 [32.672]] [[0.655]
 [0.704]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]]
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.588]
 [0.611]
 [0.555]
 [0.561]
 [0.595]
 [0.526]] [[40.623]
 [36.998]
 [44.672]
 [36.478]
 [36.867]
 [43.143]
 [35.663]] [[1.012]
 [1.042]
 [1.262]
 [0.996]
 [1.012]
 [1.207]
 [0.946]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.9008640657723
siam score:  -0.76691824
actor:  0 policy actor:  1  step number:  47 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  50.40440586831086
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.425]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]] [[36.022]
 [40.703]
 [36.022]
 [36.022]
 [36.022]
 [36.022]
 [36.022]] [[1.057]
 [1.256]
 [1.057]
 [1.057]
 [1.057]
 [1.057]
 [1.057]]
maxi score, test score, baseline:  0.12647333333333322 0.6970000000000002 0.6970000000000002
actor:  1 policy actor:  1  step number:  52 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  59.874853581081624
Printing some Q and Qe and total Qs values:  [[0.174]
 [0.264]
 [0.148]
 [0.174]
 [0.148]
 [0.174]
 [0.155]] [[31.705]
 [34.709]
 [31.745]
 [31.705]
 [32.645]
 [31.705]
 [32.206]] [[0.745]
 [0.942]
 [0.721]
 [0.745]
 [0.753]
 [0.745]
 [0.743]]
printing an ep nov before normalisation:  38.57092618942261
actor:  1 policy actor:  1  step number:  42 total reward:  0.43333333333333335  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.89231097887449
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]] [[29.139]
 [29.139]
 [29.139]
 [29.139]
 [29.139]
 [29.139]
 [29.139]] [[0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]]
maxi score, test score, baseline:  0.12647333333333322 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  0.12647333333333322 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  37.4337741888911
printing an ep nov before normalisation:  43.39084975909634
maxi score, test score, baseline:  0.12647333333333322 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  43.944840559178786
using explorer policy with actor:  1
siam score:  -0.7743339
maxi score, test score, baseline:  0.12647333333333322 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
from probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
actor:  1 policy actor:  1  step number:  64 total reward:  0.20666666666666567  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.12647333333333322 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]] [[34.374]
 [34.374]
 [34.374]
 [34.374]
 [34.374]
 [34.374]
 [34.374]] [[0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]]
siam score:  -0.77649236
printing an ep nov before normalisation:  29.365434501501994
maxi score, test score, baseline:  0.12403333333333323 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
actor:  0 policy actor:  0  step number:  46 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  0.333
using another actor
actor:  1 policy actor:  1  step number:  44 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12391333333333324 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.685]
 [0.685]
 [0.685]
 [0.779]
 [0.542]
 [0.685]] [[26.9  ]
 [35.043]
 [35.043]
 [35.043]
 [28.293]
 [31.382]
 [35.043]] [[0.765]
 [0.685]
 [0.685]
 [0.685]
 [0.779]
 [0.542]
 [0.685]]
maxi score, test score, baseline:  0.12391333333333324 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  40.47273470906093
actor:  1 policy actor:  1  step number:  47 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.12391333333333324 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
using explorer policy with actor:  1
siam score:  -0.77119094
printing an ep nov before normalisation:  40.36582759275778
printing an ep nov before normalisation:  43.232431411743164
maxi score, test score, baseline:  0.1208199999999999 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
maxi score, test score, baseline:  0.1208199999999999 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  39.383645000325195
Printing some Q and Qe and total Qs values:  [[ 0.494]
 [ 0.479]
 [-0.094]
 [ 0.369]
 [ 0.377]
 [-0.085]
 [ 0.418]] [[37.723]
 [36.326]
 [36.13 ]
 [34.17 ]
 [35.393]
 [35.168]
 [35.66 ]] [[2.206]
 [2.064]
 [1.474]
 [1.757]
 [1.876]
 [1.394]
 [1.943]]
maxi score, test score, baseline:  0.1208199999999999 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
maxi score, test score, baseline:  0.1208199999999999 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  0.1208199999999999 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
maxi score, test score, baseline:  0.1208199999999999 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
maxi score, test score, baseline:  0.1208199999999999 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
actor:  1 policy actor:  1  step number:  43 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1208199999999999 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.59 ]
 [0.549]
 [0.41 ]
 [0.407]
 [0.417]
 [0.543]] [[25.224]
 [29.759]
 [30.406]
 [26.674]
 [27.393]
 [28.809]
 [30.096]] [[0.526]
 [0.784]
 [0.753]
 [0.559]
 [0.566]
 [0.597]
 [0.742]]
printing an ep nov before normalisation:  26.587958211310696
maxi score, test score, baseline:  0.1208199999999999 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  52.45876093637345
printing an ep nov before normalisation:  45.070097828846315
maxi score, test score, baseline:  0.1208199999999999 0.6970000000000002 0.6970000000000002
actor:  1 policy actor:  1  step number:  50 total reward:  0.3399999999999993  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1208199999999999 0.6970000000000002 0.6970000000000002
siam score:  -0.7636568
maxi score, test score, baseline:  0.1208199999999999 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  51.07982679174816
actor:  1 policy actor:  1  step number:  48 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.652]
 [0.64 ]
 [0.605]
 [0.571]
 [0.62 ]
 [0.639]] [[31.216]
 [32.48 ]
 [32.753]
 [32.518]
 [29.934]
 [28.676]
 [29.2  ]] [[0.647]
 [0.652]
 [0.64 ]
 [0.605]
 [0.571]
 [0.62 ]
 [0.639]]
printing an ep nov before normalisation:  49.25866150166531
maxi score, test score, baseline:  0.1208199999999999 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.573]
 [0.406]
 [0.406]
 [0.314]
 [0.406]
 [0.406]] [[24.922]
 [29.375]
 [29.576]
 [29.576]
 [27.957]
 [29.576]
 [29.576]] [[1.113]
 [1.589]
 [1.434]
 [1.434]
 [1.24 ]
 [1.434]
 [1.434]]
maxi score, test score, baseline:  0.1208199999999999 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
actions average: 
K:  2  action  0 :  tensor([0.7342, 0.0056, 0.0474, 0.0371, 0.0785, 0.0302, 0.0670],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0094, 0.9189, 0.0123, 0.0099, 0.0071, 0.0230, 0.0193],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1152, 0.0126, 0.6021, 0.0463, 0.0450, 0.1133, 0.0654],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1990, 0.0449, 0.1100, 0.2547, 0.1125, 0.1044, 0.1745],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2093, 0.0516, 0.0987, 0.0846, 0.3650, 0.0712, 0.1197],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0883, 0.0328, 0.0951, 0.0732, 0.0731, 0.5639, 0.0735],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1065, 0.1157, 0.1310, 0.0901, 0.1094, 0.1099, 0.3374],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1208199999999999 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
Printing some Q and Qe and total Qs values:  [[-0.094]
 [-0.091]
 [-0.089]
 [-0.088]
 [-0.088]
 [-0.084]
 [-0.087]] [[33.918]
 [32.816]
 [29.132]
 [25.966]
 [25.966]
 [28.189]
 [28.208]] [[0.879]
 [0.851]
 [0.748]
 [0.658]
 [0.658]
 [0.726]
 [0.722]]
siam score:  -0.76786226
maxi score, test score, baseline:  0.1208199999999999 0.6970000000000002 0.6970000000000002
siam score:  -0.76885396
printing an ep nov before normalisation:  38.11007287349369
actor:  0 policy actor:  1  step number:  57 total reward:  0.2533333333333332  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  34.249344114570086
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.429]
 [0.329]
 [0.346]
 [0.328]
 [0.33 ]
 [0.392]] [[35.236]
 [35.422]
 [31.134]
 [32.515]
 [30.914]
 [30.991]
 [35.236]] [[0.974]
 [1.016]
 [0.792]
 [0.849]
 [0.786]
 [0.79 ]
 [0.974]]
maxi score, test score, baseline:  0.12048666666666656 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
actions average: 
K:  0  action  0 :  tensor([0.4194, 0.0086, 0.0957, 0.1221, 0.1518, 0.0973, 0.1050],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0080, 0.9509, 0.0041, 0.0070, 0.0026, 0.0022, 0.0252],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1601, 0.0065, 0.4135, 0.0951, 0.1158, 0.0970, 0.1121],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2037, 0.0794, 0.1373, 0.1624, 0.1317, 0.1266, 0.1589],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2266, 0.0041, 0.1585, 0.1237, 0.1673, 0.1506, 0.1692],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1055, 0.0070, 0.1376, 0.0936, 0.0763, 0.5247, 0.0554],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1775, 0.1296, 0.1187, 0.1000, 0.0994, 0.1035, 0.2712],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.12048666666666656 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  0.12048666666666656 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  20.53668911602269
actor:  1 policy actor:  1  step number:  37 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  27.207619458941295
actor:  1 policy actor:  1  step number:  45 total reward:  0.38666666666666616  reward:  1.0 rdn_beta:  0.333
from probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  29.451601840724816
actor:  1 policy actor:  1  step number:  38 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  1  action  0 :  tensor([0.2853, 0.0146, 0.1221, 0.1150, 0.1871, 0.1177, 0.1582],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0064, 0.9311, 0.0053, 0.0042, 0.0025, 0.0031, 0.0473],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0847, 0.0844, 0.5984, 0.0377, 0.0451, 0.0846, 0.0651],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2515, 0.0261, 0.1068, 0.1729, 0.1431, 0.1183, 0.1815],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1387, 0.0026, 0.0956, 0.0841, 0.4659, 0.0967, 0.1164],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1166, 0.0037, 0.1017, 0.0656, 0.0965, 0.5341, 0.0818],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1348, 0.0579, 0.1453, 0.1206, 0.1189, 0.0819, 0.3406],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.11765999999999989 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  36.25612258911133
maxi score, test score, baseline:  0.11765999999999989 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
actor:  1 policy actor:  1  step number:  61 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.76959197884217
printing an ep nov before normalisation:  37.870364463215374
printing an ep nov before normalisation:  34.2537784576416
printing an ep nov before normalisation:  27.311947798808955
maxi score, test score, baseline:  0.11765999999999989 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  0.003533914306785846
actor:  1 policy actor:  1  step number:  67 total reward:  0.06666666666666543  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  46 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.11765999999999989 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  0.11765999999999989 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  36.04825494592951
maxi score, test score, baseline:  0.11765999999999989 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  35.24739692799616
actor:  0 policy actor:  1  step number:  31 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  27.19871397900192
maxi score, test score, baseline:  0.11789999999999988 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
Printing some Q and Qe and total Qs values:  [[0.776]
 [0.884]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]] [[28.538]
 [36.474]
 [29.908]
 [29.908]
 [29.908]
 [29.908]
 [29.908]] [[0.776]
 [0.884]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]
 [0.77 ]]
actor:  0 policy actor:  0  step number:  48 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  46 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  1.0
siam score:  -0.7698953
printing an ep nov before normalisation:  47.11729511977001
printing an ep nov before normalisation:  45.322928380509765
actor:  1 policy actor:  1  step number:  61 total reward:  0.2933333333333328  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12087333333333324 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
maxi score, test score, baseline:  0.12087333333333324 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  31.471547632875232
actor:  1 policy actor:  1  step number:  56 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12087333333333324 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
maxi score, test score, baseline:  0.12087333333333324 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
Printing some Q and Qe and total Qs values:  [[0.168]
 [0.194]
 [0.207]
 [0.151]
 [0.156]
 [0.157]
 [0.207]] [[32.69 ]
 [33.806]
 [36.266]
 [35.618]
 [35.331]
 [35.055]
 [36.266]] [[1.075]
 [1.16 ]
 [1.305]
 [1.214]
 [1.203]
 [1.19 ]
 [1.305]]
printing an ep nov before normalisation:  34.330782946068226
printing an ep nov before normalisation:  19.18479870492451
printing an ep nov before normalisation:  32.99334389448549
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.2933333333333331  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.12087333333333324 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
maxi score, test score, baseline:  0.12087333333333324 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  0.12087333333333324 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
actor:  1 policy actor:  1  step number:  85 total reward:  0.07999999999999963  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12087333333333324 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
from probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
maxi score, test score, baseline:  0.12087333333333324 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
line 256 mcts: sample exp_bonus 42.920562545523325
actor:  1 policy actor:  1  step number:  55 total reward:  0.30666666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12087333333333324 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [ 0.098]
 [-0.146]
 [-0.006]
 [-0.008]
 [ 0.023]
 [-0.015]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.01 ]
 [ 0.098]
 [-0.146]
 [-0.006]
 [-0.008]
 [ 0.023]
 [-0.015]]
maxi score, test score, baseline:  0.12087333333333324 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
maxi score, test score, baseline:  0.12087333333333324 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  44.76014176912581
printing an ep nov before normalisation:  41.38780776249709
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.321]
 [0.231]
 [0.231]
 [0.174]
 [0.231]
 [0.231]] [[37.881]
 [38.554]
 [37.881]
 [37.881]
 [35.076]
 [37.881]
 [37.881]] [[1.758]
 [1.902]
 [1.758]
 [1.758]
 [1.474]
 [1.758]
 [1.758]]
maxi score, test score, baseline:  0.12087333333333324 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  0.12087333333333324 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
actor:  0 policy actor:  1  step number:  45 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  40.048728505391935
maxi score, test score, baseline:  0.12041999999999989 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
line 256 mcts: sample exp_bonus 30.503358615622012
printing an ep nov before normalisation:  34.581018769285805
maxi score, test score, baseline:  0.12041999999999989 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  26.4611280540074
maxi score, test score, baseline:  0.12041999999999989 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
maxi score, test score, baseline:  0.12041999999999989 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  30.852533327942442
printing an ep nov before normalisation:  0.03564332413077409
actor:  1 policy actor:  1  step number:  61 total reward:  0.279999999999999  reward:  1.0 rdn_beta:  2.0
using another actor
printing an ep nov before normalisation:  42.62288322558867
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.453]
 [0.55 ]
 [0.453]
 [0.453]
 [0.512]
 [0.453]] [[25.675]
 [25.675]
 [21.124]
 [25.675]
 [25.675]
 [28.565]
 [25.675]] [[2.219]
 [2.219]
 [2.   ]
 [2.219]
 [2.219]
 [2.477]
 [2.219]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.586666666666667  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]] [[28.386]
 [28.386]
 [28.386]
 [28.386]
 [28.386]
 [28.386]
 [28.386]] [[1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.429]]
printing an ep nov before normalisation:  50.07919668293785
printing an ep nov before normalisation:  45.81471531131488
actor:  0 policy actor:  1  step number:  39 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.12329999999999988 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4066666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12329999999999988 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
maxi score, test score, baseline:  0.12329999999999988 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
siam score:  -0.7772001
printing an ep nov before normalisation:  40.533628863585584
line 256 mcts: sample exp_bonus 46.98722189191178
maxi score, test score, baseline:  0.12329999999999988 0.6970000000000002 0.6970000000000002
actor:  1 policy actor:  1  step number:  59 total reward:  0.29333333333333345  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.439]
 [0.296]
 [0.296]
 [0.296]
 [0.296]
 [0.296]] [[30.287]
 [37.334]
 [30.287]
 [30.287]
 [30.287]
 [30.287]
 [30.287]] [[0.819]
 [1.19 ]
 [0.819]
 [0.819]
 [0.819]
 [0.819]
 [0.819]]
printing an ep nov before normalisation:  46.503314439776034
printing an ep nov before normalisation:  44.96508007730755
printing an ep nov before normalisation:  30.50042152404785
siam score:  -0.7798503
printing an ep nov before normalisation:  28.95182029855191
printing an ep nov before normalisation:  22.34258203331886
printing an ep nov before normalisation:  47.73920651914225
actor:  1 policy actor:  1  step number:  48 total reward:  0.4733333333333336  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.12329999999999988 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
Printing some Q and Qe and total Qs values:  [[-0.008]
 [-0.004]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]] [[26.925]
 [29.601]
 [26.925]
 [26.925]
 [26.925]
 [26.925]
 [26.925]] [[0.198]
 [0.24 ]
 [0.198]
 [0.198]
 [0.198]
 [0.198]
 [0.198]]
siam score:  -0.7817382
actor:  1 policy actor:  1  step number:  57 total reward:  0.33333333333333337  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.12329999999999988 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
maxi score, test score, baseline:  0.12329999999999988 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
maxi score, test score, baseline:  0.12329999999999988 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
maxi score, test score, baseline:  0.12329999999999988 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  0.12329999999999988 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  31.816039036434947
from probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  27.638711248151886
actor:  1 policy actor:  1  step number:  50 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12329999999999988 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  29.869722912903537
maxi score, test score, baseline:  0.12329999999999988 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
actor:  1 policy actor:  1  step number:  58 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.12329999999999988 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  0.12329999999999988 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  36.06476835189694
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]] [[33.724]
 [33.724]
 [33.724]
 [33.724]
 [33.724]
 [33.724]
 [33.724]] [[0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.642]]
maxi score, test score, baseline:  0.12329999999999988 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
actor:  1 policy actor:  1  step number:  39 total reward:  0.4266666666666663  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
actor:  0 policy actor:  0  step number:  49 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.796]
 [0.673]
 [0.762]
 [0.619]
 [0.678]
 [0.768]] [[37.134]
 [42.317]
 [43.911]
 [40.271]
 [37.134]
 [43.134]
 [31.366]] [[1.41 ]
 [1.699]
 [1.611]
 [1.621]
 [1.41 ]
 [1.599]
 [1.436]]
actor:  1 policy actor:  1  step number:  35 total reward:  0.56  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  43.50129321571242
maxi score, test score, baseline:  0.12596666666666656 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
maxi score, test score, baseline:  0.12596666666666656 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
Printing some Q and Qe and total Qs values:  [[0.221]
 [0.239]
 [0.226]
 [0.218]
 [0.241]
 [0.228]
 [0.228]] [[31.787]
 [31.318]
 [29.722]
 [33.367]
 [30.835]
 [29.476]
 [31.472]] [[0.452]
 [0.463]
 [0.426]
 [0.472]
 [0.458]
 [0.425]
 [0.454]]
maxi score, test score, baseline:  0.12596666666666656 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
actor:  0 policy actor:  0  step number:  47 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12612666666666655 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  37.42397611310936
maxi score, test score, baseline:  0.12612666666666655 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  42.71921555306668
siam score:  -0.7711691
maxi score, test score, baseline:  0.12612666666666655 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  29.304569953943727
actor:  1 policy actor:  1  step number:  54 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  41.0968698840947
printing an ep nov before normalisation:  32.082472057653504
Printing some Q and Qe and total Qs values:  [[0.104]
 [0.147]
 [0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.104]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.104]
 [0.147]
 [0.104]
 [0.104]
 [0.104]
 [0.104]
 [0.104]]
actions average: 
K:  2  action  0 :  tensor([0.4937, 0.0374, 0.0826, 0.0938, 0.1092, 0.0976, 0.0857],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0065, 0.9520, 0.0046, 0.0121, 0.0023, 0.0044, 0.0181],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0441, 0.0029, 0.7864, 0.0329, 0.0264, 0.0472, 0.0603],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1187, 0.0396, 0.0970, 0.4177, 0.0848, 0.0816, 0.1605],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1823, 0.0126, 0.1047, 0.1069, 0.3429, 0.0999, 0.1506],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1554, 0.0479, 0.1356, 0.1039, 0.1024, 0.3532, 0.1016],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2140, 0.0052, 0.1256, 0.1444, 0.1405, 0.1329, 0.2374],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.12612666666666655 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
maxi score, test score, baseline:  0.12612666666666655 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  0.12612666666666655 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
maxi score, test score, baseline:  0.12612666666666655 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
actor:  1 policy actor:  1  step number:  63 total reward:  0.07999999999999918  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12612666666666655 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
maxi score, test score, baseline:  0.12612666666666655 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
actions average: 
K:  3  action  0 :  tensor([0.4586, 0.0703, 0.0845, 0.0847, 0.1063, 0.0800, 0.1155],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0111, 0.9286, 0.0053, 0.0110, 0.0069, 0.0077, 0.0294],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1248, 0.0098, 0.4458, 0.0942, 0.0973, 0.0925, 0.1355],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1647, 0.1807, 0.0828, 0.2623, 0.0863, 0.0729, 0.1501],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2798, 0.0069, 0.0909, 0.1008, 0.3204, 0.1007, 0.1005],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1325, 0.0017, 0.1207, 0.0594, 0.1238, 0.3828, 0.1791],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1635, 0.1025, 0.1186, 0.0965, 0.1104, 0.1222, 0.2863],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  58 total reward:  0.13999999999999935  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  43 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  40.84393398514945
maxi score, test score, baseline:  0.12612666666666655 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
actions average: 
K:  1  action  0 :  tensor([0.4870, 0.0542, 0.0710, 0.0873, 0.1361, 0.0815, 0.0830],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0073, 0.9530, 0.0046, 0.0122, 0.0015, 0.0024, 0.0190],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1445, 0.0072, 0.4521, 0.0868, 0.0877, 0.1349, 0.0869],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1610, 0.0424, 0.0934, 0.3127, 0.1351, 0.1331, 0.1225],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2890, 0.0518, 0.0465, 0.0837, 0.3983, 0.0575, 0.0732],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1500, 0.0014, 0.1694, 0.0616, 0.0665, 0.4663, 0.0847],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2267, 0.0904, 0.0610, 0.2421, 0.0648, 0.0514, 0.2636],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.945950941295905
actions average: 
K:  0  action  0 :  tensor([0.4200, 0.0091, 0.0919, 0.1383, 0.1041, 0.1069, 0.1297],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0038, 0.9711, 0.0017, 0.0017, 0.0012, 0.0012, 0.0194],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0942, 0.0135, 0.5831, 0.0723, 0.0661, 0.0942, 0.0766],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2312, 0.0359, 0.0781, 0.2189, 0.1285, 0.0896, 0.2177],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2320, 0.0144, 0.0704, 0.0851, 0.4535, 0.0670, 0.0775],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1257, 0.0264, 0.0888, 0.0743, 0.0781, 0.5288, 0.0779],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2946, 0.0365, 0.1141, 0.0801, 0.0666, 0.0926, 0.3155],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.79929009287061
actor:  1 policy actor:  1  step number:  36 total reward:  0.6466666666666668  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.12612666666666655 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  36.717147888434944
printing an ep nov before normalisation:  51.775154361011474
printing an ep nov before normalisation:  28.837592601776123
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12612666666666655 0.6970000000000002 0.6970000000000002
actor:  1 policy actor:  1  step number:  63 total reward:  0.18666666666666598  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.874]
 [0.874]
 [0.874]
 [0.874]
 [0.874]
 [0.874]
 [0.874]] [[35.849]
 [35.849]
 [35.849]
 [35.849]
 [35.849]
 [35.849]
 [35.849]] [[2.434]
 [2.434]
 [2.434]
 [2.434]
 [2.434]
 [2.434]
 [2.434]]
maxi score, test score, baseline:  0.12612666666666655 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
maxi score, test score, baseline:  0.12612666666666655 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
Printing some Q and Qe and total Qs values:  [[0.151]
 [0.185]
 [0.151]
 [0.151]
 [0.151]
 [0.151]
 [0.167]] [[29.005]
 [39.515]
 [29.005]
 [29.005]
 [29.005]
 [29.005]
 [33.527]] [[0.3  ]
 [0.424]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.3  ]
 [0.355]]
maxi score, test score, baseline:  0.12612666666666655 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
actor:  1 policy actor:  1  step number:  75 total reward:  0.10666666666666524  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12612666666666655 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5133333333333336  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.12612666666666655 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  0.12612666666666655 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  34.26968543029178
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12612666666666655 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12612666666666655 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  33.91937755193416
maxi score, test score, baseline:  0.12612666666666655 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  34.60194104194103
printing an ep nov before normalisation:  33.46580415384785
actor:  1 policy actor:  1  step number:  35 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  35.559987479766846
actor:  0 policy actor:  0  step number:  43 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11087
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.326]
 [0.277]
 [0.293]
 [0.306]
 [0.186]
 [0.337]] [[31.324]
 [32.98 ]
 [30.641]
 [30.287]
 [31.324]
 [32.08 ]
 [31.679]] [[0.52 ]
 [0.565]
 [0.482]
 [0.491]
 [0.52 ]
 [0.411]
 [0.556]]
actor:  1 policy actor:  1  step number:  75 total reward:  0.14666666666666583  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12665999999999988 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.096349368773847
actor:  1 policy actor:  1  step number:  43 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.12665999999999988 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
actor:  1 policy actor:  1  step number:  62 total reward:  0.42000000000000004  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.233814335147105
printing an ep nov before normalisation:  28.556457549463385
maxi score, test score, baseline:  0.12665999999999988 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
from probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  34.78140495617589
Printing some Q and Qe and total Qs values:  [[0.196]
 [0.217]
 [0.196]
 [0.196]
 [0.196]
 [0.196]
 [0.196]] [[20.271]
 [40.803]
 [20.271]
 [20.271]
 [20.271]
 [20.271]
 [20.271]] [[0.308]
 [0.55 ]
 [0.308]
 [0.308]
 [0.308]
 [0.308]
 [0.308]]
maxi score, test score, baseline:  0.12423333333333321 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
siam score:  -0.772576
printing an ep nov before normalisation:  39.655741431893254
maxi score, test score, baseline:  0.12160666666666654 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
line 256 mcts: sample exp_bonus 34.75618817930261
printing an ep nov before normalisation:  30.744187382908244
actor:  0 policy actor:  0  step number:  35 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  30.73161926653441
printing an ep nov before normalisation:  31.147668029157895
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.542]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]] [[29.554]
 [45.195]
 [29.554]
 [29.554]
 [29.554]
 [29.554]
 [29.554]] [[0.673]
 [0.857]
 [0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.673]]
actor:  1 policy actor:  1  step number:  70 total reward:  0.03333333333333288  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  29.706826210021973
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.58652012930442
maxi score, test score, baseline:  0.1246199999999999 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.302922335517927
maxi score, test score, baseline:  0.1246199999999999 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
actor:  1 policy actor:  1  step number:  69 total reward:  0.31999999999999906  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  46 total reward:  0.5266666666666671  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1246199999999999 0.6970000000000002 0.6970000000000002
Printing some Q and Qe and total Qs values:  [[0.733]
 [0.877]
 [0.628]
 [0.759]
 [0.775]
 [0.741]
 [0.686]] [[23.106]
 [21.273]
 [21.776]
 [22.194]
 [20.612]
 [21.953]
 [21.758]] [[1.79 ]
 [1.85 ]
 [1.624]
 [1.774]
 [1.718]
 [1.745]
 [1.681]]
actor:  1 policy actor:  1  step number:  41 total reward:  0.48  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1246199999999999 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  0.1246199999999999 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  40.04917786459725
printing an ep nov before normalisation:  39.22475173186169
siam score:  -0.7674727
maxi score, test score, baseline:  0.1246199999999999 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  0.1246199999999999 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  48.28015150409252
actor:  1 policy actor:  1  step number:  34 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1246199999999999 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  20.134353637695312
actor:  0 policy actor:  0  step number:  43 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12485999999999989 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
actor:  1 policy actor:  1  step number:  44 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  0.333
from probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
siam score:  -0.77576685
siam score:  -0.77679145
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.819]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]] [[29.975]
 [31.525]
 [29.975]
 [29.975]
 [29.975]
 [29.975]
 [29.975]] [[0.738]
 [0.819]
 [0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]]
printing an ep nov before normalisation:  28.27545799554982
siam score:  -0.7799792
printing an ep nov before normalisation:  27.820502592107744
Printing some Q and Qe and total Qs values:  [[0.944]
 [0.782]
 [0.937]
 [0.782]
 [0.782]
 [0.871]
 [0.782]] [[35.805]
 [42.886]
 [43.081]
 [42.886]
 [42.886]
 [35.877]
 [42.886]] [[0.944]
 [0.782]
 [0.937]
 [0.782]
 [0.782]
 [0.871]
 [0.782]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  50 total reward:  0.44666666666666677  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12540666666666656 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  27.93764591217041
maxi score, test score, baseline:  0.12540666666666656 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
actor:  0 policy actor:  1  step number:  35 total reward:  0.5866666666666668  reward:  1.0 rdn_beta:  2.0
siam score:  -0.7825502
actor:  1 policy actor:  1  step number:  51 total reward:  0.3333333333333328  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12643333333333323 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  0.12643333333333323 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
siam score:  -0.7852887
maxi score, test score, baseline:  0.12643333333333323 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  23.866379261016846
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12643333333333323 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
maxi score, test score, baseline:  0.12643333333333323 0.6970000000000002 0.6970000000000002
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12643333333333323 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  32.3175239276986
maxi score, test score, baseline:  0.12643333333333323 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
maxi score, test score, baseline:  0.12643333333333323 0.6970000000000002 0.6970000000000002
maxi score, test score, baseline:  0.12643333333333323 0.6970000000000002 0.6970000000000002
printing an ep nov before normalisation:  25.80668985334839
maxi score, test score, baseline:  0.12643333333333323 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  28.42820632144699
printing an ep nov before normalisation:  34.33701611804337
maxi score, test score, baseline:  0.12643333333333323 0.6970000000000002 0.6970000000000002
probs:  [0.44875856098273287, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.10794877990949808, 0.11944631937927483]
printing an ep nov before normalisation:  16.617542505264282
actor:  1 policy actor:  1  step number:  53 total reward:  0.19999999999999973  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  32.74929079300354
maxi score, test score, baseline:  0.12643333333333323 0.6970000000000002 0.6970000000000002
actor:  1 policy actor:  1  step number:  63 total reward:  0.18666666666666587  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  36.45584518696808
