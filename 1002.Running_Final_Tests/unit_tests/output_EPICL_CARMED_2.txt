append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 50}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.Car_Driving_Env.RaceWorld'>
same_env_each_time:True
env_size:[7, 42]
observable_size:[7, 9]
game_modes:1
env_map:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
max_steps:150
actions_size:7
optimal_score:0.86
total_frames:255000
exp_gamma:0.975
atari_env:False
memory_size:30
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:7
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:episodic
rdn_beta:[0.3333333333333333, 2, 6]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
contrast_vector:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 294)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.
  1. 2. 2. 1. 2. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2.
  1. 2. 1. 1. 2. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.
  1. 1. 1. 1. 0. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 0. 1. 1. 1. 1. 1.
  1. 0. 0. 1. 1. 1. 1. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 1. 0. 1. 1. 2. 1. 1. 2. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.
  2. 0. 0. 0. 1. 1. 1. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
main train batch thing paused
add a thread
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Adding thread: now have 3 threads
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  12.581300735473633
printing an ep nov before normalisation:  12.495801448822021
using explorer policy with actor:  1
printing an ep nov before normalisation:  11.754481792449951
printing an ep nov before normalisation:  7.590403272566064
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
Starting evaluation
UNIT TEST: sample policy line 217 mcts : [0.  0.  0.  0.  0.6 0.  0.4]
siam score:  -0.004018809709867293
printing an ep nov before normalisation:  12.704496383666992
printing an ep nov before normalisation:  12.330713272094727
line 256 mcts: sample exp_bonus 8.243176688549863
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[11.833]
 [11.833]
 [11.833]
 [11.833]
 [11.833]
 [11.833]
 [11.833]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
printing an ep nov before normalisation:  8.345514794930331
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
deleting a thread, now have 2 threads
Frames:  1288 train batches done:  35 episodes:  30
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
actions average: 
K:  0  action  0 :  tensor([0.3931, 0.0492, 0.1377, 0.0769, 0.1488, 0.0982, 0.0961],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0761, 0.6069, 0.0677, 0.0629, 0.0447, 0.0675, 0.0742],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1421, 0.0868, 0.3157, 0.0947, 0.1138, 0.1349, 0.1120],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1363, 0.1214, 0.1354, 0.1735, 0.1045, 0.1743, 0.1546],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1562, 0.0862, 0.1162, 0.1060, 0.2834, 0.1427, 0.1093],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1127, 0.1029, 0.1143, 0.1094, 0.1048, 0.3242, 0.1317],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1450, 0.1795, 0.1244, 0.1109, 0.1331, 0.1074, 0.1998],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
siam score:  -0.4554554
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
deleting a thread, now have 1 threads
Frames:  1288 train batches done:  100 episodes:  30
siam score:  -0.46550596
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
actions average: 
K:  0  action  0 :  tensor([0.3931, 0.0403, 0.0761, 0.0866, 0.2393, 0.0745, 0.0900],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0431, 0.7541, 0.0323, 0.0420, 0.0292, 0.0472, 0.0519],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1387, 0.0484, 0.3234, 0.1297, 0.1126, 0.1421, 0.1052],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1363, 0.1363, 0.1539, 0.2145, 0.0931, 0.1357, 0.1302],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1649, 0.0721, 0.0925, 0.1347, 0.2801, 0.1489, 0.1068],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1308, 0.0770, 0.1247, 0.1130, 0.0935, 0.3542, 0.1068],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1358, 0.0709, 0.1127, 0.1509, 0.1542, 0.1379, 0.2375],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
maxi score, test score, baseline:  -0.9654172413793104 -1.0 -0.9654172413793104
probs:  [0.04925178162042242, 0.342788994236033, 0.04925178162042242, 0.342788994236033, 0.10795922414354457, 0.10795922414354457]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
printing an ep nov before normalisation:  22.872707843780518
using another actor
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
deleting a thread, now have 1 threads
Frames:  1896 train batches done:  162 episodes:  52
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.13198950571308812, 0.13198950571308812, 0.2369754058477755, 0.046091951057434875, 0.13198950571308812, 0.32096412595552537]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.13198950571308812, 0.13198950571308812, 0.2369754058477755, 0.046091951057434875, 0.13198950571308812, 0.32096412595552537]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.13198950571308812, 0.13198950571308812, 0.2369754058477755, 0.046091951057434875, 0.13198950571308812, 0.32096412595552537]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.13198950571308812, 0.13198950571308812, 0.2369754058477755, 0.046091951057434875, 0.13198950571308812, 0.32096412595552537]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.13198950571308812, 0.13198950571308812, 0.2369754058477755, 0.046091951057434875, 0.13198950571308812, 0.32096412595552537]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.13198950571308812, 0.13198950571308812, 0.2369754058477755, 0.046091951057434875, 0.13198950571308812, 0.32096412595552537]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.13198950571308812, 0.13198950571308812, 0.2369754058477755, 0.046091951057434875, 0.13198950571308812, 0.32096412595552537]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.13198950571308812, 0.13198950571308812, 0.2369754058477755, 0.046091951057434875, 0.13198950571308812, 0.32096412595552537]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.13198950571308812, 0.13198950571308812, 0.2369754058477755, 0.046091951057434875, 0.13198950571308812, 0.32096412595552537]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.13198950571308812, 0.13198950571308812, 0.2369754058477755, 0.046091951057434875, 0.13198950571308812, 0.32096412595552537]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.13198950571308812, 0.13198950571308812, 0.2369754058477755, 0.046091951057434875, 0.13198950571308812, 0.32096412595552537]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.13198950571308812, 0.13198950571308812, 0.2369754058477755, 0.046091951057434875, 0.13198950571308812, 0.32096412595552537]
siam score:  -0.45083702
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.13198950571308812, 0.13198950571308812, 0.2369754058477755, 0.046091951057434875, 0.13198950571308812, 0.32096412595552537]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.13198950571308812, 0.13198950571308812, 0.2369754058477755, 0.046091951057434875, 0.13198950571308812, 0.32096412595552537]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.13198950571308812, 0.13198950571308812, 0.2369754058477755, 0.046091951057434875, 0.13198950571308812, 0.32096412595552537]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.13198950571308812, 0.13198950571308812, 0.2369754058477755, 0.046091951057434875, 0.13198950571308812, 0.32096412595552537]
maxi score, test score, baseline:  -0.9704882352941177 -1.0 -0.9704882352941177
probs:  [0.13198950571308812, 0.13198950571308812, 0.2369754058477755, 0.046091951057434875, 0.13198950571308812, 0.32096412595552537]
siam score:  -0.43203956
from probs:  [0.11186433375858314, 0.07957010186631965, 0.2733354932199006, 0.053147548499922224, 0.11186433375858314, 0.3702181888966912]
siam score:  -0.43621895
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.11186433351578791, 0.07957010148044863, 0.2733354936924843, 0.05314754799698921, 0.11186433351578791, 0.3702181897985021]
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.11186433351578791, 0.07957010148044863, 0.2733354936924843, 0.05314754799698921, 0.11186433351578791, 0.3702181897985021]
siam score:  -0.45277527
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.21972993371502192, 0.04878786537132716, 0.12571179612598984, 0.04878786537132716, 0.21972993371502192, 0.3372526057013119]
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.21972993371502192, 0.04878786537132716, 0.12571179612598984, 0.04878786537132716, 0.21972993371502192, 0.3372526057013119]
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.21972993371502192, 0.04878786537132716, 0.12571179612598984, 0.04878786537132716, 0.21972993371502192, 0.3372526057013119]
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.21972993371502192, 0.04878786537132716, 0.12571179612598984, 0.04878786537132716, 0.21972993371502192, 0.3372526057013119]
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.21972993371502192, 0.04878786537132716, 0.12571179612598984, 0.04878786537132716, 0.21972993371502192, 0.3372526057013119]
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.21972993371502192, 0.04878786537132716, 0.12571179612598984, 0.04878786537132716, 0.21972993371502192, 0.3372526057013119]
actions average: 
K:  4  action  0 :  tensor([0.1610, 0.2125, 0.1650, 0.0834, 0.0664, 0.1981, 0.1136],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0638, 0.5353, 0.0550, 0.0658, 0.0710, 0.1225, 0.0866],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1847, 0.0538, 0.2051, 0.1625, 0.0931, 0.1685, 0.1324],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1875, 0.1018, 0.1525, 0.1403, 0.1189, 0.1815, 0.1175],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1879, 0.2111, 0.1197, 0.0964, 0.1155, 0.1503, 0.1192],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1120, 0.0531, 0.1781, 0.1281, 0.1203, 0.2821, 0.1263],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1044, 0.0089, 0.2937, 0.1684, 0.1091, 0.1778, 0.1376],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.21972993371502192, 0.04878786537132716, 0.12571179612598984, 0.04878786537132716, 0.21972993371502192, 0.3372526057013119]
maxi score, test score, baseline:  -0.9735842105263158 -1.0 -0.9735842105263158
probs:  [0.21972993371502192, 0.04878786537132716, 0.12571179612598984, 0.04878786537132716, 0.21972993371502192, 0.3372526057013119]
Printing some Q and Qe and total Qs values:  [[1.28]
 [1.28]
 [1.28]
 [1.28]
 [1.28]
 [1.28]
 [1.28]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.28]
 [1.28]
 [1.28]
 [1.28]
 [1.28]
 [1.28]
 [1.28]]
from probs:  [0.21972993371502192, 0.04878786537132716, 0.12571179612598984, 0.04878786537132716, 0.21972993371502192, 0.3372526057013119]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.26165178684501944, 0.03815738642536584, 0.13872986661420983, 0.03815738642536584, 0.26165178684501944, 0.26165178684501944]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.26165178684501944, 0.03815738642536584, 0.13872986661420983, 0.03815738642536584, 0.26165178684501944, 0.26165178684501944]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.20452462145119857, 0.11753187428674222, 0.11753187428674222, 0.04503791831636198, 0.20452462145119857, 0.3108490902077564]
siam score:  -0.45838767
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.22051365123978636, 0.12671567811241655, 0.0485507005062748, 0.0485507005062748, 0.22051365123978636, 0.335155618395461]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.22051365123978636, 0.12671567811241655, 0.0485507005062748, 0.0485507005062748, 0.22051365123978636, 0.335155618395461]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.22051365123978636, 0.12671567811241655, 0.0485507005062748, 0.0485507005062748, 0.22051365123978636, 0.335155618395461]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.22051365123978636, 0.12671567811241655, 0.0485507005062748, 0.0485507005062748, 0.22051365123978636, 0.335155618395461]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.22051365123978636, 0.12671567811241655, 0.0485507005062748, 0.0485507005062748, 0.22051365123978636, 0.335155618395461]
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
siam score:  -0.46616858
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.29756699785857604, 0.1589666471847896, 0.043466354956634326, 0.043466354956634326, 0.29756699785857604, 0.1589666471847896]
Printing some Q and Qe and total Qs values:  [[1.018]
 [1.018]
 [0.992]
 [0.994]
 [0.994]
 [1.018]
 [0.993]] [[37.343]
 [37.343]
 [33.175]
 [32.612]
 [32.538]
 [37.343]
 [32.812]] [[3.503]
 [3.503]
 [2.992]
 [2.929]
 [2.92 ]
 [3.503]
 [2.951]]
from probs:  [0.33644181282028424, 0.17972629329386808, 0.049130027021854504, 0.049130027021854504, 0.33644181282028424, 0.049130027021854504]
printing an ep nov before normalisation:  52.51396497090657
maxi score, test score, baseline:  -0.9742589743589744 -1.0 -0.9742589743589744
probs:  [0.27983533466414534, 0.18559523391179664, 0.10706181661817256, 0.10706181661817256, 0.27983533466414534, 0.04061046352356748]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.3383100835161466, 0.129416393222737, 0.129416393222737, 0.129416393222737, 0.22436807062883227, 0.04907266618681019]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.3678798551547061, 0.05335187104445497, 0.1407207555195247, 0.1407207555195247, 0.2439748917173344, 0.05335187104445497]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.3678798551547061, 0.05335187104445497, 0.1407207555195247, 0.1407207555195247, 0.2439748917173344, 0.05335187104445497]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
printing an ep nov before normalisation:  20.51738977432251
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.47115737397342333, 0.06889442120119435, 0.06889442120119435, 0.25326494122179927, 0.06889442120119435, 0.06889442120119435]
Printing some Q and Qe and total Qs values:  [[1.254]
 [1.254]
 [1.254]
 [1.254]
 [1.254]
 [1.254]
 [1.292]] [[64.211]
 [64.211]
 [64.211]
 [64.211]
 [64.211]
 [64.211]
 [70.216]] [[2.087]
 [2.087]
 [2.087]
 [2.087]
 [2.087]
 [2.087]
 [2.292]]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.03506240061904287, 0.23246879969047854, 0.03506240061904287, 0.23246879969047854, 0.23246879969047854, 0.23246879969047854]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.03506240061904287, 0.23246879969047854, 0.03506240061904287, 0.23246879969047854, 0.23246879969047854, 0.23246879969047854]
siam score:  -0.4666375
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.04366216610508656, 0.28967116722824676, 0.04366216610508656, 0.04366216610508656, 0.28967116722824676, 0.28967116722824676]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.04366216610508656, 0.28967116722824676, 0.04366216610508656, 0.04366216610508656, 0.28967116722824676, 0.28967116722824676]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.04366216610508656, 0.28967116722824676, 0.04366216610508656, 0.04366216610508656, 0.28967116722824676, 0.28967116722824676]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.04366216610508656, 0.28967116722824676, 0.04366216610508656, 0.04366216610508656, 0.28967116722824676, 0.28967116722824676]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.04366216610508656, 0.28967116722824676, 0.04366216610508656, 0.04366216610508656, 0.28967116722824676, 0.28967116722824676]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.04366216610508656, 0.28967116722824676, 0.04366216610508656, 0.04366216610508656, 0.28967116722824676, 0.28967116722824676]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.04366216610508656, 0.28967116722824676, 0.04366216610508656, 0.04366216610508656, 0.28967116722824676, 0.28967116722824676]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.1606574437636297, 0.29586495908195837, 0.043477597154411934, 0.043477597154411934, 0.29586495908195837, 0.1606574437636297]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.21490699298079086, 0.39580821665875715, 0.05812593245988706, 0.05812593245988706, 0.21490699298079086, 0.05812593245988706]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.2643439776646478, 0.1441257487440557, 0.03893479843853733, 0.1441257487440557, 0.2643439776646478, 0.1441257487440557]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.2643439776646478, 0.1441257487440557, 0.03893479843853733, 0.1441257487440557, 0.2643439776646478, 0.1441257487440557]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
probs:  [0.2643439776646478, 0.1441257487440557, 0.03893479843853733, 0.1441257487440557, 0.2643439776646478, 0.1441257487440557]
maxi score, test score, baseline:  -0.9749 -1.0 -0.9749
maxi score, test score, baseline:  -0.975509756097561 -1.0 -0.975509756097561
probs:  [0.19409644713157734, 0.19409644713157734, 0.029517764342113194, 0.19409644713157734, 0.19409644713157734, 0.19409644713157734]
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.19409644778165128, 0.19409644778165128, 0.029517761091743682, 0.19409644778165128, 0.19409644778165128, 0.19409644778165128]
maxi score, test score, baseline:  -0.9760904761904762 -1.0 -0.9760904761904762
probs:  [0.2893804803146843, 0.04395285301864905, 0.04395285301864905, 0.2893804803146843, 0.2893804803146843, 0.04395285301864905]
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.14440402066419447, 0.038949381705115915, 0.14440402066419447, 0.2639192781511503, 0.2639192781511503, 0.14440402066419447]
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.14440402066419447, 0.038949381705115915, 0.14440402066419447, 0.2639192781511503, 0.2639192781511503, 0.14440402066419447]
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.14440402066419447, 0.038949381705115915, 0.14440402066419447, 0.2639192781511503, 0.2639192781511503, 0.14440402066419447]
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.14440402066419447, 0.038949381705115915, 0.14440402066419447, 0.2639192781511503, 0.2639192781511503, 0.14440402066419447]
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.14440402066419447, 0.038949381705115915, 0.14440402066419447, 0.2639192781511503, 0.2639192781511503, 0.14440402066419447]
actions average: 
K:  1  action  0 :  tensor([0.2949, 0.0191, 0.1305, 0.1538, 0.1142, 0.1811, 0.1065],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0454, 0.7877, 0.0262, 0.0507, 0.0262, 0.0377, 0.0261],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1690, 0.0064, 0.2200, 0.1808, 0.1139, 0.1908, 0.1192],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1215, 0.1517, 0.1541, 0.1819, 0.1306, 0.1659, 0.0942],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1600, 0.0992, 0.1298, 0.2071, 0.1519, 0.1184, 0.1336],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1399, 0.0479, 0.1190, 0.1086, 0.0980, 0.4151, 0.0715],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1559, 0.1075, 0.0938, 0.1017, 0.0674, 0.0572, 0.4165],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9766441860465116 -1.0 -0.9766441860465116
probs:  [0.14440402066419447, 0.038949381705115915, 0.14440402066419447, 0.2639192781511503, 0.2639192781511503, 0.14440402066419447]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.28923561180472646, 0.044097721528606894, 0.28923561180472646, 0.28923561180472646, 0.044097721528606894, 0.044097721528606894]
printing an ep nov before normalisation:  60.70641630312178
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.23534340011474622, 0.23534340011474622, 0.23534340011474622, 0.03488158356359492, 0.1295441080460832, 0.1295441080460832]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
from probs:  [0.23534340011474622, 0.23534340011474622, 0.23534340011474622, 0.03488158356359492, 0.1295441080460832, 0.1295441080460832]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
probs:  [0.23534340011474622, 0.23534340011474622, 0.23534340011474622, 0.03488158356359492, 0.1295441080460832, 0.1295441080460832]
maxi score, test score, baseline:  -0.9771727272727273 -1.0 -0.9771727272727273
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.23534340237174944, 0.23534340237174944, 0.23534340237174944, 0.034881579232588784, 0.12954410682608147, 0.12954410682608147]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.23534340237174944, 0.23534340237174944, 0.23534340237174944, 0.034881579232588784, 0.12954410682608147, 0.12954410682608147]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.25995882702559525, 0.25995882702559525, 0.25995882702559525, 0.03851809474506145, 0.03851809474506145, 0.14308732943309144]
UNIT TEST: sample policy line 217 mcts : [0.2 0.4 0.  0.2 0.2 0.  0. ]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.1867403262819085, 0.1867403262819085, 0.3393001393577462, 0.05023944089826415, 0.05023944089826415, 0.1867403262819085]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.21626628149809873, 0.21626628149809873, 0.3929649093350757, 0.05816750922290898, 0.05816750922290898, 0.05816750922290898]
siam score:  -0.48469055
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.14506615068433298, 0.26288714695160786, 0.26288714695160786, 0.14506615068433298, 0.03902725404378548, 0.14506615068433298]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.16227303114439087, 0.29408209681266345, 0.29408209681266345, 0.16227303114439087, 0.04364487204294559, 0.04364487204294559]
actions average: 
K:  1  action  0 :  tensor([0.3719, 0.0494, 0.0931, 0.0849, 0.1698, 0.1338, 0.0971],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0156, 0.8267, 0.0168, 0.0399, 0.0136, 0.0706, 0.0167],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1188, 0.0857, 0.1334, 0.1447, 0.1416, 0.2431, 0.1326],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1101, 0.1838, 0.0982, 0.1908, 0.0790, 0.2092, 0.1289],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1813, 0.0689, 0.0614, 0.1028, 0.2876, 0.1881, 0.1099],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1075, 0.0543, 0.1361, 0.1244, 0.0971, 0.3721, 0.1086],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0864, 0.0440, 0.2186, 0.0727, 0.0980, 0.1240, 0.3563],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.18691184107712155, 0.33875064915553393, 0.18691184107712155, 0.18691184107712155, 0.05025691380655062, 0.05025691380655062]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.23209554155931494, 0.23209554155931494, 0.23209554155931494, 0.23209554155931494, 0.035808916881370056, 0.035808916881370056]
printing an ep nov before normalisation:  55.490171483556146
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.05889610652189706, 0.05889610652189706, 0.3822077869562059, 0.3822077869562059, 0.05889610652189706, 0.05889610652189706]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.14524392901081537, 0.14524392901081537, 0.262603274429827, 0.262603274429827, 0.14524392901081537, 0.03906166410789969]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.14524392901081537, 0.14524392901081537, 0.262603274429827, 0.262603274429827, 0.14524392901081537, 0.03906166410789969]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.14524392901081537, 0.14524392901081537, 0.262603274429827, 0.262603274429827, 0.14524392901081537, 0.03906166410789969]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.14524392901081537, 0.14524392901081537, 0.262603274429827, 0.262603274429827, 0.14524392901081537, 0.03906166410789969]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.14524392901081537, 0.14524392901081537, 0.262603274429827, 0.262603274429827, 0.14524392901081537, 0.03906166410789969]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.14524392901081537, 0.14524392901081537, 0.262603274429827, 0.262603274429827, 0.14524392901081537, 0.03906166410789969]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.14524392901081537, 0.14524392901081537, 0.262603274429827, 0.262603274429827, 0.14524392901081537, 0.03906166410789969]
maxi score, test score, baseline:  -0.9781608695652174 -1.0 -0.9781608695652174
probs:  [0.14524392901081537, 0.14524392901081537, 0.262603274429827, 0.262603274429827, 0.14524392901081537, 0.03906166410789969]
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
probs:  [0.26234222351331477, 0.03909925753780281, 0.14540543181185583, 0.14540543181185583, 0.26234222351331477, 0.14540543181185583]
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
probs:  [0.23735674772408227, 0.03492606105966465, 0.16345348116405692, 0.16345348116405692, 0.23735674772408227, 0.16345348116405692]
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
probs:  [0.2563037301944082, 0.03770495591385139, 0.17649782847293508, 0.17649782847293508, 0.17649782847293508, 0.17649782847293508]
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
probs:  [0.1180459582848807, 0.031761320874950816, 0.2125481802100421, 0.2125481802100421, 0.2125481802100421, 0.2125481802100421]
maxi score, test score, baseline:  -0.9790666666666666 -1.0 -0.9790666666666666
probs:  [0.1180459582848807, 0.031761320874950816, 0.2125481802100421, 0.2125481802100421, 0.2125481802100421, 0.2125481802100421]
printing an ep nov before normalisation:  18.975908756256104
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.03874793228238608, 0.03874793228238608, 0.2594704543572232, 0.14409277236355822, 0.2594704543572232, 0.2594704543572232]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.03874793228238608, 0.03874793228238608, 0.2594704543572232, 0.14409277236355822, 0.2594704543572232, 0.2594704543572232]
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.03874793228238608, 0.03874793228238608, 0.2594704543572232, 0.14409277236355822, 0.2594704543572232, 0.2594704543572232]
siam score:  -0.48131108
maxi score, test score, baseline:  -0.9799 -1.0 -0.9799
probs:  [0.05034348854907438, 0.05034348854907438, 0.33734647007285384, 0.18732218427633246, 0.18732218427633246, 0.18732218427633246]
using another actor
from probs:  [0.058320243516106283, 0.058320243516106283, 0.3909185657457335, 0.058320243516106283, 0.21706035185297384, 0.21706035185297384]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.08541819095520475, 0.08541819095520475, 0.5729090452239762, 0.08541819095520475, 0.08541819095520475, 0.08541819095520475]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.08541819095520475, 0.08541819095520475, 0.5729090452239762, 0.08541819095520475, 0.08541819095520475, 0.08541819095520475]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.08541819095520475, 0.08541819095520475, 0.5729090452239762, 0.08541819095520475, 0.08541819095520475, 0.08541819095520475]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.08541819095520475, 0.08541819095520475, 0.5729090452239762, 0.08541819095520475, 0.08541819095520475, 0.08541819095520475]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.08541819095520475, 0.08541819095520475, 0.5729090452239762, 0.08541819095520475, 0.08541819095520475, 0.08541819095520475]
maxi score, test score, baseline:  -0.9806692307692307 -1.0 -0.9806692307692307
probs:  [0.08541819095520475, 0.08541819095520475, 0.5729090452239762, 0.08541819095520475, 0.08541819095520475, 0.08541819095520475]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.0530538849156271, 0.14661735224001246, 0.3604767061243228, 0.14661735224001246, 0.14661735224001246, 0.14661735224001246]
Printing some Q and Qe and total Qs values:  [[-0.03]
 [-0.03]
 [-0.03]
 [-0.03]
 [-0.03]
 [-0.03]
 [-0.03]] [[39.908]
 [39.908]
 [39.908]
 [39.908]
 [39.908]
 [39.908]
 [39.908]] [[1.919]
 [1.919]
 [1.919]
 [1.919]
 [1.919]
 [1.919]
 [1.919]]
maxi score, test score, baseline:  -0.9813814814814815 -1.0 -0.9813814814814815
probs:  [0.05836877569156881, 0.21720568245504557, 0.3904823080152025, 0.21720568245504557, 0.05836877569156881, 0.05836877569156881]
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
probs:  [0.044069866708171986, 0.2065747463184278, 0.29890706427880015, 0.2065747463184278, 0.12193678818808613, 0.12193678818808613]
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
probs:  [0.044069866708171986, 0.2065747463184278, 0.29890706427880015, 0.2065747463184278, 0.12193678818808613, 0.12193678818808613]
maxi score, test score, baseline:  -0.9817181818181818 -1.0 -0.9817181818181818
probs:  [0.044069866708171986, 0.2065747463184278, 0.29890706427880015, 0.2065747463184278, 0.12193678818808613, 0.12193678818808613]
printing an ep nov before normalisation:  29.144606590270996
printing an ep nov before normalisation:  10.730998516082764
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.03922704861793656, 0.1458129109859654, 0.2616671092120836, 0.2616671092120836, 0.1458129109859654, 0.1458129109859654]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.04389518634504507, 0.04389518634504507, 0.2928964985466436, 0.2928964985466436, 0.1632083151083113, 0.1632083151083113]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.04389518634504507, 0.04389518634504507, 0.2928964985466436, 0.2928964985466436, 0.1632083151083113, 0.1632083151083113]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.04389518634504507, 0.04389518634504507, 0.2928964985466436, 0.2928964985466436, 0.1632083151083113, 0.1632083151083113]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.04389518634504507, 0.04389518634504507, 0.2928964985466436, 0.2928964985466436, 0.1632083151083113, 0.1632083151083113]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.04389518634504507, 0.04389518634504507, 0.2928964985466436, 0.2928964985466436, 0.1632083151083113, 0.1632083151083113]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.04389518634504507, 0.04389518634504507, 0.2928964985466436, 0.2928964985466436, 0.1632083151083113, 0.1632083151083113]
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.556]
 [0.594]
 [0.585]
 [0.586]
 [0.585]
 [0.576]] [[27.585]
 [28.564]
 [27.867]
 [28.299]
 [28.877]
 [28.949]
 [29.387]] [[0.833]
 [0.833]
 [0.857]
 [0.857]
 [0.869]
 [0.869]
 [0.868]]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.04389518634504507, 0.04389518634504507, 0.2928964985466436, 0.2928964985466436, 0.1632083151083113, 0.1632083151083113]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.04389518634504507, 0.04389518634504507, 0.2928964985466436, 0.2928964985466436, 0.1632083151083113, 0.1632083151083113]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
Printing some Q and Qe and total Qs values:  [[-0.005]
 [-0.006]
 [-0.005]
 [-0.007]
 [-0.009]
 [-0.01 ]
 [-0.011]] [[28.492]
 [25.103]
 [32.638]
 [38.928]
 [41.419]
 [42.426]
 [43.206]] [[0.15 ]
 [0.117]
 [0.189]
 [0.247]
 [0.268]
 [0.277]
 [0.284]]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.04389518634504507, 0.04389518634504507, 0.2928964985466436, 0.2928964985466436, 0.1632083151083113, 0.1632083151083113]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.037475464073495286, 0.1037930807797232, 0.2537285620285868, 0.2537285620285868, 0.1756371655448039, 0.1756371655448039]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.04064132901559215, 0.11258133561682608, 0.2752283070630937, 0.19051634276816265, 0.19051634276816265, 0.19051634276816265]
from probs:  [0.04064132901559215, 0.11258133561682608, 0.2752283070630937, 0.19051634276816265, 0.19051634276816265, 0.19051634276816265]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.03192186437209943, 0.1185435229900355, 0.21238365315946625, 0.21238365315946625, 0.21238365315946625, 0.21238365315946625]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.03192186437209943, 0.1185435229900355, 0.21238365315946625, 0.21238365315946625, 0.21238365315946625, 0.21238365315946625]
printing an ep nov before normalisation:  62.37621079689886
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.03192186437209943, 0.1185435229900355, 0.21238365315946625, 0.21238365315946625, 0.21238365315946625, 0.21238365315946625]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.03192186437209943, 0.1185435229900355, 0.21238365315946625, 0.21238365315946625, 0.21238365315946625, 0.21238365315946625]
printing an ep nov before normalisation:  47.76067611595262
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.03192186437209943, 0.1185435229900355, 0.21238365315946625, 0.21238365315946625, 0.21238365315946625, 0.21238365315946625]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.03192186437209943, 0.1185435229900355, 0.21238365315946625, 0.21238365315946625, 0.21238365315946625, 0.21238365315946625]
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
maxi score, test score, baseline:  -0.9820428571428571 -1.0 -0.9820428571428571
probs:  [0.03192186437209943, 0.1185435229900355, 0.21238365315946625, 0.21238365315946625, 0.21238365315946625, 0.21238365315946625]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.371]
 [0.371]
 [0.279]
 [0.371]
 [0.315]
 [0.341]] [[19.058]
 [19.058]
 [19.058]
 [19.182]
 [19.058]
 [19.488]
 [19.561]] [[0.371]
 [0.371]
 [0.371]
 [0.279]
 [0.371]
 [0.315]
 [0.341]]
printing an ep nov before normalisation:  16.729909801114598
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.031921861964073664, 0.11854352213002642, 0.21238365397647496, 0.21238365397647496, 0.21238365397647496, 0.21238365397647496]
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.031921861964073664, 0.11854352213002642, 0.21238365397647496, 0.21238365397647496, 0.21238365397647496, 0.21238365397647496]
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.031921861964073664, 0.11854352213002642, 0.21238365397647496, 0.21238365397647496, 0.21238365397647496, 0.21238365397647496]
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.031921861964073664, 0.11854352213002642, 0.21238365397647496, 0.21238365397647496, 0.21238365397647496, 0.21238365397647496]
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.031921861964073664, 0.11854352213002642, 0.21238365397647496, 0.21238365397647496, 0.21238365397647496, 0.21238365397647496]
maxi score, test score, baseline:  -0.9823561403508771 -1.0 -0.9823561403508771
probs:  [0.031921861964073664, 0.11854352213002642, 0.21238365397647496, 0.21238365397647496, 0.21238365397647496, 0.21238365397647496]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
printing an ep nov before normalisation:  78.8660305981696
using explorer policy with actor:  1
using explorer policy with actor:  0
printing an ep nov before normalisation:  45.4259353701223
printing an ep nov before normalisation:  57.637544342154
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.04395068956961029, 0.16335001863701643, 0.2926992917933733, 0.04395068956961029, 0.2926992917933733, 0.16335001863701643]
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.317]
 [0.319]
 [0.344]
 [0.355]
 [0.352]
 [0.351]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.348]
 [0.317]
 [0.319]
 [0.344]
 [0.355]
 [0.352]
 [0.351]]
printing an ep nov before normalisation:  34.03888940811157
printing an ep nov before normalisation:  30.066432440163066
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.04395068956961029, 0.16335001863701643, 0.2926992917933733, 0.04395068956961029, 0.2926992917933733, 0.16335001863701643]
maxi score, test score, baseline:  -0.9826586206896551 -1.0 -0.9826586206896551
probs:  [0.04395068956961029, 0.16335001863701643, 0.2926992917933733, 0.04395068956961029, 0.2926992917933733, 0.16335001863701643]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
using explorer policy with actor:  0
printing an ep nov before normalisation:  24.27979768273984
Printing some Q and Qe and total Qs values:  [[0.772]
 [0.938]
 [0.956]
 [0.772]
 [0.986]
 [0.975]
 [0.772]] [[47.835]
 [65.632]
 [67.579]
 [47.835]
 [63.614]
 [59.035]
 [47.835]] [[1.078]
 [1.462]
 [1.503]
 [1.078]
 [1.485]
 [1.418]
 [1.078]]
printing an ep nov before normalisation:  15.879720361591321
siam score:  -0.5360401
printing an ep nov before normalisation:  56.66804432706208
printing an ep nov before normalisation:  82.00539112511115
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.04395068403675284, 0.1633500184874796, 0.29269929747576756, 0.04395068403675284, 0.29269929747576756, 0.1633500184874796]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.04395068403675284, 0.1633500184874796, 0.29269929747576756, 0.04395068403675284, 0.29269929747576756, 0.1633500184874796]
maxi score, test score, baseline:  -0.9832333333333333 -1.0 -0.9832333333333333
probs:  [0.04395068403675284, 0.1633500184874796, 0.29269929747576756, 0.04395068403675284, 0.29269929747576756, 0.1633500184874796]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.0439506814097788, 0.16335001841648064, 0.2926993001737406, 0.0439506814097788, 0.2926993001737406, 0.16335001841648064]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.0439506814097788, 0.16335001841648064, 0.2926993001737406, 0.0439506814097788, 0.2926993001737406, 0.16335001841648064]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.049897222999011046, 0.049897222999011046, 0.3324039415497908, 0.049897222999011046, 0.3324039415497908, 0.18550044790338532]
maxi score, test score, baseline:  -0.9835065573770492 -1.0 -0.9835065573770492
probs:  [0.049897222999011046, 0.049897222999011046, 0.3324039415497908, 0.049897222999011046, 0.3324039415497908, 0.18550044790338532]
Printing some Q and Qe and total Qs values:  [[1.098]
 [1.098]
 [1.098]
 [1.098]
 [1.098]
 [1.098]
 [1.098]] [[64.81]
 [64.81]
 [64.81]
 [64.81]
 [64.81]
 [64.81]
 [64.81]] [[2.045]
 [2.045]
 [2.045]
 [2.045]
 [2.045]
 [2.045]
 [2.045]]
Printing some Q and Qe and total Qs values:  [[1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]
 [1.008]] [[41.176]
 [41.176]
 [41.176]
 [41.176]
 [41.176]
 [41.176]
 [41.176]] [[1.787]
 [1.787]
 [1.787]
 [1.787]
 [1.787]
 [1.787]
 [1.787]]
using another actor
main train batch thing paused
add a thread
Adding thread: now have 4 threads
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]
 [0.473]] [[54.56]
 [54.56]
 [54.56]
 [54.56]
 [54.56]
 [54.56]
 [54.56]] [[1.773]
 [1.773]
 [1.773]
 [1.773]
 [1.773]
 [1.773]
 [1.773]]
printing an ep nov before normalisation:  57.564291880716
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.16348071397906697, 0.04400748819408037, 0.16348071397906697, 0.04400748819408037, 0.2925117978268527, 0.2925117978268527]
printing an ep nov before normalisation:  16.795752904738485
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.16348071397906697, 0.04400748819408037, 0.16348071397906697, 0.04400748819408037, 0.2925117978268527, 0.2925117978268527]
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.16348071397906697, 0.04400748819408037, 0.16348071397906697, 0.04400748819408037, 0.2925117978268527, 0.2925117978268527]
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.16348071397906697, 0.04400748819408037, 0.16348071397906697, 0.04400748819408037, 0.2925117978268527, 0.2925117978268527]
Printing some Q and Qe and total Qs values:  [[1.005]
 [1.01 ]
 [1.037]
 [1.05 ]
 [1.031]
 [1.041]
 [1.039]] [[44.126]
 [44.797]
 [45.268]
 [46.507]
 [46.956]
 [49.087]
 [49.371]] [[1.714]
 [1.742]
 [1.786]
 [1.842]
 [1.839]
 [1.923]
 [1.931]]
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.049965924252541656, 0.049965924252541656, 0.18566446194338426, 0.049965924252541656, 0.33221888264949534, 0.33221888264949534]
siam score:  -0.53894216
maxi score, test score, baseline:  -0.9837709677419355 -1.0 -0.9837709677419355
probs:  [0.049965924252541656, 0.049965924252541656, 0.18566446194338426, 0.049965924252541656, 0.33221888264949534, 0.33221888264949534]
printing an ep nov before normalisation:  25.525221292796427
main train batch thing paused
add a thread
Adding thread: now have 5 threads
printing an ep nov before normalisation:  76.44640588019058
printing an ep nov before normalisation:  40.008573161639454
Printing some Q and Qe and total Qs values:  [[ 0.209]
 [ 0.209]
 [-0.013]
 [-0.026]
 [-0.029]
 [ 0.209]
 [ 0.209]] [[45.209]
 [45.209]
 [39.007]
 [35.339]
 [34.363]
 [45.209]
 [45.209]] [[2.209]
 [2.209]
 [1.707]
 [1.527]
 [1.48 ]
 [2.209]
 [2.209]]
printing an ep nov before normalisation:  26.446341174568033
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.335]
 [0.416]
 [0.449]
 [0.464]
 [0.45 ]
 [0.443]] [[36.423]
 [38.037]
 [34.927]
 [27.164]
 [27.392]
 [31.117]
 [32.938]] [[0.445]
 [0.335]
 [0.416]
 [0.449]
 [0.464]
 [0.45 ]
 [0.443]]
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.048126643872332384, 0.13364386342429402, 0.13364386342429402, 0.13364386342429402, 0.32520243522068715, 0.2257393306340984]
siam score:  -0.538532
printing an ep nov before normalisation:  13.359416723251343
printing an ep nov before normalisation:  102.51739031294042
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.058038334513724706, 0.058038334513724706, 0.058038334513724706, 0.16121673085967564, 0.39233633867460477, 0.27233192692454555]
siam score:  -0.53146386
printing an ep nov before normalisation:  103.4539543581608
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
printing an ep nov before normalisation:  49.39402541391813
printing an ep nov before normalisation:  35.9527150823691
printing an ep nov before normalisation:  5.85720632001923
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.058038334513724706, 0.058038334513724706, 0.058038334513724706, 0.16121673085967564, 0.39233633867460477, 0.27233192692454555]
actions average: 
K:  3  action  0 :  tensor([0.2269, 0.1496, 0.0994, 0.1030, 0.1101, 0.1525, 0.1585],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.1054, 0.5929, 0.0562, 0.0519, 0.0598, 0.0496, 0.0842],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1331, 0.1388, 0.2042, 0.1168, 0.1241, 0.1479, 0.1352],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1298, 0.1443, 0.1493, 0.1274, 0.1160, 0.1516, 0.1816],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2167, 0.0429, 0.0660, 0.0996, 0.3162, 0.1140, 0.1447],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1324, 0.0492, 0.1725, 0.1098, 0.0971, 0.3408, 0.0982],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1527, 0.0454, 0.1345, 0.1253, 0.1317, 0.1315, 0.2790],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
probs:  [0.058038334513724706, 0.058038334513724706, 0.058038334513724706, 0.16121673085967564, 0.39233633867460477, 0.27233192692454555]
maxi score, test score, baseline:  -0.9840269841269841 -1.0 -0.9840269841269841
printing an ep nov before normalisation:  42.43718729722437
main train batch thing paused
add a thread
Adding thread: now have 6 threads
printing an ep nov before normalisation:  30.680416675322782
printing an ep nov before normalisation:  42.1309100073497
printing an ep nov before normalisation:  48.18363355782436
using explorer policy with actor:  0
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  17.068637081158545
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
actions average: 
K:  2  action  0 :  tensor([0.5538, 0.0052, 0.0811, 0.0647, 0.1353, 0.0603, 0.0996],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0475, 0.7087, 0.0223, 0.0769, 0.0337, 0.0656, 0.0453],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0442, 0.1747, 0.4934, 0.0665, 0.0589, 0.1017, 0.0608],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1268, 0.3225, 0.1075, 0.1674, 0.0377, 0.1147, 0.1234],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2452, 0.0454, 0.1014, 0.1458, 0.2049, 0.1416, 0.1156],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1234, 0.0279, 0.1331, 0.1458, 0.0823, 0.3547, 0.1328],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1064, 0.0738, 0.1296, 0.1526, 0.1109, 0.1280, 0.2987],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  32.773637771606445
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.040427678067543854, 0.11232040656666556, 0.11232040656666556, 0.1895385223620189, 0.27269649321855305, 0.27269649321855305]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.040427678067543854, 0.11232040656666556, 0.11232040656666556, 0.1895385223620189, 0.27269649321855305, 0.27269649321855305]
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.040427678067543854, 0.11232040656666556, 0.11232040656666556, 0.1895385223620189, 0.27269649321855305, 0.27269649321855305]
printing an ep nov before normalisation:  69.27140154195288
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.035375915783455884, 0.13118267994147467, 0.13118267994147467, 0.23408624144453158, 0.23408624144453158, 0.23408624144453158]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
printing an ep nov before normalisation:  55.606612307130156
maxi score, test score, baseline:  -0.984275 -1.0 -0.984275
probs:  [0.035375915783455884, 0.13118267994147467, 0.13118267994147467, 0.23408624144453158, 0.23408624144453158, 0.23408624144453158]
printing an ep nov before normalisation:  16.620185830789325
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.035375913312552285, 0.13118267927366276, 0.13118267927366276, 0.23408624271337403, 0.23408624271337403, 0.23408624271337403]
printing an ep nov before normalisation:  30.790608681787774
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.035375913312552285, 0.13118267927366276, 0.13118267927366276, 0.23408624271337403, 0.23408624271337403, 0.23408624271337403]
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.035375913312552285, 0.13118267927366276, 0.13118267927366276, 0.23408624271337403, 0.23408624271337403, 0.23408624271337403]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.5329162
Printing some Q and Qe and total Qs values:  [[0.733]
 [0.737]
 [0.743]
 [0.732]
 [0.743]
 [0.743]
 [0.743]] [[67.191]
 [68.01 ]
 [65.366]
 [69.609]
 [65.366]
 [65.366]
 [65.366]] [[1.896]
 [1.927]
 [1.845]
 [1.976]
 [1.845]
 [1.845]
 [1.845]]
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.03942211420864854, 0.1462284224894719, 0.1462284224894719, 0.2609463091614678, 0.2609463091614678, 0.1462284224894719]
Printing some Q and Qe and total Qs values:  [[0.937]
 [0.883]
 [0.925]
 [0.949]
 [0.945]
 [0.944]
 [0.941]] [[48.389]
 [51.1  ]
 [44.981]
 [41.774]
 [41.368]
 [44.393]
 [44.194]] [[1.463]
 [1.465]
 [1.381]
 [1.34 ]
 [1.327]
 [1.388]
 [1.381]]
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.03942211420864854, 0.1462284224894719, 0.1462284224894719, 0.2609463091614678, 0.2609463091614678, 0.1462284224894719]
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.03942211420864854, 0.1462284224894719, 0.1462284224894719, 0.2609463091614678, 0.2609463091614678, 0.1462284224894719]
printing an ep nov before normalisation:  15.68927526473999
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.03942211420864854, 0.1462284224894719, 0.1462284224894719, 0.2609463091614678, 0.2609463091614678, 0.1462284224894719]
Printing some Q and Qe and total Qs values:  [[1.117]
 [1.114]
 [1.113]
 [1.112]
 [1.113]
 [1.116]
 [1.117]] [[74.772]
 [75.323]
 [78.513]
 [77.161]
 [78.513]
 [78.111]
 [78.48 ]] [[1.416]
 [1.417]
 [1.442]
 [1.43 ]
 [1.442]
 [1.442]
 [1.446]]
Printing some Q and Qe and total Qs values:  [[0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]] [[17.208]
 [17.208]
 [17.208]
 [17.208]
 [17.208]
 [17.208]
 [17.208]] [[1.482]
 [1.482]
 [1.482]
 [1.482]
 [1.482]
 [1.482]
 [1.482]]
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.060387777379977386, 0.060387777379977386, 0.3792244452400452, 0.3792244452400452, 0.060387777379977386, 0.060387777379977386]
actions average: 
K:  3  action  0 :  tensor([0.3693, 0.0223, 0.1002, 0.0931, 0.2093, 0.1189, 0.0869],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.1246, 0.5574, 0.0466, 0.0838, 0.0470, 0.0656, 0.0750],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0882, 0.0302, 0.3349, 0.1166, 0.0866, 0.1452, 0.1983],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1554, 0.1407, 0.1057, 0.2728, 0.1296, 0.0944, 0.1014],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1514, 0.0620, 0.1267, 0.1572, 0.2321, 0.1323, 0.1384],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0860, 0.0503, 0.1639, 0.1333, 0.0965, 0.3570, 0.1130],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2192, 0.0479, 0.1603, 0.1370, 0.1512, 0.1328, 0.1517],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  62.1149865686935
printing an ep nov before normalisation:  38.73471107835407
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
printing an ep nov before normalisation:  62.74073416539494
printing an ep nov before normalisation:  48.76223261185452
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.08862466373313409, 0.08862466373313409, 0.5568766813343295, 0.08862466373313409, 0.08862466373313409, 0.08862466373313409]
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.08862466373313409, 0.08862466373313409, 0.5568766813343295, 0.08862466373313409, 0.08862466373313409, 0.08862466373313409]
printing an ep nov before normalisation:  64.74016004659619
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.08862466373313409, 0.08862466373313409, 0.5568766813343295, 0.08862466373313409, 0.08862466373313409, 0.08862466373313409]
printing an ep nov before normalisation:  35.217595685798976
printing an ep nov before normalisation:  42.23681926727295
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.08862466373313409, 0.08862466373313409, 0.5568766813343295, 0.08862466373313409, 0.08862466373313409, 0.08862466373313409]
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.08862466373313409, 0.08862466373313409, 0.5568766813343295, 0.08862466373313409, 0.08862466373313409, 0.08862466373313409]
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.08862466373313409, 0.08862466373313409, 0.5568766813343295, 0.08862466373313409, 0.08862466373313409, 0.08862466373313409]
printing an ep nov before normalisation:  68.67005351957167
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.08862466373313409, 0.08862466373313409, 0.5568766813343295, 0.08862466373313409, 0.08862466373313409, 0.08862466373313409]
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.82049948217464
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]
 [0.339]] [[13.512]
 [13.512]
 [13.512]
 [13.512]
 [13.512]
 [13.512]
 [13.512]] [[2.158]
 [2.158]
 [2.158]
 [2.158]
 [2.158]
 [2.158]
 [2.158]]
actions average: 
K:  3  action  0 :  tensor([0.4317, 0.0470, 0.0771, 0.0912, 0.1440, 0.0862, 0.1228],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0619, 0.7416, 0.0208, 0.0533, 0.0342, 0.0319, 0.0563],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1526, 0.0533, 0.2587, 0.1332, 0.1158, 0.1575, 0.1290],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1138, 0.0977, 0.1061, 0.2359, 0.1451, 0.1300, 0.1715],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2048, 0.1154, 0.1170, 0.1387, 0.1812, 0.0788, 0.1641],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1307, 0.0400, 0.0726, 0.1085, 0.0769, 0.4618, 0.1094],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1350, 0.1465, 0.1306, 0.1005, 0.1475, 0.1017, 0.2381],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.19379503277813642, 0.19379503277813642, 0.031024836109317888, 0.19379503277813642, 0.19379503277813642, 0.19379503277813642]
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.19379503277813642, 0.19379503277813642, 0.031024836109317888, 0.19379503277813642, 0.19379503277813642, 0.19379503277813642]
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.19379503277813642, 0.19379503277813642, 0.031024836109317888, 0.19379503277813642, 0.19379503277813642, 0.19379503277813642]
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.19379503277813642, 0.19379503277813642, 0.031024836109317888, 0.19379503277813642, 0.19379503277813642, 0.19379503277813642]
printing an ep nov before normalisation:  41.61105781556443
printing an ep nov before normalisation:  82.59300843606471
actions average: 
K:  2  action  0 :  tensor([0.3869, 0.0289, 0.1111, 0.1150, 0.0890, 0.0964, 0.1727],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0450, 0.7129, 0.0284, 0.1091, 0.0257, 0.0367, 0.0421],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1606, 0.0509, 0.2420, 0.1727, 0.1031, 0.1193, 0.1513],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1245, 0.0580, 0.1199, 0.3681, 0.0842, 0.1057, 0.1396],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2657, 0.0944, 0.0746, 0.1071, 0.2070, 0.1302, 0.1209],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1370, 0.1351, 0.0897, 0.1366, 0.0856, 0.3400, 0.0760],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1649, 0.0640, 0.1413, 0.1587, 0.1305, 0.1389, 0.2017],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.89437389373779
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9845153846153847 -1.0 -0.9845153846153847
probs:  [0.13139093654853362, 0.23391102720435789, 0.03548504528985907, 0.23391102720435789, 0.23391102720435789, 0.13139093654853362]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.044244958808488506, 0.16391561705187657, 0.044244958808488506, 0.29183942413963493, 0.29183942413963493, 0.16391561705187657]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.044244958808488506, 0.16391561705187657, 0.044244958808488506, 0.29183942413963493, 0.29183942413963493, 0.16391561705187657]
printing an ep nov before normalisation:  29.800548726372345
siam score:  -0.573177
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.044244958808488506, 0.16391561705187657, 0.044244958808488506, 0.29183942413963493, 0.29183942413963493, 0.16391561705187657]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.044244958808488506, 0.16391561705187657, 0.044244958808488506, 0.29183942413963493, 0.29183942413963493, 0.16391561705187657]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.05072159228715195, 0.18796270073637358, 0.05072159228715195, 0.18796270073637358, 0.3346687132165753, 0.18796270073637358]
using explorer policy with actor:  0
printing an ep nov before normalisation:  73.88957517802675
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.04816517133516572, 0.13410262696595696, 0.13410262696595696, 0.13410262696595696, 0.3237577014614957, 0.22576924630546769]
printing an ep nov before normalisation:  53.23047888599179
Printing some Q and Qe and total Qs values:  [[1.059]
 [1.058]
 [1.067]
 [1.075]
 [1.079]
 [1.082]
 [1.086]] [[53.743]
 [55.23 ]
 [56.14 ]
 [56.101]
 [54.749]
 [53.544]
 [54.213]] [[2.716]
 [2.806]
 [2.871]
 [2.876]
 [2.798]
 [2.726]
 [2.771]]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.04816517133516572, 0.13410262696595696, 0.13410262696595696, 0.13410262696595696, 0.3237577014614957, 0.22576924630546769]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.04816517133516572, 0.13410262696595696, 0.13410262696595696, 0.13410262696595696, 0.3237577014614957, 0.22576924630546769]
Printing some Q and Qe and total Qs values:  [[0.706]
 [0.716]
 [0.765]
 [0.774]
 [0.763]
 [0.778]
 [0.778]] [[24.145]
 [39.503]
 [21.537]
 [20.648]
 [22.629]
 [19.019]
 [19.093]] [[0.706]
 [0.716]
 [0.765]
 [0.774]
 [0.763]
 [0.778]
 [0.778]]
from probs:  [0.05268494523982125, 0.1467090580476806, 0.1467090580476806, 0.05268494523982125, 0.3542105483822659, 0.24700144504273033]
maxi score, test score, baseline:  -0.9847484848484849 -1.0 -0.9847484848484849
probs:  [0.050318334697480975, 0.050318334697480975, 0.18630989154458075, 0.050318334697480975, 0.33136755218148817, 0.33136755218148817]
line 256 mcts: sample exp_bonus 0.0
siam score:  -0.5668704
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
maxi score, test score, baseline:  -0.9849746268656716 -1.0 -0.9849746268656716
probs:  [0.11285271769957624, 0.11285271769957624, 0.189840881684287, 0.04053050304727233, 0.2719615899346442, 0.2719615899346442]
printing an ep nov before normalisation:  64.6186379894923
siam score:  -0.56082815
printing an ep nov before normalisation:  30.498541199169118
printing an ep nov before normalisation:  30.159080799071045
printing an ep nov before normalisation:  34.52669873480353
printing an ep nov before normalisation:  61.822967529296875
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.495]
 [0.495]
 [0.491]
 [0.508]
 [0.503]
 [0.495]] [[19.961]
 [19.961]
 [19.961]
 [28.378]
 [23.023]
 [23.23 ]
 [19.961]] [[1.017]
 [1.017]
 [1.017]
 [1.416]
 [1.177]
 [1.182]
 [1.017]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
siam score:  -0.551606
printing an ep nov before normalisation:  54.45884043963569
printing an ep nov before normalisation:  50.21987413244623
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.04791613718631901, 0.1334606704293266, 0.22452420581704546, 0.04791613718631901, 0.32165864356394436, 0.22452420581704546]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.04791613718631901, 0.1334606704293266, 0.22452420581704546, 0.04791613718631901, 0.32165864356394436, 0.22452420581704546]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.09564102525944093, 0.15332404733003416, 0.21472855469550436, 0.04135112213417706, 0.28022669588533905, 0.21472855469550436]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.09749215341084941, 0.16387449359746353, 0.23453956540902082, 0.0350146567646246, 0.23453956540902082, 0.23453956540902082]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.09749215341084941, 0.16387449359746353, 0.23453956540902082, 0.0350146567646246, 0.23453956540902082, 0.23453956540902082]
printing an ep nov before normalisation:  35.46476364135742
printing an ep nov before normalisation:  79.70107469540227
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.54107427675259
printing an ep nov before normalisation:  48.573376257179454
Printing some Q and Qe and total Qs values:  [[1.033]
 [0.909]
 [0.909]
 [0.952]
 [0.909]
 [0.921]
 [0.909]] [[34.473]
 [36.703]
 [36.703]
 [33.794]
 [36.703]
 [34.64 ]
 [36.703]] [[1.972]
 [1.965]
 [1.965]
 [1.855]
 [1.965]
 [1.869]
 [1.965]]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.1049015362964849, 0.17633582387165478, 0.17633582387165478, 0.03766926563750146, 0.252378775161352, 0.252378775161352]
using explorer policy with actor:  1
siam score:  -0.55044585
line 256 mcts: sample exp_bonus 18.393662095069885
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.11353158028706858, 0.19085009990616153, 0.19085009990616153, 0.04076120888086332, 0.2731569111135835, 0.19085009990616153]
printing an ep nov before normalisation:  46.88160739301679
Printing some Q and Qe and total Qs values:  [[0.515]
 [0.515]
 [0.447]
 [0.515]
 [0.512]
 [0.549]
 [0.555]] [[18.365]
 [18.365]
 [13.42 ]
 [18.365]
 [13.582]
 [13.73 ]
 [14.627]] [[0.968]
 [0.968]
 [0.706]
 [0.968]
 [0.778]
 [0.82 ]
 [0.862]]
Printing some Q and Qe and total Qs values:  [[0.269]
 [0.258]
 [0.25 ]
 [0.252]
 [0.243]
 [0.238]
 [0.241]] [[24.357]
 [20.497]
 [19.93 ]
 [19.586]
 [19.373]
 [19.295]
 [19.315]] [[1.648]
 [1.262]
 [1.199]
 [1.167]
 [1.138]
 [1.125]
 [1.13 ]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  255000
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.11353158028706858, 0.19085009990616153, 0.19085009990616153, 0.04076120888086332, 0.2731569111135835, 0.19085009990616153]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.12304233294579507, 0.20684557913376347, 0.12304233294579507, 0.044168689474765314, 0.29605548636611767, 0.20684557913376347]
printing an ep nov before normalisation:  77.13889598846436
Printing some Q and Qe and total Qs values:  [[1.148]
 [1.148]
 [1.148]
 [1.215]
 [1.148]
 [1.148]
 [1.148]] [[52.003]
 [52.003]
 [52.003]
 [54.817]
 [52.003]
 [52.003]
 [52.003]] [[2.635]
 [2.635]
 [2.635]
 [2.859]
 [2.635]
 [2.635]
 [2.635]]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
actions average: 
K:  2  action  0 :  tensor([0.3566, 0.0803, 0.0872, 0.0851, 0.1332, 0.1486, 0.1090],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0395, 0.7055, 0.0407, 0.0696, 0.0340, 0.0535, 0.0573],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0821, 0.0418, 0.5340, 0.0846, 0.0692, 0.1084, 0.0799],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2515, 0.0937, 0.1066, 0.1959, 0.1181, 0.1135, 0.1208],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2022, 0.0140, 0.1440, 0.1535, 0.1500, 0.1727, 0.1635],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1087, 0.0624, 0.0819, 0.0876, 0.0989, 0.4654, 0.0951],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1592, 0.0951, 0.1424, 0.1448, 0.1858, 0.1103, 0.1625],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  60.63627053395528
printing an ep nov before normalisation:  42.53821850879232
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.813]
 [0.816]
 [0.816]] [[51.643]
 [51.643]
 [51.643]
 [51.643]
 [51.237]
 [51.643]
 [51.643]] [[2.676]
 [2.676]
 [2.676]
 [2.676]
 [2.643]
 [2.676]
 [2.676]]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.14781446998132505, 0.14781446998132505, 0.14781446998132505, 0.053043967725283456, 0.35569815234941643, 0.14781446998132505]
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.174]
 [0.141]
 [0.16 ]
 [0.173]
 [0.174]
 [0.184]] [[21.554]
 [16.427]
 [20.968]
 [21.073]
 [20.914]
 [16.427]
 [21.521]] [[0.465]
 [0.356]
 [0.412]
 [0.433]
 [0.443]
 [0.356]
 [0.466]]
printing an ep nov before normalisation:  26.518348709619644
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
siam score:  -0.5485692
using explorer policy with actor:  0
printing an ep nov before normalisation:  74.0061379059373
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.19370997141397028, 0.19370997141397028, 0.19370997141397028, 0.03145014293014869, 0.19370997141397028, 0.19370997141397028]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.19370997141397028, 0.19370997141397028, 0.19370997141397028, 0.03145014293014869, 0.19370997141397028, 0.19370997141397028]
actions average: 
K:  3  action  0 :  tensor([0.1509, 0.1291, 0.1743, 0.1137, 0.1495, 0.1451, 0.1373],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0743, 0.4421, 0.0724, 0.1309, 0.0650, 0.0793, 0.1360],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0966, 0.0228, 0.3710, 0.1036, 0.1052, 0.1323, 0.1684],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0846, 0.1715, 0.1222, 0.2559, 0.0878, 0.1105, 0.1676],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1384, 0.0657, 0.1320, 0.1388, 0.1832, 0.1959, 0.1461],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0876, 0.0767, 0.1103, 0.1420, 0.0878, 0.3879, 0.1078],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1561, 0.0102, 0.1434, 0.1199, 0.1172, 0.1581, 0.2950],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
printing an ep nov before normalisation:  24.27751365544255
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.308]
 [0.317]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.319]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.308]
 [0.317]]
printing an ep nov before normalisation:  41.36051637533371
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.23123918372423807, 0.23123918372423807, 0.23123918372423807, 0.03752163255152389, 0.23123918372423807, 0.03752163255152389]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.23123918372423807, 0.23123918372423807, 0.23123918372423807, 0.03752163255152389, 0.23123918372423807, 0.03752163255152389]
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.125]
 [0.102]
 [0.25 ]
 [0.313]
 [0.06 ]
 [0.23 ]] [[91.564]
 [99.598]
 [89.404]
 [99.728]
 [97.626]
 [88.012]
 [93.017]] [[0.171]
 [0.125]
 [0.102]
 [0.25 ]
 [0.313]
 [0.06 ]
 [0.23 ]]
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.23123918372423807, 0.23123918372423807, 0.23123918372423807, 0.03752163255152389, 0.23123918372423807, 0.03752163255152389]
printing an ep nov before normalisation:  73.2645276493286
maxi score, test score, baseline:  -0.9851941176470589 -1.0 -0.9851941176470589
probs:  [0.13174447248457524, 0.23360087218234166, 0.23360087218234166, 0.13174447248457524, 0.23360087218234166, 0.03570843848382457]
printing an ep nov before normalisation:  23.66579138467955
printing an ep nov before normalisation:  79.65380112120157
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.13174447163090652, 0.2336008738185406, 0.2336008738185406, 0.13174447163090652, 0.2336008738185406, 0.035708435282565114]
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.68165969848633
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.14668349651519397, 0.26010148926679716, 0.26010148926679716, 0.14668349651519397, 0.14668349651519397, 0.039746531920823856]
printing an ep nov before normalisation:  58.93981456756592
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.044494149392548074, 0.29125844170858917, 0.29125844170858917, 0.16424740889886275, 0.16424740889886275, 0.044494149392548074]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.044494149392548074, 0.29125844170858917, 0.29125844170858917, 0.16424740889886275, 0.16424740889886275, 0.044494149392548074]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.044494149392548074, 0.29125844170858917, 0.29125844170858917, 0.16424740889886275, 0.16424740889886275, 0.044494149392548074]
printing an ep nov before normalisation:  39.14732974085311
printing an ep nov before normalisation:  57.83744265430785
using explorer policy with actor:  1
printing an ep nov before normalisation:  24.39434028545506
Printing some Q and Qe and total Qs values:  [[0.506]
 [0.744]
 [0.745]
 [0.767]
 [0.777]
 [0.745]
 [0.703]] [[50.375]
 [50.36 ]
 [51.05 ]
 [50.826]
 [49.647]
 [48.866]
 [50.783]] [[2.027]
 [2.264]
 [2.305]
 [2.314]
 [2.255]
 [2.179]
 [2.247]]
maxi score, test score, baseline:  -0.9854072463768117 -1.0 -0.9854072463768117
probs:  [0.05053472123554957, 0.3309006013296072, 0.3309006013296072, 0.05053472123554957, 0.1865946336341369, 0.05053472123554957]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.08966971990796754, 0.5516514004601621, 0.08966971990796754, 0.08966971990796754, 0.08966971990796754, 0.08966971990796754]
siam score:  -0.5495057
Printing some Q and Qe and total Qs values:  [[1.002]
 [1.07 ]
 [1.07 ]
 [1.07 ]
 [1.069]
 [1.07 ]
 [1.07 ]] [[41.166]
 [21.467]
 [21.467]
 [21.467]
 [17.197]
 [21.467]
 [21.467]] [[1.443]
 [1.206]
 [1.206]
 [1.206]
 [1.138]
 [1.206]
 [1.206]]
printing an ep nov before normalisation:  58.119800211689025
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
printing an ep nov before normalisation:  43.19164538286805
printing an ep nov before normalisation:  72.11514072815729
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.16548420573629063, 0.29318998621689685, 0.16548420573629063, 0.16548420573629063, 0.16548420573629063, 0.04487319083794062]
printing an ep nov before normalisation:  0.0035958630633103894
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.18818325524879173, 0.3334202281781339, 0.18818325524879173, 0.05101500303774545, 0.18818325524879173, 0.05101500303774545]
maxi score, test score, baseline:  -0.9856142857142858 -1.0 -0.9856142857142858
probs:  [0.18818325524879173, 0.3334202281781339, 0.18818325524879173, 0.05101500303774545, 0.18818325524879173, 0.05101500303774545]
printing an ep nov before normalisation:  35.38136959075928
printing an ep nov before normalisation:  1.49243066971394e-05
using explorer policy with actor:  0
printing an ep nov before normalisation:  22.022500038146973
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
maxi score, test score, baseline:  -0.9858154929577465 -1.0 -0.9858154929577465
probs:  [0.20688478433734614, 0.29538236044529526, 0.12330374023539421, 0.04424059040922389, 0.20688478433734614, 0.12330374023539421]
printing an ep nov before normalisation:  60.93899913970174
Printing some Q and Qe and total Qs values:  [[0.955]
 [0.755]
 [1.056]
 [1.038]
 [1.001]
 [1.006]
 [1.049]] [[63.052]
 [62.881]
 [60.542]
 [61.205]
 [62.254]
 [62.697]
 [64.837]] [[2.354]
 [2.145]
 [2.341]
 [2.354]
 [2.364]
 [2.388]
 [2.528]]
Starting evaluation
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.2728568163964235, 0.38959294798379884, 0.058314736722328724, 0.058314736722328724, 0.058314736722328724, 0.1626060254527915]
printing an ep nov before normalisation:  24.968579211681888
using explorer policy with actor:  0
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.739074950607936
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.2728568163964235, 0.38959294798379884, 0.058314736722328724, 0.058314736722328724, 0.058314736722328724, 0.1626060254527915]
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.2728568163964235, 0.38959294798379884, 0.058314736722328724, 0.058314736722328724, 0.058314736722328724, 0.1626060254527915]
maxi score, test score, baseline:  -0.9860111111111112 -1.0 -0.9860111111111112
probs:  [0.2728568163964235, 0.38959294798379884, 0.058314736722328724, 0.058314736722328724, 0.058314736722328724, 0.1626060254527915]
line 256 mcts: sample exp_bonus 44.422820528262
Printing some Q and Qe and total Qs values:  [[0.944]
 [0.952]
 [0.956]
 [0.963]
 [0.959]
 [0.964]
 [0.961]] [[62.419]
 [59.944]
 [62.349]
 [62.449]
 [63.272]
 [63.507]
 [64.394]] [[1.81 ]
 [1.752]
 [1.821]
 [1.83 ]
 [1.848]
 [1.859]
 [1.881]]
printing an ep nov before normalisation:  45.39152270815945
printing an ep nov before normalisation:  17.482094960197514
Printing some Q and Qe and total Qs values:  [[0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]] [[56.558]
 [56.558]
 [56.558]
 [56.558]
 [56.558]
 [56.558]
 [56.558]] [[1.736]
 [1.736]
 [1.736]
 [1.736]
 [1.736]
 [1.736]
 [1.736]]
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.414]
 [0.686]
 [0.656]
 [0.263]
 [0.24 ]
 [0.109]] [[22.815]
 [28.193]
 [34.594]
 [26.514]
 [35.132]
 [33.38 ]
 [31.742]] [[1.337]
 [1.543]
 [2.083]
 [1.713]
 [1.683]
 [1.586]
 [1.386]]
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.586]
 [0.68 ]
 [0.69 ]
 [0.576]
 [0.326]
 [0.6  ]] [[69.358]
 [75.083]
 [73.105]
 [57.766]
 [60.9  ]
 [74.166]
 [66.385]] [[0.636]
 [0.586]
 [0.68 ]
 [0.69 ]
 [0.576]
 [0.326]
 [0.6  ]]
printing an ep nov before normalisation:  15.385452265294806
printing an ep nov before normalisation:  66.01883242933815
printing an ep nov before normalisation:  23.935821362750634
printing an ep nov before normalisation:  27.657695683218588
printing an ep nov before normalisation:  67.65882509750601
printing an ep nov before normalisation:  46.370607461592634
printing an ep nov before normalisation:  12.027348279953003
printing an ep nov before normalisation:  12.40297846945874
actions average: 
K:  0  action  0 :  tensor([0.3356, 0.0498, 0.1314, 0.1098, 0.1117, 0.1389, 0.1227],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0149, 0.8463, 0.0179, 0.0200, 0.0101, 0.0649, 0.0258],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1399, 0.0208, 0.1916, 0.1611, 0.1264, 0.2005, 0.1598],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1380, 0.0475, 0.1344, 0.2237, 0.1070, 0.2146, 0.1349],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1833, 0.0175, 0.0987, 0.1260, 0.2969, 0.1577, 0.1200],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1085, 0.1649, 0.1345, 0.1187, 0.0803, 0.2783, 0.1148],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1627, 0.1191, 0.1052, 0.1535, 0.0684, 0.1639, 0.2272],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  12.16442791578348
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.622]
 [0.615]
 [0.615]
 [0.628]
 [0.615]
 [0.615]] [[50.808]
 [56.685]
 [50.202]
 [50.202]
 [42.607]
 [50.202]
 [50.202]] [[0.612]
 [0.622]
 [0.615]
 [0.615]
 [0.628]
 [0.615]
 [0.615]]
printing an ep nov before normalisation:  65.99015174724653
using explorer policy with actor:  0
printing an ep nov before normalisation:  73.43995916776188
printing an ep nov before normalisation:  62.551857374555304
maxi score, test score, baseline:  -0.9862013698630137 -1.0 -0.9862013698630137
probs:  [0.24367710984018645, 0.3169700072078623, 0.10897664981310692, 0.10897664981310692, 0.046943543221688894, 0.17445604010404855]
printing an ep nov before normalisation:  10.662418603897095
printing an ep nov before normalisation:  55.589107136007094
printing an ep nov before normalisation:  36.53046241698806
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.413]
 [0.413]
 [0.418]
 [0.413]
 [0.409]
 [0.408]] [[50.359]
 [50.359]
 [50.359]
 [48.416]
 [50.359]
 [48.901]
 [49.547]] [[1.838]
 [1.838]
 [1.838]
 [1.737]
 [1.838]
 [1.755]
 [1.789]]
printing an ep nov before normalisation:  25.623153687400322
maxi score, test score, baseline:  -0.9867421052631579 -1.0 -0.9867421052631579
probs:  [0.24367711347231052, 0.3169700142967752, 0.10897664709221255, 0.10897664709221255, 0.04694353757506224, 0.17445604047142682]
printing an ep nov before normalisation:  27.299459159926982
line 256 mcts: sample exp_bonus 41.096908331452426
line 256 mcts: sample exp_bonus 13.186620620365522
printing an ep nov before normalisation:  25.063393039261374
using explorer policy with actor:  0
printing an ep nov before normalisation:  38.2962963283132
printing an ep nov before normalisation:  15.24661525952321
printing an ep nov before normalisation:  40.2410501314414
printing an ep nov before normalisation:  28.043914784660284
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.351]
 [0.361]
 [0.345]
 [0.346]
 [0.349]
 [0.347]] [[21.763]
 [37.31 ]
 [20.813]
 [20.263]
 [20.161]
 [19.87 ]
 [20.226]] [[0.347]
 [0.351]
 [0.361]
 [0.345]
 [0.346]
 [0.349]
 [0.347]]
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.618]
 [0.616]
 [0.615]
 [0.616]
 [0.617]
 [0.618]] [[19.223]
 [21.515]
 [16.63 ]
 [17.403]
 [16.294]
 [15.572]
 [15.58 ]] [[0.621]
 [0.618]
 [0.616]
 [0.615]
 [0.616]
 [0.617]
 [0.618]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.14797012756556
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.549]
 [0.549]
 [0.547]
 [0.551]
 [0.551]
 [0.55 ]] [[22.359]
 [24.594]
 [22.073]
 [22.432]
 [20.968]
 [20.561]
 [20.818]] [[0.557]
 [0.549]
 [0.549]
 [0.547]
 [0.551]
 [0.551]
 [0.55 ]]
printing an ep nov before normalisation:  53.984741787791734
Printing some Q and Qe and total Qs values:  [[0.908]
 [0.892]
 [0.908]
 [0.908]
 [0.908]
 [0.902]
 [0.902]] [[26.497]
 [33.676]
 [26.497]
 [26.497]
 [26.497]
 [29.357]
 [29.53 ]] [[1.045]
 [1.108]
 [1.045]
 [1.045]
 [1.045]
 [1.07 ]
 [1.072]]
printing an ep nov before normalisation:  32.407318075722145
maxi score, test score, baseline:  -0.9870794871794872 -1.0 -0.9870794871794872
probs:  [0.2517833185581051, 0.2517833185581051, 0.10539079331454666, 0.1766087785681694, 0.037825012432904453, 0.1766087785681694]
printing an ep nov before normalisation:  30.870677265926716
printing an ep nov before normalisation:  50.46874324603564
printing an ep nov before normalisation:  46.293242488659416
printing an ep nov before normalisation:  83.08816111582995
printing an ep nov before normalisation:  83.5945755595123
maxi score, test score, baseline:  -0.9877048780487805 -1.0 -0.9877048780487805
probs:  [0.05287883599521408, 0.35229060753120034, 0.14742992174342076, 0.24709187699152999, 0.05287883599521408, 0.14742992174342076]
siam score:  -0.6493408
printing an ep nov before normalisation:  20.943869557589412
printing an ep nov before normalisation:  67.68674352920566
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
probs:  [0.058391620759264413, 0.3890970107936037, 0.1628249018227404, 0.27290322510586273, 0.058391620759264413, 0.058391620759264413]
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
probs:  [0.06560433627951472, 0.437252906090332, 0.182967042535562, 0.182967042535562, 0.06560433627951472, 0.06560433627951472]
maxi score, test score, baseline:  -0.9887888888888889 -1.0 -0.9887888888888889
probs:  [0.06560433627951472, 0.437252906090332, 0.182967042535562, 0.182967042535562, 0.06560433627951472, 0.06560433627951472]
printing an ep nov before normalisation:  52.1090845385606
maxi score, test score, baseline:  -0.9889109890109891 -1.0 -0.9889109890109891
probs:  [0.06560433414861112, 0.4372529117956543, 0.1829670428792562, 0.1829670428792562, 0.06560433414861112, 0.06560433414861112]
maxi score, test score, baseline:  -0.9890304347826087 -1.0 -0.9890304347826087
printing an ep nov before normalisation:  14.18750847224544
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.989147311827957 -1.0 -0.989147311827957
probs:  [0.07431779778630074, 0.49542863988076996, 0.07431779778630074, 0.2073001689740272, 0.07431779778630074, 0.07431779778630074]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9892617021276596 -1.0 -0.9892617021276596
probs:  [0.12611066770315268, 0.3656642367104369, 0.12611066770315268, 0.2017591631791372, 0.12611066770315268, 0.054244597000967815]
maxi score, test score, baseline:  -0.9893736842105263 -1.0 -0.9893736842105263
maxi score, test score, baseline:  -0.9893736842105263 -1.0 -0.9893736842105263
probs:  [0.12611066719835667, 0.36566423918733804, 0.12611066719835667, 0.20175916361592977, 0.12611066719835667, 0.0542445956016623]
maxi score, test score, baseline:  -0.9893736842105263 -1.0 -0.9893736842105263
probs:  [0.12611066719835667, 0.36566423918733804, 0.12611066719835667, 0.20175916361592977, 0.12611066719835667, 0.0542445956016623]
using another actor
printing an ep nov before normalisation:  70.67308506470762
printing an ep nov before normalisation:  71.07547991599381
printing an ep nov before normalisation:  19.01298444761255
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
probs:  [0.0547654933707303, 0.3718181045904331, 0.17183107289800528, 0.2349848723798245, 0.11183496339027661, 0.0547654933707303]
printing an ep nov before normalisation:  21.432026397141605
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
probs:  [0.0547654933707303, 0.3718181045904331, 0.17183107289800528, 0.2349848723798245, 0.11183496339027661, 0.0547654933707303]
maxi score, test score, baseline:  -0.9894833333333334 -1.0 -0.9894833333333334
Printing some Q and Qe and total Qs values:  [[0.916]
 [0.915]
 [0.915]
 [0.915]
 [0.917]
 [0.916]
 [0.915]] [[0.325]
 [0.318]
 [0.318]
 [0.317]
 [0.306]
 [0.313]
 [0.313]] [[0.916]
 [0.915]
 [0.915]
 [0.915]
 [0.917]
 [0.916]
 [0.915]]
maxi score, test score, baseline:  -0.9896959183673469 -1.0 -0.9896959183673469
probs:  [0.05476549116516022, 0.3718181086339643, 0.1718310729997953, 0.2349848737263752, 0.11183496230954484, 0.05476549116516022]
printing an ep nov before normalisation:  48.62541296573849
printing an ep nov before normalisation:  53.37781223828889
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.04410631955797481, 0.2965931878889549, 0.22861595410753713, 0.16403758201519014, 0.16403758201519014, 0.10260937441515282]
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.704]
 [0.679]
 [0.662]
 [0.643]
 [0.578]
 [0.703]] [[71.587]
 [66.062]
 [63.527]
 [61.673]
 [65.299]
 [64.946]
 [66.368]] [[1.861]
 [1.735]
 [1.634]
 [1.563]
 [1.651]
 [1.576]
 [1.742]]
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.04098161126886055, 0.19101550718606244, 0.27180298960301735, 0.19101550718606244, 0.19101550718606244, 0.11416887756993471]
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.04098161126886055, 0.19101550718606244, 0.27180298960301735, 0.19101550718606244, 0.19101550718606244, 0.11416887756993471]
printing an ep nov before normalisation:  81.08466033794008
maxi score, test score, baseline:  -0.98979898989899 -1.0 -0.98979898989899
probs:  [0.044384925659854815, 0.20691901033973187, 0.2944373636288965, 0.20691901033973187, 0.12366984501589247, 0.12366984501589247]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.60723304748535
using explorer policy with actor:  1
STARTED EXPV TRAINING ON FRAME NO.  11042
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.04840688041305612, 0.22571339538682408, 0.32118613421885345, 0.13489786332708878, 0.13489786332708878, 0.13489786332708878]
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.04840688041305612, 0.22571339538682408, 0.32118613421885345, 0.13489786332708878, 0.13489786332708878, 0.13489786332708878]
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.053233024639607614, 0.14837091795262494, 0.35328330354989274, 0.14837091795262494, 0.14837091795262494, 0.14837091795262494]
printing an ep nov before normalisation:  50.45390955221707
printing an ep nov before normalisation:  51.44784479619476
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.053233024639607614, 0.14837091795262494, 0.35328330354989274, 0.14837091795262494, 0.14837091795262494, 0.14837091795262494]
printing an ep nov before normalisation:  46.36646270751953
maxi score, test score, baseline:  -0.9899 -1.0 -0.9899
probs:  [0.05882091433836148, 0.05882091433836148, 0.390446602747901, 0.16397052285845867, 0.16397052285845867, 0.16397052285845867]
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.058820912425186785, 0.058820912425186785, 0.39044660671773707, 0.16397052281062982, 0.16397052281062982, 0.16397052281062982]
siam score:  -0.8231517
maxi score, test score, baseline:  -0.9899990099009901 -1.0 -0.9899990099009901
probs:  [0.1097591128499879, 0.047154930839069666, 0.3168344841168706, 0.17541715739802397, 0.17541715739802397, 0.17541715739802397]
printing an ep nov before normalisation:  90.32955044280082
printing an ep nov before normalisation:  58.120531794135424
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.451]
 [0.451]
 [0.465]
 [0.451]
 [0.451]
 [0.451]] [[37.835]
 [38.118]
 [38.118]
 [36.452]
 [38.118]
 [38.118]
 [38.118]] [[2.366]
 [2.451]
 [2.451]
 [2.314]
 [2.451]
 [2.451]
 [2.451]]
printing an ep nov before normalisation:  52.44704494154861
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
probs:  [0.12590360663794342, 0.05407976863692049, 0.36347476310286625, 0.05407976863692049, 0.2012310464926747, 0.2012310464926747]
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
probs:  [0.058555963048288334, 0.058555963048288334, 0.38818407335275934, 0.058555963048288334, 0.2729482299129853, 0.16319980758939043]
from probs:  [0.058555963048288334, 0.058555963048288334, 0.38818407335275934, 0.058555963048288334, 0.2729482299129853, 0.16319980758939043]
printing an ep nov before normalisation:  40.728081637734334
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
using explorer policy with actor:  1
printing an ep nov before normalisation:  76.69409370708809
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
maxi score, test score, baseline:  -0.9900960784313726 -1.0 -0.9900960784313726
probs:  [0.1455873871783684, 0.1455873871783684, 0.31315286623114313, 0.04628932551746494, 0.25457306461106705, 0.0948099692835881]
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.8026818628761134
printing an ep nov before normalisation:  27.368493778174926
maxi score, test score, baseline:  -0.9901912621359223 -1.0 -0.9901912621359223
probs:  [0.15384829078120238, 0.15384829078120238, 0.2773037472216709, 0.04136665269099727, 0.2773037472216709, 0.09632927130325612]
from probs:  [0.15384829078120238, 0.15384829078120238, 0.2773037472216709, 0.04136665269099727, 0.2773037472216709, 0.09632927130325612]
printing an ep nov before normalisation:  63.6492037641303
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9902846153846154 -1.0 -0.9902846153846154
probs:  [0.1627958042402683, 0.1627958042402683, 0.2934374111312165, 0.04376678462851525, 0.2934374111312165, 0.04376678462851525]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.1744636641346821, 0.1744636641346821, 0.3144762552467928, 0.04689663667698186, 0.2428031431298793, 0.04689663667698186]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  11042
printing an ep nov before normalisation:  90.37842193641507
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.397]
 [0.298]
 [0.322]
 [0.298]
 [0.298]
 [0.298]] [[66.292]
 [71.403]
 [62.073]
 [71.747]
 [62.073]
 [62.073]
 [62.073]] [[1.548]
 [1.778]
 [1.366]
 [1.715]
 [1.366]
 [1.366]
 [1.366]]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.2064908767616438, 0.0440838015170691, 0.29957785891402205, 0.12167829302281062, 0.2064908767616438, 0.12167829302281062]
line 256 mcts: sample exp_bonus 50.95297774030253
printing an ep nov before normalisation:  40.71951866149902
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
probs:  [0.1464667806993308, 0.046521963383034104, 0.31423129548025713, 0.1464667806993308, 0.19984639903871634, 0.1464667806993308]
maxi score, test score, baseline:  -0.9904660377358491 -1.0 -0.9904660377358491
actions average: 
K:  2  action  0 :  tensor([0.1590, 0.1066, 0.1071, 0.1841, 0.1390, 0.1686, 0.1356],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0576, 0.6926, 0.0475, 0.0591, 0.0394, 0.0583, 0.0456],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1742, 0.0220, 0.1442, 0.1742, 0.1434, 0.1527, 0.1892],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1383, 0.1387, 0.1084, 0.2213, 0.1351, 0.1335, 0.1247],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1792, 0.0197, 0.1141, 0.1599, 0.1945, 0.1635, 0.1690],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1695, 0.0166, 0.1535, 0.1662, 0.1362, 0.1855, 0.1725],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1532, 0.0500, 0.1259, 0.1842, 0.1357, 0.1594, 0.1916],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.1548351585303509, 0.04163559648782528, 0.27856491239078607, 0.1548351585303509, 0.21529401553033617, 0.1548351585303509]
printing an ep nov before normalisation:  72.46318796122321
printing an ep nov before normalisation:  45.904712199947795
actor:  1 policy actor:  1  step number:  66 total reward:  0.059999999999999165  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  78.29942466675043
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.0943729605034381, 0.07693997869674972, 0.10368375760473769, 0.0943729605034381, 0.5362573821881982, 0.0943729605034381]
printing an ep nov before normalisation:  5.206568107183784
line 256 mcts: sample exp_bonus 41.66951821566046
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.09904974548627041, 0.07464163016563588, 0.10792542378468302, 0.09904974548627041, 0.52028370959087, 0.09904974548627041]
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.09904974548627041, 0.07464163016563588, 0.10792542378468302, 0.09904974548627041, 0.52028370959087, 0.09904974548627041]
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.376]
 [0.382]
 [0.383]
 [0.384]
 [0.386]
 [0.388]] [[50.905]
 [53.559]
 [52.587]
 [52.615]
 [51.894]
 [51.211]
 [50.658]] [[0.631]
 [0.691]
 [0.686]
 [0.688]
 [0.681]
 [0.676]
 [0.671]]
printing an ep nov before normalisation:  17.372845975937636
printing an ep nov before normalisation:  53.065522377023164
printing an ep nov before normalisation:  0.0038199694699869724
printing an ep nov before normalisation:  47.21372800677681
maxi score, test score, baseline:  -0.9905542056074766 -1.0 -0.9905542056074766
probs:  [0.09133486746591371, 0.07528012109375176, 0.10884913623554501, 0.09989739886440013, 0.5247410774759892, 0.09989739886440013]
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
line 256 mcts: sample exp_bonus 53.67960002340456
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.23917768049776
printing an ep nov before normalisation:  38.6243417567063
actions average: 
K:  3  action  0 :  tensor([0.2277, 0.0262, 0.1244, 0.1492, 0.1555, 0.1623, 0.1547],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0427, 0.6833, 0.0463, 0.0413, 0.0341, 0.0997, 0.0526],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1773, 0.0729, 0.1360, 0.1444, 0.1414, 0.1672, 0.1608],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1539, 0.0519, 0.1170, 0.2072, 0.1426, 0.1746, 0.1527],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1549, 0.0529, 0.1318, 0.1346, 0.1596, 0.2173, 0.1489],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1444, 0.0544, 0.1497, 0.1390, 0.1370, 0.2408, 0.1347],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2257, 0.0366, 0.1165, 0.1473, 0.1667, 0.1466, 0.1606],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  75.54017755464368
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
probs:  [0.0838571102779901, 0.07535002075460732, 0.11169849417269763, 0.10200556792787353, 0.525083238938958, 0.10200556792787353]
printing an ep nov before normalisation:  58.942301684118945
printing an ep nov before normalisation:  77.03898127819664
maxi score, test score, baseline:  -0.9906407407407407 -1.0 -0.9906407407407407
using another actor
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.03267230942782362
printing an ep nov before normalisation:  55.90780361064099
printing an ep nov before normalisation:  59.51440340033969
printing an ep nov before normalisation:  52.2903587228578
printing an ep nov before normalisation:  64.2711508069326
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.252]
 [0.276]
 [0.286]
 [0.266]
 [0.281]
 [0.291]] [[55.251]
 [65.857]
 [61.539]
 [64.659]
 [66.281]
 [66.168]
 [65.942]] [[0.526]
 [0.252]
 [0.276]
 [0.286]
 [0.266]
 [0.281]
 [0.291]]
printing an ep nov before normalisation:  71.55833479271226
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  11042
printing an ep nov before normalisation:  54.22554016113281
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.599]
 [0.492]
 [0.47 ]
 [0.41 ]
 [0.433]
 [0.42 ]] [[50.069]
 [52.155]
 [52.758]
 [51.63 ]
 [53.724]
 [54.846]
 [55.402]] [[1.71 ]
 [1.874]
 [1.796]
 [1.72 ]
 [1.76 ]
 [1.837]
 [1.851]]
printing an ep nov before normalisation:  78.77109676971978
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
from probs:  [0.08571641377407158, 0.07606049314662633, 0.09579215703749284, 0.10631571111262157, 0.5297995138165661, 0.10631571111262157]
siam score:  -0.85715765
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
probs:  [0.08580143739962863, 0.07579612737594305, 0.09624176090260486, 0.10714609878349127, 0.5278684767548409, 0.10714609878349127]
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
probs:  [0.0858892712276414, 0.07552302378288236, 0.09670622508304222, 0.10800393244312756, 0.525873615020179, 0.10800393244312756]
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
probs:  [0.0858892712276414, 0.07552302378288236, 0.09670622508304222, 0.10800393244312756, 0.525873615020179, 0.10800393244312756]
maxi score, test score, baseline:  -0.9907256880733946 -1.0 -0.9907256880733946
probs:  [0.0858892712276414, 0.07552302378288236, 0.09670622508304222, 0.10800393244312756, 0.525873615020179, 0.10800393244312756]
from probs:  [0.0858892712276414, 0.07552302378288236, 0.09670622508304222, 0.10800393244312756, 0.525873615020179, 0.10800393244312756]
using explorer policy with actor:  1
printing an ep nov before normalisation:  95.47093974378984
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.435]
 [0.427]
 [0.429]
 [0.426]
 [0.425]
 [0.422]] [[85.847]
 [79.534]
 [85.427]
 [86.617]
 [86.2  ]
 [85.763]
 [85.723]] [[1.078]
 [0.993]
 [1.071]
 [1.091]
 [1.081]
 [1.075]
 [1.07 ]]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.09259488432601067, 0.07210900153060303, 0.10350584364095602, 0.11490173448101003, 0.5019868015404102, 0.11490173448101003]
printing an ep nov before normalisation:  44.9860143661499
printing an ep nov before normalisation:  37.92062997817993
printing an ep nov before normalisation:  58.57681935585077
printing an ep nov before normalisation:  56.91913919777098
siam score:  -0.8577786
printing an ep nov before normalisation:  65.52929483977249
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.10081915188600499, 0.07079133879516557, 0.10081915188600499, 0.11169879431022219, 0.49280958672486447, 0.12306197639773787]
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.10311151224100372, 0.07240004283787341, 0.10311151224100372, 0.10311151224100372, 0.5040265642114705, 0.11423885622764504]
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.464]
 [0.463]
 [0.458]
 [0.444]
 [0.443]
 [0.441]] [[56.352]
 [46.009]
 [51.24 ]
 [52.779]
 [51.373]
 [48.863]
 [47.583]] [[1.871]
 [1.385]
 [1.625]
 [1.689]
 [1.611]
 [1.495]
 [1.434]]
printing an ep nov before normalisation:  46.44541966510671
maxi score, test score, baseline:  -0.991204347826087 -1.0 -0.991204347826087
probs:  [0.0934436382845896, 0.07317961106224595, 0.104222376168815, 0.104222376168815, 0.5094622434849193, 0.11546975483061533]
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.475]
 [0.466]
 [0.461]
 [0.461]
 [0.462]
 [0.462]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.465]
 [0.475]
 [0.466]
 [0.461]
 [0.461]
 [0.462]
 [0.462]]
printing an ep nov before normalisation:  0.006768152701397412
from probs:  [0.0934437231583491, 0.07317952496055179, 0.1042225519869647, 0.1042225519869647, 0.5094616223599105, 0.11547002554725927]
printing an ep nov before normalisation:  69.29675551169743
printing an ep nov before normalisation:  47.01143715979312
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.09450602790863873, 0.07401112946986779, 0.10540756963138931, 0.10540756963138931, 0.5152601337273257, 0.10540756963138931]
printing an ep nov before normalisation:  0.0
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.646]
 [0.664]
 [0.629]
 [0.646]
 [0.65 ]
 [0.641]] [[60.42 ]
 [62.815]
 [48.753]
 [48.777]
 [48.601]
 [48.689]
 [49.481]] [[1.542]
 [1.724]
 [1.346]
 [1.312]
 [1.324]
 [1.33 ]
 [1.343]]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
probs:  [0.09554701745484857, 0.07482604778830973, 0.09554701745484857, 0.10656880983066717, 0.5209422976406588, 0.10656880983066717]
maxi score, test score, baseline:  -0.9912793103448276 -1.0 -0.9912793103448276
printing an ep nov before normalisation:  30.39851136279092
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.244788474427615
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.545]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]] [[61.987]
 [87.609]
 [61.987]
 [61.987]
 [61.987]
 [61.987]
 [61.987]] [[1.269]
 [1.889]
 [1.269]
 [1.269]
 [1.269]
 [1.269]
 [1.269]]
printing an ep nov before normalisation:  42.95550617179369
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.504]
 [0.487]
 [0.477]
 [0.473]
 [0.471]
 [0.47 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.473]
 [0.504]
 [0.487]
 [0.477]
 [0.473]
 [0.471]
 [0.47 ]]
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
probs:  [0.0979855633434952, 0.0747930154956213, 0.0861526307680493, 0.11032202496470485, 0.5204247404634246, 0.11032202496470485]
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.568]
 [0.516]
 [0.516]
 [0.516]
 [0.516]
 [0.516]] [[68.945]
 [89.75 ]
 [60.889]
 [60.889]
 [60.889]
 [60.889]
 [60.889]] [[1.111]
 [1.46 ]
 [1.04 ]
 [1.04 ]
 [1.04 ]
 [1.04 ]
 [1.04 ]]
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
deleting a thread, now have 5 threads
Frames:  13196 train batches done:  1539 episodes:  371
maxi score, test score, baseline:  -0.9913529914529915 -1.0 -0.9913529914529915
probs:  [0.09849104771371721, 0.07449656111399021, 0.08624896271385647, 0.11125407250080617, 0.5182552834568238, 0.11125407250080617]
siam score:  -0.8597177
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.09901450908774424, 0.07418956337261734, 0.08634872045757748, 0.11221926744685437, 0.5160086721883521, 0.11221926744685437]
printing an ep nov before normalisation:  23.018509153989726
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.10223827996142315, 0.0758604969138274, 0.0758604969138274, 0.10223827996142315, 0.5275334306244608, 0.11626901562503811]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.10369245452150645, 0.07693900053644484, 0.07693900053644484, 0.10369245452150645, 0.5350446353625911, 0.10369245452150645]
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.09188183085941169, 0.07760061288645995, 0.07760061288645995, 0.10675809958123643, 0.5394007442051956, 0.10675809958123643]
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.99828875888734
actions average: 
K:  1  action  0 :  tensor([0.3323, 0.0071, 0.1001, 0.1280, 0.1518, 0.1319, 0.1488],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0185, 0.8032, 0.0250, 0.0571, 0.0152, 0.0442, 0.0368],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1723, 0.0329, 0.2060, 0.1439, 0.1189, 0.1821, 0.1438],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1902, 0.0069, 0.0976, 0.2297, 0.1602, 0.1444, 0.1711],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2103, 0.0081, 0.1216, 0.1828, 0.1494, 0.1673, 0.1604],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1957, 0.0264, 0.1065, 0.1673, 0.1427, 0.1980, 0.1634],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2011, 0.0516, 0.1224, 0.1573, 0.1356, 0.1714, 0.1605],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  25.304572116966373
maxi score, test score, baseline:  -0.9914254237288136 -1.0 -0.9914254237288136
probs:  [0.09252696343205354, 0.07716376227449982, 0.07716376227449982, 0.10853029797117186, 0.5360849160766031, 0.10853029797117186]
Printing some Q and Qe and total Qs values:  [[0.66 ]
 [0.574]
 [0.574]
 [0.595]
 [0.574]
 [0.574]
 [0.574]] [[31.129]
 [29.515]
 [29.515]
 [33.665]
 [29.515]
 [29.515]
 [29.515]] [[1.801]
 [1.607]
 [1.607]
 [1.907]
 [1.607]
 [1.607]
 [1.607]]
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.443]
 [0.452]
 [0.45 ]
 [0.446]
 [0.442]
 [0.439]] [[10.256]
 [16.11 ]
 [ 9.325]
 [ 9.392]
 [ 9.255]
 [ 9.398]
 [11.014]] [[0.774]
 [1.11 ]
 [0.725]
 [0.726]
 [0.715]
 [0.719]
 [0.81 ]]
siam score:  -0.8443619
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.0948642708709555, 0.07803079277204668, 0.07803079277204668, 0.11239914389065177, 0.5418107288233439, 0.0948642708709555]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.0948642708709555, 0.07803079277204668, 0.07803079277204668, 0.11239914389065177, 0.5418107288233439, 0.0948642708709555]
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.09531057032339421, 0.07782367232796482, 0.07782367232796482, 0.11352608906863304, 0.5402054256286489, 0.09531057032339421]
line 256 mcts: sample exp_bonus 47.169015973492975
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.1060722989592845, 0.07442061168788432, 0.0899299384508703, 0.12288725782221567, 0.5167599546288748, 0.0899299384508703]
printing an ep nov before normalisation:  44.545237737311005
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  58.76763362734684
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9915666666666667 -1.0 -0.9915666666666667
probs:  [0.10774557591285484, 0.07376133929501578, 0.09041361523775683, 0.12579970161608162, 0.511866152700534, 0.09041361523775683]
deleting a thread, now have 4 threads
Frames:  13854 train batches done:  1617 episodes:  395
printing an ep nov before normalisation:  48.95891134108305
printing an ep nov before normalisation:  53.11176054708871
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  36.76792802592357
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.491]
 [0.437]
 [0.437]
 [0.437]
 [0.434]
 [0.428]] [[57.539]
 [60.004]
 [54.021]
 [54.035]
 [54.356]
 [54.69 ]
 [55.154]] [[2.113]
 [2.281]
 [1.872]
 [1.872]
 [1.891]
 [1.908]
 [1.929]]
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.11161852214447421, 0.07641062343290521, 0.09366249380157393, 0.11161852214447421, 0.5302792150436672, 0.07641062343290521]
printing an ep nov before normalisation:  0.15656582280627163
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.11161852214447421, 0.07641062343290521, 0.09366249380157393, 0.11161852214447421, 0.5302792150436672, 0.07641062343290521]
printing an ep nov before normalisation:  41.93718213732926
printing an ep nov before normalisation:  63.860981792932186
printing an ep nov before normalisation:  28.678603077727374
printing an ep nov before normalisation:  48.240994711301376
printing an ep nov before normalisation:  69.99161813539429
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.09878501091210407, 0.0786774922082516, 0.0786774922082516, 0.11971324466509332, 0.5454692677980479, 0.0786774922082516]
deleting a thread, now have 3 threads
Frames:  14132 train batches done:  1649 episodes:  401
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.09878501091210407, 0.0786774922082516, 0.0786774922082516, 0.11971324466509332, 0.5454692677980479, 0.0786774922082516]
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.0994152974780264, 0.07848580413435222, 0.07848580413435222, 0.12119905585613619, 0.5439282342627809, 0.07848580413435222]
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.0994152974780264, 0.07848580413435222, 0.07848580413435222, 0.12119905585613619, 0.5439282342627809, 0.07848580413435222]
actions average: 
K:  0  action  0 :  tensor([0.2105, 0.0361, 0.1165, 0.1521, 0.1730, 0.1760, 0.1359],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0226, 0.8769, 0.0199, 0.0198, 0.0171, 0.0242, 0.0195],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1900, 0.0034, 0.1460, 0.1592, 0.1664, 0.1765, 0.1584],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1917, 0.0130, 0.1237, 0.2183, 0.1431, 0.1598, 0.1504],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1881, 0.0281, 0.1179, 0.1706, 0.1729, 0.1641, 0.1583],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1743, 0.0119, 0.1458, 0.1473, 0.1428, 0.2464, 0.1315],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1817, 0.0450, 0.1365, 0.1531, 0.1484, 0.1670, 0.1683],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9917032786885246 -1.0 -0.9917032786885246
probs:  [0.0994152974780264, 0.07848580413435222, 0.07848580413435222, 0.12119905585613619, 0.5439282342627809, 0.07848580413435222]
printing an ep nov before normalisation:  19.77450503682807
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.09941549426667605, 0.07848574390506045, 0.07848574390506045, 0.12119952015325565, 0.5439277538648869, 0.07848574390506045]
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.09941549426667605, 0.07848574390506045, 0.07848574390506045, 0.12119952015325565, 0.5439277538648869, 0.07848574390506045]
using another actor
maxi score, test score, baseline:  -0.991769918699187 -1.0 -0.991769918699187
probs:  [0.10162823288347689, 0.08023201385606187, 0.08023201385606187, 0.10162823288347689, 0.5560474926648606, 0.08023201385606187]
printing an ep nov before normalisation:  84.83076708928927
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.651]
 [0.615]
 [0.615]
 [0.632]
 [0.628]
 [0.596]] [[44.598]
 [52.206]
 [44.43 ]
 [43.747]
 [43.369]
 [43.583]
 [44.355]] [[1.492]
 [1.858]
 [1.506]
 [1.478]
 [1.479]
 [1.485]
 [1.484]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  102.61629982750098
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.325]] [[29.421]
 [29.421]
 [29.421]
 [29.421]
 [29.421]
 [29.421]
 [29.421]] [[1.412]
 [1.412]
 [1.412]
 [1.412]
 [1.412]
 [1.412]
 [1.412]]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.09501064022821643, 0.07562556300956196, 0.09501064022821643, 0.11517112053561711, 0.5241713957701716, 0.09501064022821643]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
printing an ep nov before normalisation:  38.18918704986572
siam score:  -0.84879774
printing an ep nov before normalisation:  59.79893483234837
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.09501064022821643, 0.07562556300956196, 0.09501064022821643, 0.11517112053561711, 0.5241713957701716, 0.09501064022821643]
maxi score, test score, baseline:  -0.9919 -1.0 -0.9919
probs:  [0.1050184066438626, 0.0708081171637384, 0.1050184066438626, 0.12314986006832856, 0.49098680283634527, 0.1050184066438626]
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.809139013290405
printing an ep nov before normalisation:  82.99685315380026
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.634]
 [0.617]
 [0.528]
 [0.617]
 [0.595]
 [0.617]] [[ 97.693]
 [ 74.606]
 [ 96.802]
 [100.247]
 [ 96.802]
 [ 99.161]
 [ 96.802]] [[2.121]
 [1.446]
 [2.113]
 [2.129]
 [2.113]
 [2.163]
 [2.113]]
siam score:  -0.85003054
printing an ep nov before normalisation:  33.35819461906347
printing an ep nov before normalisation:  77.01581424181737
line 256 mcts: sample exp_bonus 57.49915539018023
from probs:  [0.10771316552110333, 0.07166309830362214, 0.10771316552110333, 0.12681970114636829, 0.496749372857147, 0.08934149665065601]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.10858300065257304, 0.071244961499465, 0.10858300065257304, 0.1283721614037206, 0.49366191432289064, 0.08955496146877764]
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]] [[54.049]
 [54.049]
 [54.049]
 [54.049]
 [54.049]
 [54.049]
 [54.049]] [[1.399]
 [1.399]
 [1.399]
 [1.399]
 [1.399]
 [1.399]
 [1.399]]
printing an ep nov before normalisation:  45.856304290480665
printing an ep nov before normalisation:  42.49742979856783
siam score:  -0.8506848
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.10949250214482853, 0.0708077571463059, 0.10949250214482853, 0.1299954169940456, 0.490433660626333, 0.08977816094365842]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.11169360089667235, 0.07223023932789799, 0.09158246471258556, 0.132609182528123, 0.5003020478221356, 0.09158246471258556]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.11169360089667235, 0.07223023932789799, 0.09158246471258556, 0.132609182528123, 0.5003020478221356, 0.09158246471258556]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.11169360089667235, 0.07223023932789799, 0.09158246471258556, 0.132609182528123, 0.5003020478221356, 0.09158246471258556]
printing an ep nov before normalisation:  39.47423458099365
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.11274761443791542, 0.07181586601272878, 0.09188816572123382, 0.1344414411032646, 0.4972187470036237, 0.09188816572123382]
printing an ep nov before normalisation:  72.53487880832417
printing an ep nov before normalisation:  49.57686364650726
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.543]
 [0.537]
 [0.537]
 [0.541]
 [0.539]
 [0.538]] [[63.46 ]
 [57.305]
 [63.644]
 [63.741]
 [61.864]
 [64.39 ]
 [64.932]] [[0.857]
 [0.803]
 [0.858]
 [0.858]
 [0.845]
 [0.866]
 [0.871]]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.0978543169148817, 0.07647686266962493, 0.07647686266962493, 0.14317451991482583, 0.5295405751614178, 0.07647686266962493]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.0978543169148817, 0.07647686266962493, 0.07647686266962493, 0.14317451991482583, 0.5295405751614178, 0.07647686266962493]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.0978543169148817, 0.07647686266962493, 0.07647686266962493, 0.14317451991482583, 0.5295405751614178, 0.07647686266962493]
printing an ep nov before normalisation:  37.81405882777312
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.1084721553345612, 0.08944456495681685, 0.08944456495681685, 0.14881064693537896, 0.4927063676678074, 0.07112170014861866]
Printing some Q and Qe and total Qs values:  [[-0.033]
 [-0.045]
 [-0.053]
 [-0.054]
 [-0.046]
 [-0.04 ]
 [-0.035]] [[ 6.825]
 [ 9.073]
 [10.338]
 [ 3.957]
 [ 7.832]
 [10.401]
 [ 6.282]] [[ 0.029]
 [ 0.039]
 [ 0.042]
 [-0.018]
 [ 0.026]
 [ 0.055]
 [ 0.022]]
printing an ep nov before normalisation:  86.19078508861985
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.0748769710399692, 0.0958395507492509, 0.0958395507492509, 0.14023089601596472, 0.5183360604055951, 0.0748769710399692]
printing an ep nov before normalisation:  34.64884523700021
siam score:  -0.8546308
siam score:  -0.852533
Printing some Q and Qe and total Qs values:  [[0.221]
 [0.228]
 [0.248]
 [0.249]
 [0.243]
 [0.243]
 [0.243]] [[46.238]
 [46.779]
 [44.506]
 [46.827]
 [31.171]
 [31.171]
 [31.171]] [[1.396]
 [1.428]
 [1.344]
 [1.451]
 [0.733]
 [0.733]
 [0.733]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.06978415574475128, 0.10649786437036488, 0.10649786437036488, 0.1460910795548501, 0.4833279676486444, 0.08780106831102462]
maxi score, test score, baseline:  -0.9919634920634921 -1.0 -0.9919634920634921
probs:  [0.06978415574475128, 0.10649786437036488, 0.10649786437036488, 0.1460910795548501, 0.4833279676486444, 0.08780106831102462]
printing an ep nov before normalisation:  22.988095627089102
Printing some Q and Qe and total Qs values:  [[0.714]
 [0.628]
 [0.714]
 [0.699]
 [0.714]
 [0.747]
 [0.714]] [[51.303]
 [46.083]
 [51.303]
 [52.691]
 [51.303]
 [51.796]
 [51.303]] [[1.64 ]
 [1.366]
 [1.64 ]
 [1.674]
 [1.64 ]
 [1.691]
 [1.64 ]]
printing an ep nov before normalisation:  0.043712131394436755
using another actor
from probs:  [0.06931435341780355, 0.10729794449462483, 0.10729794449462483, 0.1482606407539416, 0.47987466780016874, 0.08795444903883623]
line 256 mcts: sample exp_bonus 47.71017194393519
printing an ep nov before normalisation:  106.53516708378166
printing an ep nov before normalisation:  82.0071644253201
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.06758358176854978, 0.11989118376322101, 0.08438541756077753, 0.15818171594232694, 0.46814855739759237, 0.10180954356753238]
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.06758358176854978, 0.11989118376322101, 0.08438541756077753, 0.15818171594232694, 0.46814855739759237, 0.10180954356753238]
siam score:  -0.8564337
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.06706936514871344, 0.12108116256377253, 0.08441860916688397, 0.16061921034473098, 0.4644012349975788, 0.10241041777832019]
printing an ep nov before normalisation:  48.200299357264726
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.06774640363912342, 0.12455170345718486, 0.06774640363912342, 0.16613466802988372, 0.46890551771460715, 0.10491530352007741]
printing an ep nov before normalisation:  15.432259336327911
maxi score, test score, baseline:  -0.9922076923076923 -1.0 -0.9922076923076923
probs:  [0.067218672103295, 0.12594314406602847, 0.067218672103295, 0.16893099282306223, 0.4650451923068901, 0.1056433265974293]
printing an ep nov before normalisation:  85.98676550888898
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  63.488635120749656
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.534]
 [0.458]
 [0.508]
 [0.458]
 [0.458]
 [0.458]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.458]
 [0.534]
 [0.458]
 [0.508]
 [0.458]
 [0.458]
 [0.458]]
maxi score, test score, baseline:  -0.9922664122137405 -1.0 -0.9922664122137405
printing an ep nov before normalisation:  54.70522039968525
using explorer policy with actor:  1
from probs:  [0.0666334122084095, 0.14258417107325586, 0.08460390426125271, 0.12254160970614353, 0.4610952930447949, 0.12254160970614353]
from probs:  [0.0679441424132977, 0.14539190392625667, 0.08626883598556037, 0.10525988205135971, 0.4701809354298552, 0.1249543001936703]
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.14168350114828
printing an ep nov before normalisation:  45.61068058013916
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.33843398585733
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.06742580966245854, 0.14747649522642542, 0.08636637365750431, 0.10599568543418805, 0.46638362726126736, 0.1263520087581563]
maxi score, test score, baseline:  -0.9924925925925926 -1.0 -0.9924925925925926
probs:  [0.06742580966245854, 0.14747649522642542, 0.08636637365750431, 0.10599568543418805, 0.46638362726126736, 0.1263520087581563]
printing an ep nov before normalisation:  45.30008244661161
Printing some Q and Qe and total Qs values:  [[0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]
 [0.685]] [[65.816]
 [65.816]
 [65.816]
 [65.816]
 [65.816]
 [65.816]
 [65.816]] [[2.07]
 [2.07]
 [2.07]
 [2.07]
 [2.07]
 [2.07]
 [2.07]]
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.06882510136671577, 0.15054135066680427, 0.08815974963861181, 0.10819747602948575, 0.4760788462688966, 0.10819747602948575]
printing an ep nov before normalisation:  50.853580433265506
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.06882510136671577, 0.15054135066680427, 0.08815974963861181, 0.10819747602948575, 0.4760788462688966, 0.10819747602948575]
printing an ep nov before normalisation:  48.74656425916669
line 256 mcts: sample exp_bonus 31.44368765177845
printing an ep nov before normalisation:  45.764926204650465
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.0703402576555844, 0.13181929183338426, 0.09010137578416313, 0.11058108002650817, 0.48657691467385183, 0.11058108002650817]
maxi score, test score, baseline:  -0.9925470588235294 -1.0 -0.9925470588235294
probs:  [0.0718649295146031, 0.11297965366805518, 0.09205519583995916, 0.11297965366805518, 0.4971409136412721, 0.11297965366805518]
printing an ep nov before normalisation:  10.783212718580728
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.07186481235603018, 0.11297995430012028, 0.0920552838464317, 0.11297995430012028, 0.49714004089717734, 0.11297995430012028]
printing an ep nov before normalisation:  28.37773541668689
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.07186481235603018, 0.11297995430012028, 0.0920552838464317, 0.11297995430012028, 0.49714004089717734, 0.11297995430012028]
printing an ep nov before normalisation:  36.50278023433827
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.07302979677077708, 0.09441620422158961, 0.09441620422158961, 0.11658029921606788, 0.5049771963539078, 0.11658029921606788]
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.07302979677077708, 0.09441620422158961, 0.09441620422158961, 0.11658029921606788, 0.5049771963539078, 0.11658029921606788]
printing an ep nov before normalisation:  49.667178589082674
actions average: 
K:  1  action  0 :  tensor([0.2130, 0.0965, 0.1260, 0.1374, 0.1203, 0.1454, 0.1615],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0219, 0.8791, 0.0164, 0.0214, 0.0121, 0.0178, 0.0313],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1885, 0.0083, 0.1837, 0.1472, 0.1275, 0.1870, 0.1579],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1319, 0.0830, 0.1228, 0.2335, 0.1021, 0.1610, 0.1658],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2147, 0.0023, 0.1277, 0.1439, 0.1977, 0.1506, 0.1630],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1911, 0.0148, 0.1411, 0.1519, 0.1385, 0.1985, 0.1642],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1556, 0.0748, 0.1224, 0.1411, 0.1058, 0.1435, 0.2568],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.73595428466797
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.571]
 [0.554]
 [0.571]
 [0.571]
 [0.571]
 [0.571]] [[43.313]
 [43.313]
 [38.813]
 [43.313]
 [43.313]
 [43.313]
 [43.313]] [[3.024]
 [3.024]
 [2.554]
 [3.024]
 [3.024]
 [3.024]
 [3.024]]
maxi score, test score, baseline:  -0.9926007299270073 -1.0 -0.9926007299270073
probs:  [0.0689325494992342, 0.1083850341825173, 0.1083850341825173, 0.1083850341825173, 0.47672509457405665, 0.12918725337915726]
printing an ep nov before normalisation:  45.61986124086149
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.07034281904713914, 0.09012048054022752, 0.11060448708664035, 0.11060448708664035, 0.48649435964097504, 0.1318333665983776]
printing an ep nov before normalisation:  42.00084635809786
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.07034281904713914, 0.09012048054022752, 0.11060448708664035, 0.11060448708664035, 0.48649435964097504, 0.1318333665983776]
Printing some Q and Qe and total Qs values:  [[-0.042]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.013]] [[37.093]
 [14.768]
 [14.768]
 [14.768]
 [14.768]
 [14.768]
 [14.768]] [[1.008]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]
 [0.401]]
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.07340149495511512, 0.09404076886820797, 0.1154171597067686, 0.09404076886820797, 0.5076826478949316, 0.1154171597067686]
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.07340149495511512, 0.09404076886820797, 0.1154171597067686, 0.09404076886820797, 0.5076826478949316, 0.1154171597067686]
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.07340149495511512, 0.09404076886820797, 0.1154171597067686, 0.09404076886820797, 0.5076826478949316, 0.1154171597067686]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.06989404617490519, 0.11157270636095862, 0.13352842913754026, 0.09037407747322448, 0.48305803449241275, 0.11157270636095862]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.405]
 [0.443]
 [0.443]
 [0.351]
 [0.443]
 [0.443]] [[45.987]
 [55.498]
 [69.675]
 [69.675]
 [43.131]
 [69.675]
 [69.675]] [[1.676]
 [2.405]
 [3.44 ]
 [3.44 ]
 [1.481]
 [3.44 ]
 [3.44 ]]
maxi score, test score, baseline:  -0.9926536231884058 -1.0 -0.9926536231884058
probs:  [0.06989404617490519, 0.11157270636095862, 0.13352842913754026, 0.09037407747322448, 0.48305803449241275, 0.11157270636095862]
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.05073642730713
maxi score, test score, baseline:  -0.9927057553956835 -1.0 -0.9927057553956835
probs:  [0.06989390907279217, 0.11157299247193395, 0.13352893819112455, 0.090374148329267, 0.4830570194629483, 0.11157299247193395]
maxi score, test score, baseline:  -0.9927057553956835 -1.0 -0.9927057553956835
probs:  [0.07140601859942469, 0.1139885240548371, 0.13642037960724154, 0.09233018076286018, 0.49352471621277627, 0.09233018076286018]
maxi score, test score, baseline:  -0.9927571428571429 -1.0 -0.9927571428571429
probs:  [0.07140589600006025, 0.11398883686558106, 0.1364209217858107, 0.0923302721150145, 0.4935238011185191, 0.0923302721150145]
printing an ep nov before normalisation:  28.067259788513184
using explorer policy with actor:  1
using explorer policy with actor:  0
printing an ep nov before normalisation:  14.50540542602539
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
probs:  [0.07140577515003325, 0.11398914521296337, 0.13642145622825705, 0.09233036216371457, 0.4935228990813172, 0.09233036216371457]
maxi score, test score, baseline:  -0.9928078014184397 -1.0 -0.9928078014184397
probs:  [0.07140577515003325, 0.11398914521296337, 0.13642145622825705, 0.09233036216371457, 0.4935228990813172, 0.09233036216371457]
printing an ep nov before normalisation:  71.42636584903434
printing an ep nov before normalisation:  49.37124107338261
printing an ep nov before normalisation:  3.2705916430552406
from probs:  [0.0709740601227552, 0.11509442896407436, 0.1383364089786977, 0.0926538965361619, 0.490287308862149, 0.0926538965361619]
maxi score, test score, baseline:  -0.9930034482758621 -1.0 -0.9930034482758621
probs:  [0.0709738158325449, 0.115095052319953, 0.13833748939814122, 0.09265407858928858, 0.4902854852707838, 0.09265407858928858]
printing an ep nov before normalisation:  52.452286133897466
maxi score, test score, baseline:  -0.9930506849315068 -1.0 -0.9930506849315068
probs:  [0.07004241586866777, 0.11747964068192633, 0.14246889303891103, 0.09335208668207934, 0.4833048770463361, 0.09335208668207934]
printing an ep nov before normalisation:  83.32587027158874
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9930972789115646 -1.0 -0.9930972789115646
probs:  [0.07004228234235059, 0.11747998145832024, 0.14246948367119688, 0.09335218621830112, 0.4833038800915301, 0.09335218621830112]
siam score:  -0.8504184
maxi score, test score, baseline:  -0.9930972789115646 -1.0 -0.9930972789115646
probs:  [0.07004228234235059, 0.11747998145832024, 0.14246948367119688, 0.09335218621830112, 0.4833038800915301, 0.09335218621830112]
maxi score, test score, baseline:  -0.9930972789115646 -1.0 -0.9930972789115646
maxi score, test score, baseline:  -0.9931432432432432 -1.0 -0.9931432432432432
probs:  [0.07004215063046833, 0.11748031760408853, 0.1424700662776916, 0.09335228440198871, 0.4833028966837743, 0.09335228440198871]
printing an ep nov before normalisation:  74.47575700290128
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  65.65784452172807
maxi score, test score, baseline:  -0.9931432432432432 -1.0 -0.9931432432432432
probs:  [0.07004215063046833, 0.11748031760408853, 0.1424700662776916, 0.09335228440198871, 0.4833028966837743, 0.09335228440198871]
printing an ep nov before normalisation:  60.899901908983075
printing an ep nov before normalisation:  30.409200138105277
Printing some Q and Qe and total Qs values:  [[-0.161]
 [-0.144]
 [-0.167]
 [-0.168]
 [-0.167]
 [-0.169]
 [-0.17 ]] [[35.608]
 [35.251]
 [34.396]
 [34.841]
 [34.199]
 [33.901]
 [34.482]] [[1.039]
 [1.034]
 [0.957]
 [0.984]
 [0.944]
 [0.923]
 [0.959]]
maxi score, test score, baseline:  -0.9931885906040269 -1.0 -0.9931885906040269
maxi score, test score, baseline:  -0.9931885906040269 -1.0 -0.9931885906040269
probs:  [0.07183528808884652, 0.12049078991813533, 0.12049078991813533, 0.095743595022204, 0.49569594203047485, 0.095743595022204]
actor:  1 policy actor:  1  step number:  73 total reward:  0.013333333333332753  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  33.41934681741915
maxi score, test score, baseline:  -0.9932333333333333 -1.0 -0.9932333333333333
probs:  [0.07908915982153482, 0.0971376374949257, 0.0971376374949257, 0.08795780833345959, 0.5507199485216945, 0.08795780833345959]
maxi score, test score, baseline:  -0.9932333333333333 -1.0 -0.9932333333333333
probs:  [0.0790012816094882, 0.09742055305305067, 0.09742055305305067, 0.0880521305084801, 0.5500533512674503, 0.0880521305084801]
maxi score, test score, baseline:  -0.9932333333333333 -1.0 -0.9932333333333333
probs:  [0.0790012816094882, 0.09742055305305067, 0.09742055305305067, 0.0880521305084801, 0.5500533512674503, 0.0880521305084801]
maxi score, test score, baseline:  -0.9932333333333333 -1.0 -0.9932333333333333
probs:  [0.0790012816094882, 0.09742055305305067, 0.09742055305305067, 0.0880521305084801, 0.5500533512674503, 0.0880521305084801]
maxi score, test score, baseline:  -0.9932333333333333 -1.0 -0.9932333333333333
probs:  [0.07959603668963791, 0.09896907996990946, 0.08911554933597819, 0.08911554933597819, 0.5540882353325179, 0.08911554933597819]
printing an ep nov before normalisation:  55.844306751001106
printing an ep nov before normalisation:  54.25606542227556
from probs:  [0.07959603668963791, 0.09896907996990946, 0.08911554933597819, 0.08911554933597819, 0.5540882353325179, 0.08911554933597819]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.08112056348048877, 0.09103398302277509, 0.08112056348048877, 0.09103398302277509, 0.5646569239706973, 0.09103398302277509]
printing an ep nov before normalisation:  64.44091590795557
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.08112056348048877, 0.09103398302277509, 0.08112056348048877, 0.09103398302277509, 0.5646569239706973, 0.09103398302277509]
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.08112056348048877, 0.09103398302277509, 0.08112056348048877, 0.09103398302277509, 0.5646569239706973, 0.09103398302277509]
siam score:  -0.8673158
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.08112056348048877, 0.09103398302277509, 0.08112056348048877, 0.09103398302277509, 0.5646569239706973, 0.09103398302277509]
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]] [[33.101]
 [33.101]
 [33.101]
 [33.101]
 [33.101]
 [33.101]
 [33.101]] [[0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.56585357702141
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.08112056348048877, 0.09103398302277509, 0.08112056348048877, 0.09103398302277509, 0.5646569239706973, 0.09103398302277509]
printing an ep nov before normalisation:  62.37794811699875
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.8689235
printing an ep nov before normalisation:  45.84971100139157
printing an ep nov before normalisation:  26.309903229000405
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
maxi score, test score, baseline:  -0.993321052631579 -1.0 -0.993321052631579
probs:  [0.08107293793878129, 0.09119655921193613, 0.08107293793878129, 0.09119655921193613, 0.564264446486629, 0.09119655921193613]
printing an ep nov before normalisation:  21.70046329498291
printing an ep nov before normalisation:  24.994381413231103
printing an ep nov before normalisation:  60.4641217292386
printing an ep nov before normalisation:  61.70726370739032
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.08190138320096074, 0.09212867123308095, 0.08190138320096074, 0.09212867123308095, 0.5700385079309558, 0.08190138320096074]
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.08190138320096074, 0.09212867123308095, 0.08190138320096074, 0.09212867123308095, 0.5700385079309558, 0.08190138320096074]
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
printing an ep nov before normalisation:  21.28144798446008
printing an ep nov before normalisation:  63.61880527549936
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.08186994499155271, 0.09231666534168481, 0.08186994499155271, 0.09231666534168481, 0.5697568343419721, 0.08186994499155271]
maxi score, test score, baseline:  -0.9933640522875817 -1.0 -0.9933640522875817
probs:  [0.08183777281189389, 0.09250904844335284, 0.08183777281189389, 0.09250904844335284, 0.5694685846776126, 0.08183777281189389]
maxi score, test score, baseline:  -0.9934064935064936 -1.0 -0.9934064935064936
probs:  [0.08183776555652893, 0.09250909094650793, 0.08183776555652893, 0.09250909094650793, 0.5694685214373973, 0.08183776555652893]
Printing some Q and Qe and total Qs values:  [[0.277]
 [0.275]
 [0.275]
 [0.275]
 [0.275]
 [0.275]
 [0.275]] [[53.537]
 [53.175]
 [53.175]
 [53.175]
 [53.175]
 [53.175]
 [53.175]] [[0.798]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]]
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
probs:  [0.0818377583953433, 0.09250913289797916, 0.0818377583953433, 0.09250913289797916, 0.5694684590180118, 0.0818377583953433]
printing an ep nov before normalisation:  90.35190718353171
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
probs:  [0.08271977543481035, 0.08271977543481035, 0.08271977543481035, 0.09350634959597857, 0.57561454866478, 0.08271977543481035]
printing an ep nov before normalisation:  35.42230770848232
using explorer policy with actor:  1
printing an ep nov before normalisation:  3.9871160263686534
printing an ep nov before normalisation:  64.66782906961186
using explorer policy with actor:  1
printing an ep nov before normalisation:  67.07549334385891
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.74013267894225
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
probs:  [0.07656857374225365, 0.09518650279963375, 0.09518650279963375, 0.10497696549360096, 0.532894952365244, 0.09518650279963375]
printing an ep nov before normalisation:  45.14037609100342
printing an ep nov before normalisation:  31.11303013489362
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
probs:  [0.07729946809049063, 0.09609542654996979, 0.08654081433306784, 0.10597950815366142, 0.5379893563228404, 0.09609542654996979]
printing an ep nov before normalisation:  97.77414877941109
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
probs:  [0.07883046339531832, 0.09799933783012597, 0.08825515999243207, 0.09799933783012597, 0.5486605409595656, 0.08825515999243207]
printing an ep nov before normalisation:  60.7073974609375
printing an ep nov before normalisation:  56.954407691955566
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
probs:  [0.07864775359544271, 0.09858871677776004, 0.08845206049341542, 0.09858871677776004, 0.5472706918622062, 0.08845206049341542]
Printing some Q and Qe and total Qs values:  [[0.211]
 [0.183]
 [0.211]
 [0.211]
 [0.211]
 [0.211]
 [0.211]] [[32.769]
 [51.39 ]
 [32.769]
 [32.769]
 [32.769]
 [32.769]
 [32.769]] [[0.63]
 [0.85]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]]
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
probs:  [0.07417419086338897, 0.10920872007695226, 0.1000121561583919, 0.10920872007695226, 0.5162790938986288, 0.09111711892568589]
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([0.3265, 0.0078, 0.1073, 0.1176, 0.1609, 0.1376, 0.1424],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0230, 0.8918, 0.0140, 0.0121, 0.0069, 0.0081, 0.0439],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1447, 0.0084, 0.2187, 0.1281, 0.1312, 0.2080, 0.1609],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1776, 0.0292, 0.1275, 0.2026, 0.1339, 0.1577, 0.1713],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1828, 0.0073, 0.1053, 0.1351, 0.2506, 0.1616, 0.1573],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1657, 0.0069, 0.1361, 0.1233, 0.1397, 0.2416, 0.1867],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1452, 0.0472, 0.1140, 0.1559, 0.1377, 0.1658, 0.2341],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
probs:  [0.07417419086338897, 0.10920872007695226, 0.1000121561583919, 0.10920872007695226, 0.5162790938986288, 0.09111711892568589]
printing an ep nov before normalisation:  47.68545768946569
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  35.243378763575514
printing an ep nov before normalisation:  66.19585990905762
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
probs:  [0.07551745551529383, 0.10182417788112139, 0.10182417788112139, 0.11118758753675505, 0.5256431061975672, 0.08400349498814141]
siam score:  -0.85417557
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
probs:  [0.07607385699056313, 0.10311473319660283, 0.0938055790928843, 0.11273945184621029, 0.5294696585909054, 0.08479672028283394]
printing an ep nov before normalisation:  46.64297580718994
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.41 ]
 [0.373]
 [0.363]
 [0.361]
 [0.363]
 [0.371]] [[33.907]
 [47.159]
 [34.785]
 [33.364]
 [33.25 ]
 [33.51 ]
 [33.345]] [[0.364]
 [0.41 ]
 [0.373]
 [0.363]
 [0.361]
 [0.363]
 [0.371]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
probs:  [0.0759382912898694, 0.10348183000769019, 0.09399962815401415, 0.11328546243267724, 0.52847148433626, 0.084823303779489]
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.414]
 [0.38 ]
 [0.428]
 [0.415]
 [0.388]
 [0.381]] [[46.821]
 [55.145]
 [52.552]
 [50.828]
 [50.963]
 [46.821]
 [51.015]] [[1.395]
 [1.82 ]
 [1.662]
 [1.628]
 [1.62 ]
 [1.395]
 [1.589]]
maxi score, test score, baseline:  -0.9934483870967742 -1.0 -0.9934483870967742
probs:  [0.0759382912898694, 0.10348183000769019, 0.09399962815401415, 0.11328546243267724, 0.52847148433626, 0.084823303779489]
printing an ep nov before normalisation:  29.914832291656438
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]] [[40.227]
 [42.111]
 [42.111]
 [42.111]
 [42.111]
 [42.111]
 [42.111]] [[0.727]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]]
printing an ep nov before normalisation:  39.36624520841974
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.07593826057204878, 0.10348191275409944, 0.09399967183896729, 0.11328558556465985, 0.5284712596071917, 0.08482330966303289]
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.07593826057204878, 0.10348191275409944, 0.09399967183896729, 0.11328558556465985, 0.5284712596071917, 0.08482330966303289]
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.07580016424879482, 0.10385586220101825, 0.09419734323385931, 0.11384178859079272, 0.5274544523308615, 0.0848503893946733]
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.07649177918984443, 0.10480391331521322, 0.09505711304254527, 0.11488111359712415, 0.5322743016654286, 0.07649177918984443]
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.491]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]] [[53.048]
 [58.129]
 [53.048]
 [53.048]
 [53.048]
 [53.048]
 [53.048]] [[0.63 ]
 [0.759]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]]
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.07649177918984443, 0.10480391331521322, 0.09505711304254527, 0.11488111359712415, 0.5322743016654286, 0.07649177918984443]
maxi score, test score, baseline:  -0.9934897435897436 -1.0 -0.9934897435897436
probs:  [0.07649177918984443, 0.10480391331521322, 0.09505711304254527, 0.11488111359712415, 0.5322743016654286, 0.07649177918984443]
using another actor
printing an ep nov before normalisation:  40.81893477192139
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.39 ]
 [0.352]
 [0.322]
 [0.359]
 [0.367]
 [0.359]] [[48.295]
 [49.245]
 [48.524]
 [49.068]
 [47.953]
 [48.237]
 [48.403]] [[0.617]
 [0.696]
 [0.65 ]
 [0.627]
 [0.65 ]
 [0.661]
 [0.656]]
maxi score, test score, baseline:  -0.9935305732484077 -1.0 -0.9935305732484077
probs:  [0.07636272226056738, 0.10520633864823765, 0.09527656907215443, 0.11547271058283198, 0.531318937175641, 0.07636272226056738]
printing an ep nov before normalisation:  23.36542143104167
printing an ep nov before normalisation:  64.28052165640229
printing an ep nov before normalisation:  18.346996307373047
from probs:  [0.07636272226056738, 0.10520633864823765, 0.09527656907215443, 0.11547271058283198, 0.531318937175641, 0.07636272226056738]
using another actor
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([0.3462, 0.0333, 0.1221, 0.1086, 0.1369, 0.1189, 0.1341],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0082, 0.9101, 0.0118, 0.0304, 0.0073, 0.0113, 0.0210],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1612, 0.0181, 0.1761, 0.1549, 0.1525, 0.1672, 0.1699],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1708, 0.0047, 0.1325, 0.2009, 0.1580, 0.1689, 0.1642],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1519, 0.0052, 0.1020, 0.1417, 0.3036, 0.1529, 0.1427],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1823, 0.0119, 0.1374, 0.1608, 0.1540, 0.1978, 0.1559],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1688, 0.0054, 0.1303, 0.1695, 0.1608, 0.2023, 0.1629],
       grad_fn=<DivBackward0>)
from probs:  [0.07352817591424421, 0.11073347244276936, 0.1009747061402054, 0.12082304438270823, 0.5115540212704655, 0.08238657984960734]
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
probs:  [0.07425206296215878, 0.10196929557251472, 0.10196929557251472, 0.12201339599130301, 0.5165981134513514, 0.08319783645015719]
printing an ep nov before normalisation:  76.43586797261094
maxi score, test score, baseline:  -0.99365 -1.0 -0.99365
probs:  [0.07425206296215878, 0.10196929557251472, 0.10196929557251472, 0.12201339599130301, 0.5165981134513514, 0.08319783645015719]
printing an ep nov before normalisation:  53.073930740356445
printing an ep nov before normalisation:  45.42206036837355
printing an ep nov before normalisation:  74.07740342770852
using explorer policy with actor:  1
siam score:  -0.8545721
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.07470947550526043, 0.10373895376933023, 0.10373895376933023, 0.11406054604099942, 0.5196732929013101, 0.0840787780137698]
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
printing an ep nov before normalisation:  77.76065855126203
printing an ep nov before normalisation:  47.071569986721244
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.07548787771223296, 0.10482035606447777, 0.10482035606447777, 0.10482035606447777, 0.5250960801943393, 0.08495497389999451]
printing an ep nov before normalisation:  50.95035522380858
printing an ep nov before normalisation:  42.26955706137743
Printing some Q and Qe and total Qs values:  [[0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]
 [0.63]]
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.07612480678699061, 0.10630670252682316, 0.10630670252682316, 0.09592153410021413, 0.5294742004989849, 0.08586605356016408]
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.07599020555929888, 0.10673307979980208, 0.10673307979980208, 0.09615488651274728, 0.5284762394427507, 0.08591250888559886]
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.67 ]
 [0.67 ]
 [0.67 ]
 [0.638]
 [0.635]
 [0.633]] [[30.484]
 [30.484]
 [30.484]
 [30.484]
 [31.613]
 [30.809]
 [30.422]] [[0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.773]
 [0.764]
 [0.758]]
printing an ep nov before normalisation:  57.02934799961616
printing an ep nov before normalisation:  31.893842498538167
printing an ep nov before normalisation:  0.00014481086907380813
printing an ep nov before normalisation:  22.364007339940187
using another actor
Printing some Q and Qe and total Qs values:  [[ 0.138]
 [ 0.177]
 [ 0.115]
 [ 0.141]
 [ 0.086]
 [ 0.013]
 [-0.008]] [[67.19 ]
 [65.217]
 [ 0.   ]
 [60.175]
 [59.024]
 [54.918]
 [51.769]] [[ 0.138]
 [ 0.177]
 [ 0.115]
 [ 0.141]
 [ 0.086]
 [ 0.013]
 [-0.008]]
printing an ep nov before normalisation:  47.14189017451601
printing an ep nov before normalisation:  59.787466015279946
maxi score, test score, baseline:  -0.9936888198757764 -1.0 -0.9936888198757764
probs:  [0.07300433237449239, 0.11251634851910744, 0.11251634851910744, 0.10216037654572041, 0.5076694284542107, 0.09213316558736162]
Printing some Q and Qe and total Qs values:  [[0.609]
 [0.702]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]] [[81.147]
 [84.204]
 [81.147]
 [81.147]
 [81.147]
 [81.147]
 [81.147]] [[1.449]
 [1.604]
 [1.449]
 [1.449]
 [1.449]
 [1.449]
 [1.449]]
from probs:  [0.07300429136251851, 0.11251646342107102, 0.11251646342107102, 0.10216045058314399, 0.5076691311546634, 0.09213320005753207]
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.07065485912506834, 0.11677091795612869, 0.11677091795612869, 0.1069526602695158, 0.4914045513900455, 0.09744609330311298]
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.07065485912506834, 0.11677091795612869, 0.11677091795612869, 0.1069526602695158, 0.4914045513900455, 0.09744609330311298]
maxi score, test score, baseline:  -0.9937271604938271 -1.0 -0.9937271604938271
probs:  [0.07065485912506834, 0.11677091795612869, 0.11677091795612869, 0.1069526602695158, 0.4914045513900455, 0.09744609330311298]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.07065481204160014, 0.116771041267141, 0.116771041267141, 0.10695274730299364, 0.49140421291040015, 0.0974461452107239]
printing an ep nov before normalisation:  36.5844464302063
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.43 ]
 [0.463]
 [0.468]
 [0.462]
 [0.463]
 [0.449]] [[88.56 ]
 [76.199]
 [82.938]
 [89.677]
 [84.838]
 [83.267]
 [85.011]] [[1.043]
 [0.861]
 [0.978]
 [1.067]
 [1.001]
 [0.982]
 [0.99 ]]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.07272791633165819, 0.10030682719775454, 0.12019981208477465, 0.11009289234378863, 0.5058459754544901, 0.09082657658753389]
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.581]
 [0.488]
 [0.606]
 [0.531]
 [0.666]
 [0.447]] [[45.261]
 [44.5  ]
 [41.033]
 [43.412]
 [39.119]
 [35.455]
 [41.579]] [[1.846]
 [1.881]
 [1.654]
 [1.864]
 [1.624]
 [1.617]
 [1.634]]
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.0732980407517568, 0.1016420414235571, 0.11169959004903458, 0.11169959004903458, 0.5097619465339912, 0.09189879119262574]
Printing some Q and Qe and total Qs values:  [[ 0.025]
 [ 0.065]
 [-0.001]
 [-0.008]
 [ 0.007]
 [-0.018]
 [-0.019]] [[21.825]
 [30.942]
 [22.397]
 [22.078]
 [21.721]
 [22.176]
 [22.48 ]] [[0.192]
 [0.372]
 [0.175]
 [0.163]
 [0.172]
 [0.154]
 [0.158]]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.841984
maxi score, test score, baseline:  -0.9937650306748467 -1.0 -0.9937650306748467
probs:  [0.0732980407517568, 0.1016420414235571, 0.11169959004903458, 0.11169959004903458, 0.5097619465339912, 0.09189879119262574]
using explorer policy with actor:  1
siam score:  -0.84673554
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.0733445470131937, 0.09348268053636928, 0.11492004848039507, 0.11492004848039507, 0.5098499949532777, 0.09348268053636928]
printing an ep nov before normalisation:  60.500355692104854
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.07393655713082807, 0.08413141932731272, 0.11668920505156984, 0.11668920505156984, 0.513908742471282, 0.09464487096743729]
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.07477297611637859, 0.08508337749317435, 0.10668612323503196, 0.11801014318036064, 0.5197314010620596, 0.0957159789129949]
from probs:  [0.07477297611637859, 0.08508337749317435, 0.10668612323503196, 0.11801014318036064, 0.5197314010620596, 0.0957159789129949]
maxi score, test score, baseline:  -0.9938024390243902 -1.0 -0.9938024390243902
probs:  [0.0746180592664353, 0.08511432460595958, 0.10710649960305803, 0.11863465585153703, 0.5185878624356661, 0.09593859823734388]
printing an ep nov before normalisation:  22.550992542297067
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.07545990804040245, 0.0860749025540399, 0.09702161564622856, 0.11997440116210807, 0.5244475569509924, 0.09702161564622856]
printing an ep nov before normalisation:  74.04236547073204
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.07545990804040245, 0.0860749025540399, 0.09702161564622856, 0.11997440116210807, 0.5244475569509924, 0.09702161564622856]
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.516]
 [0.494]
 [0.5  ]
 [0.377]
 [0.369]
 [0.349]] [[60.023]
 [67.934]
 [64.519]
 [65.517]
 [59.042]
 [59.189]
 [59.387]] [[1.422]
 [1.843]
 [1.706]
 [1.746]
 [1.404]
 [1.401]
 [1.388]]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
printing an ep nov before normalisation:  45.34959100085623
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.07349053003571059, 0.09340565390383035, 0.10382997655354925, 0.11458523008103708, 0.5108586328723235, 0.10382997655354925]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.07349053003571059, 0.09340565390383035, 0.10382997655354925, 0.11458523008103708, 0.5108586328723235, 0.10382997655354925]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.07349053003571059, 0.09340565390383035, 0.10382997655354925, 0.11458523008103708, 0.5108586328723235, 0.10382997655354925]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.07349053003571059, 0.09340565390383035, 0.10382997655354925, 0.11458523008103708, 0.5108586328723235, 0.10382997655354925]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.537]
 [0.529]
 [0.54 ]
 [0.536]
 [0.564]
 [0.578]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.429]
 [0.537]
 [0.529]
 [0.54 ]
 [0.536]
 [0.564]
 [0.578]]
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.07583593975979722, 0.08595617304275852, 0.0963877981190417, 0.11824453637411134, 0.5271877545852496, 0.0963877981190417]
printing an ep nov before normalisation:  28.906371593475342
maxi score, test score, baseline:  -0.9938759036144579 -1.0 -0.9938759036144579
probs:  [0.07570076234423301, 0.08600265530204054, 0.09662152958162691, 0.11887059950076016, 0.5261829236897124, 0.09662152958162691]
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.373]
 [0.363]
 [0.315]
 [0.319]
 [0.315]
 [0.26 ]] [[44.494]
 [53.477]
 [49.832]
 [44.494]
 [42.916]
 [44.494]
 [50.516]] [[0.526]
 [0.67 ]
 [0.626]
 [0.526]
 [0.516]
 [0.526]
 [0.528]]
Printing some Q and Qe and total Qs values:  [[0.772]
 [0.769]
 [0.364]
 [0.76 ]
 [0.364]
 [0.364]
 [0.364]] [[ 4.627]
 [ 4.592]
 [56.17 ]
 [ 4.717]
 [56.17 ]
 [56.17 ]
 [56.17 ]] [[0.772]
 [0.769]
 [0.686]
 [0.761]
 [0.686]
 [0.686]
 [0.686]]
from probs:  [0.07570076234423301, 0.08600265530204054, 0.09662152958162691, 0.11887059950076016, 0.5261829236897124, 0.09662152958162691]
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
probs:  [0.07452714987316761, 0.09408387273173387, 0.10431354315006085, 0.11486289076896061, 0.5181286707443431, 0.09408387273173387]
maxi score, test score, baseline:  -0.9939119760479042 -1.0 -0.9939119760479042
probs:  [0.07513188262435117, 0.0850311251332745, 0.105743386382714, 0.11658496063046749, 0.5222783005412703, 0.09523034468792264]
from probs:  [0.07254898830297819, 0.09099679692837226, 0.11057985531532911, 0.12083036243975184, 0.5044040273946494, 0.10063996961891926]
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.07218772788801357, 0.09125160874130638, 0.11148865149326342, 0.12208147855874095, 0.5017737141313298, 0.10121681918734587]
siam score:  -0.8569055
printing an ep nov before normalisation:  19.194712186363127
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.07218772788801357, 0.09125160874130638, 0.11148865149326342, 0.12208147855874095, 0.5017737141313298, 0.10121681918734587]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
probs:  [0.07218772788801357, 0.09125160874130638, 0.11148865149326342, 0.12208147855874095, 0.5017737141313298, 0.10121681918734587]
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.356]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]] [[39.016]
 [47.188]
 [39.016]
 [39.016]
 [39.016]
 [39.016]
 [39.016]] [[1.313]
 [1.747]
 [1.313]
 [1.313]
 [1.313]
 [1.313]
 [1.313]]
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
maxi score, test score, baseline:  -0.9939476190476191 -1.0 -0.9939476190476191
printing an ep nov before normalisation:  18.52720546940212
printing an ep nov before normalisation:  16.316190449403123
maxi score, test score, baseline:  -0.9939828402366864 -1.0 -0.9939828402366864
probs:  [0.07346742159284818, 0.08320963383574934, 0.11423483282468068, 0.1252229241332604, 0.5106225292290526, 0.09324265838440876]
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
probs:  [0.07346738388856958, 0.08320963311019676, 0.11423494986214767, 0.12522308287846368, 0.5106222545191978, 0.09324269574142455]
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
probs:  [0.07403232147320457, 0.07403232147320457, 0.11590609777134345, 0.12719238903920113, 0.5144926423565654, 0.09434422788648082]
printing an ep nov before normalisation:  77.35554437316776
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
probs:  [0.07403232147320457, 0.07403232147320457, 0.11590609777134345, 0.12719238903920113, 0.5144926423565654, 0.09434422788648082]
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
printing an ep nov before normalisation:  65.26241052102836
printing an ep nov before normalisation:  73.24029793934656
printing an ep nov before normalisation:  79.3080481314631
printing an ep nov before normalisation:  60.29218189043925
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
printing an ep nov before normalisation:  62.74330515338743
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
probs:  [0.07292864092400705, 0.07292864092400705, 0.1226681970775408, 0.1226681970775408, 0.5069248071029296, 0.10188151689397447]
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]] [[57.528]
 [57.528]
 [57.528]
 [57.528]
 [57.528]
 [57.528]
 [57.528]] [[1.864]
 [1.864]
 [1.864]
 [1.864]
 [1.864]
 [1.864]
 [1.864]]
printing an ep nov before normalisation:  44.19252872467041
Printing some Q and Qe and total Qs values:  [[0.466]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]] [[36.045]
 [32.815]
 [36.718]
 [32.815]
 [32.815]
 [32.815]
 [32.815]] [[1.899]
 [1.665]
 [1.944]
 [1.665]
 [1.665]
 [1.665]
 [1.665]]
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
probs:  [0.07292864092400705, 0.07292864092400705, 0.1226681970775408, 0.1226681970775408, 0.5069248071029296, 0.10188151689397447]
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
probs:  [0.0703885634019557, 0.07915919037970295, 0.12706030695047654, 0.12706030695047654, 0.48928970244126496, 0.10704192987612345]
maxi score, test score, baseline:  -0.9940176470588236 -1.0 -0.9940176470588236
probs:  [0.07018528343046272, 0.07909328754234195, 0.1277446946149132, 0.1277446946149132, 0.48781927649636614, 0.1074127633010028]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.07091630451026902, 0.07991730518977143, 0.1290766165932076, 0.11864888387126658, 0.4929084630825198, 0.10853242675296562]
printing an ep nov before normalisation:  44.50511866947199
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
probs:  [0.07071909953602926, 0.07986266988418912, 0.1298006310164464, 0.11920773017020991, 0.49147883407374177, 0.10893103531938363]
maxi score, test score, baseline:  -0.9940520467836257 -1.0 -0.9940520467836257
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
probs:  [0.07147538183397203, 0.08071695865640449, 0.12048374377111358, 0.12048374377111358, 0.4967432750546023, 0.110096896912794]
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.482]
 [0.332]
 [0.477]
 [0.44 ]
 [0.218]
 [0.401]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.404]
 [0.482]
 [0.332]
 [0.477]
 [0.44 ]
 [0.218]
 [0.401]]
maxi score, test score, baseline:  -0.9940860465116279 -1.0 -0.9940860465116279
probs:  [0.07128480720974227, 0.08067449351142847, 0.12107859820353281, 0.12107859820353281, 0.49535821559527315, 0.11052528727649061]
from probs:  [0.07128480720974227, 0.08067449351142847, 0.12107859820353281, 0.12107859820353281, 0.49535821559527315, 0.11052528727649061]
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.07128476354815629, 0.08067448368324001, 0.12107873396147908, 0.12107873396147908, 0.4953578998374378, 0.11052538500820772]
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.07202170315349073, 0.08150867567358437, 0.1223314059115628, 0.1223314059115628, 0.5004871028254311, 0.10131970652436807]
line 256 mcts: sample exp_bonus 47.64347240351646
printing an ep nov before normalisation:  53.682449093132405
maxi score, test score, baseline:  -0.9941196531791907 -1.0 -0.9941196531791907
probs:  [0.07262385105192844, 0.08237034591968838, 0.12430980868398851, 0.11335547139480573, 0.5046172024531078, 0.10272332049648118]
printing an ep nov before normalisation:  51.456832698508634
printing an ep nov before normalisation:  74.75567524523709
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.07262381149533652, 0.08237034213230807, 0.12430995881261023, 0.11335558132148661, 0.5046169148346272, 0.10272339140363129]
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
probs:  [0.07262381149533652, 0.08237034213230807, 0.12430995881261023, 0.11335558132148661, 0.5046169148346272, 0.10272339140363129]
Starting evaluation
printing an ep nov before normalisation:  36.369749279347786
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.03]
 [0.03]
 [0.03]
 [0.03]
 [0.03]
 [0.03]
 [0.03]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.03]
 [0.03]
 [0.03]
 [0.03]
 [0.03]
 [0.03]
 [0.03]]
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
maxi score, test score, baseline:  -0.9941528735632184 -1.0 -0.9941528735632184
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.552]
 [0.486]
 [0.486]
 [0.486]
 [0.521]
 [0.486]] [[26.195]
 [45.447]
 [26.195]
 [26.195]
 [26.195]
 [27.223]
 [26.195]] [[0.486]
 [0.552]
 [0.486]
 [0.486]
 [0.486]
 [0.521]
 [0.486]]
printing an ep nov before normalisation:  69.44326281776002
line 256 mcts: sample exp_bonus 53.872495454328316
printing an ep nov before normalisation:  43.584043959620224
printing an ep nov before normalisation:  59.02602177447212
printing an ep nov before normalisation:  62.919790262780204
using explorer policy with actor:  0
printing an ep nov before normalisation:  59.27374726753133
printing an ep nov before normalisation:  41.56748854531912
Printing some Q and Qe and total Qs values:  [[0.796]
 [0.751]
 [0.751]
 [0.751]
 [0.708]
 [0.751]
 [0.751]] [[31.41 ]
 [31.361]
 [31.361]
 [31.361]
 [32.197]
 [31.361]
 [31.361]] [[0.796]
 [0.751]
 [0.751]
 [0.751]
 [0.708]
 [0.751]
 [0.751]]
line 256 mcts: sample exp_bonus 54.08100003813176
printing an ep nov before normalisation:  30.086333239958584
printing an ep nov before normalisation:  47.42829833329225
using explorer policy with actor:  0
printing an ep nov before normalisation:  8.058814679433226
Printing some Q and Qe and total Qs values:  [[0.724]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]] [[18.327]
 [17.503]
 [17.503]
 [17.503]
 [17.503]
 [17.503]
 [17.503]] [[0.724]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]]
siam score:  -0.8488216
printing an ep nov before normalisation:  39.4062206615547
printing an ep nov before normalisation:  10.327069759368896
printing an ep nov before normalisation:  10.890416459319649
printing an ep nov before normalisation:  36.52505912396225
Printing some Q and Qe and total Qs values:  [[0.705]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]] [[31.214]
 [35.706]
 [35.706]
 [35.706]
 [35.706]
 [35.706]
 [35.706]] [[0.705]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]]
printing an ep nov before normalisation:  30.978955170638823
printing an ep nov before normalisation:  34.903367965349126
printing an ep nov before normalisation:  19.800807842729427
printing an ep nov before normalisation:  17.216153139145973
printing an ep nov before normalisation:  10.970795077096245
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]] [[12.075]
 [12.075]
 [12.075]
 [12.075]
 [12.075]
 [12.075]
 [12.075]] [[0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]
 [0.621]]
printing an ep nov before normalisation:  11.280593872070312
printing an ep nov before normalisation:  20.752701484875445
printing an ep nov before normalisation:  13.034809045398509
printing an ep nov before normalisation:  13.8877281618886
printing an ep nov before normalisation:  11.884189107448586
printing an ep nov before normalisation:  62.82334068883036
printing an ep nov before normalisation:  18.19200038518396
printing an ep nov before normalisation:  19.2401155336305
printing an ep nov before normalisation:  15.357566408964729
printing an ep nov before normalisation:  11.15485059404854
printing an ep nov before normalisation:  58.53724520351112
printing an ep nov before normalisation:  11.518916299470195
printing an ep nov before normalisation:  10.275017655400802
printing an ep nov before normalisation:  46.32650573506604
printing an ep nov before normalisation:  49.91926281098982
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.64 ]
 [0.576]
 [0.575]
 [0.583]
 [0.591]
 [0.606]] [[23.794]
 [24.341]
 [19.27 ]
 [19.339]
 [19.038]
 [19.173]
 [22.01 ]] [[0.633]
 [0.64 ]
 [0.576]
 [0.575]
 [0.583]
 [0.591]
 [0.606]]
printing an ep nov before normalisation:  24.708611965179443
printing an ep nov before normalisation:  26.372249844494817
printing an ep nov before normalisation:  33.277125846101555
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]] [[29.933]
 [33.825]
 [33.825]
 [33.825]
 [33.825]
 [33.825]
 [33.825]] [[0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  18.235979293331184
printing an ep nov before normalisation:  71.05916792590463
printing an ep nov before normalisation:  35.46645210002084
printing an ep nov before normalisation:  34.378852443529695
printing an ep nov before normalisation:  33.089173877049575
using explorer policy with actor:  0
printing an ep nov before normalisation:  32.25682400091329
maxi score, test score, baseline:  -0.9947186528497409 -1.0 -0.9947186528497409
probs:  [0.07384490794515666, 0.07384490794515666, 0.11684976923120757, 0.11684976923120757, 0.5129864100605844, 0.10562423558668693]
line 256 mcts: sample exp_bonus 28.588067398341153
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.523]
 [0.49 ]
 [0.483]
 [0.466]
 [0.473]
 [0.502]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.475]
 [0.523]
 [0.49 ]
 [0.483]
 [0.466]
 [0.473]
 [0.502]]
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.51 ]
 [0.473]
 [0.459]
 [0.454]
 [0.449]
 [0.463]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.456]
 [0.51 ]
 [0.473]
 [0.459]
 [0.454]
 [0.449]
 [0.463]]
maxi score, test score, baseline:  -0.9947717948717949 -1.0 -0.9947717948717949
probs:  [0.0708467574358757, 0.08058731743692482, 0.1224571872921803, 0.1224571872921803, 0.4921236307496407, 0.11152791979319814]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.07084668540095028, 0.08058730128180432, 0.1224574113368184, 0.1224574113368184, 0.49212310950497407, 0.11152808113863455]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.07084668540095028, 0.08058730128180432, 0.1224574113368184, 0.1224574113368184, 0.49212310950497407, 0.11152808113863455]
printing an ep nov before normalisation:  53.57440948486328
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.451]
 [0.451]
 [0.509]
 [0.451]
 [0.451]
 [0.451]] [[57.173]
 [57.173]
 [57.173]
 [80.961]
 [57.173]
 [57.173]
 [57.173]] [[1.18 ]
 [1.18 ]
 [1.18 ]
 [1.721]
 [1.18 ]
 [1.18 ]
 [1.18 ]]
printing an ep nov before normalisation:  56.83661697609704
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.07064724047561656, 0.0805430257959393, 0.12308013284449054, 0.12308013284449054, 0.49067277122602826, 0.11197669681343488]
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.353]
 [0.311]
 [0.313]
 [0.324]
 [0.324]
 [0.34 ]] [[26.896]
 [21.263]
 [26.784]
 [26.625]
 [26.525]
 [25.928]
 [25.557]] [[1.541]
 [1.331]
 [1.558]
 [1.552]
 [1.558]
 [1.529]
 [1.527]]
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.07044426401732472, 0.0804979663325561, 0.12371388076220748, 0.12371388076220748, 0.48919675208594254, 0.11243325603976166]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.9948238578680203 -1.0 -0.9948238578680203
probs:  [0.07023766138996006, 0.08045210188304835, 0.12435895056975588, 0.12435895056975588, 0.4876943639028571, 0.11289797168462261]
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.133]
 [0.437]
 [0.414]
 [0.366]
 [0.405]
 [0.337]] [[20.245]
 [21.287]
 [19.157]
 [19.499]
 [20.785]
 [20.719]
 [21.766]] [[1.38 ]
 [1.246]
 [1.432]
 [1.428]
 [1.452]
 [1.487]
 [1.477]]
actions average: 
K:  2  action  0 :  tensor([0.2850, 0.0077, 0.1303, 0.1336, 0.1633, 0.1543, 0.1257],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0279, 0.7821, 0.0305, 0.0364, 0.0202, 0.0278, 0.0751],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1814, 0.0274, 0.1729, 0.1444, 0.1620, 0.1871, 0.1248],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1247, 0.0771, 0.1167, 0.2505, 0.1303, 0.1843, 0.1165],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1682, 0.0347, 0.1193, 0.1498, 0.2268, 0.1642, 0.1370],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1592, 0.0708, 0.1278, 0.1344, 0.1262, 0.2478, 0.1339],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1738, 0.0604, 0.1264, 0.1518, 0.1679, 0.1784, 0.1411],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.574]
 [0.525]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]] [[73.458]
 [63.343]
 [73.458]
 [73.458]
 [73.458]
 [73.458]
 [73.458]] [[2.197]
 [1.706]
 [2.197]
 [2.197]
 [2.197]
 [2.197]
 [2.197]]
maxi score, test score, baseline:  -0.9948494949494949 -1.0 -0.9948494949494949
probs:  [0.06618403025243205, 0.0935202902389665, 0.13377667310470864, 0.13377667310470864, 0.4596772774659804, 0.11306505583320368]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  7.078385465320025e-05
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.425]
 [0.425]
 [0.487]
 [0.425]
 [0.381]
 [0.425]] [[74.009]
 [59.078]
 [59.078]
 [72.137]
 [59.078]
 [72.234]
 [59.078]] [[1.06 ]
 [0.848]
 [0.848]
 [1.093]
 [0.848]
 [0.988]
 [0.848]]
siam score:  -0.84516406
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.06713343212709567, 0.09588902561967992, 0.12718187736161005, 0.12718187736161005, 0.46616520714267967, 0.11644858038732474]
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.06783994993180205, 0.09689877143202671, 0.12852160659403597, 0.12852160659403597, 0.47107951882075527, 0.10713854662734397]
printing an ep nov before normalisation:  57.28536108832316
printing an ep nov before normalisation:  59.601735021001865
actions average: 
K:  2  action  0 :  tensor([0.3214, 0.0429, 0.1129, 0.1329, 0.1443, 0.1201, 0.1256],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0456, 0.7992, 0.0364, 0.0262, 0.0202, 0.0287, 0.0436],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1590, 0.0662, 0.1638, 0.1744, 0.1320, 0.1493, 0.1552],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1294, 0.0389, 0.1109, 0.2655, 0.1648, 0.1482, 0.1422],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1979, 0.0233, 0.1390, 0.1576, 0.1802, 0.1548, 0.1472],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1416, 0.0045, 0.1334, 0.1332, 0.1367, 0.3227, 0.1278],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2176, 0.0453, 0.1370, 0.1388, 0.1355, 0.1643, 0.1614],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  50.2431495852472
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.06852132111892258, 0.08781705483181154, 0.1298136517363345, 0.1298136517363345, 0.4758189186768741, 0.10821540189972263]
printing an ep nov before normalisation:  38.54485746389152
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.06852132111892258, 0.08781705483181154, 0.1298136517363345, 0.1298136517363345, 0.4758189186768741, 0.10821540189972263]
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.06852132111892258, 0.08781705483181154, 0.1298136517363345, 0.1298136517363345, 0.4758189186768741, 0.10821540189972263]
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.06906403864499853, 0.08887203670608483, 0.12073707706522364, 0.1319835618978609, 0.47953136531517027, 0.10981192037066183]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  73.58910269827135
printing an ep nov before normalisation:  82.14475345435328
printing an ep nov before normalisation:  54.997982297624866
maxi score, test score, baseline:  -0.9948748743718593 -1.0 -0.9948748743718593
probs:  [0.06906403864499853, 0.08887203670608483, 0.12073707706522364, 0.1319835618978609, 0.47953136531517027, 0.10981192037066183]
using explorer policy with actor:  0
printing an ep nov before normalisation:  63.77726621403896
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.06906399856498442, 0.08887205198463044, 0.12073718139884376, 0.1319836976626838, 0.47953107621774405, 0.10981199417111348]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.02539030678252
maxi score, test score, baseline:  -0.9949 -1.0 -0.9949
probs:  [0.06884490407568342, 0.08895638615362372, 0.12130963993118009, 0.13272843538208232, 0.47794353867855544, 0.11021709577887502]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.06884486344665545, 0.0889564016427295, 0.12130974569728312, 0.13272857301065508, 0.4779432456098122, 0.11021717059286482]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.06884486344665545, 0.0889564016427295, 0.12130974569728312, 0.13272857301065508, 0.4779432456098122, 0.11021717059286482]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.06961622542421295, 0.0899535372537046, 0.11145298118773868, 0.13421709829436299, 0.48330717665224215, 0.11145298118773868]
printing an ep nov before normalisation:  60.711197888032835
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.06940280333514061, 0.09005577309206336, 0.11188891254938153, 0.13500635432771857, 0.4817572441463142, 0.11188891254938153]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.06940280333514061, 0.09005577309206336, 0.11188891254938153, 0.13500635432771857, 0.4817572441463142, 0.11188891254938153]
printing an ep nov before normalisation:  48.69042869738712
printing an ep nov before normalisation:  0.7873071476399218
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.07052960656323548, 0.08127313586069716, 0.10366809833991293, 0.13973057468512093, 0.4894531129183866, 0.11534547163264691]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.07052960656323548, 0.08127313586069716, 0.10366809833991293, 0.13973057468512093, 0.4894531129183866, 0.11534547163264691]
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.76839637756348
printing an ep nov before normalisation:  57.13427076433275
printing an ep nov before normalisation:  77.5656151748271
printing an ep nov before normalisation:  18.221701781889564
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.07173505694042166, 0.08329965873558853, 0.10740615261847163, 0.11997596728597487, 0.4976071971335685, 0.11997596728597487]
printing an ep nov before normalisation:  59.80791160352071
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.07173505694042166, 0.08329965873558853, 0.10740615261847163, 0.11997596728597487, 0.4976071971335685, 0.11997596728597487]
printing an ep nov before normalisation:  50.61884203250494
printing an ep nov before normalisation:  65.03903006433958
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.07154244419555876, 0.08329854934235076, 0.10780423331031136, 0.12058219709360503, 0.496190378964569, 0.12058219709360503]
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.07154244419555876, 0.08329854934235076, 0.10780423331031136, 0.12058219709360503, 0.496190378964569, 0.12058219709360503]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949248756218906 -1.0 -0.9949248756218906
probs:  [0.07134592336852423, 0.0832974174396993, 0.10821039099651487, 0.12120072720828295, 0.49474481377869567, 0.12120072720828295]
siam score:  -0.85342544
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.07094063164251248, 0.0832950829448607, 0.10904802368778344, 0.12247634278945034, 0.4917635761459426, 0.12247634278945034]
from probs:  [0.07094063164251248, 0.0832950829448607, 0.10904802368778344, 0.12247634278945034, 0.4917635761459426, 0.12247634278945034]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.07073164433420168, 0.08329387923668115, 0.10947994635734201, 0.12313410992740109, 0.49022631021697316, 0.12313410992740109]
from probs:  [0.07073164433420168, 0.08329387923668115, 0.10947994635734201, 0.12313410992740109, 0.49022631021697316, 0.12313410992740109]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.07168220841259826, 0.08441358604733147, 0.09749861306080729, 0.12479024083177133, 0.4968251108157203, 0.12479024083177133]
Printing some Q and Qe and total Qs values:  [[0.599]
 [0.612]
 [0.663]
 [0.597]
 [0.595]
 [0.585]
 [0.58 ]] [[43.502]
 [44.733]
 [39.23 ]
 [43.045]
 [41.849]
 [43.091]
 [43.513]] [[1.764]
 [1.843]
 [1.602]
 [1.738]
 [1.673]
 [1.728]
 [1.745]]
maxi score, test score, baseline:  -0.9949495049504951 -1.0 -0.9949495049504951
probs:  [0.071482153859852, 0.08443151104557604, 0.0977405725975702, 0.1254994724060151, 0.49534681768497163, 0.1254994724060151]
using another actor
from probs:  [0.071482153859852, 0.08443151104557604, 0.0977405725975702, 0.1254994724060151, 0.49534681768497163, 0.1254994724060151]
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.07148207541018836, 0.08443151777437626, 0.09774066687090281, 0.12549974927222957, 0.4953462414000735, 0.12549974927222957]
printing an ep nov before normalisation:  58.9411071283169
printing an ep nov before normalisation:  43.46604413206414
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.0735208790362996, 0.0868403535810303, 0.10052981352978127, 0.1290821157086047, 0.5094970246145027, 0.10052981352978127]
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
printing an ep nov before normalisation:  79.5426213337109
maxi score, test score, baseline:  -0.9949980392156863 -1.0 -0.9949980392156863
probs:  [0.0735208790362996, 0.0868403535810303, 0.10052981352978127, 0.1290821157086047, 0.5094970246145027, 0.10052981352978127]
from probs:  [0.07451236259563411, 0.07451236259563411, 0.10188619382135701, 0.1308242439742642, 0.5163786431917535, 0.10188619382135701]
printing an ep nov before normalisation:  73.86897874851088
printing an ep nov before normalisation:  50.48169527888752
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
probs:  [0.07451233254272428, 0.07451233254272428, 0.1018862562420782, 0.13082440415282381, 0.5163784182775715, 0.1018862562420782]
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
probs:  [0.07451233254272428, 0.07451233254272428, 0.1018862562420782, 0.13082440415282381, 0.5163784182775715, 0.1018862562420782]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actions average: 
K:  1  action  0 :  tensor([0.2718, 0.0072, 0.1315, 0.1431, 0.1322, 0.1575, 0.1568],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0065, 0.9711, 0.0031, 0.0051, 0.0019, 0.0026, 0.0096],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1726, 0.0032, 0.1533, 0.1728, 0.1425, 0.1819, 0.1737],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1482, 0.0443, 0.1322, 0.2210, 0.1324, 0.1700, 0.1519],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1769, 0.0024, 0.1412, 0.1631, 0.1583, 0.1812, 0.1768],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1462, 0.0222, 0.1421, 0.1518, 0.1206, 0.2794, 0.1377],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1803, 0.1007, 0.1228, 0.1319, 0.1431, 0.1535, 0.1677],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
probs:  [0.0743534443259964, 0.0743534443259964, 0.10221858514088895, 0.13167601971663243, 0.515179921349597, 0.10221858514088895]
maxi score, test score, baseline:  -0.9950219512195122 -1.0 -0.9950219512195122
probs:  [0.0743534443259964, 0.0743534443259964, 0.10221858514088895, 0.13167601971663243, 0.515179921349597, 0.10221858514088895]
printing an ep nov before normalisation:  53.33557757718953
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.07435341368039915, 0.07435341368039915, 0.1022186487977694, 0.13167618306470366, 0.5151796919789593, 0.1022186487977694]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.07089236251424579, 0.08327414645519216, 0.10906952966549732, 0.1363389347735341, 0.4913554969260333, 0.10906952966549732]
printing an ep nov before normalisation:  52.10695709420843
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.07089236251424579, 0.08327414645519216, 0.10906952966549732, 0.1363389347735341, 0.4913554969260333, 0.10906952966549732]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.07068427031087712, 0.08327260328440374, 0.10949829697925045, 0.1372226017423742, 0.4898239307038439, 0.10949829697925045]
printing an ep nov before normalisation:  68.62172377063979
printing an ep nov before normalisation:  35.6076484948554
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.07068427031087712, 0.08327260328440374, 0.10949829697925045, 0.1372226017423742, 0.4898239307038439, 0.10949829697925045]
Printing some Q and Qe and total Qs values:  [[0.241]
 [0.271]
 [0.271]
 [0.271]
 [0.271]
 [0.271]
 [0.272]] [[27.164]
 [28.386]
 [28.386]
 [28.386]
 [28.386]
 [28.386]
 [25.308]] [[1.296]
 [1.373]
 [1.373]
 [1.373]
 [1.373]
 [1.373]
 [1.254]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.572]
 [0.494]
 [0.496]
 [0.498]
 [0.496]
 [0.521]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.507]
 [0.572]
 [0.494]
 [0.496]
 [0.498]
 [0.496]
 [0.521]]
printing an ep nov before normalisation:  66.46422386169434
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
printing an ep nov before normalisation:  50.30330266724759
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.07003306936071275, 0.08326777410643578, 0.11084007566002536, 0.13998793730239179, 0.48503106791040895, 0.11084007566002536]
printing an ep nov before normalisation:  39.506660309006165
printing an ep nov before normalisation:  50.3631541036171
printing an ep nov before normalisation:  54.00982038271469
siam score:  -0.853477
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.06980653848350815, 0.08326609419807433, 0.11130683527008683, 0.14094990440335722, 0.4833637923748865, 0.11130683527008683]
maxi score, test score, baseline:  -0.9950456310679612 -1.0 -0.9950456310679612
probs:  [0.06980653848350815, 0.08326609419807433, 0.11130683527008683, 0.14094990440335722, 0.4833637923748865, 0.11130683527008683]
printing an ep nov before normalisation:  24.14504669915521
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.54 ]
 [0.535]
 [0.517]
 [0.522]
 [0.522]
 [0.518]] [[28.381]
 [36.106]
 [35.163]
 [29.773]
 [29.689]
 [29.454]
 [29.037]] [[0.525]
 [0.54 ]
 [0.535]
 [0.517]
 [0.522]
 [0.522]
 [0.518]]
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.0708705504088995, 0.08453570638066141, 0.11300478132183205, 0.12784077812216033, 0.4907434024446146, 0.11300478132183205]
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.602]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]] [[45.744]
 [69.002]
 [45.744]
 [45.744]
 [45.744]
 [45.744]
 [45.744]] [[1.319]
 [2.259]
 [1.319]
 [1.319]
 [1.319]
 [1.319]
 [1.319]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9950690821256039 -1.0 -0.9950690821256039
probs:  [0.07190704143247384, 0.0857724284953802, 0.1146586515431017, 0.12971203538487208, 0.49793215396772383, 0.10001768917644825]
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
maxi score, test score, baseline:  -0.9951153110047847 -1.0 -0.9951153110047847
probs:  [0.06835913566190237, 0.09408640338703253, 0.12124296376355853, 0.13539497410062154, 0.4734378433497849, 0.10747867973710024]
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
probs:  [0.06640579397014561, 0.10232639645556472, 0.11495601468103156, 0.1412825428129909, 0.46007323739923567, 0.11495601468103156]
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
probs:  [0.06614487467781827, 0.10261806580139696, 0.1154419731827466, 0.14217349842837665, 0.458179614726915, 0.1154419731827466]
Printing some Q and Qe and total Qs values:  [[0.887]
 [0.887]
 [0.887]
 [1.472]
 [0.887]
 [0.887]
 [0.887]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.887]
 [0.887]
 [0.887]
 [1.472]
 [0.887]
 [0.887]
 [0.887]]
printing an ep nov before normalisation:  61.510301771394786
printing an ep nov before normalisation:  59.25117537362779
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
probs:  [0.06587901662362547, 0.10291525595571248, 0.11593713005877504, 0.14308131832994764, 0.45625014897316435, 0.11593713005877504]
Printing some Q and Qe and total Qs values:  [[1.099]
 [1.161]
 [1.099]
 [1.099]
 [1.099]
 [1.099]
 [1.099]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.099]
 [1.161]
 [1.099]
 [1.099]
 [1.099]
 [1.099]
 [1.099]]
printing an ep nov before normalisation:  48.571168755008394
using another actor
maxi score, test score, baseline:  -0.9951380952380953 -1.0 -0.9951380952380953
probs:  [0.06622012099164201, 0.10493537970253511, 0.1185475939524837, 0.14692235041716545, 0.45843917523363875, 0.10493537970253511]
from probs:  [0.06622012099164201, 0.10493537970253511, 0.1185475939524837, 0.14692235041716545, 0.45843917523363875, 0.10493537970253511]
printing an ep nov before normalisation:  80.46109060435782
maxi score, test score, baseline:  -0.9951830188679245 -1.0 -0.9951830188679245
probs:  [0.06660631586546817, 0.1071293654384497, 0.1071293654384497, 0.1510768980739368, 0.46092868974524615, 0.1071293654384497]
printing an ep nov before normalisation:  29.402661275489596
printing an ep nov before normalisation:  46.495513917737185
using another actor
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.06864260672005429, 0.11040717131096138, 0.11040717131096138, 0.1250915159388148, 0.47504436340824663, 0.11040717131096138]
printing an ep nov before normalisation:  36.866796645940134
maxi score, test score, baseline:  -0.9952051643192489 -1.0 -0.9952051643192489
probs:  [0.06864260672005429, 0.11040717131096138, 0.11040717131096138, 0.1250915159388148, 0.47504436340824663, 0.11040717131096138]
printing an ep nov before normalisation:  53.39903350685501
printing an ep nov before normalisation:  54.257755279541016
maxi score, test score, baseline:  -0.9952271028037384 -1.0 -0.9952271028037384
probs:  [0.06740066134166968, 0.11970529067517732, 0.09286475720140357, 0.13367707522316916, 0.46664692488340304, 0.11970529067517732]
actions average: 
K:  0  action  0 :  tensor([0.2655, 0.0198, 0.1313, 0.1399, 0.1439, 0.1553, 0.1442],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0057, 0.9322, 0.0088, 0.0093, 0.0035, 0.0050, 0.0357],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1385, 0.0253, 0.2356, 0.1297, 0.1209, 0.1697, 0.1803],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1567, 0.0157, 0.1412, 0.2246, 0.1448, 0.1748, 0.1421],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1765, 0.0171, 0.1555, 0.1656, 0.1511, 0.1798, 0.1544],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1495, 0.0556, 0.1429, 0.1643, 0.1389, 0.1986, 0.1503],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1904, 0.0071, 0.1578, 0.1641, 0.1394, 0.1805, 0.1606],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  43.03631793585532
Printing some Q and Qe and total Qs values:  [[2.992]
 [2.992]
 [2.992]
 [2.992]
 [2.992]
 [2.992]
 [2.992]] [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]] [[2.992]
 [2.992]
 [2.992]
 [2.992]
 [2.992]
 [2.992]
 [2.992]]
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.06714713919619984, 0.12028230726342835, 0.09301557628156115, 0.13447594804851004, 0.46479672194687216, 0.12028230726342835]
maxi score, test score, baseline:  -0.9952488372093024 -1.0 -0.9952488372093024
probs:  [0.0680866769041349, 0.10795796957362751, 0.09431779050248525, 0.13635943832449876, 0.47131132256207464, 0.12196680213317887]
Printing some Q and Qe and total Qs values:  [[0.144]
 [0.153]
 [0.16 ]
 [0.224]
 [0.159]
 [0.222]
 [0.228]] [[22.   ]
 [20.056]
 [20.49 ]
 [16.085]
 [20.115]
 [15.132]
 [15.641]] [[1.156]
 [1.075]
 [1.102]
 [0.962]
 [1.084]
 [0.916]
 [0.945]]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.06783950791533058, 0.10835575848979301, 0.09449493592484538, 0.13721664930995817, 0.4695019504792523, 0.12259119788082053]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.0675871908268002, 0.10876183301829188, 0.09467577121593948, 0.13809171512729962, 0.467654890617988, 0.1232285991936809]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.0675871908268002, 0.10876183301829188, 0.09467577121593948, 0.13809171512729962, 0.467654890617988, 0.1232285991936809]
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.37 ]
 [0.319]
 [0.321]
 [0.316]
 [0.325]
 [0.309]] [[29.3  ]
 [31.009]
 [29.221]
 [28.683]
 [28.48 ]
 [28.644]
 [31.321]] [[1.552]
 [1.701]
 [1.571]
 [1.549]
 [1.536]
 [1.552]
 [1.654]]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.0675871908268002, 0.10876183301829188, 0.09467577121593948, 0.13809171512729962, 0.467654890617988, 0.1232285991936809]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.06732951445308137, 0.10917653268279957, 0.09486044749894867, 0.13898536758616079, 0.46576859869117354, 0.12387953908783594]
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.492982295802697
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.07034861471514282, 0.11407580144859933, 0.09911650072399562, 0.14522393446421233, 0.48668939538308365, 0.08454575326496634]
printing an ep nov before normalisation:  56.94394217835201
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.07138767588323412, 0.11576194615553577, 0.0857949064911244, 0.1473710153906, 0.4938895495883812, 0.0857949064911244]
using another actor
printing an ep nov before normalisation:  46.277591132021556
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
printing an ep nov before normalisation:  44.01724415455254
printing an ep nov before normalisation:  13.372150829444161
printing an ep nov before normalisation:  67.20784105211897
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.07074657387732249, 0.11749570742892215, 0.08592486399147813, 0.15079646009581502, 0.4891115306149841, 0.08592486399147813]
printing an ep nov before normalisation:  62.07229486483433
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.07074657387732249, 0.11749570742892215, 0.08592486399147813, 0.15079646009581502, 0.4891115306149841, 0.08592486399147813]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.07074657387732249, 0.11749570742892215, 0.08592486399147813, 0.15079646009581502, 0.4891115306149841, 0.08592486399147813]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.07074657387732249, 0.11749570742892215, 0.08592486399147813, 0.15079646009581502, 0.4891115306149841, 0.08592486399147813]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.07074657387732249, 0.11749570742892215, 0.08592486399147813, 0.15079646009581502, 0.4891115306149841, 0.08592486399147813]
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.536]
 [0.517]
 [0.539]
 [0.538]
 [0.534]
 [0.527]] [[43.65 ]
 [43.876]
 [49.237]
 [45.463]
 [45.546]
 [45.583]
 [48.619]] [[1.328]
 [1.286]
 [1.469]
 [1.349]
 [1.351]
 [1.348]
 [1.456]]
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
maxi score, test score, baseline:  -0.9952703703703704 -1.0 -0.9952703703703704
probs:  [0.07074657387732249, 0.11749570742892215, 0.08592486399147813, 0.15079646009581502, 0.4891115306149841, 0.08592486399147813]
printing an ep nov before normalisation:  29.599759109904866
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.07052243055796475, 0.11810186779328657, 0.08597029979021215, 0.15199406965954337, 0.48744103240878095, 0.08597029979021215]
maxi score, test score, baseline:  -0.9952917050691245 -1.0 -0.9952917050691245
probs:  [0.07052243055796475, 0.11810186779328657, 0.08597029979021215, 0.15199406965954337, 0.48744103240878095, 0.08597029979021215]
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.684]
 [0.546]
 [0.546]
 [0.546]
 [0.525]
 [0.546]] [[72.836]
 [74.985]
 [77.88 ]
 [77.88 ]
 [77.88 ]
 [68.56 ]
 [77.88 ]] [[1.969]
 [2.232]
 [2.213]
 [2.213]
 [2.213]
 [1.808]
 [2.213]]
printing an ep nov before normalisation:  82.26592051278472
using explorer policy with actor:  1
printing an ep nov before normalisation:  6.870105153211625e-05
printing an ep nov before normalisation:  60.42572469120278
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.07195430867501104, 0.12482293751681357, 0.07195430867501104, 0.1624827827191933, 0.4968313537389601, 0.07195430867501104]
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.07195430867501104, 0.12482293751681357, 0.07195430867501104, 0.1624827827191933, 0.4968313537389601, 0.07195430867501104]
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.14255457986973
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.431]
 [0.405]
 [0.388]
 [0.366]
 [0.386]
 [0.372]] [[60.566]
 [67.736]
 [60.322]
 [56.932]
 [54.8  ]
 [56.814]
 [51.171]] [[0.422]
 [0.431]
 [0.405]
 [0.388]
 [0.366]
 [0.386]
 [0.372]]
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
printing an ep nov before normalisation:  64.62825677406101
maxi score, test score, baseline:  -0.9953128440366973 -1.0 -0.9953128440366973
probs:  [0.0717369778514714, 0.12561240967585416, 0.0717369778514714, 0.16398942960555168, 0.4951872271641802, 0.0717369778514714]
using explorer policy with actor:  1
siam score:  -0.8448716
printing an ep nov before normalisation:  84.33721762087163
Printing some Q and Qe and total Qs values:  [[0.603]
 [1.176]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.603]
 [1.176]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]]
printing an ep nov before normalisation:  29.986967901628727
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]] [[59.389]
 [59.389]
 [59.389]
 [59.389]
 [59.389]
 [59.389]
 [59.389]] [[1.861]
 [1.861]
 [1.861]
 [1.861]
 [1.861]
 [1.861]
 [1.861]]
printing an ep nov before normalisation:  62.76044675999407
printing an ep nov before normalisation:  53.03462303981652
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
probs:  [0.07188518217110863, 0.1448203465174574, 0.07188518217110863, 0.12586680051955745, 0.49612481502024874, 0.08941767360051919]
using explorer policy with actor:  1
from probs:  [0.07188518217110863, 0.1448203465174574, 0.07188518217110863, 0.12586680051955745, 0.49612481502024874, 0.08941767360051919]
printing an ep nov before normalisation:  60.27006777833322
maxi score, test score, baseline:  -0.9953337899543379 -1.0 -0.9953337899543379
from probs:  [0.07166555804344968, 0.14599586780378432, 0.07166555804344968, 0.12667976757001315, 0.49445982757269613, 0.089533420966607]
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
probs:  [0.07143970053728652, 0.1472047503811358, 0.07143970053728652, 0.12751580650724093, 0.4927475891334539, 0.08965245290359639]
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
probs:  [0.07283505841485188, 0.15008271360337133, 0.07283505841485188, 0.11045566970796214, 0.5023872935622552, 0.09140420629670752]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.995375113122172 -1.0 -0.995375113122172
probs:  [0.07283505841485188, 0.15008271360337133, 0.07283505841485188, 0.11045566970796214, 0.5023872935622552, 0.09140420629670752]
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.07283501835169848, 0.15008296601029836, 0.07283501835169848, 0.11045577208153617, 0.5023869886659064, 0.09140423653886211]
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.07262591315075546, 0.15140761579018983, 0.07262591315075546, 0.11099362547515534, 0.500783109994063, 0.09156382243908108]
printing an ep nov before normalisation:  34.36649224986938
printing an ep nov before normalisation:  29.23257457472772
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.07262591315075546, 0.15140761579018983, 0.07262591315075546, 0.11099362547515534, 0.500783109994063, 0.09156382243908108]
siam score:  -0.8507135
printing an ep nov before normalisation:  55.00197538682073
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.07262591315075546, 0.15140761579018983, 0.07262591315075546, 0.11099362547515534, 0.500783109994063, 0.09156382243908108]
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
printing an ep nov before normalisation:  24.29449929909165
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
from probs:  [0.07262591315075546, 0.15140761579018983, 0.07262591315075546, 0.11099362547515534, 0.500783109994063, 0.09156382243908108]
maxi score, test score, baseline:  -0.9953954954954956 -1.0 -0.9953954954954956
probs:  [0.07395346305859557, 0.13470065238399978, 0.07395346305859557, 0.11392563958440276, 0.5097834624937133, 0.09368331942069295]
maxi score, test score, baseline:  -0.9954156950672646 -1.0 -0.9954156950672646
probs:  [0.0739534257199442, 0.13470085472205082, 0.0739534257199442, 0.11392575995509648, 0.5097831739572076, 0.09368335992575667]
maxi score, test score, baseline:  -0.9954156950672646 -1.0 -0.9954156950672646
probs:  [0.07375874943266467, 0.13576250457697445, 0.07375874943266467, 0.1145577571466694, 0.5082655182220907, 0.0938967211889361]
printing an ep nov before normalisation:  61.8371265159973
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.06891133036011614, 0.14194693039347928, 0.08647685441877283, 0.12297664467052781, 0.4751954636268881, 0.1044927765302159]
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.06864167114238878, 0.1430397258784569, 0.08653487418017732, 0.12371555581714051, 0.47318129568597894, 0.10488687729585768]
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]] [[58.245]
 [58.245]
 [58.245]
 [58.245]
 [58.245]
 [58.245]
 [58.245]] [[2.242]
 [2.242]
 [2.242]
 [2.242]
 [2.242]
 [2.242]
 [2.242]]
printing an ep nov before normalisation:  46.493825912475586
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.06864167114238878, 0.1430397258784569, 0.08653487418017732, 0.12371555581714051, 0.47318129568597894, 0.10488687729585768]
Printing some Q and Qe and total Qs values:  [[0.631]
 [0.725]
 [0.725]
 [0.725]
 [0.725]
 [0.725]
 [0.725]] [[42.58 ]
 [41.421]
 [41.421]
 [41.421]
 [41.421]
 [41.421]
 [41.421]] [[2.146]
 [2.165]
 [2.165]
 [2.165]
 [2.165]
 [2.165]
 [2.165]]
printing an ep nov before normalisation:  40.47128200531006
printing an ep nov before normalisation:  38.09866745088581
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.06912804484324267, 0.14940266559268323, 0.08843459920070307, 0.12855211474867265, 0.4760479764139954, 0.08843459920070307]
printing an ep nov before normalisation:  49.306804310061764
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.07178373628460992, 0.13496845558746318, 0.07178373628460992, 0.13496845558746318, 0.49418351125259535, 0.09231210500325848]
printing an ep nov before normalisation:  51.40177057346829
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.07178373628460992, 0.13496845558746318, 0.07178373628460992, 0.13496845558746318, 0.49418351125259535, 0.09231210500325848]
printing an ep nov before normalisation:  40.81684589385986
maxi score, test score, baseline:  -0.9954357142857143 -1.0 -0.9954357142857143
probs:  [0.07178373628460992, 0.13496845558746318, 0.07178373628460992, 0.13496845558746318, 0.49418351125259535, 0.09231210500325848]
printing an ep nov before normalisation:  41.32959206341121
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.46231137395341
printing an ep nov before normalisation:  42.61808395385742
printing an ep nov before normalisation:  31.300529435505847
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.07336692763295236, 0.1158690529649013, 0.07336692763295236, 0.13794807911136828, 0.5051000231521392, 0.09434898950568664]
printing an ep nov before normalisation:  52.34450273203258
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.07315454092425337, 0.11655937966831187, 0.07315454092425337, 0.13910734784704407, 0.5034419445091209, 0.09458224612701646]
Printing some Q and Qe and total Qs values:  [[-0.002]
 [ 0.161]
 [ 0.028]
 [ 0.012]
 [-0.002]
 [ 0.001]
 [ 0.019]] [[26.724]
 [34.065]
 [27.544]
 [24.377]
 [23.393]
 [23.111]
 [26.027]] [[0.399]
 [0.761]
 [0.451]
 [0.35 ]
 [0.309]
 [0.305]
 [0.401]]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
printing an ep nov before normalisation:  51.53340471899315
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.0866675389082602, 0.12523676010016158, 0.06809717314919622, 0.1452727191608896, 0.46901776817216695, 0.10570804050932524]
printing an ep nov before normalisation:  42.09534186713757
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.0866675389082602, 0.12523676010016158, 0.06809717314919622, 0.1452727191608896, 0.46901776817216695, 0.10570804050932524]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.0866675389082602, 0.12523676010016158, 0.06809717314919622, 0.1452727191608896, 0.46901776817216695, 0.10570804050932524]
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.08673039438884476, 0.12603405089596284, 0.0678064116261583, 0.1464515347957647, 0.46684414297053084, 0.1061334653227386]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9954555555555555 -1.0 -0.9954555555555555
probs:  [0.08673039438884476, 0.12603405089596284, 0.0678064116261583, 0.1464515347957647, 0.46684414297053084, 0.1061334653227386]
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.086730406708574, 0.12603420944992091, 0.0678063535368143, 0.146451769315556, 0.46684371115508555, 0.1061335498340492]
line 256 mcts: sample exp_bonus 24.76515087016972
printing an ep nov before normalisation:  37.03145583203883
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.08679504719317137, 0.12685414218586, 0.06750733478928411, 0.14766406166258145, 0.46460835805549056, 0.10657105611361252]
printing an ep nov before normalisation:  55.26828766623285
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.08679504719317137, 0.12685414218586, 0.06750733478928411, 0.14766406166258145, 0.46460835805549056, 0.10657105611361252]
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.08686154937920211, 0.1276976896882914, 0.06719970404519576, 0.1489112690696368, 0.4623086248681618, 0.107021162949512]
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.0998635241115063, 0.1371960845353385, 0.06435206419615308, 0.1371960845353385, 0.4430987203262401, 0.1182935222954235]
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.0998635241115063, 0.1371960845353385, 0.06435206419615308, 0.1371960845353385, 0.4430987203262401, 0.1182935222954235]
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]] [[21.413]
 [21.413]
 [21.413]
 [21.413]
 [21.413]
 [21.413]
 [21.413]] [[0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.266]]
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.0998635241115063, 0.1371960845353385, 0.06435206419615308, 0.1371960845353385, 0.4430987203262401, 0.1182935222954235]
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.0998635241115063, 0.1371960845353385, 0.06435206419615308, 0.1371960845353385, 0.4430987203262401, 0.1182935222954235]
printing an ep nov before normalisation:  16.95856561072501
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.10867033625641065, 0.1425268470773306, 0.06094489329198108, 0.1425268470773306, 0.41994676634413275, 0.12538430995281433]
printing an ep nov before normalisation:  88.47593938341129
printing an ep nov before normalisation:  21.5242576795805
using explorer policy with actor:  0
printing an ep nov before normalisation:  74.49844873690252
siam score:  -0.8455698
maxi score, test score, baseline:  -0.9954752212389381 -1.0 -0.9954752212389381
probs:  [0.11051675968026356, 0.14494923438040505, 0.061979415825847234, 0.14494923438040505, 0.4270885960528156, 0.11051675968026356]
maxi score, test score, baseline:  -0.9954947136563876 -1.0 -0.9954947136563876
probs:  [0.11096381424576911, 0.14596325545291866, 0.06162725254412498, 0.14596325545291866, 0.42451860805849956, 0.11096381424576911]
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.11296502759801991, 0.14859652236306478, 0.06273749883283615, 0.1305552591908902, 0.4321806644171691, 0.11296502759801991]
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.371]
 [0.357]
 [0.362]
 [0.347]
 [0.35 ]
 [0.363]] [[40.779]
 [39.721]
 [39.584]
 [39.728]
 [38.353]
 [40.255]
 [38.915]] [[1.25 ]
 [1.215]
 [1.196]
 [1.206]
 [1.136]
 [1.216]
 [1.174]]
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
probs:  [0.11296502759801991, 0.14859652236306478, 0.06273749883283615, 0.1305552591908902, 0.4321806644171691, 0.11296502759801991]
printing an ep nov before normalisation:  43.50185019352902
maxi score, test score, baseline:  -0.9955140350877193 -1.0 -0.9955140350877193
printing an ep nov before normalisation:  56.908061536145375
printing an ep nov before normalisation:  61.23126604872925
maxi score, test score, baseline:  -0.9955331877729258 -1.0 -0.9955331877729258
probs:  [0.1078394487301938, 0.15759750069522235, 0.06163554333409609, 0.12401081561882824, 0.4249058760028314, 0.12401081561882824]
Printing some Q and Qe and total Qs values:  [[-0.027]
 [-0.014]
 [-0.018]
 [-0.017]
 [-0.015]
 [-0.016]
 [-0.014]] [[27.043]
 [27.256]
 [24.74 ]
 [24.738]
 [25.672]
 [26.456]
 [27.016]] [[0.364]
 [0.38 ]
 [0.339]
 [0.341]
 [0.356]
 [0.367]
 [0.377]]
printing an ep nov before normalisation:  43.436487007322135
printing an ep nov before normalisation:  29.671407522225373
maxi score, test score, baseline:  -0.9955331877729258 -1.0 -0.9955331877729258
probs:  [0.10961127473817504, 0.16018787375609272, 0.06264728993582322, 0.10961127473817504, 0.4318936174127356, 0.12604866941899837]
maxi score, test score, baseline:  -0.9955331877729258 -1.0 -0.9955331877729258
probs:  [0.11153853097919114, 0.145415511799638, 0.06374779017891823, 0.11153853097919114, 0.43949434580377456, 0.1282652902592868]
printing an ep nov before normalisation:  58.706443167373926
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9955331877729258 -1.0 -0.9955331877729258
probs:  [0.11200191145102059, 0.1464361121897505, 0.06342509255174118, 0.11200191145102059, 0.4371311742906986, 0.12900379806576864]
maxi score, test score, baseline:  -0.9955331877729258 -1.0 -0.9955331877729258
probs:  [0.11200191145102059, 0.1464361121897505, 0.06342509255174118, 0.11200191145102059, 0.4371311742906986, 0.12900379806576864]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.11200200188595769, 0.14643631157751763, 0.0634250292853644, 0.11200200188595769, 0.437130713069037, 0.12900394229616563]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.11200200188595769, 0.14643631157751763, 0.0634250292853644, 0.11200200188595769, 0.437130713069037, 0.12900394229616563]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.1159948071164134, 0.13360357849010016, 0.06568403176302368, 0.1159948071164134, 0.45272796839763596, 0.1159948071164134]
printing an ep nov before normalisation:  54.30271099367719
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.1159948071164134, 0.13360357849010016, 0.06568403176302368, 0.1159948071164134, 0.45272796839763596, 0.1159948071164134]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.1159948071164134, 0.13360357849010016, 0.06568403176302368, 0.1159948071164134, 0.45272796839763596, 0.1159948071164134]
printing an ep nov before normalisation:  57.897756927568075
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.11655471799958422, 0.13446566223902814, 0.06538059160117353, 0.11655471799958422, 0.4504895921610457, 0.11655471799958422]
actor:  1 policy actor:  1  step number:  58 total reward:  0.206666666666666  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.508]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]] [[47.671]
 [50.81 ]
 [47.671]
 [47.671]
 [47.671]
 [47.671]
 [47.671]] [[1.082]
 [1.175]
 [1.082]
 [1.082]
 [1.082]
 [1.082]
 [1.082]]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.08512709975822935, 0.08512709975822935, 0.0742975730569653, 0.08142921259194404, 0.15579477678040324, 0.5182242380542287]
printing an ep nov before normalisation:  40.84832304056091
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.08559713870056557, 0.08187880122520454, 0.07470772180843699, 0.08187880122520454, 0.15484811656352296, 0.5210894204770653]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.08559713870056557, 0.08187880122520454, 0.07470772180843699, 0.08187880122520454, 0.15484811656352296, 0.5210894204770653]
printing an ep nov before normalisation:  60.152630415907666
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.08559713870056557, 0.08187880122520454, 0.07470772180843699, 0.08187880122520454, 0.15484811656352296, 0.5210894204770653]
siam score:  -0.83961874
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.08559713870056557, 0.08187880122520454, 0.07470772180843699, 0.08187880122520454, 0.15484811656352296, 0.5210894204770653]
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.37 ]
 [0.359]
 [0.49 ]
 [0.359]
 [0.359]
 [0.359]] [[46.673]
 [46.131]
 [46.673]
 [44.768]
 [46.673]
 [46.673]
 [46.673]] [[2.359]
 [2.331]
 [2.359]
 [2.354]
 [2.359]
 [2.359]
 [2.359]]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.08559713870056557, 0.08187880122520454, 0.07470772180843699, 0.08187880122520454, 0.15484811656352296, 0.5210894204770653]
printing an ep nov before normalisation:  35.974914639204634
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.08559713870056557, 0.08187880122520454, 0.07470772180843699, 0.08187880122520454, 0.15484811656352296, 0.5210894204770653]
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.08559713870056557, 0.08187880122520454, 0.07470772180843699, 0.08187880122520454, 0.15484811656352296, 0.5210894204770653]
printing an ep nov before normalisation:  47.55290177667871
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.08600193215407051, 0.07835746533581756, 0.07467174026273132, 0.08213308614239377, 0.1580560077416588, 0.5207797683633282]
using explorer policy with actor:  1
siam score:  -0.8438084
maxi score, test score, baseline:  -0.9955521739130435 -1.0 -0.9955521739130435
probs:  [0.08616230471612021, 0.07850356072059521, 0.07481095200846712, 0.0822862330598484, 0.15648479637971635, 0.5217521531152527]
printing an ep nov before normalisation:  49.41268705143471
printing an ep nov before normalisation:  42.06680649304771
actor:  1 policy actor:  1  step number:  48 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9955709956709957 -1.0 -0.9955709956709957
probs:  [0.08474966869736555, 0.08096091058282948, 0.07913418792046387, 0.08283218745647228, 0.11953781985627876, 0.5527852254865901]
printing an ep nov before normalisation:  35.64422155708968
printing an ep nov before normalisation:  71.55244085523817
printing an ep nov before normalisation:  71.48006951032723
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.407]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]] [[42.482]
 [44.402]
 [42.482]
 [42.482]
 [42.482]
 [42.482]
 [42.482]] [[1.488]
 [1.632]
 [1.488]
 [1.488]
 [1.488]
 [1.488]
 [1.488]]
line 256 mcts: sample exp_bonus 54.21966776283337
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.09256007724968
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
probs:  [0.0848485547095559, 0.08100088793192238, 0.0791457628784205, 0.08290125993794868, 0.11924863973304127, 0.5528548948091112]
maxi score, test score, baseline:  -0.9956081545064378 -1.0 -0.9956081545064378
using another actor
from probs:  [0.08487003594851016, 0.0809666329611361, 0.07908463509222358, 0.08289453321709522, 0.1197682525108912, 0.5524159102701438]
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.624]
 [0.532]
 [0.523]
 [0.534]
 [0.548]
 [0.554]] [[66.254]
 [64.915]
 [67.586]
 [67.912]
 [67.319]
 [66.468]
 [65.952]] [[1.732]
 [1.723]
 [1.72 ]
 [1.722]
 [1.714]
 [1.699]
 [1.688]]
printing an ep nov before normalisation:  57.800992771780024
printing an ep nov before normalisation:  51.05254093220295
maxi score, test score, baseline:  -0.9956627118644068 -1.0 -0.9956627118644068
probs:  [0.0833768304188182, 0.07943367098732841, 0.07943367098732841, 0.08138149673059444, 0.1215399415511102, 0.5548343893248203]
printing an ep nov before normalisation:  57.93288480516061
actions average: 
K:  1  action  0 :  tensor([0.3625, 0.0139, 0.1219, 0.1274, 0.1114, 0.1306, 0.1322],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0362, 0.8492, 0.0231, 0.0264, 0.0118, 0.0185, 0.0348],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1628, 0.0316, 0.1699, 0.1500, 0.1345, 0.1794, 0.1718],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1290, 0.0125, 0.1131, 0.3073, 0.1393, 0.1491, 0.1497],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1540, 0.0030, 0.1235, 0.1690, 0.2422, 0.1709, 0.1373],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1401, 0.0199, 0.1397, 0.1379, 0.1209, 0.3155, 0.1258],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1366, 0.0457, 0.1438, 0.1525, 0.1153, 0.1766, 0.2295],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9956627118644068 -1.0 -0.9956627118644068
probs:  [0.08483333883931805, 0.0808168842341647, 0.07887953554226719, 0.0808168842341647, 0.12370582323094229, 0.550947533919143]
actions average: 
K:  2  action  0 :  tensor([0.2712, 0.0562, 0.1267, 0.1290, 0.1302, 0.1303, 0.1564],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0141, 0.9101, 0.0093, 0.0173, 0.0107, 0.0113, 0.0271],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2316, 0.0222, 0.1526, 0.1598, 0.1353, 0.1564, 0.1420],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1670, 0.0414, 0.1008, 0.3135, 0.1291, 0.1151, 0.1332],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2267, 0.0276, 0.1575, 0.1463, 0.1394, 0.1556, 0.1469],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1758, 0.0375, 0.1422, 0.1480, 0.1356, 0.2217, 0.1392],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1680, 0.0738, 0.1339, 0.1516, 0.1677, 0.1549, 0.1500],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9956627118644068 -1.0 -0.9956627118644068
probs:  [0.08483333883931805, 0.0808168842341647, 0.07887953554226719, 0.0808168842341647, 0.12370582323094229, 0.550947533919143]
maxi score, test score, baseline:  -0.9956627118644068 -1.0 -0.9956627118644068
probs:  [0.08483333883931805, 0.0808168842341647, 0.07887953554226719, 0.0808168842341647, 0.12370582323094229, 0.550947533919143]
maxi score, test score, baseline:  -0.9956627118644068 -1.0 -0.9956627118644068
probs:  [0.08483333883931805, 0.0808168842341647, 0.07887953554226719, 0.0808168842341647, 0.12370582323094229, 0.550947533919143]
maxi score, test score, baseline:  -0.9956627118644068 -1.0 -0.9956627118644068
actions average: 
K:  0  action  0 :  tensor([0.3599, 0.0103, 0.1246, 0.1242, 0.1209, 0.1321, 0.1280],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0038, 0.9556, 0.0091, 0.0118, 0.0033, 0.0109, 0.0055],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1555, 0.0178, 0.2025, 0.1440, 0.1229, 0.1902, 0.1670],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1343, 0.0093, 0.1184, 0.3246, 0.1203, 0.1404, 0.1526],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1965, 0.0053, 0.1480, 0.1401, 0.1974, 0.1525, 0.1600],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1955, 0.0112, 0.1636, 0.1389, 0.1274, 0.2221, 0.1413],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1758, 0.0113, 0.1638, 0.1601, 0.1500, 0.1736, 0.1654],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.23]
 [0.23]
 [0.23]
 [0.23]
 [0.23]
 [0.23]
 [0.23]] [[33.304]
 [33.304]
 [33.304]
 [33.304]
 [33.304]
 [33.304]
 [33.304]] [[1.527]
 [1.527]
 [1.527]
 [1.527]
 [1.527]
 [1.527]
 [1.527]]
from probs:  [0.08483333883931805, 0.0808168842341647, 0.07887953554226719, 0.0808168842341647, 0.12370582323094229, 0.550947533919143]
maxi score, test score, baseline:  -0.9956983193277311 -1.0 -0.9956983193277311
maxi score, test score, baseline:  -0.99571589958159 -1.0 -0.99571589958159
probs:  [0.08491688857947172, 0.08089638491056278, 0.07895708314085374, 0.08089638491056278, 0.1228433126375992, 0.5514899458209498]
maxi score, test score, baseline:  -0.99571589958159 -1.0 -0.99571589958159
maxi score, test score, baseline:  -0.99571589958159 -1.0 -0.99571589958159
maxi score, test score, baseline:  -0.99571589958159 -1.0 -0.99571589958159
probs:  [0.08495998922265856, 0.08082770490942959, 0.07883448541716621, 0.08082770490942959, 0.12394086826132905, 0.5506092472799872]
printing an ep nov before normalisation:  56.129042460956626
maxi score, test score, baseline:  -0.99571589958159 -1.0 -0.99571589958159
probs:  [0.08495998922265856, 0.08082770490942959, 0.07883448541716621, 0.08082770490942959, 0.12394086826132905, 0.5506092472799872]
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.471]
 [0.475]
 [0.45 ]
 [0.448]
 [0.45 ]
 [0.45 ]] [[63.909]
 [57.956]
 [65.861]
 [63.909]
 [65.41 ]
 [63.909]
 [63.909]] [[2.355]
 [2.087]
 [2.475]
 [2.355]
 [2.426]
 [2.355]
 [2.355]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.568177482256914
maxi score, test score, baseline:  -0.99571589958159 -1.0 -0.99571589958159
probs:  [0.08634026911889457, 0.0820894960121897, 0.0780364332825409, 0.0820894960121897, 0.12643888279918578, 0.5450054227749994]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.99571589958159 -1.0 -0.99571589958159
probs:  [0.08634026911889457, 0.0820894960121897, 0.0780364332825409, 0.0820894960121897, 0.12643888279918578, 0.5450054227749994]
printing an ep nov before normalisation:  31.208527088165283
printing an ep nov before normalisation:  30.321670896071005
printing an ep nov before normalisation:  16.549577575269154
printing an ep nov before normalisation:  66.86320676455291
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.08637950183837177, 0.0820726683745531, 0.07796615274626084, 0.0820726683745531, 0.12700675631838804, 0.5445022523478732]
printing an ep nov before normalisation:  37.93551359600687
printing an ep nov before normalisation:  24.145895202723747
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.0864189201912609, 0.08205573862984356, 0.07789549574570138, 0.08205573862984356, 0.12757771797794054, 0.54399638882541]
printing an ep nov before normalisation:  45.34148958422084
actions average: 
K:  2  action  0 :  tensor([0.3852, 0.0166, 0.1073, 0.1068, 0.1218, 0.1369, 0.1255],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0419, 0.7865, 0.0328, 0.0360, 0.0233, 0.0222, 0.0574],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1594, 0.0239, 0.2258, 0.1434, 0.1159, 0.1752, 0.1563],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1259, 0.1389, 0.1015, 0.2435, 0.1047, 0.1378, 0.1477],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1775, 0.0126, 0.1158, 0.1553, 0.2124, 0.1790, 0.1474],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1812, 0.0099, 0.1520, 0.1459, 0.1505, 0.1964, 0.1641],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1531, 0.0227, 0.1486, 0.1794, 0.1307, 0.1771, 0.1883],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
printing an ep nov before normalisation:  33.49002335405841
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.08664314439706564, 0.08007735510554484, 0.07799055385785215, 0.08221384209723018, 0.12842567284077808, 0.544649431701529]
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
actions average: 
K:  0  action  0 :  tensor([0.3323, 0.0042, 0.1146, 0.1497, 0.1276, 0.1436, 0.1280],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0065, 0.9474, 0.0071, 0.0113, 0.0034, 0.0039, 0.0204],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1651, 0.0048, 0.1694, 0.1593, 0.1295, 0.2347, 0.1372],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1484, 0.0084, 0.1161, 0.2881, 0.1343, 0.1592, 0.1456],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1983, 0.0087, 0.1218, 0.1703, 0.1974, 0.1556, 0.1480],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1489, 0.0033, 0.1147, 0.1430, 0.1257, 0.3237, 0.1407],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2182, 0.0495, 0.1137, 0.1605, 0.1338, 0.1289, 0.1954],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9957333333333334 -1.0 -0.9957333333333334
probs:  [0.08664314439706564, 0.08007735510554484, 0.07799055385785215, 0.08221384209723018, 0.12842567284077808, 0.544649431701529]
printing an ep nov before normalisation:  35.343755495803464
printing an ep nov before normalisation:  35.020920423569635
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.408]
 [0.408]
 [0.155]
 [0.408]
 [0.408]
 [0.408]] [[54.494]
 [54.494]
 [54.494]
 [54.561]
 [54.494]
 [54.494]
 [54.494]] [[1.988]
 [1.988]
 [1.988]
 [1.74 ]
 [1.988]
 [1.988]
 [1.988]]
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
from probs:  [0.0866432653397494, 0.08007733014148011, 0.07799048252032476, 0.08221386461075822, 0.12842614371276326, 0.5446489136749243]
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.08668560984178561, 0.08003482136937808, 0.07792100487814779, 0.0821989668246853, 0.12900845912919476, 0.5441511379568086]
Printing some Q and Qe and total Qs values:  [[0.308]
 [0.411]
 [0.307]
 [0.309]
 [0.31 ]
 [0.309]
 [0.305]] [[18.627]
 [29.505]
 [18.793]
 [19.313]
 [19.776]
 [19.015]
 [18.952]] [[0.783]
 [1.44 ]
 [0.791]
 [0.819]
 [0.844]
 [0.805]
 [0.797]]
maxi score, test score, baseline:  -0.9957847736625515 -1.0 -0.9957847736625515
probs:  [0.08668560984178561, 0.08003482136937808, 0.07792100487814779, 0.0821989668246853, 0.12900845912919476, 0.5441511379568086]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9958016393442624 -1.0 -0.9958016393442624
probs:  [0.08469955877085128, 0.08030431288186772, 0.0781833512028815, 0.08247577364844887, 0.12835112226976614, 0.5459858812261846]
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
probs:  [0.08469958429515884, 0.08030430644328793, 0.07818332934035022, 0.08247578300105748, 0.12835127493203055, 0.5459857219881149]
printing an ep nov before normalisation:  31.9065358219083
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
probs:  [0.08469958429515884, 0.08030430644328793, 0.07818332934035022, 0.08247578300105748, 0.12835127493203055, 0.5459857219881149]
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.857231676019097
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
probs:  [0.08479125159115701, 0.08039120939507705, 0.07826793321906172, 0.08256503976575942, 0.12740715694792684, 0.546577409081018]
line 256 mcts: sample exp_bonus 35.581830157918915
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
probs:  [0.08497583257116127, 0.0805661969065099, 0.07843829132414906, 0.0805661969065099, 0.1276846539342927, 0.5477688283573771]
actor:  1 policy actor:  1  step number:  71 total reward:  0.13333333333333297  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
probs:  [0.08453310970783338, 0.08133666800458941, 0.07979419904314028, 0.08133666800458941, 0.11549173488311834, 0.5575076203567291]
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
probs:  [0.08453310970783338, 0.08133666800458941, 0.07979419904314028, 0.08133666800458941, 0.11549173488311834, 0.5575076203567291]
maxi score, test score, baseline:  -0.9958183673469387 -1.0 -0.9958183673469387
probs:  [0.08454706995218371, 0.08131237297561081, 0.07975144362063669, 0.08131237297561081, 0.11587621046230683, 0.5572005300136512]
maxi score, test score, baseline:  -0.9958349593495935 -1.0 -0.9958349593495935
probs:  [0.08454708892413126, 0.08131237147652041, 0.07975143224308029, 0.08131237147652041, 0.11587628944492658, 0.5572004464348209]
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]] [[57.742]
 [57.742]
 [57.742]
 [57.742]
 [57.742]
 [57.742]
 [57.742]] [[0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]
 [0.216]]
printing an ep nov before normalisation:  0.15757743572123672
printing an ep nov before normalisation:  42.00166535987745
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
printing an ep nov before normalisation:  45.201809085985026
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.08457520948843938, 0.08126346598322261, 0.07966535719872846, 0.08126346598322261, 0.11665028988593946, 0.5565822114604474]
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.08457520948843938, 0.08126346598322261, 0.07966535719872846, 0.08126346598322261, 0.11665028988593946, 0.5565822114604474]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.08559483572876421, 0.08231382536498479, 0.0791836660524136, 0.08231382536498479, 0.11737225781398326, 0.5532215896748695]
line 256 mcts: sample exp_bonus 32.09230943537719
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.08559483572876421, 0.08231382536498479, 0.0791836660524136, 0.08231382536498479, 0.11737225781398326, 0.5532215896748695]
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.381]
 [0.377]
 [0.385]
 [0.522]
 [0.377]
 [0.428]] [[35.494]
 [38.959]
 [38.853]
 [32.308]
 [27.008]
 [38.875]
 [22.17 ]] [[0.625]
 [0.697]
 [0.692]
 [0.634]
 [0.717]
 [0.691]
 [0.574]]
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.08407425724834018, 0.08245058437996237, 0.07931521608240527, 0.08245058437996237, 0.11756735853940468, 0.5541419993699249]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.08409072336314694, 0.08242926723284291, 0.07922093815363512, 0.08242926723284291, 0.118363217254438, 0.5534665867630941]
printing an ep nov before normalisation:  38.012214125762995
printing an ep nov before normalisation:  73.05560641818576
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.08417788634365174, 0.08247690446157972, 0.079192249792751, 0.08247690446157972, 0.11842613142159318, 0.5532499235188446]
line 256 mcts: sample exp_bonus 53.993069013049
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.531]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]] [[46.455]
 [47.825]
 [46.455]
 [46.455]
 [46.455]
 [46.455]
 [46.455]] [[2.211]
 [2.35 ]
 [2.211]
 [2.211]
 [2.211]
 [2.211]
 [2.211]]
printing an ep nov before normalisation:  37.8688028708538
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.0843389392959045, 0.08259652322864798, 0.07923185771946295, 0.08259652322864798, 0.11772525280372936, 0.5535109037236072]
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
probs:  [0.0843606677198464, 0.08257931253729092, 0.0791394542537355, 0.08257931253729092, 0.11849309091726742, 0.5528481620345688]
printing an ep nov before normalisation:  31.42605409441416
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.713]
 [0.646]
 [0.52 ]
 [0.514]
 [0.619]
 [0.505]] [[15.929]
 [12.553]
 [13.657]
 [16.288]
 [16.302]
 [16.046]
 [16.314]] [[1.596]
 [1.551]
 [1.558]
 [1.609]
 [1.604]
 [1.691]
 [1.595]]
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.496]
 [0.491]
 [0.503]
 [0.507]
 [0.511]
 [0.49 ]] [[52.715]
 [53.786]
 [51.643]
 [48.415]
 [48.067]
 [48.533]
 [52.657]] [[0.502]
 [0.496]
 [0.491]
 [0.503]
 [0.507]
 [0.511]
 [0.49 ]]
maxi score, test score, baseline:  -0.9958514170040486 -1.0 -0.9958514170040486
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.08546970157671815, 0.08368670661495262, 0.07858085740626033, 0.08368670661495262, 0.11963339481703861, 0.5489426329700777]
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.08549244845377599, 0.0836901683579578, 0.07852909353811477, 0.0836901683579578, 0.12002566119246935, 0.5485724600997243]
printing an ep nov before normalisation:  32.37123849294691
printing an ep nov before normalisation:  89.95323353502745
maxi score, test score, baseline:  -0.9958677419354839 -1.0 -0.9958677419354839
probs:  [0.0855152877084178, 0.08369364415948428, 0.07847711945117464, 0.08369364415948428, 0.12041952060504278, 0.5482007839163963]
printing an ep nov before normalisation:  36.2601886654053
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.08551531378866042, 0.08369365882506254, 0.07847710142930492, 0.08369365882506254, 0.12041961373532097, 0.5482006533965887]
printing an ep nov before normalisation:  51.360075261896526
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.08551531378866042, 0.08369365882506254, 0.07847710142930492, 0.08369365882506254, 0.12041961373532097, 0.5482006533965887]
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.08551531378866042, 0.08369365882506254, 0.07847710142930492, 0.08369365882506254, 0.12041961373532097, 0.5482006533965887]
printing an ep nov before normalisation:  51.57947204061819
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.08551531378866042, 0.08369365882506254, 0.07847710142930492, 0.08369365882506254, 0.12041961373532097, 0.5482006533965887]
printing an ep nov before normalisation:  40.06706611286788
actor:  1 policy actor:  1  step number:  62 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8453961
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
printing an ep nov before normalisation:  33.546093219739255
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.08502772710308998, 0.08361945281392826, 0.07958666734951063, 0.08361945281392826, 0.11201133770901196, 0.5561353622105311]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9958839357429718 -1.0 -0.9958839357429718
probs:  [0.08502772710308998, 0.08361945281392826, 0.07958666734951063, 0.08361945281392826, 0.11201133770901196, 0.5561353622105311]
actions average: 
K:  2  action  0 :  tensor([0.2678, 0.0078, 0.1312, 0.1409, 0.1592, 0.1575, 0.1356],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0213, 0.8687, 0.0224, 0.0218, 0.0140, 0.0178, 0.0340],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1842, 0.0671, 0.1950, 0.1376, 0.1388, 0.1561, 0.1213],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1684, 0.0238, 0.1040, 0.3024, 0.1301, 0.1351, 0.1362],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1845, 0.0679, 0.1178, 0.1536, 0.1897, 0.1559, 0.1307],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1688, 0.0016, 0.1548, 0.1302, 0.1409, 0.2715, 0.1323],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1720, 0.0022, 0.1552, 0.1702, 0.1568, 0.1782, 0.1654],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.39992461204078
printing an ep nov before normalisation:  48.30589100885331
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.439]
 [0.43 ]
 [0.428]
 [0.43 ]
 [0.432]
 [0.448]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.43 ]
 [0.439]
 [0.43 ]
 [0.428]
 [0.43 ]
 [0.432]
 [0.448]]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.08518056257149009, 0.0837419565655098, 0.07962231209383891, 0.08233680651315693, 0.1127452332189402, 0.5563731290370639]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.08518056257149009, 0.0837419565655098, 0.07962231209383891, 0.08233680651315693, 0.1127452332189402, 0.5563731290370639]
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.08519853962761016, 0.0837456760720038, 0.07958520316276745, 0.08232660004094643, 0.1130363946341188, 0.5561075864625533]
printing an ep nov before normalisation:  48.10453370705139
from probs:  [0.08519853962761016, 0.0837456760720038, 0.07958520316276745, 0.08232660004094643, 0.1130363946341188, 0.5561075864625533]
printing an ep nov before normalisation:  55.29261680528162
siam score:  -0.85033846
line 256 mcts: sample exp_bonus 6.582695493518915
maxi score, test score, baseline:  -0.9959 -1.0 -0.9959
probs:  [0.08533596097337122, 0.08386673648215287, 0.07965941180275485, 0.08102961332669674, 0.11348730264666299, 0.5566209747683614]
printing an ep nov before normalisation:  46.253461837768555
actions average: 
K:  1  action  0 :  tensor([0.2855, 0.0348, 0.1120, 0.1352, 0.1236, 0.1456, 0.1633],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0045, 0.9499, 0.0144, 0.0062, 0.0028, 0.0165, 0.0057],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1997, 0.0072, 0.1374, 0.1628, 0.1408, 0.1848, 0.1674],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1548, 0.0125, 0.1150, 0.2802, 0.1426, 0.1457, 0.1492],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1740, 0.0464, 0.1205, 0.1717, 0.1820, 0.1591, 0.1463],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1855, 0.0076, 0.1313, 0.1694, 0.1512, 0.2153, 0.1399],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1493, 0.0047, 0.1417, 0.1587, 0.1403, 0.1462, 0.2590],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.08623333663249115, 0.08476165700359684, 0.07920565301136659, 0.08191979289262849, 0.11443159984520652, 0.5534479606147105]
printing an ep nov before normalisation:  53.52286075644107
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.634]
 [0.606]
 [0.587]
 [0.587]
 [0.574]
 [0.592]] [[52.069]
 [48.869]
 [52.662]
 [55.509]
 [55.401]
 [55.466]
 [54.743]] [[2.17 ]
 [2.034]
 [2.213]
 [2.35 ]
 [2.345]
 [2.334]
 [2.313]]
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.08635176548466056, 0.08487806159204053, 0.07931441543563233, 0.08065790953590136, 0.11458881480287668, 0.5542090331488886]
printing an ep nov before normalisation:  69.04289985482855
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.08635176548466056, 0.08487806159204053, 0.07931441543563233, 0.08065790953590136, 0.11458881480287668, 0.5542090331488886]
Printing some Q and Qe and total Qs values:  [[0.589]
 [0.61 ]
 [0.565]
 [0.546]
 [0.555]
 [0.53 ]
 [0.553]] [[46.516]
 [50.478]
 [51.386]
 [52.185]
 [51.605]
 [50.132]
 [50.387]] [[1.948]
 [2.215]
 [2.226]
 [2.256]
 [2.23 ]
 [2.114]
 [2.152]]
printing an ep nov before normalisation:  55.63154453649786
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.504]
 [0.5  ]
 [0.489]
 [0.482]
 [0.524]
 [0.5  ]] [[49.48 ]
 [59.271]
 [47.694]
 [49.306]
 [49.281]
 [56.47 ]
 [47.694]] [[1.544]
 [2.152]
 [1.557]
 [1.628]
 [1.619]
 [2.029]
 [1.557]]
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.459]
 [0.507]
 [0.474]
 [0.41 ]
 [0.431]
 [0.405]] [[39.01 ]
 [43.324]
 [40.89 ]
 [41.133]
 [39.332]
 [41.404]
 [38.06 ]] [[1.797]
 [2.13 ]
 [2.011]
 [1.994]
 [1.808]
 [1.971]
 [1.715]]
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.0864385629259908, 0.08492200847335767, 0.07919658941622586, 0.08057914799536281, 0.1154966546411649, 0.553367036547898]
actor:  1 policy actor:  1  step number:  76 total reward:  0.1266666666666657  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  24.62942332202589
printing an ep nov before normalisation:  57.595604700639
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.08477403136671058, 0.08477403136671058, 0.07998250792492936, 0.0811395519378595, 0.11036152677540545, 0.5589683506283846]
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.08477403136671058, 0.08477403136671058, 0.07998250792492936, 0.0811395519378595, 0.11036152677540545, 0.5589683506283846]
printing an ep nov before normalisation:  37.7718448638916
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.08363796488088167, 0.08487917219816829, 0.08008169672461102, 0.08124017801794162, 0.11049845240090467, 0.5596625357774926]
printing an ep nov before normalisation:  30.29349689000451
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.08363796488088167, 0.08487917219816829, 0.08008169672461102, 0.08124017801794162, 0.11049845240090467, 0.5596625357774926]
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.08363796488088167, 0.08487917219816829, 0.08008169672461102, 0.08124017801794162, 0.11049845240090467, 0.5596625357774926]
printing an ep nov before normalisation:  33.12688316707943
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.08363796488088167, 0.08487917219816829, 0.08008169672461102, 0.08124017801794162, 0.11049845240090467, 0.5596625357774926]
printing an ep nov before normalisation:  27.634938455002434
printing an ep nov before normalisation:  25.098770872658992
printing an ep nov before normalisation:  30.249533653259277
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.08363796488088167, 0.08487917219816829, 0.08008169672461102, 0.08124017801794162, 0.11049845240090467, 0.5596625357774926]
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.45 ]
 [0.474]
 [0.536]
 [0.474]
 [0.474]
 [0.498]] [[29.751]
 [35.938]
 [29.316]
 [28.865]
 [29.316]
 [29.316]
 [31.394]] [[1.158]
 [1.412]
 [1.145]
 [1.187]
 [1.145]
 [1.145]
 [1.261]]
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.08364043425658835, 0.0848929385764691, 0.08005179828389632, 0.0812208236386369, 0.11074539584066745, 0.5594486094037419]
actor:  1 policy actor:  1  step number:  72 total reward:  0.04666666666666586  reward:  1.0 rdn_beta:  2.0
using another actor
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
probs:  [0.08360566547173598, 0.08469910810990851, 0.08047276802528659, 0.08149333310253905, 0.10726843471861756, 0.5624606905719122]
maxi score, test score, baseline:  -0.9959159362549801 -1.0 -0.9959159362549801
printing an ep nov before normalisation:  45.08949061774275
from probs:  [0.0836938704309117, 0.0847980842425478, 0.08053011175712291, 0.08053011175712291, 0.10758973447699777, 0.562858087335297]
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.08369387972098648, 0.08479809908952218, 0.08053010512574374, 0.08053010512574374, 0.10758977214062225, 0.5628580387973817]
maxi score, test score, baseline:  -0.9959317460317461 -1.0 -0.9959317460317461
probs:  [0.08369387972098648, 0.08479809908952218, 0.08053010512574374, 0.08053010512574374, 0.10758977214062225, 0.5628580387973817]
printing an ep nov before normalisation:  24.030001256184935
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.08373929405930591, 0.08484411895346022, 0.080573784531111, 0.080573784531111, 0.1071052543746485, 0.5631637635503633]
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.08373929405930591, 0.08484411895346022, 0.080573784531111, 0.080573784531111, 0.1071052543746485, 0.5631637635503633]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9959474308300396 -1.0 -0.9959474308300396
probs:  [0.08373929405930591, 0.08484411895346022, 0.080573784531111, 0.080573784531111, 0.1071052543746485, 0.5631637635503633]
siam score:  -0.83701074
line 256 mcts: sample exp_bonus 50.86160405450764
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.576]
 [0.622]
 [0.625]
 [0.576]
 [0.625]
 [0.62 ]] [[48.279]
 [35.66 ]
 [45.578]
 [45.966]
 [35.66 ]
 [43.917]
 [42.593]] [[0.623]
 [0.576]
 [0.622]
 [0.625]
 [0.576]
 [0.625]
 [0.62 ]]
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.594]
 [0.574]
 [0.574]
 [0.574]
 [0.574]
 [0.574]] [[62.137]
 [61.795]
 [50.128]
 [50.128]
 [50.128]
 [50.128]
 [50.128]] [[2.264]
 [2.245]
 [1.728]
 [1.728]
 [1.728]
 [1.728]
 [1.728]]
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
printing an ep nov before normalisation:  77.81237471425038
actions average: 
K:  1  action  0 :  tensor([0.3661, 0.0334, 0.0966, 0.1271, 0.1254, 0.1305, 0.1209],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0296, 0.8226, 0.0264, 0.0348, 0.0262, 0.0376, 0.0227],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1215, 0.0428, 0.2931, 0.1414, 0.0986, 0.1760, 0.1265],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1511, 0.0754, 0.0997, 0.2370, 0.1375, 0.1692, 0.1300],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2293, 0.0078, 0.0915, 0.1408, 0.2653, 0.1368, 0.1286],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1135, 0.0145, 0.1444, 0.1618, 0.1064, 0.3215, 0.1379],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1526, 0.1370, 0.1197, 0.1502, 0.1293, 0.1809, 0.1303],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.08452732356555501, 0.08452732356555501, 0.08129475872120011, 0.08026512695596115, 0.10838815546636554, 0.5609973117253633]
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.08452732356555501, 0.08452732356555501, 0.08129475872120011, 0.08026512695596115, 0.10838815546636554, 0.5609973117253633]
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.296]
 [0.296]
 [0.296]
 [0.296]
 [0.296]
 [0.296]] [[41.735]
 [41.735]
 [41.735]
 [41.735]
 [41.735]
 [41.735]
 [41.735]] [[1.758]
 [1.758]
 [1.758]
 [1.758]
 [1.758]
 [1.758]
 [1.758]]
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.084557178728414, 0.084557178728414, 0.0812414647279555, 0.08018534841669835, 0.10903176723798907, 0.5604270621605291]
using another actor
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.08523393589683605, 0.08523393589683605, 0.08193864152885516, 0.07986248537027382, 0.10955779907173571, 0.5581732022354631]
maxi score, test score, baseline:  -0.9959629921259843 -1.0 -0.9959629921259843
probs:  [0.08523393589683605, 0.08523393589683605, 0.08193864152885516, 0.07986248537027382, 0.10955779907173571, 0.5581732022354631]
printing an ep nov before normalisation:  52.804564532901885
printing an ep nov before normalisation:  53.18929592442151
maxi score, test score, baseline:  -0.995978431372549 -1.0 -0.995978431372549
probs:  [0.08524953233077626, 0.08524953233077626, 0.08192672786307863, 0.07983323933397975, 0.10977636457175603, 0.557964603569633]
printing an ep nov before normalisation:  41.24266189304852
maxi score, test score, baseline:  -0.995978431372549 -1.0 -0.995978431372549
probs:  [0.08524953233077626, 0.08524953233077626, 0.08192672786307863, 0.07983323933397975, 0.10977636457175603, 0.557964603569633]
printing an ep nov before normalisation:  30.96080707845708
maxi score, test score, baseline:  -0.99599375 -1.0 -0.99599375
printing an ep nov before normalisation:  67.54139638198983
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]]
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
probs:  [0.08524956600242425, 0.08524956600242425, 0.08192672932919749, 0.07983322050943556, 0.10977645057151535, 0.557964467585003]
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.563]
 [0.558]
 [0.539]
 [0.53 ]
 [0.528]
 [0.567]] [[46.818]
 [44.252]
 [43.454]
 [45.362]
 [47.61 ]
 [47.368]
 [46.818]] [[2.507]
 [2.309]
 [2.243]
 [2.368]
 [2.53 ]
 [2.51 ]
 [2.507]]
printing an ep nov before normalisation:  48.52224997017121
actor:  1 policy actor:  1  step number:  65 total reward:  0.13333333333333275  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.529]
 [0.523]
 [0.493]
 [0.523]
 [0.496]
 [0.518]] [[33.785]
 [31.705]
 [33.114]
 [33.947]
 [32.393]
 [33.273]
 [31.317]] [[2.449]
 [2.262]
 [2.413]
 [2.475]
 [2.332]
 [2.404]
 [2.208]]
from probs:  [0.08503244266412177, 0.08503244266412177, 0.08209277054494518, 0.08024066942956652, 0.10673107122415958, 0.560870603473085]
printing an ep nov before normalisation:  43.65968322268954
printing an ep nov before normalisation:  26.15108776448747
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
probs:  [0.08513171777183061, 0.08412026181381273, 0.08216553793989052, 0.08029673599449236, 0.10702600795327155, 0.5612597385267023]
maxi score, test score, baseline:  -0.9960089494163424 -1.0 -0.9960089494163424
probs:  [0.08513171777183061, 0.08412026181381273, 0.08216553793989052, 0.08029673599449236, 0.10702600795327155, 0.5612597385267023]
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
probs:  [0.08514577541801219, 0.0841262677685627, 0.08215598332243562, 0.08027230478602843, 0.10721427258410213, 0.5610853961208588]
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
probs:  [0.08517393727111497, 0.08413829176075406, 0.08213681953848356, 0.08022332411719202, 0.10759175828777397, 0.5607358690246813]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
probs:  [0.08517393727111497, 0.08413829176075406, 0.08213681953848356, 0.08022332411719202, 0.10759175828777397, 0.5607358690246813]
printing an ep nov before normalisation:  61.7232641302996
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
probs:  [0.08525634940476048, 0.08421970026030848, 0.08124797271287942, 0.08030093865930313, 0.10769589531588931, 0.561279143646859]
printing an ep nov before normalisation:  52.423918209991804
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.792]
 [0.825]
 [0.792]
 [0.765]
 [0.764]
 [0.792]
 [0.792]] [[35.846]
 [37.548]
 [35.846]
 [36.414]
 [37.956]
 [35.846]
 [35.846]] [[2.361]
 [2.534]
 [2.361]
 [2.38 ]
 [2.506]
 [2.361]
 [2.361]]
Printing some Q and Qe and total Qs values:  [[0.763]
 [0.859]
 [0.763]
 [0.897]
 [0.906]
 [0.763]
 [0.763]] [[47.743]
 [42.375]
 [47.743]
 [45.991]
 [46.465]
 [47.743]
 [47.743]] [[2.548]
 [2.294]
 [2.548]
 [2.568]
 [2.608]
 [2.548]
 [2.548]]
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
printing an ep nov before normalisation:  62.714376983433944
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.6377],
        [-0.5999],
        [-0.5343],
        [-0.3832],
        [-0.5685],
        [-0.5886],
        [-0.5921],
        [-0.6233],
        [-0.4488],
        [-0.6010]], dtype=torch.float64)
-0.032346567066 -0.6700252634797177
-0.032346567066 -0.6322938856855811
-0.09703970119800001 -0.6313832120344048
-0.09703970119800001 -0.48019036123026093
-0.032346567066 -0.6008360385075151
-0.032346567066 -0.6209408547487191
-0.032346567066 -0.6244657899283041
-0.032346567066 -0.6556922621003994
-0.09703970119800001 -0.5458314177276492
-0.032346567066 -0.6333316855665547
siam score:  -0.83755493
printing an ep nov before normalisation:  55.08336955513542
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.59 ]
 [0.599]
 [0.574]
 [0.566]
 [0.599]
 [0.599]] [[39.427]
 [42.059]
 [34.088]
 [40.201]
 [40.564]
 [34.088]
 [34.088]] [[2.05 ]
 [2.238]
 [1.702]
 [2.095]
 [2.112]
 [1.702]
 [1.702]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  71.07692608147025
printing an ep nov before normalisation:  36.42671823501587
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960240310077519 -1.0 -0.9960240310077519
probs:  [0.08425848469042885, 0.08425848469042885, 0.08082637581278755, 0.08082637581278755, 0.10481928059156934, 0.5650109984019979]
printing an ep nov before normalisation:  34.283270835876465
printing an ep nov before normalisation:  70.99436029269515
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.08484145965058715, 0.08484145965058715, 0.08058478494633374, 0.08139869857110306, 0.10501210988710484, 0.5633214872942842]
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.382]
 [0.375]
 [0.38 ]
 [0.375]
 [0.379]
 [0.369]] [[55.66 ]
 [55.911]
 [56.668]
 [56.044]
 [55.307]
 [55.795]
 [55.65 ]] [[1.225]
 [1.252]
 [1.268]
 [1.254]
 [1.226]
 [1.246]
 [1.231]]
printing an ep nov before normalisation:  37.14445794886103
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.08484145965058715, 0.08484145965058715, 0.08058478494633374, 0.08139869857110306, 0.10501210988710484, 0.5633214872942842]
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.55 ]
 [0.532]
 [0.51 ]
 [0.52 ]
 [0.519]
 [0.516]] [[50.046]
 [56.435]
 [52.958]
 [53.205]
 [52.255]
 [51.564]
 [50.997]] [[0.727]
 [0.827]
 [0.778]
 [0.758]
 [0.76 ]
 [0.752]
 [0.744]]
maxi score, test score, baseline:  -0.9960389961389962 -1.0 -0.9960389961389962
probs:  [0.08484145965058715, 0.08484145965058715, 0.08058478494633374, 0.08139869857110306, 0.10501210988710484, 0.5633214872942842]
printing an ep nov before normalisation:  52.2848775479345
printing an ep nov before normalisation:  48.20024013519287
Sims:  50 1 epoch:  29160 pick best:  False frame count:  29160
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.08536266647138131, 0.08536266647138131, 0.0803335854435822, 0.08193625961727642, 0.10543742778837906, 0.5615673942079996]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]] [[40.855]
 [40.855]
 [40.855]
 [40.855]
 [40.855]
 [40.855]
 [40.855]] [[1.842]
 [1.842]
 [1.842]
 [1.842]
 [1.842]
 [1.842]
 [1.842]]
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.08455170047476102, 0.08543829049273009, 0.08040474716490574, 0.08200884339025635, 0.10553086413119099, 0.562065554346156]
printing an ep nov before normalisation:  59.403829862770365
printing an ep nov before normalisation:  53.172372991964544
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.08456937875562658, 0.0854691947347181, 0.08036056207923083, 0.08198858787053995, 0.1058615051087684, 0.5617507714511162]
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.08456937875562658, 0.0854691947347181, 0.08036056207923083, 0.08198858787053995, 0.1058615051087684, 0.5617507714511162]
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.08465491290238132, 0.08465491290238132, 0.08041124632634093, 0.08205275251839174, 0.10612334260244465, 0.5621028327480599]
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.08465491290238132, 0.08465491290238132, 0.08041124632634093, 0.08205275251839174, 0.10612334260244465, 0.5621028327480599]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.08465491290238132, 0.08465491290238132, 0.08041124632634093, 0.08205275251839174, 0.10612334260244465, 0.5621028327480599]
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.08465491290238132, 0.08465491290238132, 0.08041124632634093, 0.08205275251839174, 0.10612334260244465, 0.5621028327480599]
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.08465491290238132, 0.08465491290238132, 0.08041124632634093, 0.08205275251839174, 0.10612334260244465, 0.5621028327480599]
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
printing an ep nov before normalisation:  31.338072114315825
Printing some Q and Qe and total Qs values:  [[0.278]
 [0.421]
 [0.278]
 [0.278]
 [0.278]
 [0.278]
 [0.278]] [[36.246]
 [48.715]
 [36.246]
 [36.246]
 [36.246]
 [36.246]
 [36.246]] [[1.316]
 [2.02 ]
 [1.316]
 [1.316]
 [1.316]
 [1.316]
 [1.316]]
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.08466435478591955, 0.08466435478591955, 0.08038958562254776, 0.08204312270552236, 0.10629013042672768, 0.561948451673363]
using another actor
printing an ep nov before normalisation:  28.123903442956717
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
probs:  [0.08384582294390222, 0.08474000114678651, 0.08046140652008214, 0.08211642334271943, 0.10638512955463088, 0.5624512164918789]
actor:  1 policy actor:  1  step number:  53 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9960538461538462 -1.0 -0.9960538461538462
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.613]
 [0.613]
 [0.627]
 [0.613]
 [0.613]
 [0.623]] [[46.931]
 [46.931]
 [46.931]
 [52.172]
 [46.931]
 [46.931]
 [49.72 ]] [[0.613]
 [0.613]
 [0.613]
 [0.627]
 [0.613]
 [0.613]
 [0.623]]
printing an ep nov before normalisation:  38.37301731109619
siam score:  -0.8372524
actor:  1 policy actor:  1  step number:  47 total reward:  0.4266666666666663  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.7112139165618
printing an ep nov before normalisation:  65.00156630579588
printing an ep nov before normalisation:  78.57344124118121
printing an ep nov before normalisation:  45.98823945517312
actions average: 
K:  4  action  0 :  tensor([0.2888, 0.0675, 0.1190, 0.1169, 0.1263, 0.1415, 0.1401],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0213, 0.8635, 0.0214, 0.0285, 0.0128, 0.0282, 0.0244],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1740, 0.0270, 0.1475, 0.1471, 0.1370, 0.2035, 0.1639],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1678, 0.0955, 0.1097, 0.2114, 0.1307, 0.1433, 0.1417],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2122, 0.0220, 0.1200, 0.1273, 0.2249, 0.1502, 0.1434],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1852, 0.1109, 0.1520, 0.1131, 0.1145, 0.1814, 0.1429],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1556, 0.1030, 0.1338, 0.1299, 0.1201, 0.1554, 0.2023],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.31001604814796
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.0837274016975304, 0.08440058545968489, 0.081179437350451, 0.08242541991578103, 0.100696106173246, 0.5675710494033066]
printing an ep nov before normalisation:  39.75192015810935
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.0837274016975304, 0.08440058545968489, 0.081179437350451, 0.08242541991578103, 0.100696106173246, 0.5675710494033066]
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.08378666782090462, 0.08378666782090462, 0.08121920421723948, 0.0824747221333175, 0.10088523169707792, 0.5678475063105559]
printing an ep nov before normalisation:  61.96580328916839
actor:  1 policy actor:  1  step number:  69 total reward:  0.05333333333333257  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.07865728003022324, 0.13995803232842138, 0.07623062048912324, 0.07741728367130951, 0.09481812963681258, 0.5329186538441102]
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.07865728003022324, 0.13995803232842138, 0.07623062048912324, 0.07741728367130951, 0.09481812963681258, 0.5329186538441102]
printing an ep nov before normalisation:  38.89301366382142
printing an ep nov before normalisation:  51.789404771171114
Printing some Q and Qe and total Qs values:  [[0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]] [[58.726]
 [58.726]
 [58.726]
 [58.726]
 [58.726]
 [58.726]
 [58.726]] [[0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]
 [0.62]]
maxi score, test score, baseline:  -0.9960685823754789 -1.0 -0.9960685823754789
probs:  [0.07862682076427528, 0.14032405382430074, 0.07618446608424988, 0.07737880436184473, 0.09489219544965365, 0.5325936595156756]
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.07862680700852046, 0.14032428403686065, 0.07618443898877325, 0.07737878378963864, 0.09489221782517082, 0.5325934683510362]
printing an ep nov before normalisation:  53.39710163110788
printing an ep nov before normalisation:  32.71031253032135
printing an ep nov before normalisation:  47.174167946000594
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.07899931496072836, 0.1408773974345069, 0.07596999145532644, 0.07774763843541944, 0.09531233909817029, 0.5310933186158486]
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.07943899016408291, 0.14114434525206557, 0.07585210760035381, 0.077587067101288, 0.09570647788699747, 0.5302710119952122]
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
printing an ep nov before normalisation:  47.683513591277205
printing an ep nov before normalisation:  47.73898201625798
printing an ep nov before normalisation:  48.47886779402586
printing an ep nov before normalisation:  34.80077601735232
printing an ep nov before normalisation:  54.21426179717001
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.425]
 [0.355]
 [0.353]
 [0.413]
 [0.413]
 [0.403]] [[36.606]
 [44.417]
 [37.239]
 [37.147]
 [41.016]
 [41.016]
 [38.355]] [[0.865]
 [1.194]
 [0.881]
 [0.876]
 [1.067]
 [1.067]
 [0.967]]
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.07943899016408291, 0.14114434525206557, 0.07585210760035381, 0.077587067101288, 0.09570647788699747, 0.5302710119952122]
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.07943899016408291, 0.14114434525206557, 0.07585210760035381, 0.077587067101288, 0.09570647788699747, 0.5302710119952122]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9960832061068703 -1.0 -0.9960832061068703
probs:  [0.07943899016408291, 0.14114434525206557, 0.07585210760035381, 0.077587067101288, 0.09570647788699747, 0.5302710119952122]
actor:  1 policy actor:  1  step number:  75 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
Starting evaluation
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
printing an ep nov before normalisation:  58.407738787843016
siam score:  -0.84032625
maxi score, test score, baseline:  -0.9960977186311787 -1.0 -0.9960977186311787
probs:  [0.07974605984175097, 0.13703262232278937, 0.07641603314939564, 0.07802675258211099, 0.09453887111228501, 0.5342396609916679]
printing an ep nov before normalisation:  38.235427289803965
using explorer policy with actor:  0
printing an ep nov before normalisation:  44.760765803515696
printing an ep nov before normalisation:  41.62667302447539
using explorer policy with actor:  0
printing an ep nov before normalisation:  33.615616566926704
printing an ep nov before normalisation:  25.600643157958984
maxi score, test score, baseline:  -0.9961406015037594 -1.0 -0.9961406015037594
probs:  [0.07974603559676423, 0.13703325023434892, 0.07641595627491372, 0.07802670116428707, 0.09453893781603616, 0.53423911891365]
printing an ep nov before normalisation:  24.576034545898438
using another actor
printing an ep nov before normalisation:  36.86992901360799
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.632]
 [0.642]
 [0.628]
 [0.628]
 [0.628]
 [0.627]] [[44.5  ]
 [39.573]
 [39.596]
 [44.5  ]
 [44.5  ]
 [44.5  ]
 [41.763]] [[0.628]
 [0.632]
 [0.642]
 [0.628]
 [0.628]
 [0.628]
 [0.627]]
printing an ep nov before normalisation:  27.62062480975088
printing an ep nov before normalisation:  25.583569022691485
Printing some Q and Qe and total Qs values:  [[0.609]
 [0.55 ]
 [0.609]
 [0.603]
 [0.609]
 [0.609]
 [0.603]] [[21.792]
 [31.653]
 [21.792]
 [22.772]
 [21.792]
 [21.792]
 [24.722]] [[0.609]
 [0.55 ]
 [0.609]
 [0.603]
 [0.609]
 [0.609]
 [0.603]]
Printing some Q and Qe and total Qs values:  [[0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]]
printing an ep nov before normalisation:  19.972286224365234
printing an ep nov before normalisation:  22.032827181655733
printing an ep nov before normalisation:  30.07955195700731
printing an ep nov before normalisation:  24.161178644299497
printing an ep nov before normalisation:  17.426958513903777
printing an ep nov before normalisation:  31.47330366605231
printing an ep nov before normalisation:  20.467801482648884
printing an ep nov before normalisation:  13.05342435836792
printing an ep nov before normalisation:  18.696981902534873
actor:  0 policy actor:  1  step number:  42 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.452]
 [0.429]
 [0.42 ]
 [0.431]
 [0.427]
 [0.418]] [[44.06 ]
 [49.464]
 [44.81 ]
 [44.053]
 [44.139]
 [42.464]
 [39.956]] [[0.449]
 [0.452]
 [0.429]
 [0.42 ]
 [0.431]
 [0.427]
 [0.418]]
actor:  0 policy actor:  1  step number:  46 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  11.82864785194397
printing an ep nov before normalisation:  13.688156917833341
actor:  0 policy actor:  1  step number:  49 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.461]
 [0.482]
 [0.48 ]
 [0.482]
 [0.475]
 [0.473]] [[38.252]
 [40.9  ]
 [34.499]
 [36.536]
 [37.201]
 [36.838]
 [35.877]] [[0.478]
 [0.461]
 [0.482]
 [0.48 ]
 [0.482]
 [0.475]
 [0.473]]
printing an ep nov before normalisation:  48.50336099555338
maxi score, test score, baseline:  -0.9816142857142857 -1.0 -0.9816142857142857
actor:  1 policy actor:  1  step number:  58 total reward:  0.31333333333333324  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  0
printing an ep nov before normalisation:  18.73317273699053
maxi score, test score, baseline:  -0.9816142857142857 -1.0 -0.9816142857142857
actor:  0 policy actor:  1  step number:  62 total reward:  0.1666666666666664  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.9776856301531213 -1.0 -0.9776856301531213
probs:  [0.08038913795316649, 0.1327078099565838, 0.07688475608920357, 0.07882196097052468, 0.09365001091228012, 0.5375463241182413]
maxi score, test score, baseline:  -0.9778415204678362 -1.0 -0.9778415204678362
probs:  [0.08038906977242524, 0.13271005951056897, 0.07688447427039896, 0.07882179724977995, 0.09365026164707635, 0.5375443375497506]
actions average: 
K:  3  action  0 :  tensor([0.2880, 0.0748, 0.1422, 0.1105, 0.1236, 0.1472, 0.1136],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0307, 0.7966, 0.0195, 0.0207, 0.0094, 0.0137, 0.1095],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1661, 0.0375, 0.2088, 0.1429, 0.1401, 0.1705, 0.1340],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1389, 0.0701, 0.1226, 0.2653, 0.1172, 0.1695, 0.1163],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2099, 0.0515, 0.1201, 0.1207, 0.2475, 0.1447, 0.1057],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1585, 0.0070, 0.2073, 0.1323, 0.1505, 0.2233, 0.1210],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1427, 0.1133, 0.1358, 0.1588, 0.1266, 0.1740, 0.1487],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]] [[58.768]
 [58.768]
 [58.768]
 [58.768]
 [58.768]
 [58.768]
 [58.768]] [[2.257]
 [2.257]
 [2.257]
 [2.257]
 [2.257]
 [2.257]
 [2.257]]
maxi score, test score, baseline:  -0.9778415204678362 -1.0 -0.9778415204678362
probs:  [0.08041136358324118, 0.13274688726935713, 0.07690579455970958, 0.07884365569694134, 0.09339867698756743, 0.5376936219031833]
printing an ep nov before normalisation:  13.248194166057248
maxi score, test score, baseline:  -0.9778415204678362 -1.0 -0.9778415204678362
probs:  [0.08041136358324118, 0.13274688726935713, 0.07690579455970958, 0.07884365569694134, 0.09339867698756743, 0.5376936219031833]
maxi score, test score, baseline:  -0.9778415204678362 -1.0 -0.9778415204678362
maxi score, test score, baseline:  -0.9778415204678362 -1.0 -0.9778415204678362
probs:  [0.08041136358324118, 0.13274688726935713, 0.07690579455970958, 0.07884365569694134, 0.09339867698756743, 0.5376936219031833]
maxi score, test score, baseline:  -0.9778415204678362 -1.0 -0.9778415204678362
probs:  [0.08041136358324118, 0.13274688726935713, 0.07690579455970958, 0.07884365569694134, 0.09339867698756743, 0.5376936219031833]
maxi score, test score, baseline:  -0.9778415204678362 -1.0 -0.9778415204678362
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]
 [0.469]] [[42.213]
 [42.213]
 [42.213]
 [42.213]
 [42.213]
 [42.213]
 [42.213]] [[1.526]
 [1.526]
 [1.526]
 [1.526]
 [1.526]
 [1.526]
 [1.526]]
printing an ep nov before normalisation:  36.9369013259106
printing an ep nov before normalisation:  34.83912944793701
maxi score, test score, baseline:  -0.9778415204678362 -1.0 -0.9778415204678362
probs:  [0.08052294702649317, 0.1321542988353864, 0.0769900588396482, 0.07843903141397632, 0.09361147154191528, 0.5382821923425806]
Printing some Q and Qe and total Qs values:  [[0.718]
 [0.657]
 [0.718]
 [0.72 ]
 [0.774]
 [0.721]
 [0.711]] [[12.853]
 [28.468]
 [14.828]
 [17.951]
 [16.235]
 [15.091]
 [16.43 ]] [[0.718]
 [0.657]
 [0.718]
 [0.72 ]
 [0.774]
 [0.721]
 [0.711]]
printing an ep nov before normalisation:  48.12745347399073
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [1.3  ]
 [1.391]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [1.3  ]
 [1.391]]
maxi score, test score, baseline:  -0.9778415204678362 -1.0 -0.9778415204678362
probs:  [0.08050549004308892, 0.13245379710429142, 0.07695091411367261, 0.0784087816607604, 0.0936743625727466, 0.5380066545054402]
printing an ep nov before normalisation:  58.19560780583058
maxi score, test score, baseline:  -0.9779186480186479 -0.7356666666666667 -0.7356666666666667
probs:  [0.08062626930801173, 0.12818249281475705, 0.07748241581406039, 0.07877183038070402, 0.09318343360970878, 0.541753558072758]
printing an ep nov before normalisation:  55.83975639109671
printing an ep nov before normalisation:  46.877324599670516
maxi score, test score, baseline:  -0.9779186480186479 -0.7356666666666667 -0.7356666666666667
probs:  [0.08092718844385359, 0.12837801528032922, 0.07737913485982256, 0.0790768594242245, 0.09320629997847514, 0.5410325020132949]
maxi score, test score, baseline:  -0.9779186480186479 -0.7356666666666667 -0.7356666666666667
probs:  [0.08096261816444843, 0.12843425310961934, 0.07741300869004508, 0.07867330886247285, 0.09324711432784312, 0.5412696968455712]
printing an ep nov before normalisation:  58.46930641777671
using explorer policy with actor:  1
printing an ep nov before normalisation:  67.23063222363562
maxi score, test score, baseline:  -0.9779186480186479 -0.7356666666666667 -0.7356666666666667
maxi score, test score, baseline:  -0.9779186480186479 -0.7356666666666667 -0.7356666666666667
probs:  [0.08102816492251903, 0.12784440940352784, 0.07745347864270162, 0.07872268241492403, 0.09339944694844887, 0.5415518176678785]
Printing some Q and Qe and total Qs values:  [[0.132]
 [0.167]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]] [[60.996]
 [64.683]
 [60.996]
 [60.996]
 [60.996]
 [60.996]
 [60.996]] [[1.654]
 [1.86 ]
 [1.654]
 [1.654]
 [1.654]
 [1.654]
 [1.654]]
maxi score, test score, baseline:  -0.9779186480186479 -0.7356666666666667 -0.7356666666666667
probs:  [0.08101404737466637, 0.12811292339114877, 0.07741778057118029, 0.07869464657720526, 0.09346001532368822, 0.541300586762111]
maxi score, test score, baseline:  -0.9779186480186479 -0.7356666666666667 -0.7356666666666667
probs:  [0.08101404737466637, 0.12811292339114877, 0.07741778057118029, 0.07869464657720526, 0.09346001532368822, 0.541300586762111]
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.157]
 [0.065]
 [0.065]
 [0.065]
 [0.065]
 [0.065]] [[37.26 ]
 [40.743]
 [37.26 ]
 [37.26 ]
 [37.26 ]
 [37.26 ]
 [37.26 ]] [[1.2  ]
 [1.476]
 [1.2  ]
 [1.2  ]
 [1.2  ]
 [1.2  ]
 [1.2  ]]
printing an ep nov before normalisation:  0.0012688071771549403
Printing some Q and Qe and total Qs values:  [[0.092]
 [0.153]
 [0.125]
 [0.055]
 [0.107]
 [0.043]
 [0.068]] [[45.207]
 [48.429]
 [49.337]
 [48.831]
 [50.647]
 [47.815]
 [46.358]] [[1.286]
 [1.529]
 [1.553]
 [1.454]
 [1.61 ]
 [1.384]
 [1.327]]
printing an ep nov before normalisation:  66.74833139333067
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.912676948916754
printing an ep nov before normalisation:  29.4677472114563
printing an ep nov before normalisation:  35.56555327541368
UNIT TEST: sample policy line 217 mcts : [0.041 0.286 0.204 0.143 0.02  0.204 0.102]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9779186480186479 -0.7356666666666667 -0.7356666666666667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9779186480186479 -0.7356666666666667 -0.7356666666666667
probs:  [0.08102821916194812, 0.12900868574790636, 0.07736463790921355, 0.07822268720261717, 0.09345109595203607, 0.5409246740262788]
printing an ep nov before normalisation:  56.01511557217869
printing an ep nov before normalisation:  31.86638909845989
maxi score, test score, baseline:  -0.9779186480186479 -0.7356666666666667 -0.7356666666666667
probs:  [0.08102821916194812, 0.12900868574790636, 0.07736463790921355, 0.07822268720261717, 0.09345109595203607, 0.5409246740262788]
actor:  1 policy actor:  1  step number:  65 total reward:  0.03999999999999926  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9779186480186479 -0.7356666666666667 -0.7356666666666667
printing an ep nov before normalisation:  54.32310400881391
printing an ep nov before normalisation:  53.00515363676368
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.4  ]
 [0.39 ]
 [0.385]
 [0.37 ]
 [0.363]
 [0.372]] [[43.774]
 [52.366]
 [44.419]
 [43.793]
 [42.978]
 [38.87 ]
 [44.118]] [[0.37 ]
 [0.4  ]
 [0.39 ]
 [0.385]
 [0.37 ]
 [0.363]
 [0.372]]
printing an ep nov before normalisation:  60.64378443393994
using explorer policy with actor:  0
printing an ep nov before normalisation:  42.012330702534484
maxi score, test score, baseline:  -0.9780712962962964 -0.7356666666666667 -0.7356666666666667
probs:  [0.08118333004647339, 0.12739364351865254, 0.0776549100421292, 0.0776549100421292, 0.09314788711612527, 0.5429653192344903]
maxi score, test score, baseline:  -0.9780712962962964 -0.7356666666666667 -0.7356666666666667
probs:  [0.08118333004647339, 0.12739364351865254, 0.0776549100421292, 0.0776549100421292, 0.09314788711612527, 0.5429653192344903]
from probs:  [0.08118333004647339, 0.12739364351865254, 0.0776549100421292, 0.0776549100421292, 0.09314788711612527, 0.5429653192344903]
maxi score, test score, baseline:  -0.9780712962962964 -0.7356666666666667 -0.7356666666666667
probs:  [0.0812031597690786, 0.12742477905902239, 0.0776738765013075, 0.0776738765013075, 0.09292617960602537, 0.5430981285632586]
maxi score, test score, baseline:  -0.9780712962962964 -0.7356666666666667 -0.7356666666666667
probs:  [0.0812031597690786, 0.12742477905902239, 0.0776738765013075, 0.0776738765013075, 0.09292617960602537, 0.5430981285632586]
maxi score, test score, baseline:  -0.9780712962962964 -0.7356666666666667 -0.7356666666666667
probs:  [0.0812031597690786, 0.12742477905902239, 0.0776738765013075, 0.0776738765013075, 0.09292617960602537, 0.5430981285632586]
printing an ep nov before normalisation:  50.63124349718583
printing an ep nov before normalisation:  50.438033440525636
maxi score, test score, baseline:  -0.9780712962962964 -0.7356666666666667 -0.7356666666666667
probs:  [0.0812031597690786, 0.12742477905902239, 0.0776738765013075, 0.0776738765013075, 0.09292617960602537, 0.5430981285632586]
maxi score, test score, baseline:  -0.9780712962962964 -0.7356666666666667 -0.7356666666666667
probs:  [0.08119053102081605, 0.12768195522375184, 0.07764064661488465, 0.07764064661488465, 0.09298198048865199, 0.5428642400370108]
printing an ep nov before normalisation:  39.79817094160897
maxi score, test score, baseline:  -0.9780712962962964 -0.7356666666666667 -0.7356666666666667
probs:  [0.08126870496652186, 0.12684130958692996, 0.07771539695215302, 0.07771539695215302, 0.09307152644011107, 0.543387665102131]
maxi score, test score, baseline:  -0.9780712962962964 -0.7356666666666667 -0.7356666666666667
probs:  [0.08126870496652186, 0.12684130958692996, 0.07771539695215302, 0.07771539695215302, 0.09307152644011107, 0.543387665102131]
actions average: 
K:  2  action  0 :  tensor([0.2956, 0.0293, 0.1110, 0.1257, 0.1712, 0.1221, 0.1451],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0319, 0.8368, 0.0260, 0.0246, 0.0256, 0.0247, 0.0303],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2016, 0.0133, 0.1404, 0.1383, 0.1494, 0.2004, 0.1567],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1377, 0.0679, 0.0824, 0.2700, 0.1161, 0.1423, 0.1837],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1996, 0.0297, 0.1053, 0.1204, 0.2950, 0.1303, 0.1198],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1570, 0.0256, 0.1323, 0.1499, 0.1309, 0.2844, 0.1200],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1902, 0.0510, 0.1233, 0.1606, 0.1453, 0.1651, 0.1645],
       grad_fn=<DivBackward0>)
siam score:  -0.83584505
printing an ep nov before normalisation:  50.99625001700337
Printing some Q and Qe and total Qs values:  [[-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]] [[48.145]
 [48.145]
 [48.145]
 [48.145]
 [48.145]
 [48.145]
 [48.145]] [[0.83]
 [0.83]
 [0.83]
 [0.83]
 [0.83]
 [0.83]
 [0.83]]
maxi score, test score, baseline:  -0.978146828143022 -0.7356666666666667 -0.7356666666666667
probs:  [0.08126870496652186, 0.12684130958692996, 0.07771539695215302, 0.07771539695215302, 0.09307152644011107, 0.543387665102131]
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]] [[64.864]
 [64.864]
 [64.864]
 [64.864]
 [64.864]
 [64.864]
 [64.864]] [[1.888]
 [1.888]
 [1.888]
 [1.888]
 [1.888]
 [1.888]
 [1.888]]
printing an ep nov before normalisation:  42.077127743056764
printing an ep nov before normalisation:  69.55872675641575
using another actor
maxi score, test score, baseline:  -0.978146828143022 -0.7356666666666667 -0.7356666666666667
siam score:  -0.83484805
using explorer policy with actor:  0
printing an ep nov before normalisation:  46.81318039768176
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.577]
 [0.51 ]
 [0.557]
 [0.51 ]
 [0.51 ]
 [0.51 ]] [[32.993]
 [32.319]
 [32.993]
 [33.7  ]
 [32.993]
 [32.993]
 [32.993]] [[0.51 ]
 [0.577]
 [0.51 ]
 [0.557]
 [0.51 ]
 [0.51 ]
 [0.51 ]]
maxi score, test score, baseline:  -0.978146828143022 -0.7356666666666667 -0.7356666666666667
probs:  [0.08180835580948378, 0.12716366571893822, 0.07747720110779958, 0.07827199032109834, 0.09355490025651499, 0.5417238867861651]
printing an ep nov before normalisation:  42.31277450291356
printing an ep nov before normalisation:  38.74397317743751
printing an ep nov before normalisation:  41.75804615020752
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.555]
 [0.544]
 [0.547]
 [0.552]
 [0.54 ]
 [0.556]] [[34.621]
 [38.119]
 [34.692]
 [35.233]
 [35.098]
 [34.388]
 [33.639]] [[0.522]
 [0.555]
 [0.544]
 [0.547]
 [0.552]
 [0.54 ]
 [0.556]]
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.569]
 [0.523]
 [0.588]
 [0.523]
 [0.543]
 [0.517]] [[35.457]
 [38.401]
 [35.535]
 [40.288]
 [35.535]
 [36.009]
 [40.756]] [[0.534]
 [0.569]
 [0.523]
 [0.588]
 [0.523]
 [0.543]
 [0.517]]
printing an ep nov before normalisation:  48.90789585398293
maxi score, test score, baseline:  -0.9782218390804599 -0.7356666666666667 -0.7356666666666667
probs:  [0.08179934612011293, 0.12741688916922833, 0.0774431497574182, 0.07824253424459311, 0.09361380617033725, 0.5414842745383102]
printing an ep nov before normalisation:  78.26923825853589
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9782218390804599 -0.7356666666666667 -0.7356666666666667
probs:  [0.08138060713522054, 0.12778357787217584, 0.07747783521477065, 0.07787607724746962, 0.09375592225977132, 0.5417259802705919]
printing an ep nov before normalisation:  54.02070604997107
printing an ep nov before normalisation:  49.3700289016309
maxi score, test score, baseline:  -0.9782218390804599 -0.7356666666666667 -0.7356666666666667
probs:  [0.08138060713522054, 0.12778357787217584, 0.07747783521477065, 0.07787607724746962, 0.09375592225977132, 0.5417259802705919]
maxi score, test score, baseline:  -0.9782218390804599 -0.7356666666666667 -0.7356666666666667
probs:  [0.08138060713522054, 0.12778357787217584, 0.07747783521477065, 0.07787607724746962, 0.09375592225977132, 0.5417259802705919]
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.9782218390804599 -0.7356666666666667 -0.7356666666666667
probs:  [0.08094699899573794, 0.12784387531183802, 0.07751437002898033, 0.07791280017690756, 0.09380014625746383, 0.5419818092290722]
maxi score, test score, baseline:  -0.9782218390804599 -0.7356666666666667 -0.7356666666666667
probs:  [0.08094699899573794, 0.12784387531183802, 0.07751437002898033, 0.07791280017690756, 0.09380014625746383, 0.5419818092290722]
actor:  1 policy actor:  1  step number:  63 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  40.179223576864906
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.035]
 [0.058]
 [0.062]
 [0.063]
 [0.062]
 [0.071]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.063]
 [0.035]
 [0.058]
 [0.062]
 [0.063]
 [0.062]
 [0.071]]
maxi score, test score, baseline:  -0.9782218390804599 -0.7356666666666667 -0.7356666666666667
probs:  [0.07863071323817328, 0.12490695041128455, 0.07534617150000582, 0.07611703333651451, 0.11819892614794357, 0.5268002053660783]
printing an ep nov before normalisation:  44.18064295796999
printing an ep nov before normalisation:  33.58575112706373
maxi score, test score, baseline:  -0.9782218390804599 -0.7356666666666667 -0.7356666666666667
printing an ep nov before normalisation:  89.59268751879142
printing an ep nov before normalisation:  34.54655262668904
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  47.15831648882897
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.13 ]
 [0.045]
 [0.045]
 [0.045]
 [0.045]
 [0.045]] [[32.525]
 [76.852]
 [32.525]
 [32.525]
 [32.525]
 [32.525]
 [32.525]] [[0.436]
 [1.561]
 [0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.436]]
maxi score, test score, baseline:  -0.9782218390804599 -0.7356666666666667 -0.7356666666666667
probs:  [0.07869121495761465, 0.1252747432257932, 0.07538486266419958, 0.07576893389020234, 0.1178102422173925, 0.5270700030447978]
maxi score, test score, baseline:  -0.9782218390804599 -0.7356666666666667 -0.7356666666666667
probs:  [0.07869121495761465, 0.1252747432257932, 0.07538486266419958, 0.07576893389020234, 0.1178102422173925, 0.5270700030447978]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9782218390804599 -0.7356666666666667 -0.7356666666666667
probs:  [0.07869121495761465, 0.1252747432257932, 0.07538486266419958, 0.07576893389020234, 0.1178102422173925, 0.5270700030447978]
maxi score, test score, baseline:  -0.9782218390804599 -0.7356666666666667 -0.7356666666666667
probs:  [0.0786654817383474, 0.1255054018246762, 0.0753409315602407, 0.07572711668193996, 0.11799981683109279, 0.526761251363703]
printing an ep nov before normalisation:  46.72297290296336
printing an ep nov before normalisation:  46.99474651414491
maxi score, test score, baseline:  -0.9782963344788088 -0.7356666666666667 -0.7356666666666667
probs:  [0.0786654817383474, 0.1255054018246762, 0.0753409315602407, 0.07572711668193996, 0.11799981683109279, 0.526761251363703]
maxi score, test score, baseline:  -0.9782963344788088 -0.7356666666666667 -0.7356666666666667
probs:  [0.0786887739743074, 0.12513481106405838, 0.07532463962472044, 0.07571542290775327, 0.11849144823534886, 0.5266449041938116]
printing an ep nov before normalisation:  42.64500654380346
printing an ep nov before normalisation:  53.858018311699766
maxi score, test score, baseline:  -0.9783703196347033 -0.7356666666666667 -0.7356666666666667
probs:  [0.0786887739743074, 0.12513481106405838, 0.07532463962472044, 0.07571542290775327, 0.11849144823534886, 0.5266449041938116]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9783703196347033 -0.7356666666666667 -0.7356666666666667
probs:  [0.0786887739743074, 0.12513481106405838, 0.07532463962472044, 0.07571542290775327, 0.11849144823534886, 0.5266449041938116]
maxi score, test score, baseline:  -0.9783703196347033 -0.7356666666666667 -0.7356666666666667
probs:  [0.0787195087477513, 0.12518371779960802, 0.07535405818421478, 0.07535405818421478, 0.11853775576232994, 0.5268509013218811]
maxi score, test score, baseline:  -0.9783703196347033 -0.7356666666666667 -0.7356666666666667
maxi score, test score, baseline:  -0.9784437997724689 -0.7356666666666667 -0.7356666666666667
probs:  [0.07830255107208535, 0.12524035472001208, 0.0753881266329419, 0.0753881266329419, 0.11859138264399077, 0.5270894582980281]
printing an ep nov before normalisation:  59.235638580706656
siam score:  -0.83022153
printing an ep nov before normalisation:  26.518005414445565
printing an ep nov before normalisation:  60.400366920073864
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9784437997724689 -0.7356666666666667 -0.7356666666666667
probs:  [0.078192782745786, 0.1261479995254741, 0.07521518590688236, 0.07521518590688236, 0.11935490584645661, 0.5258739400685185]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actor:  1 policy actor:  1  step number:  66 total reward:  0.12666666666666593  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9784437997724689 -0.7356666666666667 -0.7356666666666667
probs:  [0.07845411166121723, 0.12398714091894554, 0.07562691134625703, 0.07562691134625703, 0.11753716208435495, 0.5287677626429682]
maxi score, test score, baseline:  -0.9784437997724689 -0.7356666666666667 -0.7356666666666667
printing an ep nov before normalisation:  76.54308131292126
Printing some Q and Qe and total Qs values:  [[0.086]
 [0.15 ]
 [0.086]
 [0.086]
 [0.086]
 [0.088]
 [0.086]] [[71.628]
 [60.061]
 [71.628]
 [71.628]
 [71.628]
 [72.282]
 [71.628]] [[1.393]
 [1.111]
 [1.393]
 [1.393]
 [1.393]
 [1.414]
 [1.393]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.29514722437412
printing an ep nov before normalisation:  34.88649555638646
siam score:  -0.8395824
maxi score, test score, baseline:  -0.9784437997724689 -0.7356666666666667 -0.7356666666666667
probs:  [0.07897059949891996, 0.1245585187803554, 0.0757676478697646, 0.07540260558850058, 0.11810076450237982, 0.5271998637600795]
Printing some Q and Qe and total Qs values:  [[0.116]
 [0.287]
 [0.116]
 [0.116]
 [0.116]
 [0.116]
 [0.116]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.116]
 [0.287]
 [0.116]
 [0.116]
 [0.116]
 [0.116]
 [0.116]]
maxi score, test score, baseline:  -0.9785167800453516 -0.7356666666666667 -0.7356666666666667
probs:  [0.07894761759861466, 0.12477386358264426, 0.0757279214258965, 0.07536097075915289, 0.11828234914830504, 0.5269072774853866]
actions average: 
K:  4  action  0 :  tensor([0.3294, 0.0154, 0.1235, 0.1341, 0.1225, 0.1407, 0.1343],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0522, 0.7780, 0.0316, 0.0478, 0.0184, 0.0271, 0.0449],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1757, 0.0290, 0.1600, 0.1510, 0.1389, 0.1899, 0.1555],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1926, 0.0407, 0.1217, 0.1634, 0.1551, 0.1628, 0.1637],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2039, 0.1404, 0.1242, 0.1243, 0.1252, 0.1425, 0.1396],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1820, 0.0140, 0.1405, 0.1642, 0.1431, 0.1837, 0.1724],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1817, 0.1291, 0.1160, 0.1278, 0.1213, 0.1413, 0.1828],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9785167800453516 -0.7356666666666667 -0.7356666666666667
probs:  [0.07894761759861466, 0.12477386358264426, 0.0757279214258965, 0.07536097075915289, 0.11828234914830504, 0.5269072774853866]
maxi score, test score, baseline:  -0.9785892655367233 -0.7356666666666667 -0.7356666666666667
probs:  [0.07894761759861466, 0.12477386358264426, 0.0757279214258965, 0.07536097075915289, 0.11828234914830504, 0.5269072774853866]
printing an ep nov before normalisation:  60.62730832976553
maxi score, test score, baseline:  -0.9785892655367233 -0.7356666666666667 -0.7356666666666667
probs:  [0.07894761759861466, 0.12477386358264426, 0.0757279214258965, 0.07536097075915289, 0.11828234914830504, 0.5269072774853866]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.184]
 [0.179]
 [0.114]
 [0.179]
 [0.179]
 [0.179]] [[53.968]
 [53.096]
 [52.711]
 [53.673]
 [52.711]
 [52.711]
 [52.711]] [[1.973]
 [1.915]
 [1.885]
 [1.882]
 [1.885]
 [1.885]
 [1.885]]
maxi score, test score, baseline:  -0.9785892655367233 -0.7356666666666667 -0.7356666666666667
probs:  [0.07892464117235311, 0.12498915709209582, 0.07568820444444402, 0.07531934584677792, 0.11846389054270248, 0.5266147609016266]
actions average: 
K:  3  action  0 :  tensor([0.2495, 0.0147, 0.1149, 0.1534, 0.1399, 0.1597, 0.1679],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0456, 0.6525, 0.0550, 0.0575, 0.0349, 0.0535, 0.1010],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1578, 0.0675, 0.1905, 0.1350, 0.1269, 0.1689, 0.1533],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1581, 0.0540, 0.1143, 0.2525, 0.1200, 0.1501, 0.1510],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2287, 0.0329, 0.1246, 0.1394, 0.1753, 0.1569, 0.1421],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1503, 0.0601, 0.1297, 0.1610, 0.1313, 0.2140, 0.1536],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1617, 0.1143, 0.1206, 0.1305, 0.1105, 0.1461, 0.2163],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  64.83988978594758
printing an ep nov before normalisation:  53.22415804218521
printing an ep nov before normalisation:  48.16794395446777
printing an ep nov before normalisation:  61.71579031732016
printing an ep nov before normalisation:  46.318421363830566
maxi score, test score, baseline:  -0.9785892655367233 -0.7356666666666667 -0.7356666666666667
probs:  [0.07915314125919244, 0.12535509883740759, 0.07590704804792424, 0.07517431341528359, 0.11881036299720295, 0.5256000354429893]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.217]
 [0.217]
 [0.217]
 [0.182]
 [0.217]
 [0.217]
 [0.217]] [[43.611]
 [43.611]
 [43.611]
 [38.798]
 [43.611]
 [43.611]
 [43.611]] [[2.217]
 [2.217]
 [2.217]
 [1.849]
 [2.217]
 [2.217]
 [2.217]]
maxi score, test score, baseline:  -0.9785892655367233 -0.7356666666666667 -0.7356666666666667
probs:  [0.07915314125919244, 0.12535509883740759, 0.07590704804792424, 0.07517431341528359, 0.11881036299720295, 0.5256000354429893]
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.565]
 [0.559]
 [0.556]
 [0.551]
 [0.545]
 [0.537]] [[37.444]
 [34.591]
 [35.148]
 [35.767]
 [35.501]
 [35.55 ]
 [35.532]] [[0.485]
 [0.565]
 [0.559]
 [0.556]
 [0.551]
 [0.545]
 [0.537]]
printing an ep nov before normalisation:  32.34053755857106
printing an ep nov before normalisation:  0.11913051262311569
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.56 ]
 [0.56 ]
 [0.627]
 [0.56 ]
 [0.56 ]
 [0.56 ]] [[25.197]
 [25.197]
 [25.197]
 [22.987]
 [25.197]
 [25.197]
 [25.197]] [[0.56 ]
 [0.56 ]
 [0.56 ]
 [0.627]
 [0.56 ]
 [0.56 ]
 [0.56 ]]
actions average: 
K:  3  action  0 :  tensor([0.2463, 0.0068, 0.1377, 0.1527, 0.1476, 0.1578, 0.1511],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0294, 0.8183, 0.0211, 0.0305, 0.0173, 0.0177, 0.0657],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1301, 0.0814, 0.1905, 0.1472, 0.1319, 0.1560, 0.1630],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1274, 0.0127, 0.1297, 0.3136, 0.1173, 0.1392, 0.1602],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1891, 0.0166, 0.1257, 0.1475, 0.2606, 0.1365, 0.1239],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1467, 0.0135, 0.1507, 0.1382, 0.1200, 0.3044, 0.1266],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1403, 0.0296, 0.1456, 0.1736, 0.1280, 0.1807, 0.2022],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9785892655367233 -0.7356666666666667 -0.7356666666666667
actor:  1 policy actor:  1  step number:  46 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9786612612612614 -0.7356666666666667 -0.7356666666666667
probs:  [0.07953872800670306, 0.12180690957810655, 0.07656901710172011, 0.0758986697372458, 0.11549725848682664, 0.5306894170893978]
maxi score, test score, baseline:  -0.9786612612612614 -0.7356666666666667 -0.7356666666666667
maxi score, test score, baseline:  -0.9786612612612614 -0.7356666666666667 -0.7356666666666667
probs:  [0.07953872800670306, 0.12180690957810655, 0.07656901710172011, 0.0758986697372458, 0.11549725848682664, 0.5306894170893978]
maxi score, test score, baseline:  -0.9786612612612614 -0.7356666666666667 -0.7356666666666667
probs:  [0.07953872800670306, 0.12180690957810655, 0.07656901710172011, 0.0758986697372458, 0.11549725848682664, 0.5306894170893978]
maxi score, test score, baseline:  -0.9786612612612614 -0.7356666666666667 -0.7356666666666667
probs:  [0.07951913617893448, 0.12200365189001505, 0.07653422590104657, 0.07586044761016408, 0.1156617071705319, 0.530420831249308]
siam score:  -0.8330975
printing an ep nov before normalisation:  59.02984427005175
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.790422439575195
maxi score, test score, baseline:  -0.9786612612612614 -0.7356666666666667 -0.7356666666666667
probs:  [0.07951913617893448, 0.12200365189001505, 0.07653422590104657, 0.07586044761016408, 0.1156617071705319, 0.530420831249308]
printing an ep nov before normalisation:  33.267714764562015
printing an ep nov before normalisation:  28.23295042083338
Printing some Q and Qe and total Qs values:  [[-0.049]
 [-0.06 ]
 [-0.051]
 [-0.046]
 [-0.051]
 [-0.045]
 [-0.035]] [[20.648]
 [20.483]
 [19.072]
 [20.93 ]
 [17.863]
 [20.581]
 [21.244]] [[1.292]
 [1.27 ]
 [1.187]
 [1.314]
 [1.109]
 [1.292]
 [1.345]]
Printing some Q and Qe and total Qs values:  [[-0.136]
 [-0.108]
 [-0.107]
 [-0.108]
 [-0.108]
 [-0.106]
 [-0.108]] [[23.488]
 [20.413]
 [21.843]
 [20.413]
 [20.413]
 [21.875]
 [20.413]] [[1.424]
 [1.143]
 [1.287]
 [1.143]
 [1.143]
 [1.292]
 [1.143]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9786612612612614 -0.7356666666666667 -0.7356666666666667
maxi score, test score, baseline:  -0.9786612612612614 -0.7356666666666667 -0.7356666666666667
probs:  [0.07951913617893448, 0.12200365189001505, 0.07653422590104657, 0.07586044761016408, 0.1156617071705319, 0.530420831249308]
maxi score, test score, baseline:  -0.9786612612612614 -0.7356666666666667 -0.7356666666666667
probs:  [0.0794799662569709, 0.12239699860046076, 0.07646466788780248, 0.07578403014913097, 0.11599048926191377, 0.5298838478437211]
maxi score, test score, baseline:  -0.9786612612612614 -0.7356666666666667 -0.7356666666666667
probs:  [0.0794799662569709, 0.12239699860046076, 0.07646466788780248, 0.07578403014913097, 0.11599048926191377, 0.5298838478437211]
maxi score, test score, baseline:  -0.9786612612612614 -0.7356666666666667 -0.7356666666666667
maxi score, test score, baseline:  -0.9786612612612614 -0.7356666666666667 -0.7356666666666667
maxi score, test score, baseline:  -0.9786612612612614 -0.7356666666666667 -0.7356666666666667
probs:  [0.0794799662569709, 0.12239699860046076, 0.07646466788780248, 0.07578403014913097, 0.11599048926191377, 0.5298838478437211]
Printing some Q and Qe and total Qs values:  [[0.058]
 [0.106]
 [0.122]
 [0.198]
 [0.203]
 [0.183]
 [0.193]] [[30.345]
 [31.585]
 [28.033]
 [21.148]
 [22.006]
 [26.109]
 [35.331]] [[1.43 ]
 [1.559]
 [1.342]
 [0.967]
 [1.028]
 [1.277]
 [1.892]]
from probs:  [0.0794799662569709, 0.12239699860046076, 0.07646466788780248, 0.07578403014913097, 0.11599048926191377, 0.5298838478437211]
maxi score, test score, baseline:  -0.9786612612612614 -0.7356666666666667 -0.7356666666666667
probs:  [0.0797074680513089, 0.12253957514275726, 0.07669813642681036, 0.07568899757676818, 0.11614574315841146, 0.5292200796439437]
UNIT TEST: sample policy line 217 mcts : [0.02  0.143 0.02  0.224 0.041 0.02  0.531]
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.114]
 [0.114]
 [0.225]
 [0.114]
 [0.114]
 [0.114]] [[45.419]
 [45.419]
 [45.419]
 [50.54 ]
 [45.419]
 [45.419]
 [45.419]] [[1.543]
 [1.543]
 [1.543]
 [1.964]
 [1.543]
 [1.543]
 [1.543]]
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.731]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]] [[34.461]
 [35.621]
 [34.461]
 [34.461]
 [34.461]
 [34.461]
 [34.461]] [[0.598]
 [0.731]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]]
printing an ep nov before normalisation:  45.959638976733196
Printing some Q and Qe and total Qs values:  [[0.162]
 [0.722]
 [0.162]
 [0.291]
 [0.256]
 [0.162]
 [0.162]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.162]
 [0.722]
 [0.162]
 [0.291]
 [0.256]
 [0.162]
 [0.162]]
printing an ep nov before normalisation:  69.58487260603609
printing an ep nov before normalisation:  68.614352884232
Printing some Q and Qe and total Qs values:  [[0.133]
 [0.131]
 [0.133]
 [0.153]
 [0.147]
 [0.111]
 [0.122]] [[46.199]
 [45.85 ]
 [47.951]
 [46.605]
 [49.998]
 [47.212]
 [45.329]] [[1.283]
 [1.264]
 [1.368]
 [1.322]
 [1.481]
 [1.31 ]
 [1.23 ]]
printing an ep nov before normalisation:  37.66643425027968
maxi score, test score, baseline:  -0.9787327721661057 -0.7356666666666667 -0.7356666666666667
probs:  [0.07987804458886355, 0.12327046445915006, 0.07682934607164202, 0.0754792081568725, 0.11679299087951865, 0.5277499458439533]
maxi score, test score, baseline:  -0.9787327721661057 -0.7356666666666667 -0.7356666666666667
probs:  [0.07987804458886355, 0.12327046445915006, 0.07682934607164202, 0.0754792081568725, 0.11679299087951865, 0.5277499458439533]
maxi score, test score, baseline:  -0.9787327721661057 -0.7356666666666667 -0.7356666666666667
probs:  [0.07990421817563766, 0.12331088016594678, 0.07685451902442696, 0.07550393797174793, 0.11650332616136491, 0.5279231185008758]
maxi score, test score, baseline:  -0.9787327721661057 -0.7356666666666667 -0.7356666666666667
probs:  [0.07991319828949127, 0.12354858378851916, 0.0768474293034115, 0.07548973160957616, 0.11637833050497011, 0.5278227265040318]
maxi score, test score, baseline:  -0.9787327721661057 -0.7356666666666667 -0.7356666666666667
probs:  [0.07991319828949127, 0.12354858378851916, 0.0768474293034115, 0.07548973160957616, 0.11637833050497011, 0.5278227265040318]
printing an ep nov before normalisation:  22.127549724218774
printing an ep nov before normalisation:  2.537060604967678
from probs:  [0.07991319828949127, 0.12354858378851916, 0.0768474293034115, 0.07548973160957616, 0.11637833050497011, 0.5278227265040318]
maxi score, test score, baseline:  -0.9788743589743591 -0.7356666666666667 -0.7356666666666667
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]] [[59.186]
 [59.186]
 [59.186]
 [59.186]
 [59.186]
 [59.186]
 [59.186]] [[0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]
 [0.253]]
maxi score, test score, baseline:  -0.9788743589743591 -0.7356666666666667 -0.7356666666666667
probs:  [0.0795845955850408, 0.12303316375055391, 0.07691410296079247, 0.07554797340063278, 0.11669050746999211, 0.5282296568329878]
Printing some Q and Qe and total Qs values:  [[0.139]
 [0.115]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.139]
 [0.115]
 [0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]]
printing an ep nov before normalisation:  47.89754435185982
printing an ep nov before normalisation:  37.83129146499465
printing an ep nov before normalisation:  49.19053077697754
maxi score, test score, baseline:  -0.9788743589743591 -0.7356666666666667 -0.7356666666666667
probs:  [0.0795845955850408, 0.12303316375055391, 0.07691410296079247, 0.07554797340063278, 0.11669050746999211, 0.5282296568329878]
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.206055339191366
Printing some Q and Qe and total Qs values:  [[-0.084]
 [-0.075]
 [-0.084]
 [-0.086]
 [-0.086]
 [-0.085]
 [-0.085]] [[15.847]
 [28.825]
 [15.801]
 [15.774]
 [15.657]
 [15.614]
 [15.565]] [[0.218]
 [0.703]
 [0.216]
 [0.214]
 [0.21 ]
 [0.209]
 [0.207]]
printing an ep nov before normalisation:  13.89085784752961
printing an ep nov before normalisation:  37.36902615056681
Printing some Q and Qe and total Qs values:  [[-0.111]
 [-0.106]
 [-0.112]
 [-0.113]
 [-0.113]
 [-0.113]
 [-0.113]] [[13.966]
 [21.803]
 [14.055]
 [14.071]
 [14.024]
 [14.05 ]
 [14.018]] [[0.365]
 [0.894]
 [0.37 ]
 [0.37 ]
 [0.367]
 [0.369]
 [0.367]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9789444444444446 -0.7356666666666667 -0.7356666666666667
probs:  [0.07959416860164947, 0.12327097305251915, 0.07655622351636243, 0.07553634195201608, 0.11689499865884362, 0.5281472942186093]
maxi score, test score, baseline:  -0.9789444444444446 -0.7356666666666667 -0.7356666666666667
probs:  [0.07959416860164947, 0.12327097305251915, 0.07655622351636243, 0.07553634195201608, 0.11689499865884362, 0.5281472942186093]
actions average: 
K:  1  action  0 :  tensor([0.3291, 0.0058, 0.1221, 0.1347, 0.1343, 0.1407, 0.1333],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0171, 0.8336, 0.0178, 0.0625, 0.0230, 0.0145, 0.0314],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1413, 0.0333, 0.1874, 0.1740, 0.1463, 0.1751, 0.1427],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1709, 0.0916, 0.1161, 0.2353, 0.1273, 0.1258, 0.1330],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1636, 0.0117, 0.1351, 0.1462, 0.2255, 0.1618, 0.1562],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1708, 0.0073, 0.1371, 0.1393, 0.1528, 0.2518, 0.1408],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2254, 0.0098, 0.1280, 0.1532, 0.1416, 0.1560, 0.1860],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9789444444444446 -0.7356666666666667 -0.7356666666666667
printing an ep nov before normalisation:  36.431164060279535
maxi score, test score, baseline:  -0.9789444444444446 -0.7356666666666667 -0.7356666666666667
probs:  [0.0796202401562104, 0.12331137519722642, 0.07658129830510106, 0.07556108211222864, 0.1166054658072653, 0.5283205384219681]
maxi score, test score, baseline:  -0.9789444444444446 -0.7356666666666667 -0.7356666666666667
probs:  [0.0796202401562104, 0.12331137519722642, 0.07658129830510106, 0.07556108211222864, 0.1166054658072653, 0.5283205384219681]
maxi score, test score, baseline:  -0.9789444444444446 -0.7356666666666667 -0.7356666666666667
probs:  [0.0796202401562104, 0.12331137519722642, 0.07658129830510106, 0.07556108211222864, 0.1166054658072653, 0.5283205384219681]
printing an ep nov before normalisation:  29.743385314941406
actor:  1 policy actor:  1  step number:  62 total reward:  0.2866666666666665  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.9789444444444446 -0.7356666666666667 -0.7356666666666667
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.031]
 [0.012]
 [0.012]
 [0.018]
 [0.015]
 [0.013]] [[47.831]
 [46.05 ]
 [45.959]
 [47.085]
 [46.497]
 [46.533]
 [44.386]] [[1.792]
 [1.665]
 [1.64 ]
 [1.721]
 [1.685]
 [1.685]
 [1.527]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  43.92624040911092
printing an ep nov before normalisation:  53.341967802975034
printing an ep nov before normalisation:  65.5262657859817
printing an ep nov before normalisation:  82.42722373486325
Printing some Q and Qe and total Qs values:  [[ 0.089]
 [-0.008]
 [ 0.089]
 [ 0.089]
 [ 0.089]
 [ 0.089]
 [ 0.089]] [[68.845]
 [70.119]
 [68.845]
 [68.845]
 [68.845]
 [68.845]
 [68.845]] [[1.731]
 [1.688]
 [1.731]
 [1.731]
 [1.731]
 [1.731]
 [1.731]]
maxi score, test score, baseline:  -0.9789444444444446 -0.7356666666666667 -0.7356666666666667
probs:  [0.0776710207970097, 0.1175892584447788, 0.07489450325182685, 0.07396238664737262, 0.138746177629771, 0.5171366532292411]
maxi score, test score, baseline:  -0.9789444444444446 -0.7356666666666667 -0.7356666666666667
probs:  [0.0776710207970097, 0.1175892584447788, 0.07489450325182685, 0.07396238664737262, 0.138746177629771, 0.5171366532292411]
maxi score, test score, baseline:  -0.9789444444444446 -0.7356666666666667 -0.7356666666666667
probs:  [0.07764423785289684, 0.11775018563122386, 0.07485466410830205, 0.07391816435118809, 0.13900659237619956, 0.5168261556801896]
using explorer policy with actor:  1
using another actor
printing an ep nov before normalisation:  53.237123964148296
printing an ep nov before normalisation:  17.337863785879954
Printing some Q and Qe and total Qs values:  [[0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]
 [0.056]]
printing an ep nov before normalisation:  49.983994102803436
maxi score, test score, baseline:  -0.9790832229580575 -0.7356666666666667 -0.7356666666666667
maxi score, test score, baseline:  -0.9790832229580575 -0.7356666666666667 -0.7356666666666667
probs:  [0.07759071162255572, 0.11807180171695018, 0.0747750448116364, 0.0738297852393992, 0.13952703626925403, 0.5162056203402045]
maxi score, test score, baseline:  -0.9790832229580575 -0.7356666666666667 -0.7356666666666667
probs:  [0.07759071162255572, 0.11807180171695018, 0.0747750448116364, 0.0738297852393992, 0.13952703626925403, 0.5162056203402045]
maxi score, test score, baseline:  -0.9790832229580575 -0.7356666666666667 -0.7356666666666667
probs:  [0.07759071162255572, 0.11807180171695018, 0.0747750448116364, 0.0738297852393992, 0.13952703626925403, 0.5162056203402045]
printing an ep nov before normalisation:  0.0010615006209491185
maxi score, test score, baseline:  -0.9790832229580575 -0.7356666666666667 -0.7356666666666667
probs:  [0.07758898265925081, 0.11827064322180089, 0.07443656897412938, 0.07380942212460258, 0.13983218139459788, 0.5160622016256186]
printing an ep nov before normalisation:  57.0311565675122
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.9790832229580575 -0.7356666666666667 -0.7356666666666667
probs:  [0.07758898265925081, 0.11827064322180089, 0.07443656897412938, 0.07380942212460258, 0.13983218139459788, 0.5160622016256186]
printing an ep nov before normalisation:  47.309369908529405
printing an ep nov before normalisation:  0.026840279217879015
line 256 mcts: sample exp_bonus 19.430427610874176
printing an ep nov before normalisation:  59.32442620637388
siam score:  -0.8364255
actions average: 
K:  0  action  0 :  tensor([0.2884, 0.0080, 0.1277, 0.1495, 0.1433, 0.1361, 0.1469],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0124, 0.9136, 0.0093, 0.0251, 0.0058, 0.0076, 0.0262],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1317, 0.0229, 0.2985, 0.1256, 0.1122, 0.1887, 0.1205],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1666, 0.0086, 0.1134, 0.3264, 0.1267, 0.1483, 0.1100],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1951, 0.0066, 0.1577, 0.1715, 0.1542, 0.1605, 0.1544],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1632, 0.0076, 0.1522, 0.1693, 0.1391, 0.2407, 0.1279],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1941, 0.0050, 0.1425, 0.1762, 0.1583, 0.1737, 0.1502],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9790832229580575 -0.7356666666666667 -0.7356666666666667
probs:  [0.07756861108900757, 0.11864253330648211, 0.07438580112723389, 0.07375260718774876, 0.13998774677582576, 0.5156627005137019]
printing an ep nov before normalisation:  55.35161446262939
maxi score, test score, baseline:  -0.9790832229580575 -0.7356666666666667 -0.7356666666666667
probs:  [0.07754215228194432, 0.11880351138526553, 0.07434481787362965, 0.07370873441462102, 0.14024613167929087, 0.5153546523652486]
from probs:  [0.07754215228194432, 0.11880351138526553, 0.07434481787362965, 0.07370873441462102, 0.14024613167929087, 0.5153546523652486]
printing an ep nov before normalisation:  80.50232948164518
printing an ep nov before normalisation:  36.744608879089355
printing an ep nov before normalisation:  61.07979549448824
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.487]
 [0.483]
 [0.489]
 [0.483]
 [0.488]
 [0.489]] [[51.808]
 [55.211]
 [47.763]
 [51.489]
 [47.763]
 [51.215]
 [50.985]] [[0.49 ]
 [0.487]
 [0.483]
 [0.489]
 [0.483]
 [0.488]
 [0.489]]
printing an ep nov before normalisation:  36.20535267723931
maxi score, test score, baseline:  -0.9792201754385966 -0.7356666666666667 -0.7356666666666667
probs:  [0.07751570633707536, 0.11896441120914802, 0.07430385454286506, 0.07366488296899042, 0.14050439097630582, 0.5150467539656153]
maxi score, test score, baseline:  -0.9792201754385966 -0.7356666666666667 -0.7356666666666667
probs:  [0.07751570633707536, 0.11896441120914802, 0.07430385454286506, 0.07366488296899042, 0.14050439097630582, 0.5150467539656153]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9792201754385966 -0.7356666666666667 -0.7356666666666667
probs:  [0.07751570633707536, 0.11896441120914802, 0.07430385454286506, 0.07366488296899042, 0.14050439097630582, 0.5150467539656153]
printing an ep nov before normalisation:  2.9877261791602905
maxi score, test score, baseline:  -0.9792201754385966 -0.7356666666666667 -0.7356666666666667
probs:  [0.07746285299642298, 0.11928597632034754, 0.07422198759177352, 0.07357724399804433, 0.14102053311716722, 0.5144314059762443]
printing an ep nov before normalisation:  57.79501669342956
maxi score, test score, baseline:  -0.9792201754385966 -0.7356666666666667 -0.7356666666666667
probs:  [0.07746285299642298, 0.11928597632034754, 0.07422198759177352, 0.07357724399804433, 0.14102053311716722, 0.5144314059762443]
printing an ep nov before normalisation:  59.753428964950196
printing an ep nov before normalisation:  40.747388345795656
siam score:  -0.8434374
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.049]
 [0.01 ]
 [0.014]
 [0.003]
 [0.   ]
 [0.011]] [[38.986]
 [39.559]
 [38.21 ]
 [38.158]
 [36.671]
 [36.464]
 [39.801]] [[1.113]
 [1.185]
 [1.073]
 [1.074]
 [0.984]
 [0.97 ]
 [1.16 ]]
maxi score, test score, baseline:  -0.9792879781420766 -0.7356666666666667 -0.7356666666666667
printing an ep nov before normalisation:  57.48176574707031
printing an ep nov before normalisation:  57.81919155012685
Printing some Q and Qe and total Qs values:  [[-0.088]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]] [[32.365]
 [24.279]
 [24.279]
 [24.279]
 [24.279]
 [24.279]
 [24.279]] [[0.886]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.9792879781420766 -0.7356666666666667 -0.7356666666666667
probs:  [0.07768228382358879, 0.11942695897592923, 0.07444749735195864, 0.07349130260877237, 0.14112074798437274, 0.5138312092553783]
maxi score, test score, baseline:  -0.9792879781420766 -0.7356666666666667 -0.7356666666666667
probs:  [0.07768228382358879, 0.11942695897592923, 0.07444749735195864, 0.07349130260877237, 0.14112074798437274, 0.5138312092553783]
maxi score, test score, baseline:  -0.9792879781420766 -0.7356666666666667 -0.7356666666666667
probs:  [0.07765690581722794, 0.11958795109007829, 0.07440767756248794, 0.07344721386454592, 0.1413785925487341, 0.5135216591169259]
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.247]
 [0.247]
 [0.247]
 [0.247]
 [0.247]
 [0.247]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.247]
 [0.247]
 [0.247]
 [0.247]
 [0.247]
 [0.247]
 [0.247]]
maxi score, test score, baseline:  -0.9794222584147667 -0.7356666666666667 -0.7356666666666667
probs:  [0.07763154056120254, 0.11974886231909282, 0.0743678777791464, 0.07340314727124302, 0.14163630756766776, 0.5132122645016476]
maxi score, test score, baseline:  -0.9794222584147667 -0.7356666666666667 -0.7356666666666667
probs:  [0.07763154056120254, 0.11974886231909282, 0.0743678777791464, 0.07340314727124302, 0.14163630756766776, 0.5132122645016476]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9794222584147667 -0.7356666666666667 -0.7356666666666667
probs:  [0.07763154056120254, 0.11974886231909282, 0.0743678777791464, 0.07340314727124302, 0.14163630756766776, 0.5132122645016476]
Printing some Q and Qe and total Qs values:  [[-0.097]
 [-0.125]
 [-0.125]
 [-0.125]
 [-0.125]
 [-0.125]
 [-0.125]] [[26.995]
 [17.623]
 [17.623]
 [17.623]
 [17.623]
 [17.623]
 [17.623]] [[1.009]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.597]]
siam score:  -0.84476435
printing an ep nov before normalisation:  48.41464309908445
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9794222584147667 -0.7356666666666667 -0.7356666666666667
probs:  [0.0776397912866028, 0.11996164475141728, 0.07436027938376266, 0.07339086391562752, 0.14152198581371972, 0.51312543484887]
maxi score, test score, baseline:  -0.9794222584147667 -0.7356666666666667 -0.7356666666666667
probs:  [0.07761458834527281, 0.12012269305556048, 0.07432064387092548, 0.07334696217096118, 0.1417779174948166, 0.5128171950624634]
printing an ep nov before normalisation:  64.1183853149414
maxi score, test score, baseline:  -0.9794222584147667 -0.7356666666666667 -0.7356666666666667
probs:  [0.07761458834527281, 0.12012269305556048, 0.07432064387092548, 0.07334696217096118, 0.1417779174948166, 0.5128171950624634]
maxi score, test score, baseline:  -0.9794222584147667 -0.7356666666666667 -0.7356666666666667
probs:  [0.07761458834527281, 0.12012269305556048, 0.07432064387092548, 0.07334696217096118, 0.1417779174948166, 0.5128171950624634]
printing an ep nov before normalisation:  36.084042957276445
maxi score, test score, baseline:  -0.9794222584147667 -0.7356666666666667 -0.7356666666666667
maxi score, test score, baseline:  -0.9794222584147667 -0.7356666666666667 -0.7356666666666667
probs:  [0.07761458834527281, 0.12012269305556048, 0.07432064387092548, 0.07334696217096118, 0.1417779174948166, 0.5128171950624634]
printing an ep nov before normalisation:  45.43517667886894
maxi score, test score, baseline:  -0.9794222584147667 -0.7356666666666667 -0.7356666666666667
printing an ep nov before normalisation:  25.566622691384566
maxi score, test score, baseline:  -0.9794222584147667 -0.7356666666666667 -0.7356666666666667
actor:  1 policy actor:  1  step number:  69 total reward:  0.07999999999999952  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
siam score:  -0.8408328
maxi score, test score, baseline:  -0.9794222584147667 -0.7356666666666667 -0.7356666666666667
probs:  [0.11790367801737929, 0.11507007909057639, 0.07074576408180179, 0.07012808716853143, 0.13587635840281587, 0.49027603323889524]
printing an ep nov before normalisation:  25.555017983265238
actions average: 
K:  4  action  0 :  tensor([0.2052, 0.0325, 0.1307, 0.1622, 0.1340, 0.1890, 0.1463],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0352, 0.7415, 0.0348, 0.0673, 0.0370, 0.0312, 0.0530],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1886, 0.0439, 0.1260, 0.1962, 0.1287, 0.1647, 0.1519],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1375, 0.0519, 0.1187, 0.2166, 0.1434, 0.1573, 0.1745],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1680, 0.0762, 0.1239, 0.1767, 0.1326, 0.1553, 0.1674],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1565, 0.0625, 0.1629, 0.1463, 0.1249, 0.1915, 0.1554],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1292, 0.1093, 0.1220, 0.1647, 0.1256, 0.1933, 0.1559],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.054]
 [-0.062]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.062]
 [-0.065]] [[22.219]
 [34.399]
 [22.219]
 [22.219]
 [22.219]
 [18.365]
 [17.837]] [[0.501]
 [0.798]
 [0.501]
 [0.501]
 [0.501]
 [0.396]
 [0.38 ]]
siam score:  -0.83988434
maxi score, test score, baseline:  -0.9794222584147667 -0.7356666666666667 -0.7356666666666667
probs:  [0.1181855442151628, 0.1153452704658274, 0.07091654514910138, 0.0699965266999357, 0.13620056102685626, 0.4893555524431165]
printing an ep nov before normalisation:  69.87716222063422
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.75481986999512
maxi score, test score, baseline:  -0.9794222584147667 -0.7356666666666667 -0.7356666666666667
probs:  [0.1182346746605201, 0.11539321885972445, 0.07094600340990852, 0.07002560207189223, 0.13584134636170533, 0.4895591546362494]
maxi score, test score, baseline:  -0.9794222584147667 -0.7356666666666667 -0.7356666666666667
printing an ep nov before normalisation:  0.0006578135571544408
printing an ep nov before normalisation:  55.3365941050305
printing an ep nov before normalisation:  41.50268229565168
printing an ep nov before normalisation:  14.818609472157103
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [ 0.03 ]
 [-0.003]
 [-0.006]
 [-0.001]
 [-0.01 ]
 [-0.01 ]] [[46.924]
 [47.382]
 [48.465]
 [49.022]
 [49.027]
 [46.924]
 [46.924]] [[1.339]
 [1.393]
 [1.394]
 [1.409]
 [1.414]
 [1.339]
 [1.339]]
maxi score, test score, baseline:  -0.9794887445887447 -0.7356666666666667 -0.7356666666666667
probs:  [0.11820965304721053, 0.11534529752004664, 0.07088821423155622, 0.06995062872283767, 0.13657514113675545, 0.4890310653415934]
maxi score, test score, baseline:  -0.9794887445887447 -0.7356666666666667 -0.7356666666666667
from probs:  [0.11820965304721053, 0.11534529752004664, 0.07088821423155622, 0.06995062872283767, 0.13657514113675545, 0.4890310653415934]
maxi score, test score, baseline:  -0.9795548004314996 -0.7356666666666667 -0.7356666666666667
probs:  [0.11747483174963047, 0.11544137964112683, 0.07094722097649814, 0.07000885356008736, 0.1366889280728893, 0.48943878599976787]
maxi score, test score, baseline:  -0.9795548004314996 -0.7356666666666667 -0.7356666666666667
probs:  [0.11688986774659099, 0.11566674276706046, 0.07095474665100646, 0.07001178511319577, 0.13701831635489561, 0.4894585413672506]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9795548004314996 -0.7356666666666667 -0.7356666666666667
probs:  [0.11631265979554152, 0.11589120502554971, 0.07096161949232108, 0.07001406906407262, 0.13734668532425232, 0.4894737612982628]
printing an ep nov before normalisation:  0.013926282785519106
printing an ep nov before normalisation:  68.58359633980444
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.979620430107527 -0.7356666666666667 -0.7356666666666667
probs:  [0.11631265979554152, 0.11589120502554971, 0.07096161949232108, 0.07001406906407262, 0.13734668532425232, 0.4894737612982628]
maxi score, test score, baseline:  -0.979620430107527 -0.7356666666666667 -0.7356666666666667
probs:  [0.11631265979554152, 0.11589120502554971, 0.07096161949232108, 0.07001406906407262, 0.13734668532425232, 0.4894737612982628]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.979620430107527 -0.7356666666666667 -0.7356666666666667
probs:  [0.11671146583624632, 0.11628490942492581, 0.07081145857859864, 0.06985243820678808, 0.1380001048171875, 0.4883396231362537]
siam score:  -0.8260515
maxi score, test score, baseline:  -0.979620430107527 -0.7356666666666667 -0.7356666666666667
probs:  [0.11671146583624632, 0.11628490942492581, 0.07081145857859864, 0.06985243820678808, 0.1380001048171875, 0.4883396231362537]
maxi score, test score, baseline:  -0.979620430107527 -0.7356666666666667 -0.7356666666666667
maxi score, test score, baseline:  -0.979620430107527 -0.7356666666666667 -0.7356666666666667
probs:  [0.11671146583624632, 0.11628490942492581, 0.07081145857859864, 0.06985243820678808, 0.1380001048171875, 0.4883396231362537]
printing an ep nov before normalisation:  31.982219999640904
printing an ep nov before normalisation:  35.16961769232773
maxi score, test score, baseline:  -0.979620430107527 -0.7356666666666667 -0.7356666666666667
printing an ep nov before normalisation:  41.70211748819379
maxi score, test score, baseline:  -0.979620430107527 -0.7356666666666667 -0.7356666666666667
actor:  1 policy actor:  1  step number:  50 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.979620430107527 -0.7356666666666667 -0.7356666666666667
printing an ep nov before normalisation:  58.74275025297951
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.0637573117093
printing an ep nov before normalisation:  44.51212406158447
maxi score, test score, baseline:  -0.979620430107527 -0.7356666666666667 -0.7356666666666667
line 256 mcts: sample exp_bonus 60.88138298703454
Printing some Q and Qe and total Qs values:  [[-0.052]
 [-0.056]
 [-0.048]
 [-0.048]
 [-0.047]
 [-0.043]
 [-0.041]] [[20.663]
 [21.325]
 [20.558]
 [20.598]
 [20.201]
 [39.423]
 [19.262]] [[0.917]
 [0.944]
 [0.916]
 [0.917]
 [0.9  ]
 [1.806]
 [0.862]]
maxi score, test score, baseline:  -0.979620430107527 -0.7356666666666667 -0.7356666666666667
probs:  [0.11344576285170634, 0.113032573150129, 0.06898409307520034, 0.06717682766969675, 0.16775392143222198, 0.4696068218210455]
maxi score, test score, baseline:  -0.979620430107527 -0.7356666666666667 -0.7356666666666667
maxi score, test score, baseline:  -0.979620430107527 -0.7356666666666667 -0.7356666666666667
probs:  [0.11367348625846997, 0.113257168373645, 0.06887520535630828, 0.06705425747676441, 0.16839280192066894, 0.46874708061414333]
printing an ep nov before normalisation:  72.16843302096795
maxi score, test score, baseline:  -0.9796856377277601 -0.7356666666666667 -0.7356666666666667
probs:  [0.11385067443051106, 0.11343256159670564, 0.06885924634067991, 0.0670304474687599, 0.16824744037217704, 0.46857962979116635]
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.73085297322504
printing an ep nov before normalisation:  46.91850649252857
printing an ep nov before normalisation:  67.00021862718638
printing an ep nov before normalisation:  41.76487195457068
maxi score, test score, baseline:  -0.9796856377277601 -0.7356666666666667 -0.7356666666666667
probs:  [0.11401454071138295, 0.11278949715064977, 0.06832476523399994, 0.0671268460336531, 0.168489692124305, 0.4692546587460091]
printing an ep nov before normalisation:  23.1708452182329
maxi score, test score, baseline:  -0.9796856377277601 -0.7356666666666667 -0.7356666666666667
probs:  [0.1141046356170707, 0.11208812544580987, 0.06837871359492624, 0.06717984668495414, 0.16862288394781214, 0.46962579470942684]
from probs:  [0.1141046356170707, 0.11208812544580987, 0.06837871359492624, 0.06717984668495414, 0.16862288394781214, 0.46962579470942684]
maxi score, test score, baseline:  -0.9796856377277601 -0.7356666666666667 -0.7356666666666667
probs:  [0.1142423543293661, 0.11222930070579232, 0.06859481218958315, 0.06711227492123754, 0.16866715164366544, 0.46915410621035547]
siam score:  -0.8276872
maxi score, test score, baseline:  -0.9796856377277601 -0.7356666666666667 -0.7356666666666667
probs:  [0.11435771156331083, 0.11233713680530619, 0.06853962190464866, 0.06705154560745483, 0.16898584981804976, 0.46872813430122967]
printing an ep nov before normalisation:  51.925171825764686
actor:  1 policy actor:  1  step number:  54 total reward:  0.2866666666666665  reward:  1.0 rdn_beta:  2.0
from probs:  [0.11435771156331083, 0.11233713680530619, 0.06853962190464866, 0.06705154560745483, 0.16898584981804976, 0.46872813430122967]
maxi score, test score, baseline:  -0.9796856377277601 -0.7356666666666667 -0.7356666666666667
probs:  [0.11276664378614741, 0.11084980447872039, 0.06930083540612537, 0.06788915631451464, 0.1645901966061618, 0.47460336340833037]
maxi score, test score, baseline:  -0.9796856377277601 -0.7356666666666667 -0.7356666666666667
probs:  [0.11276664378614741, 0.11084980447872039, 0.06930083540612537, 0.06788915631451464, 0.1645901966061618, 0.47460336340833037]
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.563]
 [0.53 ]
 [0.51 ]
 [0.531]
 [0.493]
 [0.481]] [[34.312]
 [34.604]
 [34.415]
 [34.595]
 [32.783]
 [32.845]
 [31.505]] [[2.346]
 [2.386]
 [2.337]
 [2.332]
 [2.198]
 [2.165]
 [2.038]]
printing an ep nov before normalisation:  67.46273606982574
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.593]
 [0.572]
 [0.583]
 [0.393]
 [0.548]
 [0.557]] [[35.082]
 [33.115]
 [32.495]
 [31.523]
 [36.598]
 [32.45 ]
 [31.673]] [[1.997]
 [2.055]
 [2.006]
 [1.972]
 [2.014]
 [1.98 ]
 [1.953]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8293138
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  12.792502819811702
maxi score, test score, baseline:  -0.9798787685774948 -0.7356666666666667 -0.7356666666666667
probs:  [0.1111220707435167, 0.108622608384409, 0.07017306817133226, 0.06884312969996963, 0.15994487829773868, 0.48129424470303367]
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.22 ]
 [0.137]
 [0.134]
 [0.137]
 [0.148]
 [0.137]] [[51.151]
 [56.191]
 [51.151]
 [52.989]
 [51.151]
 [53.466]
 [51.151]] [[1.482]
 [1.806]
 [1.482]
 [1.567]
 [1.482]
 [1.604]
 [1.482]]
maxi score, test score, baseline:  -0.9798787685774948 -0.7356666666666667 -0.7356666666666667
probs:  [0.1111220707435167, 0.108622608384409, 0.07017306817133226, 0.06884312969996963, 0.15994487829773868, 0.48129424470303367]
printing an ep nov before normalisation:  67.85934185749885
actor:  1 policy actor:  1  step number:  52 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9798787685774948 -0.7356666666666667 -0.7356666666666667
probs:  [0.11014359194569724, 0.1070827106382469, 0.07071861874005186, 0.06943817757408696, 0.15714932489991265, 0.48546757620200437]
maxi score, test score, baseline:  -0.9798787685774948 -0.7356666666666667 -0.7356666666666667
probs:  [0.11024288889032505, 0.1071706569140713, 0.07067171602239546, 0.06938652659534582, 0.15742293324476597, 0.4851052783330965]
printing an ep nov before normalisation:  59.47742763487746
actions average: 
K:  0  action  0 :  tensor([0.3464, 0.0091, 0.1089, 0.1194, 0.1253, 0.1505, 0.1404],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0165, 0.8224, 0.0375, 0.0291, 0.0083, 0.0131, 0.0731],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1541, 0.0361, 0.2495, 0.1258, 0.1113, 0.1797, 0.1434],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1135, 0.0166, 0.1217, 0.3009, 0.1122, 0.1858, 0.1492],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1791, 0.0087, 0.1238, 0.1484, 0.2266, 0.1685, 0.1449],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1766, 0.0096, 0.1393, 0.1554, 0.1426, 0.1940, 0.1826],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1602, 0.0312, 0.1308, 0.1373, 0.1382, 0.1570, 0.2454],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.421]
 [0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]] [[39.649]
 [42.37 ]
 [39.649]
 [39.649]
 [39.649]
 [39.649]
 [39.649]] [[1.909]
 [2.179]
 [1.909]
 [1.909]
 [1.909]
 [1.909]
 [1.909]]
maxi score, test score, baseline:  -0.9799423280423282 -0.7356666666666667 -0.7356666666666667
probs:  [0.11024288889032505, 0.1071706569140713, 0.07067171602239546, 0.06938652659534582, 0.15742293324476597, 0.4851052783330965]
maxi score, test score, baseline:  -0.9799423280423282 -0.7356666666666667 -0.7356666666666667
probs:  [0.11024288889032505, 0.1071706569140713, 0.07067171602239546, 0.06938652659534582, 0.15742293324476597, 0.4851052783330965]
printing an ep nov before normalisation:  46.89133629490472
maxi score, test score, baseline:  -0.9800054852320677 -0.7356666666666667 -0.7356666666666667
probs:  [0.11024288889032505, 0.1071706569140713, 0.07067171602239546, 0.06938652659534582, 0.15742293324476597, 0.4851052783330965]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.9800054852320677 -0.7356666666666667 -0.7356666666666667
maxi score, test score, baseline:  -0.9800054852320677 -0.7356666666666667 -0.7356666666666667
probs:  [0.10963649910092683, 0.10724366002217524, 0.07071982563180873, 0.06943375966257627, 0.15753021008642165, 0.4854360454960912]
Printing some Q and Qe and total Qs values:  [[0.03 ]
 [0.02 ]
 [0.034]
 [0.037]
 [0.034]
 [0.034]
 [0.034]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.03 ]
 [0.02 ]
 [0.034]
 [0.037]
 [0.034]
 [0.034]
 [0.034]]
maxi score, test score, baseline:  -0.9800054852320677 -0.7356666666666667 -0.7356666666666667
probs:  [0.10963649910092683, 0.10724366002217524, 0.07071982563180873, 0.06943375966257627, 0.15753021008642165, 0.4854360454960912]
printing an ep nov before normalisation:  64.30059564413182
maxi score, test score, baseline:  -0.9800054852320677 -0.7356666666666667 -0.7356666666666667
probs:  [0.10963649910092683, 0.10724366002217524, 0.07071982563180873, 0.06943375966257627, 0.15753021008642165, 0.4854360454960912]
printing an ep nov before normalisation:  45.2576255237787
printing an ep nov before normalisation:  53.059193953659836
maxi score, test score, baseline:  -0.9800054852320677 -0.7356666666666667 -0.7356666666666667
probs:  [0.10963649910092683, 0.10724366002217524, 0.07071982563180873, 0.06943375966257627, 0.15753021008642165, 0.4854360454960912]
maxi score, test score, baseline:  -0.9800054852320677 -0.7356666666666667 -0.7356666666666667
probs:  [0.10963649910092683, 0.10724366002217524, 0.07071982563180873, 0.06943375966257627, 0.15753021008642165, 0.4854360454960912]
siam score:  -0.8296406
maxi score, test score, baseline:  -0.9800054852320677 -0.7356666666666667 -0.7356666666666667
printing an ep nov before normalisation:  64.31237884711344
actor:  1 policy actor:  1  step number:  69 total reward:  0.10666666666666602  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9800054852320677 -0.7356666666666667 -0.7356666666666667
probs:  [0.10902481346341569, 0.10545587099072135, 0.0711674749100442, 0.06991641641867961, 0.15561482925173822, 0.48882059496540087]
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.778499603271484
maxi score, test score, baseline:  -0.9800054852320677 -0.7356666666666667 -0.7356666666666667
probs:  [0.10907579433153122, 0.10550518095118455, 0.07120073172764049, 0.06994908751565343, 0.15521981683388042, 0.4890493886401099]
printing an ep nov before normalisation:  58.63404899780319
maxi score, test score, baseline:  -0.9800054852320677 -0.7356666666666667 -0.7356666666666667
probs:  [0.10912639928766639, 0.10555412732025403, 0.07123374332312454, 0.06998151770937824, 0.15482771707533496, 0.4892764952842418]
printing an ep nov before normalisation:  38.89202514970046
siam score:  -0.8378575
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([0.2142, 0.0562, 0.1420, 0.1646, 0.1250, 0.1611, 0.1369],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0428, 0.6700, 0.0458, 0.0877, 0.0480, 0.0402, 0.0655],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1587, 0.0368, 0.2972, 0.1251, 0.0937, 0.1431, 0.1453],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1494, 0.0578, 0.1224, 0.2704, 0.1198, 0.1449, 0.1353],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2181, 0.0134, 0.1213, 0.1598, 0.1455, 0.1895, 0.1525],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1865, 0.0281, 0.1892, 0.1564, 0.1106, 0.1899, 0.1393],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1324, 0.0433, 0.0871, 0.1286, 0.0914, 0.1106, 0.4066],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  25.40553092956543
printing an ep nov before normalisation:  27.629024982452393
printing an ep nov before normalisation:  0.144471498448695
maxi score, test score, baseline:  -0.9800054852320677 -0.7356666666666667 -0.7356666666666667
probs:  [0.10934316949303402, 0.10576279105145124, 0.0713645246005417, 0.06987189103730988, 0.1551481962243608, 0.48850942759330235]
maxi score, test score, baseline:  -0.9800054852320677 -0.7356666666666667 -0.7356666666666667
probs:  [0.10937163761828052, 0.10579032586733647, 0.0711226276928092, 0.0698900700508646, 0.15518860450089, 0.4886367342698192]
maxi score, test score, baseline:  -0.9800054852320677 -0.7356666666666667 -0.7356666666666667
probs:  [0.10937163761828052, 0.10579032586733647, 0.0711226276928092, 0.0698900700508646, 0.15518860450089, 0.4886367342698192]
maxi score, test score, baseline:  -0.9800054852320677 -0.7356666666666667 -0.7356666666666667
probs:  [0.1094672973264155, 0.10587281166876371, 0.07107758786863222, 0.06984049624570073, 0.15545280257968858, 0.4882890043107993]
printing an ep nov before normalisation:  55.2666488356381
maxi score, test score, baseline:  -0.980068243953733 -0.7356666666666667 -0.7356666666666667
probs:  [0.1094672973264155, 0.10587281166876371, 0.07107758786863222, 0.06984049624570073, 0.15545280257968858, 0.4882890043107993]
printing an ep nov before normalisation:  48.589517185006414
printing an ep nov before normalisation:  48.51561402006311
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.24 ]
 [0.125]
 [0.127]
 [0.122]
 [0.096]
 [0.106]] [[63.32 ]
 [61.415]
 [64.26 ]
 [63.942]
 [65.079]
 [67.861]
 [63.46 ]] [[1.853]
 [1.88 ]
 [1.911]
 [1.896]
 [1.95 ]
 [2.067]
 [1.851]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.980068243953733 -0.7356666666666667 -0.7356666666666667
maxi score, test score, baseline:  -0.980068243953733 -0.7356666666666667 -0.7356666666666667
probs:  [0.107838848803309, 0.10616026384464791, 0.07116996345515055, 0.06992593617875349, 0.15601822093803655, 0.4888867667801025]
maxi score, test score, baseline:  -0.980068243953733 -0.7356666666666667 -0.7356666666666667
probs:  [0.107838848803309, 0.10616026384464791, 0.07116996345515055, 0.06992593617875349, 0.15601822093803655, 0.4888867667801025]
maxi score, test score, baseline:  -0.980068243953733 -0.7356666666666667 -0.7356666666666667
probs:  [0.107838848803309, 0.10616026384464791, 0.07116996345515055, 0.06992593617875349, 0.15601822093803655, 0.4888867667801025]
actions average: 
K:  0  action  0 :  tensor([0.2130, 0.0105, 0.1420, 0.1579, 0.1479, 0.1624, 0.1663],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0126, 0.8678, 0.0112, 0.0420, 0.0192, 0.0087, 0.0384],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1347, 0.0041, 0.2099, 0.1557, 0.1114, 0.2487, 0.1355],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1695, 0.0414, 0.1357, 0.2080, 0.1348, 0.1591, 0.1515],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1632, 0.0102, 0.1349, 0.1590, 0.2183, 0.1668, 0.1475],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1619, 0.0024, 0.1462, 0.1890, 0.1387, 0.1995, 0.1624],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1276, 0.0315, 0.1433, 0.2120, 0.1179, 0.1599, 0.2079],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.980068243953733 -0.7356666666666667 -0.7356666666666667
probs:  [0.10806820159549288, 0.1063765668841424, 0.07111424286887974, 0.06986054420336886, 0.15615274938117096, 0.48842769506694517]
line 256 mcts: sample exp_bonus 58.37105324152006
maxi score, test score, baseline:  -0.980068243953733 -0.7356666666666667 -0.7356666666666667
using explorer policy with actor:  1
printing an ep nov before normalisation:  62.815340270882736
line 256 mcts: sample exp_bonus 53.43979163111912
printing an ep nov before normalisation:  46.662759819768155
printing an ep nov before normalisation:  28.29903223202858
maxi score, test score, baseline:  -0.980068243953733 -0.7356666666666667 -0.7356666666666667
probs:  [0.10824700886970114, 0.10654312891005499, 0.07102555125749553, 0.06976277743447923, 0.15667962661567883, 0.48774190691259034]
siam score:  -0.82816863
printing an ep nov before normalisation:  40.541168121532394
Printing some Q and Qe and total Qs values:  [[0.133]
 [0.133]
 [0.133]
 [0.133]
 [0.133]
 [0.133]
 [0.133]] [[51.339]
 [51.339]
 [51.339]
 [51.339]
 [51.339]
 [51.339]
 [51.339]] [[1.519]
 [1.519]
 [1.519]
 [1.519]
 [1.519]
 [1.519]
 [1.519]]
maxi score, test score, baseline:  -0.980068243953733 -0.7356666666666667 -0.7356666666666667
probs:  [0.1076849271191256, 0.10661025038490209, 0.07107026887568377, 0.06980669851738568, 0.15677837330210323, 0.48804948180079943]
printing an ep nov before normalisation:  36.9707055673798
maxi score, test score, baseline:  -0.980068243953733 -0.7356666666666667 -0.7356666666666667
maxi score, test score, baseline:  -0.980068243953733 -0.7356666666666667 -0.7356666666666667
probs:  [0.1076849271191256, 0.10661025038490209, 0.07107026887568377, 0.06980669851738568, 0.15677837330210323, 0.48804948180079943]
actor:  1 policy actor:  1  step number:  62 total reward:  0.20666666666666633  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.361]
 [0.624]
 [0.361]
 [0.371]
 [0.361]
 [0.381]
 [0.361]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.361]
 [0.624]
 [0.361]
 [0.371]
 [0.361]
 [0.381]
 [0.361]]
siam score:  -0.828659
printing an ep nov before normalisation:  37.81312258852545
from probs:  [0.10573279234997508, 0.10467176929477888, 0.06958332031130571, 0.06833580350990641, 0.1739284749345152, 0.47774783959951883]
maxi score, test score, baseline:  -0.980068243953733 -0.7356666666666667 -0.7356666666666667
probs:  [0.10585835450656028, 0.104798833156915, 0.06976004616556743, 0.0682783740081166, 0.17395751719496283, 0.47734687496787775]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.229904725278715
maxi score, test score, baseline:  -0.980068243953733 -0.7356666666666667 -0.7356666666666667
probs:  [0.10585835450656028, 0.104798833156915, 0.06976004616556743, 0.0682783740081166, 0.17395751719496283, 0.47734687496787775]
maxi score, test score, baseline:  -0.980068243953733 -0.7356666666666667 -0.7356666666666667
printing an ep nov before normalisation:  56.50387910518706
maxi score, test score, baseline:  -0.9801306079664571 -0.7356666666666667 -0.7356666666666667
maxi score, test score, baseline:  -0.9801306079664571 -0.7356666666666667 -0.7356666666666667
probs:  [0.10543681126071185, 0.10498719447836717, 0.06997573272721558, 0.06826361477084666, 0.17409194463869695, 0.4772447021241618]
printing an ep nov before normalisation:  44.35291926624634
maxi score, test score, baseline:  -0.9802541666666668 -0.7356666666666667 -0.7356666666666667
probs:  [0.10543681126071185, 0.10498719447836717, 0.06997573272721558, 0.06826361477084666, 0.17409194463869695, 0.4772447021241618]
maxi score, test score, baseline:  -0.9802541666666668 -0.7356666666666667 -0.7356666666666667
maxi score, test score, baseline:  -0.9802541666666668 -0.7356666666666667 -0.7356666666666667
maxi score, test score, baseline:  -0.9802541666666668 -0.7356666666666667 -0.7356666666666667
probs:  [0.10551363323884266, 0.10506245142688911, 0.06992912150612703, 0.06821104399619869, 0.1744077419414974, 0.47687600789044515]
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [ 0.091]
 [-0.011]
 [ 0.003]
 [-0.005]
 [-0.006]
 [-0.011]] [[40.04 ]
 [30.434]
 [41.432]
 [40.997]
 [41.175]
 [42.586]
 [41.618]] [[1.618]
 [1.182]
 [1.695]
 [1.685]
 [1.686]
 [1.765]
 [1.705]]
actor:  1 policy actor:  1  step number:  78 total reward:  0.00666666666666571  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  69.4839164849482
maxi score, test score, baseline:  -0.9802541666666668 -0.7356666666666667 -0.7356666666666667
probs:  [0.10516530941306419, 0.10472165602459249, 0.0699198088908191, 0.06848515148601877, 0.1729098504426052, 0.4787982237429004]
printing an ep nov before normalisation:  66.54849973564862
printing an ep nov before normalisation:  57.11133111446531
maxi score, test score, baseline:  -0.9802541666666668 -0.7356666666666667 -0.7356666666666667
printing an ep nov before normalisation:  33.18581581115723
printing an ep nov before normalisation:  52.415531196556806
printing an ep nov before normalisation:  0.011244497486870841
printing an ep nov before normalisation:  64.69136548733032
Printing some Q and Qe and total Qs values:  [[0.177]
 [0.245]
 [0.177]
 [0.222]
 [0.177]
 [0.177]
 [0.177]] [[54.764]
 [57.662]
 [54.764]
 [60.401]
 [54.764]
 [54.764]
 [54.764]] [[1.576]
 [1.783]
 [1.576]
 [1.891]
 [1.576]
 [1.576]
 [1.576]]
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.182]
 [0.182]
 [0.182]
 [0.182]
 [0.182]
 [0.182]] [[63.069]
 [63.069]
 [63.069]
 [63.069]
 [63.069]
 [63.069]
 [63.069]] [[1.986]
 [1.986]
 [1.986]
 [1.986]
 [1.986]
 [1.986]
 [1.986]]
printing an ep nov before normalisation:  43.89839853218922
printing an ep nov before normalisation:  37.60413136513221
actor:  1 policy actor:  1  step number:  36 total reward:  0.5  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  56.71711032956572
printing an ep nov before normalisation:  59.26776727040609
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9803153686396678 -0.7356666666666667 -0.7356666666666667
probs:  [0.10431983739383102, 0.10221801915410443, 0.07066544562902638, 0.06954231129417203, 0.16704440470468349, 0.48620998182418274]
printing an ep nov before normalisation:  52.57676777712928
maxi score, test score, baseline:  -0.9803153686396678 -0.7356666666666667 -0.7356666666666667
probs:  [0.10431983739383102, 0.10221801915410443, 0.07066544562902638, 0.06954231129417203, 0.16704440470468349, 0.48620998182418274]
printing an ep nov before normalisation:  62.411436497970854
actions average: 
K:  4  action  0 :  tensor([0.2541, 0.0533, 0.1069, 0.1432, 0.1401, 0.1352, 0.1671],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0388, 0.7631, 0.0364, 0.0348, 0.0267, 0.0292, 0.0710],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.2108, 0.0632, 0.1740, 0.1281, 0.1208, 0.1331, 0.1701],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1569, 0.1200, 0.1132, 0.1519, 0.1402, 0.1423, 0.1755],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1513, 0.0579, 0.1253, 0.1931, 0.1442, 0.1464, 0.1818],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1012, 0.1100, 0.1382, 0.1053, 0.0859, 0.3412, 0.1180],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1425, 0.0560, 0.1328, 0.1834, 0.1562, 0.1472, 0.1819],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  39.352402687072754
Printing some Q and Qe and total Qs values:  [[0.027]
 [0.28 ]
 [0.127]
 [0.104]
 [0.078]
 [0.127]
 [0.118]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.027]
 [0.28 ]
 [0.127]
 [0.104]
 [0.078]
 [0.127]
 [0.118]]
printing an ep nov before normalisation:  59.686146089950746
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  59.95015764845059
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.3703],
        [-0.5034],
        [-0.0000],
        [-0.4616],
        [-0.0000],
        [-0.6425],
        [-0.6441],
        [-0.6738],
        [-0.4483],
        [-0.6539]], dtype=torch.float64)
-0.071551887066 -0.4418150239931776
-0.09703970119800001 -0.6004142346563455
-0.8448 -0.8448
-0.083839701198 -0.545450123235837
-0.9446210399999999 -0.9446210399999999
-0.032346567066 -0.6748448085772201
-0.032346567066 -0.6764043399602918
-0.032346567066 -0.706135087556014
-0.09703970119800001 -0.5453654659566832
-0.032346567066 -0.6862769058863811
using explorer policy with actor:  1
printing an ep nov before normalisation:  71.13639833079345
printing an ep nov before normalisation:  38.42010890306125
maxi score, test score, baseline:  -0.9803761904761906 -0.7356666666666667 -0.7356666666666667
probs:  [0.10412832558565815, 0.10257526064424868, 0.07081160360545223, 0.06946662408398704, 0.1673381520250422, 0.4856800340556118]
maxi score, test score, baseline:  -0.9803761904761906 -0.7356666666666667 -0.7356666666666667
probs:  [0.10412832558565815, 0.10257526064424868, 0.07081160360545223, 0.06946662408398704, 0.1673381520250422, 0.4856800340556118]
printing an ep nov before normalisation:  37.46363304124862
printing an ep nov before normalisation:  24.486032421324232
printing an ep nov before normalisation:  4.858604186643447
maxi score, test score, baseline:  -0.9803761904761906 -0.7356666666666667 -0.7356666666666667
probs:  [0.10363323943206987, 0.1026319153790802, 0.07085069170773618, 0.06950496835627028, 0.16743062335096132, 0.4859485617738821]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  64 total reward:  0.09999999999999964  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9803761904761906 -0.7356666666666667 -0.7356666666666667
printing an ep nov before normalisation:  70.09667145175955
maxi score, test score, baseline:  -0.9803761904761906 -0.7356666666666667 -0.7356666666666667
probs:  [0.10339797980707445, 0.10241028256518825, 0.0710615629671394, 0.06973415328531242, 0.1658404341250433, 0.4875555872502422]
maxi score, test score, baseline:  -0.9803761904761906 -0.7356666666666667 -0.7356666666666667
probs:  [0.10339797980707445, 0.10241028256518825, 0.0710615629671394, 0.06973415328531242, 0.1658404341250433, 0.4875555872502422]
Printing some Q and Qe and total Qs values:  [[-0.002]
 [ 0.086]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.002]
 [-0.001]] [[21.167]
 [63.583]
 [23.509]
 [30.622]
 [23.495]
 [23.352]
 [23.566]] [[0.334]
 [1.145]
 [0.374]
 [0.496]
 [0.374]
 [0.371]
 [0.375]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  86.3149546227986
printing an ep nov before normalisation:  82.34555880793968
printing an ep nov before normalisation:  43.35563264822178
printing an ep nov before normalisation:  60.190705193677374
Printing some Q and Qe and total Qs values:  [[-0.049]
 [-0.043]
 [-0.054]
 [-0.048]
 [-0.053]
 [-0.057]
 [-0.046]] [[45.467]
 [37.219]
 [48.716]
 [41.792]
 [43.164]
 [39.891]
 [37.263]] [[0.613]
 [0.403]
 [0.692]
 [0.518]
 [0.548]
 [0.459]
 [0.401]]
printing an ep nov before normalisation:  41.235870350543834
maxi score, test score, baseline:  -0.9803761904761906 -0.7356666666666667 -0.7356666666666667
probs:  [0.10350335442258769, 0.10197834091113715, 0.07113394048160147, 0.06980517626901048, 0.16552621949410665, 0.4880529684215566]
printing an ep nov before normalisation:  44.198044229870725
maxi score, test score, baseline:  -0.9803761904761906 -0.7356666666666667 -0.7356666666666667
probs:  [0.10359566865638749, 0.10206512980296806, 0.07087660974771846, 0.06977539718584673, 0.16584325143488907, 0.48784394317219015]
maxi score, test score, baseline:  -0.9803761904761906 -0.7356666666666667 -0.7356666666666667
printing an ep nov before normalisation:  60.06057787232562
Printing some Q and Qe and total Qs values:  [[-0.009]
 [-0.008]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]] [[22.777]
 [40.454]
 [22.777]
 [22.777]
 [22.777]
 [22.777]
 [22.777]] [[0.103]
 [0.262]
 [0.103]
 [0.103]
 [0.103]
 [0.103]
 [0.103]]
printing an ep nov before normalisation:  48.504486083984375
printing an ep nov before normalisation:  46.52511441655268
maxi score, test score, baseline:  -0.9803761904761906 -0.7356666666666667 -0.7356666666666667
probs:  [0.10359566865638749, 0.10206512980296806, 0.07087660974771846, 0.06977539718584673, 0.16584325143488907, 0.48784394317219015]
printing an ep nov before normalisation:  37.07139177399937
maxi score, test score, baseline:  -0.9803761904761906 -0.7356666666666667 -0.7356666666666667
probs:  [0.10359566865638749, 0.10206512980296806, 0.07087660974771846, 0.06977539718584673, 0.16584325143488907, 0.48784394317219015]
printing an ep nov before normalisation:  39.284845207782176
printing an ep nov before normalisation:  37.22892793291396
maxi score, test score, baseline:  -0.9803761904761906 -0.7356666666666667 -0.7356666666666667
probs:  [0.10316323237938115, 0.10216915183503936, 0.07094880373443462, 0.06984646737947163, 0.16553069020138642, 0.48834165447028666]
UNIT TEST: sample policy line 217 mcts : [0.102 0.102 0.102 0.204 0.163 0.122 0.204]
maxi score, test score, baseline:  -0.9803761904761906 -0.7356666666666667 -0.7356666666666667
probs:  [0.10268929222196502, 0.10222311357019202, 0.07098625457742645, 0.06988333525149135, 0.16561816070856628, 0.4885998436703589]
printing an ep nov before normalisation:  0.822929998057873
maxi score, test score, baseline:  -0.9803761904761906 -0.7356666666666667 -0.7356666666666667
probs:  [0.10273838533605621, 0.10227198352671928, 0.07102017159406317, 0.06991672430574912, 0.16521906485729992, 0.48883367038011244]
using another actor
from probs:  [0.10273838533605621, 0.10227198352671928, 0.07102017159406317, 0.06991672430574912, 0.16521906485729992, 0.48883367038011244]
maxi score, test score, baseline:  -0.9803761904761906 -0.7356666666666667 -0.7356666666666667
probs:  [0.10280392071607145, 0.10233594104989555, 0.0709784029296457, 0.06987122263297824, 0.1654959749908265, 0.4885145376805825]
maxi score, test score, baseline:  -0.9803761904761906 -0.7356666666666667 -0.7356666666666667
probs:  [0.10280392071607145, 0.10233594104989555, 0.0709784029296457, 0.06987122263297824, 0.1654959749908265, 0.4885145376805825]
maxi score, test score, baseline:  -0.9803761904761906 -0.7356666666666667 -0.7356666666666667
probs:  [0.10282749924604227, 0.10235941210740117, 0.07076520424295041, 0.06988723810304569, 0.16553395087629943, 0.48862669542426096]
printing an ep nov before normalisation:  0.026302540132689955
printing an ep nov before normalisation:  45.51787150768772
printing an ep nov before normalisation:  0.0241422930440649
maxi score, test score, baseline:  -0.9803761904761906 -0.7356666666666667 -0.7356666666666667
maxi score, test score, baseline:  -0.9804366357069144 -0.7356666666666667 -0.7356666666666667
probs:  [0.10301364897665059, 0.10200711694910831, 0.07071809537162954, 0.06983374188002976, 0.16617630061497962, 0.4882510962076022]
printing an ep nov before normalisation:  61.62950997538291
printing an ep nov before normalisation:  70.77161361820822
maxi score, test score, baseline:  -0.9804967078189302 -0.7356666666666667 -0.7356666666666667
probs:  [0.1030793285493535, 0.10206943170233967, 0.07067581145718833, 0.06978850158688736, 0.16645313185925764, 0.4879337948449734]
printing an ep nov before normalisation:  49.71101770345794
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.013]
 [-0.001]
 [-0.001]] [[46.685]
 [46.685]
 [46.685]
 [46.685]
 [30.175]
 [46.685]
 [46.685]] [[2.755]
 [2.755]
 [2.755]
 [2.755]
 [1.32 ]
 [2.755]
 [2.755]]
printing an ep nov before normalisation:  46.638561771532714
maxi score, test score, baseline:  -0.9804967078189302 -0.7356666666666667 -0.7356666666666667
probs:  [0.1030793285493535, 0.10206943170233967, 0.07067581145718833, 0.06978850158688736, 0.16645313185925764, 0.4879337948449734]
printing an ep nov before normalisation:  24.532227516174316
maxi score, test score, baseline:  -0.9804967078189302 -0.7356666666666667 -0.7356666666666667
probs:  [0.1030793285493535, 0.10206943170233967, 0.07067581145718833, 0.06978850158688736, 0.16645313185925764, 0.4879337948449734]
printing an ep nov before normalisation:  41.428697109785986
maxi score, test score, baseline:  -0.9804967078189302 -0.7356666666666667 -0.7356666666666667
probs:  [0.1030793285493535, 0.10206943170233967, 0.07067581145718833, 0.06978850158688736, 0.16645313185925764, 0.4879337948449734]
maxi score, test score, baseline:  -0.9804967078189302 -0.7356666666666667 -0.7356666666666667
probs:  [0.1026068348689933, 0.10212317113752628, 0.07071300058602151, 0.06982522293755195, 0.16654081353982547, 0.48819095693008135]
maxi score, test score, baseline:  -0.9804967078189302 -0.7356666666666667 -0.7356666666666667
probs:  [0.1026068348689933, 0.10212317113752628, 0.07071300058602151, 0.06982522293755195, 0.16654081353982547, 0.48819095693008135]
printing an ep nov before normalisation:  41.33878161650317
printing an ep nov before normalisation:  44.231200981147225
maxi score, test score, baseline:  -0.9804967078189302 -0.7356666666666667 -0.7356666666666667
probs:  [0.10267091906849204, 0.10218564437493464, 0.07067085445522178, 0.06978011984297068, 0.16681784572203592, 0.48787461653634495]
using explorer policy with actor:  1
printing an ep nov before normalisation:  14.849415899337753
Printing some Q and Qe and total Qs values:  [[0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]
 [0.248]] [[44.703]
 [44.703]
 [44.703]
 [44.703]
 [44.703]
 [44.703]
 [44.703]] [[1.497]
 [1.497]
 [1.497]
 [1.497]
 [1.497]
 [1.497]
 [1.497]]
actor:  1 policy actor:  1  step number:  69 total reward:  0.1999999999999993  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8382598
maxi score, test score, baseline:  -0.9805564102564104 -0.7356666666666667 -0.7356666666666667
probs:  [0.09836886667813416, 0.09789919590464738, 0.0673977583593284, 0.10469460969046941, 0.16045316049801348, 0.4711864088694072]
printing an ep nov before normalisation:  66.3491045359769
printing an ep nov before normalisation:  26.95378303527832
printing an ep nov before normalisation:  0.10266847673847224
printing an ep nov before normalisation:  0.006169294723576968
maxi score, test score, baseline:  -0.9805564102564104 -0.7356666666666667 -0.7356666666666667
probs:  [0.09836886667813416, 0.09789919590464738, 0.0673977583593284, 0.10469460969046941, 0.16045316049801348, 0.4711864088694072]
printing an ep nov before normalisation:  0.030705600121905263
using another actor
printing an ep nov before normalisation:  50.25015817286891
using another actor
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  47.24599547051471
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.449386909198054
maxi score, test score, baseline:  -0.980615746421268 -0.7356666666666667 -0.7356666666666667
probs:  [0.0985574742226976, 0.09808434597677333, 0.06735837268094738, 0.10492978407151977, 0.16016039672777602, 0.47090962632028593]
maxi score, test score, baseline:  -0.980615746421268 -0.7356666666666667 -0.7356666666666667
probs:  [0.0985574742226976, 0.09808434597677333, 0.06735837268094738, 0.10492978407151977, 0.16016039672777602, 0.47090962632028593]
actions average: 
K:  2  action  0 :  tensor([0.2783, 0.0104, 0.1025, 0.1552, 0.1516, 0.1582, 0.1437],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0143, 0.8850, 0.0096, 0.0331, 0.0181, 0.0159, 0.0241],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1008, 0.1160, 0.2999, 0.1090, 0.0976, 0.1773, 0.0994],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1243, 0.1037, 0.1263, 0.2021, 0.1249, 0.1865, 0.1322],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1517, 0.0020, 0.1229, 0.1762, 0.1401, 0.2415, 0.1656],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1971, 0.0276, 0.1220, 0.1652, 0.1405, 0.2046, 0.1431],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1577, 0.0358, 0.1191, 0.1948, 0.1491, 0.1632, 0.1803],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.980615746421268 -0.7356666666666667 -0.7356666666666667
probs:  [0.09861015439437469, 0.09813677291331922, 0.06739435397945423, 0.10445106707029159, 0.16024604898323092, 0.47116160265932927]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  39.28957646864887
maxi score, test score, baseline:  -0.9806747196738024 -0.7356666666666667 -0.7356666666666667
probs:  [0.09865874015737615, 0.09818384986280312, 0.06734344531034873, 0.1045182696325406, 0.16049108744800553, 0.47080460758892595]
maxi score, test score, baseline:  -0.9806747196738024 -0.7356666666666667 -0.7356666666666667
probs:  [0.09865874015737615, 0.09818384986280312, 0.06734344531034873, 0.1045182696325406, 0.16049108744800553, 0.47080460758892595]
maxi score, test score, baseline:  -0.9806747196738024 -0.7356666666666667 -0.7356666666666667
probs:  [0.09865874015737615, 0.09818384986280312, 0.06734344531034873, 0.1045182696325406, 0.16049108744800553, 0.47080460758892595]
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.440768585693085
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.064108839352954
printing an ep nov before normalisation:  43.46882231041286
Printing some Q and Qe and total Qs values:  [[0.168]
 [0.188]
 [0.177]
 [0.188]
 [0.18 ]
 [0.174]
 [0.176]] [[37.912]
 [38.292]
 [38.464]
 [38.539]
 [38.545]
 [38.241]
 [38.414]] [[1.736]
 [1.787]
 [1.79 ]
 [1.807]
 [1.8  ]
 [1.769]
 [1.785]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.185425929100795
actor:  1 policy actor:  1  step number:  73 total reward:  0.25333333333333274  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9807333333333335 -0.7356666666666667 -0.7356666666666667
probs:  [0.09820359035072578, 0.09774283457084115, 0.06782035602387426, 0.10388871827172044, 0.1581955752861251, 0.4741489254967133]
printing an ep nov before normalisation:  28.90833909766208
maxi score, test score, baseline:  -0.9807333333333335 -0.7356666666666667 -0.7356666666666667
probs:  [0.09820359035072578, 0.09774283457084115, 0.06782035602387426, 0.10388871827172044, 0.1581955752861251, 0.4741489254967133]
maxi score, test score, baseline:  -0.9807333333333335 -0.7356666666666667 -0.7356666666666667
probs:  [0.09834437512406644, 0.09787924732332712, 0.06767284027032147, 0.10408344810428183, 0.15890561218977547, 0.4731144769882277]
actions average: 
K:  1  action  0 :  tensor([0.3074, 0.0095, 0.1063, 0.1356, 0.1643, 0.1477, 0.1290],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0220, 0.8850, 0.0140, 0.0224, 0.0108, 0.0141, 0.0315],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1708, 0.0092, 0.2213, 0.1258, 0.1251, 0.1748, 0.1730],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1328, 0.1016, 0.1099, 0.2705, 0.1165, 0.1323, 0.1363],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1781, 0.0065, 0.1282, 0.1779, 0.1482, 0.1758, 0.1854],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1443, 0.0790, 0.1138, 0.1428, 0.1145, 0.2682, 0.1375],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1959, 0.0810, 0.1272, 0.1365, 0.1335, 0.1626, 0.1635],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.061]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.061]] [[48.864]
 [48.864]
 [48.864]
 [48.864]
 [48.864]
 [48.864]
 [48.864]] [[1.302]
 [1.302]
 [1.302]
 [1.302]
 [1.302]
 [1.302]
 [1.302]]
printing an ep nov before normalisation:  41.833981215364595
siam score:  -0.83882475
maxi score, test score, baseline:  -0.9807333333333335 -0.7356666666666667 -0.7356666666666667
probs:  [0.0984379935169414, 0.09796995843044723, 0.06757474594205254, 0.10421293862755805, 0.15937776917255372, 0.472426594310447]
printing an ep nov before normalisation:  59.54257660769851
maxi score, test score, baseline:  -0.9807333333333335 -0.7356666666666667 -0.7356666666666667
printing an ep nov before normalisation:  50.11534996174388
maxi score, test score, baseline:  -0.9807333333333335 -0.7356666666666667 -0.7356666666666667
probs:  [0.09848473149395306, 0.09801524497627531, 0.06752577340237505, 0.10427758538034089, 0.15961348847438445, 0.4720831762726712]
printing an ep nov before normalisation:  3.3041663982658065e-05
siam score:  -0.83575165
maxi score, test score, baseline:  -0.9807333333333335 -0.7356666666666667 -0.7356666666666667
probs:  [0.09853142208761803, 0.09806048561022959, 0.06747685051146238, 0.10434216659371236, 0.15984896880204202, 0.47174010639493563]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9807333333333335 -0.7356666666666667 -0.7356666666666667
probs:  [0.09807712966967921, 0.0981098722559893, 0.06751081322755964, 0.104394721214938, 0.15992951659707724, 0.47197794703475665]
maxi score, test score, baseline:  -0.9807333333333335 -0.7356666666666667 -0.7356666666666667
printing an ep nov before normalisation:  25.985405445098877
printing an ep nov before normalisation:  8.135978512444808
maxi score, test score, baseline:  -0.9807333333333335 -0.7356666666666667 -0.7356666666666667
maxi score, test score, baseline:  -0.9807333333333335 -0.7356666666666667 -0.7356666666666667
probs:  [0.098167621709948, 0.0982005657134911, 0.06741327543955447, 0.10452407615370517, 0.160400495887562, 0.47129396509573923]
using explorer policy with actor:  1
printing an ep nov before normalisation:  65.46302021578116
siam score:  -0.8313487
maxi score, test score, baseline:  -0.9807333333333335 -0.7356666666666667 -0.7356666666666667
probs:  [0.098167621709948, 0.0982005657134911, 0.06741327543955447, 0.10452407615370517, 0.160400495887562, 0.47129396509573923]
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.057]
 [0.041]
 [0.041]
 [0.041]
 [0.041]
 [0.041]] [[49.918]
 [52.429]
 [49.918]
 [49.918]
 [49.918]
 [49.918]
 [49.918]] [[1.14 ]
 [1.253]
 [1.14 ]
 [1.14 ]
 [1.14 ]
 [1.14 ]
 [1.14 ]]
printing an ep nov before normalisation:  13.988079991197253
actor:  1 policy actor:  1  step number:  74 total reward:  0.2333333333333334  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 43.17068474181285
printing an ep nov before normalisation:  72.61010322211277
maxi score, test score, baseline:  -0.9807333333333335 -0.7356666666666667 -0.7356666666666667
probs:  [0.09772064365494917, 0.09775259277449158, 0.0678950553910501, 0.10388513798010549, 0.1580741320816372, 0.47467243811776644]
maxi score, test score, baseline:  -0.9807333333333335 -0.7356666666666667 -0.7356666666666667
probs:  [0.09772064365494917, 0.09775259277449158, 0.0678950553910501, 0.10388513798010549, 0.1580741320816372, 0.47467243811776644]
maxi score, test score, baseline:  -0.9807333333333335 -0.7356666666666667 -0.7356666666666667
probs:  [0.09772064365494917, 0.09775259277449158, 0.0678950553910501, 0.10388513798010549, 0.1580741320816372, 0.47467243811776644]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.9807333333333335 -0.7356666666666667 -0.7356666666666667
probs:  [0.09772064365494917, 0.09775259277449158, 0.0678950553910501, 0.10388513798010549, 0.1580741320816372, 0.47467243811776644]
maxi score, test score, baseline:  -0.9807333333333335 -0.7356666666666667 -0.7356666666666667
probs:  [0.09772064365494917, 0.09775259277449158, 0.0678950553910501, 0.10388513798010549, 0.1580741320816372, 0.47467243811776644]
maxi score, test score, baseline:  -0.9807333333333335 -0.7356666666666667 -0.7356666666666667
probs:  [0.09772064365494917, 0.09775259277449158, 0.0678950553910501, 0.10388513798010549, 0.1580741320816372, 0.47467243811776644]
maxi score, test score, baseline:  -0.9807333333333335 -0.7356666666666667 -0.7356666666666667
probs:  [0.09786386799867963, 0.09789594738884728, 0.06791666768139819, 0.10303672946195154, 0.1584634441669209, 0.4748233433022025]
printing an ep nov before normalisation:  72.38592978577404
maxi score, test score, baseline:  -0.9807333333333335 -0.7356666666666667 -0.7356666666666667
maxi score, test score, baseline:  -0.9807333333333335 -0.7356666666666667 -0.7356666666666667
probs:  [0.09786386799867963, 0.09789594738884728, 0.06791666768139819, 0.10303672946195154, 0.1584634441669209, 0.4748233433022025]
maxi score, test score, baseline:  -0.9807333333333335 -0.7356666666666667 -0.7356666666666667
probs:  [0.09786386799867963, 0.09789594738884728, 0.06791666768139819, 0.10303672946195154, 0.1584634441669209, 0.4748233433022025]
Printing some Q and Qe and total Qs values:  [[0.211]
 [0.245]
 [0.173]
 [0.135]
 [0.134]
 [0.138]
 [0.206]] [[57.655]
 [59.182]
 [61.053]
 [60.973]
 [61.176]
 [61.681]
 [59.288]] [[1.641]
 [1.738]
 [1.744]
 [1.703]
 [1.71 ]
 [1.734]
 [1.704]]
siam score:  -0.8347136
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.057]
 [0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]] [[51.82 ]
 [52.161]
 [51.82 ]
 [51.82 ]
 [51.82 ]
 [51.82 ]
 [51.82 ]] [[1.77 ]
 [1.804]
 [1.77 ]
 [1.77 ]
 [1.77 ]
 [1.77 ]
 [1.77 ]]
maxi score, test score, baseline:  -0.9807333333333335 -0.7356666666666667 -0.7356666666666667
probs:  [0.09790813600521212, 0.09794022992603907, 0.06794737082390885, 0.10308334056438644, 0.15808256405744403, 0.4750383586230094]
Printing some Q and Qe and total Qs values:  [[0.054]
 [0.145]
 [0.095]
 [0.073]
 [0.062]
 [0.064]
 [0.079]] [[64.846]
 [59.696]
 [59.29 ]
 [64.072]
 [65.756]
 [64.424]
 [62.89 ]] [[1.848]
 [1.677]
 [1.606]
 [1.827]
 [1.903]
 [1.836]
 [1.774]]
from probs:  [0.09821759358962118, 0.09825037050182125, 0.06761923276331656, 0.10350293178086242, 0.1596725948526633, 0.4727372765117152]
maxi score, test score, baseline:  -0.980849494949495 -0.7356666666666667 -0.7356666666666667
probs:  [0.09777056852420973, 0.09829904437880468, 0.06765271116911967, 0.10355421134439331, 0.15975173901932135, 0.4729717255641511]
maxi score, test score, baseline:  -0.980849494949495 -0.7356666666666667 -0.7356666666666667
probs:  [0.09781330221036047, 0.0983433459526455, 0.06760609088121784, 0.10361410400252509, 0.15997835909004693, 0.4726447978632042]
printing an ep nov before normalisation:  60.45117378234863
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.980849494949495 -0.7356666666666667 -0.7356666666666667
probs:  [0.09781330221036047, 0.0983433459526455, 0.06760609088121784, 0.10361410400252509, 0.15997835909004693, 0.4726447978632042]
printing an ep nov before normalisation:  55.56185326929857
maxi score, test score, baseline:  -0.980849494949495 -0.7356666666666667 -0.7356666666666667
printing an ep nov before normalisation:  26.486226926564555
printing an ep nov before normalisation:  41.915834158191885
printing an ep nov before normalisation:  13.325014012910836
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9809070493454181 -0.7356666666666667 -0.7356666666666667
maxi score, test score, baseline:  -0.9809070493454181 -0.7356666666666667 -0.7356666666666667
probs:  [0.09785831053247274, 0.09838859850449833, 0.06763718054191184, 0.10366178517633555, 0.15959160690076746, 0.47286251834401405]
Printing some Q and Qe and total Qs values:  [[0.266]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.266]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]]
maxi score, test score, baseline:  -0.9809642570281125 -0.7356666666666667 -0.7356666666666667
probs:  [0.09790115602984835, 0.098433011831668, 0.06759067536473164, 0.10372178901207767, 0.15981697077766044, 0.47253639698401384]
maxi score, test score, baseline:  -0.9809642570281125 -0.7356666666666667 -0.7356666666666667
probs:  [0.09803698375695394, 0.09857224550427587, 0.06753239813906552, 0.10338168159683234, 0.1603493005078266, 0.4721273904950459]
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.121]
 [0.026]
 [0.024]
 [0.027]
 [0.027]
 [0.039]] [[26.529]
 [52.068]
 [28.09 ]
 [33.146]
 [28.   ]
 [28.344]
 [27.086]] [[0.5  ]
 [1.542]
 [0.558]
 [0.743]
 [0.556]
 [0.569]
 [0.534]]
maxi score, test score, baseline:  -0.9809642570281125 -0.7356666666666667 -0.7356666666666667
probs:  [0.09803698375695394, 0.09857224550427587, 0.06753239813906552, 0.10338168159683234, 0.1603493005078266, 0.4721273904950459]
maxi score, test score, baseline:  -0.9809642570281125 -0.7356666666666667 -0.7356666666666667
probs:  [0.09808218338208077, 0.09861769224586753, 0.06756351458825756, 0.10342934873031807, 0.15996196309587862, 0.4723452979575976]
maxi score, test score, baseline:  -0.9809642570281125 -0.7356666666666667 -0.7356666666666667
probs:  [0.09817026749328076, 0.09870758961013955, 0.06754826134374378, 0.10353553854368296, 0.15980030952353205, 0.47223803348562093]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.135601398925836
actions average: 
K:  2  action  0 :  tensor([0.2690, 0.0165, 0.1217, 0.1275, 0.1502, 0.1641, 0.1511],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0464, 0.7615, 0.0389, 0.0348, 0.0432, 0.0239, 0.0513],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1378, 0.0219, 0.2121, 0.1364, 0.1416, 0.1742, 0.1760],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1220, 0.0679, 0.1179, 0.2708, 0.1372, 0.1458, 0.1384],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1641, 0.0023, 0.1289, 0.1311, 0.2700, 0.1667, 0.1370],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1292, 0.0700, 0.1549, 0.1395, 0.1143, 0.2481, 0.1439],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1348, 0.1204, 0.1446, 0.1075, 0.1096, 0.1241, 0.2589],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9809642570281125 -0.7356666666666667 -0.7356666666666667
probs:  [0.09817026749328076, 0.09870758961013955, 0.06754826134374378, 0.10353553854368296, 0.15980030952353205, 0.47223803348562093]
printing an ep nov before normalisation:  29.41072029206758
printing an ep nov before normalisation:  50.35583287865535
maxi score, test score, baseline:  -0.9809642570281125 -0.7356666666666667 -0.7356666666666667
probs:  [0.0977235592019366, 0.09875645308538859, 0.06758167908826351, 0.10358679524123017, 0.15987945676142307, 0.472472056621758]
printing an ep nov before normalisation:  71.07024134452782
maxi score, test score, baseline:  -0.9809642570281125 -0.7356666666666667 -0.7356666666666667
probs:  [0.09776543190003331, 0.09880133827476716, 0.06753564135091636, 0.10364576838665732, 0.16010261052092847, 0.4721492095666974]
siam score:  -0.83261335
printing an ep nov before normalisation:  40.01186140216572
Starting evaluation
line 256 mcts: sample exp_bonus 0.0
actions average: 
K:  4  action  0 :  tensor([0.3504, 0.0165, 0.1156, 0.1098, 0.1279, 0.1428, 0.1369],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0631, 0.6423, 0.0566, 0.0613, 0.0514, 0.0568, 0.0684],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1634, 0.0978, 0.1346, 0.1441, 0.1425, 0.1666, 0.1510],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1374, 0.0300, 0.1264, 0.2256, 0.1619, 0.1677, 0.1509],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1521, 0.0108, 0.1253, 0.1592, 0.2507, 0.1616, 0.1402],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1161, 0.0320, 0.1726, 0.1059, 0.0959, 0.3500, 0.1275],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1675, 0.0428, 0.1597, 0.1430, 0.1319, 0.1561, 0.1989],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  39.182825484409406
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  56.093064943949386
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.446]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]] [[56.952]
 [50.514]
 [56.952]
 [56.952]
 [56.952]
 [56.952]
 [56.952]] [[0.409]
 [0.446]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]]
printing an ep nov before normalisation:  36.83749556211132
printing an ep nov before normalisation:  43.73477809714712
Printing some Q and Qe and total Qs values:  [[0.085]
 [0.287]
 [0.085]
 [0.085]
 [0.085]
 [0.085]
 [0.085]] [[40.421]
 [48.143]
 [40.421]
 [40.421]
 [40.421]
 [40.421]
 [40.421]] [[0.942]
 [1.522]
 [0.942]
 [0.942]
 [0.942]
 [0.942]
 [0.942]]
printing an ep nov before normalisation:  53.237923621023214
printing an ep nov before normalisation:  5.8545173260426964e-05
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.191]
 [0.231]
 [0.223]
 [0.231]
 [0.206]
 [0.231]] [[33.342]
 [46.283]
 [33.342]
 [34.525]
 [33.342]
 [34.631]
 [33.342]] [[0.231]
 [0.191]
 [0.231]
 [0.223]
 [0.231]
 [0.206]
 [0.231]]
printing an ep nov before normalisation:  59.95392224375198
maxi score, test score, baseline:  -0.9815715257531585 -0.7356666666666667 -0.7356666666666667
maxi score, test score, baseline:  -0.9815715257531585 -0.7356666666666667 -0.7356666666666667
probs:  [0.0978072651159652, 0.09884618114147428, 0.06748964702298908, 0.10370468592574542, 0.1603255538666341, 0.4718266669271919]
printing an ep nov before normalisation:  43.812631534376074
line 256 mcts: sample exp_bonus 47.19089998350466
printing an ep nov before normalisation:  44.129110937274554
maxi score, test score, baseline:  -0.9817304431599231 -0.7356666666666667 -0.7356666666666667
probs:  [0.09785211376067142, 0.09889150682159838, 0.06752057483361323, 0.10375224246842545, 0.15994030937037432, 0.4720432527453172]
printing an ep nov before normalisation:  0.004184037489949333
actor:  1 policy actor:  1  step number:  52 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9818348659003833 -0.7356666666666667 -0.7356666666666667
probs:  [0.09602635918389782, 0.09704633249193827, 0.06626152693670295, 0.10181625140286774, 0.1756233229113753, 0.4632262070732178]
siam score:  -0.8270034
maxi score, test score, baseline:  -0.9818348659003833 -0.7356666666666667 -0.7356666666666667
probs:  [0.09602635918389782, 0.09704633249193827, 0.06626152693670295, 0.10181625140286774, 0.1756233229113753, 0.4632262070732178]
actions average: 
K:  4  action  0 :  tensor([0.3017, 0.0120, 0.1090, 0.1497, 0.1470, 0.1304, 0.1502],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0248, 0.7508, 0.0774, 0.0495, 0.0232, 0.0341, 0.0401],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1528, 0.0202, 0.1733, 0.1564, 0.1192, 0.2447, 0.1334],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1368, 0.1239, 0.1074, 0.2605, 0.1326, 0.0926, 0.1462],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1672, 0.0670, 0.0957, 0.1400, 0.2765, 0.1241, 0.1295],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1806, 0.0112, 0.1603, 0.1456, 0.1383, 0.2167, 0.1473],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2124, 0.0611, 0.1421, 0.1436, 0.1289, 0.1568, 0.1552],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9818866284622733 -0.7356666666666667 -0.7356666666666667
probs:  [0.09602635918389782, 0.09704633249193827, 0.06626152693670295, 0.10181625140286774, 0.1756233229113753, 0.4632262070732178]
printing an ep nov before normalisation:  18.38011893983209
printing an ep nov before normalisation:  24.152114139895247
maxi score, test score, baseline:  -0.9819380952380954 -0.7356666666666667 -0.7356666666666667
probs:  [0.09602635918389782, 0.09704633249193827, 0.06626152693670295, 0.10181625140286774, 0.1756233229113753, 0.4632262070732178]
printing an ep nov before normalisation:  34.222627400586816
maxi score, test score, baseline:  -0.9819380952380954 -0.7356666666666667 -0.7356666666666667
printing an ep nov before normalisation:  18.412769038267342
maxi score, test score, baseline:  -0.9820401515151517 -1.0 -0.9820401515151517
probs:  [0.09824298719027424, 0.09993578878202229, 0.06571991105577632, 0.1020646937841281, 0.17461906792150028, 0.4594175512662987]
printing an ep nov before normalisation:  39.83524402819371
maxi score, test score, baseline:  -0.9820401515151517 -1.0 -0.9820401515151517
probs:  [0.09824298719027424, 0.09993578878202229, 0.06571991105577632, 0.1020646937841281, 0.17461906792150028, 0.4594175512662987]
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.4  ]
 [0.355]
 [0.355]
 [0.358]
 [0.361]
 [0.356]] [[36.864]
 [51.935]
 [32.095]
 [27.917]
 [27.697]
 [29.936]
 [33.302]] [[0.359]
 [0.4  ]
 [0.355]
 [0.355]
 [0.358]
 [0.361]
 [0.356]]
actions average: 
K:  3  action  0 :  tensor([0.1889, 0.0057, 0.1601, 0.1610, 0.1656, 0.1607, 0.1581],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0362, 0.7767, 0.0313, 0.0296, 0.0240, 0.0184, 0.0839],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1729, 0.0770, 0.1391, 0.1425, 0.1745, 0.1512, 0.1428],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1006, 0.0942, 0.1218, 0.2522, 0.1074, 0.1236, 0.2002],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1579, 0.0133, 0.1685, 0.1497, 0.1878, 0.1654, 0.1574],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1616, 0.0084, 0.1663, 0.1418, 0.1713, 0.2002, 0.1503],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1552, 0.0369, 0.1402, 0.1389, 0.1362, 0.1225, 0.2700],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9821410546139361 -1.0 -0.9821410546139361
probs:  [0.09829611447379194, 0.09999004954283716, 0.06575482714966314, 0.10158487883046856, 0.17471207717130768, 0.45966205283193157]
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  39.738922119140625
printing an ep nov before normalisation:  21.16621459395217
maxi score, test score, baseline:  -0.9821410546139361 -1.0 -0.9821410546139361
probs:  [0.09833841238652907, 0.10003714668557225, 0.0657049296027563, 0.10163649441827198, 0.17497087557878127, 0.4593121413280892]
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.683]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.632]
 [0.683]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]]
maxi score, test score, baseline:  -0.9822408239700375 -1.0 -0.9822408239700375
probs:  [0.09785628498656813, 0.1000915644376064, 0.0657399110841347, 0.10169087104849366, 0.17506426788269386, 0.45955710056050325]
maxi score, test score, baseline:  -0.9822408239700375 -1.0 -0.9822408239700375
probs:  [0.09738139087065217, 0.10014422201599857, 0.06577447237440082, 0.10174437113402617, 0.1751564207258167, 0.45979912287910546]
printing an ep nov before normalisation:  77.1259721722514
maxi score, test score, baseline:  -0.9822408239700375 -1.0 -0.9822408239700375
probs:  [0.09738139087065217, 0.10014422201599857, 0.06577447237440082, 0.10174437113402617, 0.1751564207258167, 0.45979912287910546]
from probs:  [0.09738139087065217, 0.10014422201599857, 0.06577447237440082, 0.10174437113402617, 0.1751564207258167, 0.45979912287910546]
maxi score, test score, baseline:  -0.9822408239700375 -1.0 -0.9822408239700375
maxi score, test score, baseline:  -0.9822408239700375 -1.0 -0.9822408239700375
probs:  [0.09747255774109809, 0.1002446632719035, 0.06575953983815655, 0.10185018383644076, 0.17497899477564166, 0.4596940605367593]
actions average: 
K:  4  action  0 :  tensor([0.2693, 0.1069, 0.0991, 0.1283, 0.1135, 0.1414, 0.1414],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0471, 0.7446, 0.0280, 0.0551, 0.0339, 0.0343, 0.0571],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1502, 0.0157, 0.2022, 0.1703, 0.1354, 0.1700, 0.1563],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1692, 0.0414, 0.1000, 0.2625, 0.1495, 0.1461, 0.1313],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1755, 0.0237, 0.1157, 0.1256, 0.2466, 0.1585, 0.1544],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1254, 0.1369, 0.1284, 0.1321, 0.1059, 0.2383, 0.1330],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1580, 0.0184, 0.1242, 0.1640, 0.1353, 0.1846, 0.2156],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9822408239700375 -1.0 -0.9822408239700375
probs:  [0.09747255774109809, 0.1002446632719035, 0.06575953983815655, 0.10185018383644076, 0.17497899477564166, 0.4596940605367593]
maxi score, test score, baseline:  -0.9822408239700375 -1.0 -0.9822408239700375
probs:  [0.097563675603865, 0.10034505428546565, 0.06574457241884521, 0.10195594558143231, 0.17480199820326112, 0.4595887539071307]
actions average: 
K:  3  action  0 :  tensor([0.2727, 0.1007, 0.1180, 0.1178, 0.1226, 0.1314, 0.1368],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0274, 0.8330, 0.0315, 0.0281, 0.0150, 0.0213, 0.0437],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1490, 0.0458, 0.1866, 0.1673, 0.1325, 0.1601, 0.1585],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1496, 0.0958, 0.1283, 0.1951, 0.1252, 0.1545, 0.1515],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1611, 0.0027, 0.1104, 0.1327, 0.3063, 0.1473, 0.1394],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1600, 0.0254, 0.1307, 0.1626, 0.1377, 0.2347, 0.1489],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1577, 0.1104, 0.1044, 0.1684, 0.1075, 0.1300, 0.2216],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.513]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.481]
 [0.513]
 [0.481]
 [0.481]
 [0.481]
 [0.481]
 [0.481]]
actions average: 
K:  1  action  0 :  tensor([0.2410, 0.0079, 0.1282, 0.1545, 0.1472, 0.1555, 0.1657],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0244, 0.8475, 0.0262, 0.0293, 0.0148, 0.0163, 0.0415],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0864, 0.0575, 0.3954, 0.0902, 0.0673, 0.1729, 0.1303],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1293, 0.0221, 0.1108, 0.3615, 0.1122, 0.1280, 0.1361],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1513, 0.0031, 0.1133, 0.1394, 0.2921, 0.1469, 0.1538],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1699, 0.0264, 0.1465, 0.1672, 0.1372, 0.1742, 0.1786],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1471, 0.0382, 0.1126, 0.1591, 0.1277, 0.1349, 0.2803],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9822902894491131 -1.0 -0.9822902894491131
printing an ep nov before normalisation:  71.30392184689293
UNIT TEST: sample policy line 217 mcts : [0.163 0.224 0.082 0.143 0.163 0.122 0.102]
maxi score, test score, baseline:  -0.98233947858473 -1.0 -0.98233947858473
probs:  [0.09779896965986412, 0.10060066035585848, 0.06574970352136128, 0.1011609594780552, 0.17506601677596487, 0.45962369020889615]
maxi score, test score, baseline:  -0.98233947858473 -1.0 -0.98233947858473
actions average: 
K:  0  action  0 :  tensor([0.2645, 0.0058, 0.1383, 0.1577, 0.1378, 0.1627, 0.1332],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0160, 0.9124, 0.0089, 0.0187, 0.0112, 0.0050, 0.0278],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1077, 0.0354, 0.3480, 0.1064, 0.0987, 0.1821, 0.1218],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1839, 0.0026, 0.1449, 0.2007, 0.1420, 0.1716, 0.1544],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1613, 0.0195, 0.1423, 0.1492, 0.2072, 0.1592, 0.1613],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1719, 0.0205, 0.2005, 0.1202, 0.1200, 0.2349, 0.1319],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1531, 0.0574, 0.1395, 0.1329, 0.1137, 0.1388, 0.2646],
       grad_fn=<DivBackward0>)
UNIT TEST: sample policy line 217 mcts : [0.143 0.327 0.102 0.102 0.184 0.102 0.041]
maxi score, test score, baseline:  -0.9823883936861654 -1.0 -0.9823883936861654
probs:  [0.09779928374794145, 0.10060110408157537, 0.06574962894085219, 0.10116094621952339, 0.17506587183202196, 0.45962316517808566]
maxi score, test score, baseline:  -0.9823883936861654 -1.0 -0.9823883936861654
probs:  [0.09779928374794145, 0.10060110408157537, 0.06574962894085219, 0.10116094621952339, 0.17506587183202196, 0.45962316517808566]
siam score:  -0.8303189
printing an ep nov before normalisation:  44.30649628622934
printing an ep nov before normalisation:  19.477334022521973
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.636]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]] [[53.725]
 [54.979]
 [53.725]
 [53.725]
 [53.725]
 [53.725]
 [53.725]] [[2.171]
 [2.305]
 [2.171]
 [2.171]
 [2.171]
 [2.171]
 [2.171]]
siam score:  -0.82945746
printing an ep nov before normalisation:  47.24977970123291
maxi score, test score, baseline:  -0.9824370370370371 -1.0 -0.9824370370370371
probs:  [0.09790793715478611, 0.10012298182212274, 0.0658223428279547, 0.10075460455869897, 0.17525977993879138, 0.46013235369764616]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.312]
 [0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]] [[43.68 ]
 [61.365]
 [43.68 ]
 [43.68 ]
 [43.68 ]
 [43.68 ]
 [43.68 ]] [[1.198]
 [1.911]
 [1.198]
 [1.198]
 [1.198]
 [1.198]
 [1.198]]
maxi score, test score, baseline:  -0.9824370370370371 -1.0 -0.9824370370370371
probs:  [0.09790793715478611, 0.10012298182212274, 0.0658223428279547, 0.10075460455869897, 0.17525977993879138, 0.46013235369764616]
printing an ep nov before normalisation:  39.23651580575262
maxi score, test score, baseline:  -0.9824370370370371 -1.0 -0.9824370370370371
probs:  [0.09794854676814058, 0.10016977741662168, 0.06577334662683382, 0.10080316409340037, 0.1755164109480167, 0.4597887541469868]
maxi score, test score, baseline:  -0.9824370370370371 -1.0 -0.9824370370370371
probs:  [0.09794854676814058, 0.10016977741662168, 0.06577334662683382, 0.10080316409340037, 0.1755164109480167, 0.4597887541469868]
maxi score, test score, baseline:  -0.9824370370370371 -1.0 -0.9824370370370371
probs:  [0.09794854676814058, 0.10016977741662168, 0.06577334662683382, 0.10080316409340037, 0.1755164109480167, 0.4597887541469868]
printing an ep nov before normalisation:  35.848546528885805
maxi score, test score, baseline:  -0.9824854108956603 -1.0 -0.9824854108956603
probs:  [0.09794886003270582, 0.10017020939742219, 0.06577327357081626, 0.10080314740460783, 0.1755162697825887, 0.45978823981185923]
actions average: 
K:  1  action  0 :  tensor([0.2326, 0.0216, 0.1013, 0.2269, 0.1720, 0.1314, 0.1140],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0051, 0.9362, 0.0236, 0.0097, 0.0035, 0.0091, 0.0129],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1873, 0.0133, 0.2302, 0.1423, 0.1226, 0.1770, 0.1272],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1781, 0.0090, 0.1426, 0.1948, 0.1568, 0.1679, 0.1508],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2587, 0.0295, 0.0970, 0.1159, 0.2650, 0.1122, 0.1216],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1569, 0.0105, 0.1583, 0.1518, 0.1296, 0.2547, 0.1381],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1678, 0.0855, 0.1091, 0.1617, 0.1555, 0.1355, 0.1849],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.198]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.202]] [[27.324]
 [27.114]
 [27.114]
 [27.114]
 [27.114]
 [27.114]
 [26.265]] [[2.198]
 [2.2  ]
 [2.2  ]
 [2.2  ]
 [2.2  ]
 [2.2  ]
 [2.051]]
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.538]
 [0.517]
 [0.486]
 [0.469]
 [0.506]
 [0.479]] [[63.86 ]
 [63.689]
 [67.908]
 [75.411]
 [65.944]
 [73.848]
 [65.927]] [[1.333]
 [1.445]
 [1.537]
 [1.708]
 [1.436]
 [1.685]
 [1.446]]
printing an ep nov before normalisation:  71.70265130095311
maxi score, test score, baseline:  -0.9825335174953961 -1.0 -0.9825335174953961
probs:  [0.09820024109962444, 0.09984898803360878, 0.0658653467701399, 0.10055423163504561, 0.1750986942933133, 0.460432498168268]
printing an ep nov before normalisation:  45.6451615114238
maxi score, test score, baseline:  -0.9825335174953961 -1.0 -0.9825335174953961
probs:  [0.09820024109962444, 0.09984898803360878, 0.0658653467701399, 0.10055423163504561, 0.1750986942933133, 0.460432498168268]
actions average: 
K:  3  action  0 :  tensor([0.3282, 0.0316, 0.1093, 0.1333, 0.1491, 0.1256, 0.1228],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0488, 0.7125, 0.0532, 0.0494, 0.0273, 0.0399, 0.0689],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1494, 0.0998, 0.1153, 0.2052, 0.1327, 0.1450, 0.1526],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1258, 0.0713, 0.1629, 0.2019, 0.1069, 0.1594, 0.1717],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2162, 0.0109, 0.1339, 0.1756, 0.1438, 0.1695, 0.1500],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1869, 0.0031, 0.1577, 0.1595, 0.1444, 0.1961, 0.1523],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1462, 0.0419, 0.1363, 0.1661, 0.1311, 0.1917, 0.1866],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9825335174953961 -1.0 -0.9825335174953961
probs:  [0.09825129169118385, 0.09990089691076882, 0.0658995648365683, 0.10060650763975906, 0.1746696234850313, 0.46067211543668873]
printing an ep nov before normalisation:  56.6536515979568
printing an ep nov before normalisation:  46.48513160837432
maxi score, test score, baseline:  -0.9825335174953961 -1.0 -0.9825335174953961
probs:  [0.09829265646415263, 0.09994684597687123, 0.06585102337165645, 0.10065441761528483, 0.17492335666564554, 0.46033169990638945]
printing an ep nov before normalisation:  36.56652212142944
printing an ep nov before normalisation:  32.76712560814223
maxi score, test score, baseline:  -0.9825813590449956 -1.0 -0.9825813590449956
probs:  [0.09829297081900992, 0.09994726892995677, 0.06585095184576138, 0.10065439808314487, 0.17492321402807143, 0.4603311962940556]
printing an ep nov before normalisation:  77.6893717063409
printing an ep nov before normalisation:  43.01027774810791
maxi score, test score, baseline:  -0.9825813590449956 -1.0 -0.9825813590449956
probs:  [0.09833429529394967, 0.09999317343616805, 0.06580245849377764, 0.10070226032213306, 0.17517669428552682, 0.4599911181684448]
maxi score, test score, baseline:  -0.9825813590449956 -1.0 -0.9825813590449956
maxi score, test score, baseline:  -0.9825813590449956 -1.0 -0.9825813590449956
probs:  [0.09833429529394967, 0.09999317343616805, 0.06580245849377764, 0.10070226032213306, 0.17517669428552682, 0.4599911181684448]
maxi score, test score, baseline:  -0.9825813590449956 -1.0 -0.9825813590449956
probs:  [0.09833429529394967, 0.09999317343616805, 0.06580245849377764, 0.10070226032213306, 0.17517669428552682, 0.4599911181684448]
printing an ep nov before normalisation:  40.816749692518215
printing an ep nov before normalisation:  46.76954049394395
printing an ep nov before normalisation:  19.725535657793447
printing an ep nov before normalisation:  67.51958910738209
printing an ep nov before normalisation:  41.707130426428506
printing an ep nov before normalisation:  49.340091778207196
maxi score, test score, baseline:  -0.9825813590449956 -1.0 -0.9825813590449956
probs:  [0.09847766611462812, 0.10014284829031647, 0.06582220251944981, 0.10085462983319964, 0.17457377153204293, 0.46012888171036304]
printing an ep nov before normalisation:  35.09327860900397
printing an ep nov before normalisation:  52.58107059574391
maxi score, test score, baseline:  -0.9825813590449956 -1.0 -0.9825813590449956
probs:  [0.09847766611462812, 0.10014284829031647, 0.06582220251944981, 0.10085462983319964, 0.17457377153204293, 0.46012888171036304]
actions average: 
K:  3  action  0 :  tensor([0.2371, 0.0283, 0.1307, 0.1589, 0.1433, 0.1650, 0.1367],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0404, 0.7754, 0.0384, 0.0431, 0.0321, 0.0338, 0.0368],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1765, 0.0229, 0.1507, 0.1563, 0.1410, 0.1978, 0.1549],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1528, 0.1033, 0.1225, 0.1628, 0.1388, 0.1912, 0.1285],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2274, 0.0234, 0.1164, 0.1197, 0.2500, 0.1394, 0.1237],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1785, 0.0156, 0.1194, 0.1711, 0.1429, 0.2476, 0.1249],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1395, 0.1090, 0.1211, 0.1598, 0.1316, 0.1563, 0.1828],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9826289377289378 -1.0 -0.9826289377289378
probs:  [0.09851954812818255, 0.10018941975917628, 0.06577387114855064, 0.10090271507726896, 0.17482451029032217, 0.45978993559649944]
maxi score, test score, baseline:  -0.9826762557077626 -1.0 -0.9826762557077626
probs:  [0.09851986230490727, 0.1001898423459083, 0.06577379995151676, 0.10090269599513906, 0.17482436511091135, 0.4597894342916173]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.498]
 [0.498]
 [0.474]
 [0.475]
 [0.498]
 [0.498]] [[48.677]
 [48.677]
 [48.677]
 [51.339]
 [52.658]
 [48.677]
 [48.677]] [[1.791]
 [1.791]
 [1.791]
 [1.913]
 [1.986]
 [1.791]
 [1.791]]
maxi score, test score, baseline:  -0.9826762557077626 -1.0 -0.9826762557077626
probs:  [0.09851986230490727, 0.1001898423459083, 0.06577379995151676, 0.10090269599513906, 0.17482436511091135, 0.4597894342916173]
maxi score, test score, baseline:  -0.9826762557077626 -1.0 -0.9826762557077626
using explorer policy with actor:  1
siam score:  -0.8337887
maxi score, test score, baseline:  -0.9826762557077626 -1.0 -0.9826762557077626
probs:  [0.09856138905788941, 0.10023594561128478, 0.0657255874522048, 0.10095075280695379, 0.1750750012509883, 0.45945132382067905]
Printing some Q and Qe and total Qs values:  [[0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.202]
 [0.202]] [[20.048]
 [20.048]
 [20.048]
 [20.048]
 [20.048]
 [20.048]
 [20.048]] [[1.558]
 [1.558]
 [1.558]
 [1.558]
 [1.558]
 [1.558]
 [1.558]]
maxi score, test score, baseline:  -0.9826762557077626 -1.0 -0.9826762557077626
using explorer policy with actor:  1
printing an ep nov before normalisation:  24.46685401595588
maxi score, test score, baseline:  -0.9826762557077626 -1.0 -0.9826762557077626
probs:  [0.09860287478715651, 0.10028200333187177, 0.06567742258136583, 0.10099876214409705, 0.17532538979102785, 0.459113547364481]
siam score:  -0.83161217
actor:  1 policy actor:  1  step number:  63 total reward:  0.17333333333333267  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  62.89347019653881
printing an ep nov before normalisation:  53.50903480083904
using another actor
printing an ep nov before normalisation:  56.013213767921705
printing an ep nov before normalisation:  52.196080252553074
Printing some Q and Qe and total Qs values:  [[0.35 ]
 [0.374]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.339]
 [0.35 ]] [[47.548]
 [51.396]
 [47.548]
 [47.548]
 [47.548]
 [48.338]
 [47.548]] [[0.35 ]
 [0.374]
 [0.35 ]
 [0.35 ]
 [0.35 ]
 [0.339]
 [0.35 ]]
printing an ep nov before normalisation:  40.274584856745925
maxi score, test score, baseline:  -0.9827233151183972 -1.0 -0.9827233151183972
probs:  [0.09703717027426123, 0.09869349943714179, 0.06456049980096598, 0.09940004477541589, 0.18901710177819805, 0.45129168393401714]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.46117859267226
maxi score, test score, baseline:  -0.9827233151183972 -1.0 -0.9827233151183972
printing an ep nov before normalisation:  44.64860586523361
maxi score, test score, baseline:  -0.9827233151183972 -1.0 -0.9827233151183972
probs:  [0.09709302537190612, 0.09817437344698884, 0.06459763545815748, 0.09945726182574495, 0.18912597379000337, 0.4515517301071992]
Printing some Q and Qe and total Qs values:  [[0.42]
 [0.42]
 [0.42]
 [0.42]
 [0.42]
 [0.42]
 [0.42]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.42]
 [0.42]
 [0.42]
 [0.42]
 [0.42]
 [0.42]
 [0.42]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9827233151183972 -1.0 -0.9827233151183972
probs:  [0.09714830657093027, 0.09823027117182062, 0.06463438955478705, 0.09951389098369438, 0.18866403735458415, 0.4518091043641835]
maxi score, test score, baseline:  -0.9827701180744779 -1.0 -0.9827701180744779
probs:  [0.09724053307337627, 0.09775837597701979, 0.06462094178793726, 0.09961345000227839, 0.18905225568873954, 0.4517144434706487]
printing an ep nov before normalisation:  46.75774213620883
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9827701180744779 -1.0 -0.9827701180744779
maxi score, test score, baseline:  -0.9827701180744779 -1.0 -0.9827701180744779
probs:  [0.09731431347666852, 0.09783491023568654, 0.06452125329526913, 0.09969984942572513, 0.18961428494895163, 0.45101538861769913]
maxi score, test score, baseline:  -0.9827701180744779 -1.0 -0.9827701180744779
probs:  [0.09731431347666852, 0.09783491023568654, 0.06452125329526913, 0.09969984942572513, 0.18961428494895163, 0.45101538861769913]
from probs:  [0.09731431347666852, 0.09783491023568654, 0.06452125329526913, 0.09969984942572513, 0.18961428494895163, 0.45101538861769913]
printing an ep nov before normalisation:  47.794079433837574
printing an ep nov before normalisation:  57.096869438172604
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.43147721965828
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.419]
 [0.021]
 [0.419]
 [0.399]
 [0.419]
 [0.419]] [[ 0.   ]
 [ 0.   ]
 [43.715]
 [ 0.   ]
 [42.522]
 [ 0.   ]
 [ 0.   ]] [[0.418]
 [0.418]
 [0.558]
 [0.418]
 [0.922]
 [0.418]
 [0.418]]
printing an ep nov before normalisation:  40.976399984191616
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
probs:  [0.09738824251345256, 0.09791167570034663, 0.06442170263738932, 0.09978604054316893, 0.19017504027640772, 0.4503172983292348]
printing an ep nov before normalisation:  59.347444832443585
printing an ep nov before normalisation:  52.10268928238459
printing an ep nov before normalisation:  31.11622017804405
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
probs:  [0.09743847086213432, 0.09796217438145718, 0.06445490507900849, 0.09932145985813962, 0.19027318931683593, 0.45054980050242455]
printing an ep nov before normalisation:  66.38781800625105
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.322]
 [0.334]
 [0.495]
 [0.313]
 [0.327]
 [0.326]] [[50.444]
 [45.375]
 [47.361]
 [43.343]
 [50.723]
 [47.859]
 [51.186]] [[0.902]
 [0.773]
 [0.832]
 [0.898]
 [0.892]
 [0.837]
 [0.916]]
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
probs:  [0.0975908817806511, 0.09811376251469897, 0.06439725291904547, 0.09947091243178806, 0.1902797485529872, 0.4501474418008291]
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
printing an ep nov before normalisation:  57.30880758336708
printing an ep nov before normalisation:  35.3022025094648
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
probs:  [0.0975908817806511, 0.09811376251469897, 0.06439725291904547, 0.09947091243178806, 0.1902797485529872, 0.4501474418008291]
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
probs:  [0.0975908817806511, 0.09811376251469897, 0.06439725291904547, 0.09947091243178806, 0.1902797485529872, 0.4501474418008291]
printing an ep nov before normalisation:  53.535916817416805
printing an ep nov before normalisation:  26.215485034641546
printing an ep nov before normalisation:  25.904678575082805
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
probs:  [0.0975908817806511, 0.09811376251469897, 0.06439725291904547, 0.09947091243178806, 0.1902797485529872, 0.4501474418008291]
printing an ep nov before normalisation:  34.66377737935923
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
probs:  [0.09764581782220388, 0.09760575249689302, 0.06443347799762779, 0.09952690823044193, 0.1903869325111863, 0.45040111094164703]
printing an ep nov before normalisation:  15.050623659701614
printing an ep nov before normalisation:  52.52891950384382
actions average: 
K:  2  action  0 :  tensor([0.2774, 0.0098, 0.1246, 0.1431, 0.1748, 0.1466, 0.1237],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0297, 0.8936, 0.0134, 0.0147, 0.0087, 0.0109, 0.0291],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1517, 0.0139, 0.2252, 0.1685, 0.1436, 0.1706, 0.1264],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1388, 0.0125, 0.1359, 0.2439, 0.1733, 0.1553, 0.1402],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1766, 0.0151, 0.1322, 0.1531, 0.2294, 0.1581, 0.1355],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1576, 0.0084, 0.1422, 0.1366, 0.1233, 0.2966, 0.1352],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1981, 0.0173, 0.1590, 0.1550, 0.1326, 0.1752, 0.1627],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]
 [0.491]] [[76.889]
 [76.889]
 [76.889]
 [76.889]
 [76.889]
 [76.889]
 [76.889]] [[2.491]
 [2.491]
 [2.491]
 [2.491]
 [2.491]
 [2.491]
 [2.491]]
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
probs:  [0.0976832283124205, 0.09764305801854077, 0.0643838742990583, 0.09956924705623414, 0.1906673187418779, 0.4500532735718683]
actor:  1 policy actor:  1  step number:  78 total reward:  0.03333333333333255  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
probs:  [0.09740678504614998, 0.09736739041315752, 0.06475041877287414, 0.09925638605857584, 0.18859541631609236, 0.45262360339315]
Printing some Q and Qe and total Qs values:  [[0.37 ]
 [0.399]
 [0.351]
 [0.361]
 [0.361]
 [0.369]
 [0.369]] [[55.333]
 [54.617]
 [57.996]
 [58.89 ]
 [59.655]
 [50.744]
 [50.744]] [[1.456]
 [1.455]
 [1.548]
 [1.595]
 [1.626]
 [1.264]
 [1.264]]
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
probs:  [0.09755392316855117, 0.09751458826732791, 0.06469433124568014, 0.09940071973955046, 0.18860429047050903, 0.4522321471083812]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
probs:  [0.09764597935162206, 0.09760651957130842, 0.06468206592743223, 0.09949863906880312, 0.18842100542767481, 0.4521457906531593]
line 256 mcts: sample exp_bonus 38.91866650995848
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
probs:  [0.09779044413762215, 0.0977510432459549, 0.06462696338417631, 0.09964033899848299, 0.18843000016365063, 0.451761210070113]
actor:  1 policy actor:  1  step number:  64 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
probs:  [0.0974444771485378, 0.097406021321039, 0.06507645091490213, 0.09925000069508497, 0.1859099657101008, 0.45491308421033544]
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
probs:  [0.0974444771485378, 0.097406021321039, 0.06507645091490213, 0.09925000069508497, 0.1859099657101008, 0.45491308421033544]
printing an ep nov before normalisation:  54.49551316514673
printing an ep nov before normalisation:  53.64009823715183
siam score:  -0.8404355
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.238]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]] [[43.242]
 [49.91 ]
 [43.242]
 [43.242]
 [43.242]
 [43.242]
 [43.242]] [[1.763]
 [1.849]
 [1.763]
 [1.763]
 [1.763]
 [1.763]
 [1.763]]
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
probs:  [0.09755421591296595, 0.09751546031647931, 0.0649338760224875, 0.09937381378845915, 0.18670930632921842, 0.4539133276303897]
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
probs:  [0.09759072061054168, 0.09755186529563463, 0.06488644835773648, 0.09941500032407481, 0.18697520769700715, 0.4535807577150052]
Printing some Q and Qe and total Qs values:  [[0.439]
 [0.538]
 [0.439]
 [0.472]
 [0.439]
 [0.439]
 [0.439]] [[53.27 ]
 [57.982]
 [53.27 ]
 [53.81 ]
 [53.27 ]
 [53.27 ]
 [53.27 ]] [[2.172]
 [2.538]
 [2.172]
 [2.236]
 [2.172]
 [2.172]
 [2.172]]
siam score:  -0.83660746
Printing some Q and Qe and total Qs values:  [[1.433]
 [1.433]
 [1.433]
 [1.433]
 [1.433]
 [1.433]
 [1.433]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.433]
 [1.433]
 [1.433]
 [1.433]
 [1.433]
 [1.433]
 [1.433]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
probs:  [0.09764455796480583, 0.09760568118549137, 0.06492221925634377, 0.09946984544479515, 0.18652644548121497, 0.45383125066734886]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
probs:  [0.09764455796480583, 0.09760568118549137, 0.06492221925634377, 0.09946984544479515, 0.18652644548121497, 0.45383125066734886]
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.652]
 [0.67 ]
 [0.678]
 [0.684]
 [0.651]
 [0.651]] [[51.504]
 [53.771]
 [55.96 ]
 [54.821]
 [56.382]
 [56.458]
 [56.404]] [[2.366]
 [2.397]
 [2.506]
 [2.467]
 [2.537]
 [2.508]
 [2.505]]
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.445]
 [0.303]
 [0.311]
 [0.33 ]
 [0.338]
 [0.442]] [[45.566]
 [47.662]
 [45.217]
 [45.461]
 [45.608]
 [45.622]
 [45.332]] [[1.814]
 [2.015]
 [1.704]
 [1.729]
 [1.758]
 [1.767]
 [1.851]]
printing an ep nov before normalisation:  50.35746896636702
printing an ep nov before normalisation:  35.16181697589733
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
probs:  [0.0977177719112113, 0.0976786957803455, 0.06482764026567664, 0.09955241906289151, 0.18705542614685794, 0.45316804683301726]
actor:  1 policy actor:  1  step number:  63 total reward:  0.31999999999999995  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
probs:  [0.09727104184716215, 0.09723318210109284, 0.06540473357416249, 0.09904857902268024, 0.1838277410097292, 0.45721472244517314]
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
probs:  [0.09727104184716215, 0.09723318210109284, 0.06540473357416249, 0.09904857902268024, 0.1838277410097292, 0.45721472244517314]
maxi score, test score, baseline:  -0.9828166666666668 -1.0 -0.9828166666666668
probs:  [0.09730656763048474, 0.09726861115257177, 0.06535884077785424, 0.09908864642252244, 0.18408441959942567, 0.4568929144171412]
printing an ep nov before normalisation:  71.45437430422093
maxi score, test score, baseline:  -0.982862962962963 -1.0 -0.982862962962963
probs:  [0.097342352237076, 0.0973043739783107, 0.06531292673210728, 0.09912865130789969, 0.18434074016274046, 0.4565709555818659]
printing an ep nov before normalisation:  40.8225377290642
Printing some Q and Qe and total Qs values:  [[0.973]
 [0.973]
 [0.088]
 [0.088]
 [0.088]
 [0.973]
 [0.088]] [[ 0.001]
 [ 0.001]
 [29.714]
 [29.714]
 [29.714]
 [ 0.001]
 [29.714]] [[0.973]
 [0.973]
 [1.528]
 [1.528]
 [1.528]
 [0.973]
 [1.528]]
from probs:  [0.097342352237076, 0.0973043739783107, 0.06531292673210728, 0.09912865130789969, 0.18434074016274046, 0.4565709555818659]
maxi score, test score, baseline:  -0.982862962962963 -1.0 -0.982862962962963
probs:  [0.09739455778108187, 0.0973565591263337, 0.06534793107638694, 0.09918181617248668, 0.18390305327588086, 0.4568160825678299]
printing an ep nov before normalisation:  69.27546279825515
printing an ep nov before normalisation:  41.41936283222693
maxi score, test score, baseline:  -0.982862962962963 -1.0 -0.982862962962963
probs:  [0.09746573942676506, 0.09742754795584896, 0.06525649886646546, 0.09926206688433607, 0.18441320405627507, 0.4561749428103093]
printing an ep nov before normalisation:  47.741074562072754
maxi score, test score, baseline:  -0.982862962962963 -1.0 -0.982862962962963
probs:  [0.0976036895226991, 0.09756555237624366, 0.06520316189346602, 0.09939746184117729, 0.18442747800120488, 0.4558026563652091]
siam score:  -0.8292383
maxi score, test score, baseline:  -0.982862962962963 -1.0 -0.982862962962963
siam score:  -0.83091855
maxi score, test score, baseline:  -0.982862962962963 -1.0 -0.982862962962963
probs:  [0.09763952457920713, 0.097601291440385, 0.06515744382870113, 0.09943781187759858, 0.184681851194875, 0.45548207707923327]
siam score:  -0.8286015
maxi score, test score, baseline:  -0.982862962962963 -1.0 -0.982862962962963
probs:  [0.09763952457920713, 0.097601291440385, 0.06515744382870113, 0.09943781187759858, 0.184681851194875, 0.45548207707923327]
printing an ep nov before normalisation:  26.02902659164969
maxi score, test score, baseline:  -0.9829090090090091 -1.0 -0.9829090090090091
probs:  [0.09763982142333054, 0.09760166323146845, 0.06515737644327382, 0.09943779143217889, 0.18468174468287818, 0.4554816027868701]
printing an ep nov before normalisation:  0.1344878069517108
maxi score, test score, baseline:  -0.9829090090090091 -1.0 -0.9829090090090091
probs:  [0.09763982142333054, 0.09760166323146845, 0.06515737644327382, 0.09943779143217889, 0.18468174468287818, 0.4554816027868701]
maxi score, test score, baseline:  -0.9829090090090091 -1.0 -0.9829090090090091
probs:  [0.09763982142333054, 0.09760166323146845, 0.06515737644327382, 0.09943779143217889, 0.18468174468287818, 0.4554816027868701]
siam score:  -0.8279345
maxi score, test score, baseline:  -0.9829548068283919 -1.0 -0.9829548068283919
probs:  [0.09767591784929523, 0.09763773867292343, 0.06511163610869135, 0.09947808124065634, 0.18493576056428523, 0.45516086556414825]
printing an ep nov before normalisation:  38.273317351961325
printing an ep nov before normalisation:  40.289856052486975
actor:  1 policy actor:  1  step number:  67 total reward:  0.06666666666666621  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  39.73367691040039
printing an ep nov before normalisation:  5.3015541456559845
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.567]
 [0.458]
 [0.482]
 [0.482]
 [0.482]
 [0.538]] [[60.733]
 [66.855]
 [70.215]
 [55.006]
 [55.006]
 [55.006]
 [65.343]] [[0.632]
 [0.859]
 [0.774]
 [0.688]
 [0.688]
 [0.688]
 [0.819]]
printing an ep nov before normalisation:  42.686603874314066
printing an ep nov before normalisation:  53.72428894042969
maxi score, test score, baseline:  -0.9829548068283919 -1.0 -0.9829548068283919
probs:  [0.09626908828459059, 0.0966994082042831, 0.06556624460783521, 0.09955979557706245, 0.18355871169155666, 0.458346751634672]
maxi score, test score, baseline:  -0.9829548068283919 -1.0 -0.9829548068283919
probs:  [0.09626908828459059, 0.0966994082042831, 0.06556624460783521, 0.09955979557706245, 0.18355871169155666, 0.458346751634672]
maxi score, test score, baseline:  -0.9829548068283919 -1.0 -0.9829548068283919
probs:  [0.09626908828459059, 0.0966994082042831, 0.06556624460783521, 0.09955979557706245, 0.18355871169155666, 0.458346751634672]
UNIT TEST: sample policy line 217 mcts : [0.082 0.224 0.102 0.061 0.388 0.041 0.102]
maxi score, test score, baseline:  -0.9829548068283919 -1.0 -0.9829548068283919
probs:  [0.09626908828459059, 0.0966994082042831, 0.06556624460783521, 0.09955979557706245, 0.18355871169155666, 0.458346751634672]
maxi score, test score, baseline:  -0.9829548068283919 -1.0 -0.9829548068283919
probs:  [0.09626908828459059, 0.0966994082042831, 0.06556624460783521, 0.09955979557706245, 0.18355871169155666, 0.458346751634672]
maxi score, test score, baseline:  -0.9829548068283919 -1.0 -0.9829548068283919
maxi score, test score, baseline:  -0.9830003584229392 -1.0 -0.9830003584229392
probs:  [0.09640563479853, 0.09683543609896435, 0.06551370800088863, 0.09969148442276866, 0.18357370230634726, 0.45798003437250107]
printing an ep nov before normalisation:  2.4466765911256516
actions average: 
K:  2  action  0 :  tensor([0.2289, 0.0039, 0.1344, 0.1583, 0.1546, 0.1594, 0.1605],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0298, 0.8820, 0.0179, 0.0180, 0.0080, 0.0110, 0.0333],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1247, 0.0330, 0.3039, 0.1440, 0.1107, 0.1475, 0.1363],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1427, 0.0462, 0.1226, 0.2733, 0.1325, 0.1359, 0.1468],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2071, 0.0083, 0.1218, 0.1641, 0.1951, 0.1689, 0.1348],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1753, 0.0084, 0.1298, 0.1340, 0.1184, 0.3126, 0.1216],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1919, 0.0175, 0.1218, 0.1886, 0.1774, 0.1524, 0.1503],
       grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 53.6405514935199
maxi score, test score, baseline:  -0.9830456657730117 -1.0 -0.9830456657730117
probs:  [0.09596729451181583, 0.09688275978444173, 0.06554540804481342, 0.09973982852185996, 0.1836626877966795, 0.4582020213403896]
printing an ep nov before normalisation:  46.9730065190209
printing an ep nov before normalisation:  41.82936552543532
maxi score, test score, baseline:  -0.9830456657730117 -1.0 -0.9830456657730117
probs:  [0.09596729451181583, 0.09688275978444173, 0.06554540804481342, 0.09973982852185996, 0.1836626877966795, 0.4582020213403896]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  46.44520774460077
siam score:  -0.82992476
maxi score, test score, baseline:  -0.9830907308377898 -1.0 -0.9830907308377898
probs:  [0.09601814537121967, 0.09693417852519615, 0.06557987805057, 0.09979239026663525, 0.18323200196361625, 0.45844340582276266]
maxi score, test score, baseline:  -0.9830907308377898 -1.0 -0.9830907308377898
probs:  [0.09601814537121967, 0.09693417852519615, 0.06557987805057, 0.09979239026663525, 0.18323200196361625, 0.45844340582276266]
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.366]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]] [[41.141]
 [38.846]
 [39.081]
 [39.081]
 [39.081]
 [39.081]
 [39.081]] [[1.819]
 [1.723]
 [1.679]
 [1.679]
 [1.679]
 [1.679]
 [1.679]]
printing an ep nov before normalisation:  49.45049605800909
printing an ep nov before normalisation:  49.760958433404745
maxi score, test score, baseline:  -0.9831355555555557 -1.0 -0.9831355555555557
probs:  [0.096100336446875, 0.09701921837842672, 0.06556989321540987, 0.09988569116476027, 0.18305179277841352, 0.4583730680161146]
printing an ep nov before normalisation:  70.79662222375086
siam score:  -0.8389186
using another actor
from probs:  [0.09623516152584308, 0.09715287007375667, 0.06551810098196159, 0.10001505902745297, 0.1830672628736525, 0.4580115455173332]
maxi score, test score, baseline:  -0.9832244916003537 -1.0 -0.9832244916003537
printing an ep nov before normalisation:  37.32982374510708
maxi score, test score, baseline:  -0.9832686067019402 -1.0 -0.9832686067019402
probs:  [0.09623568457569669, 0.097153559474985, 0.06551798399478838, 0.10001503050760839, 0.18306701958969415, 0.45801072185722735]
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.565]
 [0.565]
 [0.537]
 [0.565]
 [0.565]
 [0.565]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.565]
 [0.565]
 [0.565]
 [0.537]
 [0.565]
 [0.565]
 [0.565]]
maxi score, test score, baseline:  -0.9832686067019402 -1.0 -0.9832686067019402
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.87509086771165
actor:  1 policy actor:  1  step number:  44 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.9832686067019402 -1.0 -0.9832686067019402
probs:  [0.09135082460747782, 0.14378611553713022, 0.06213188776785948, 0.0944875255782898, 0.17394552556144843, 0.43429812094779424]
maxi score, test score, baseline:  -0.9832686067019402 -1.0 -0.9832686067019402
maxi score, test score, baseline:  -0.9832686067019402 -1.0 -0.9832686067019402
probs:  [0.09141490074480775, 0.14399927644196123, 0.062112888206969176, 0.09456052002858534, 0.17374773239763894, 0.4341646821800375]
printing an ep nov before normalisation:  56.17952953828014
maxi score, test score, baseline:  -0.9832686067019402 -1.0 -0.9832686067019402
printing an ep nov before normalisation:  46.1934352500228
printing an ep nov before normalisation:  41.57060398761246
maxi score, test score, baseline:  -0.9832686067019402 -1.0 -0.9832686067019402
printing an ep nov before normalisation:  41.171110170259134
printing an ep nov before normalisation:  28.43687433374491
printing an ep nov before normalisation:  42.643026721530326
maxi score, test score, baseline:  -0.9833124890061566 -1.0 -0.9833124890061566
probs:  [0.09143400825954381, 0.1441409417705108, 0.062063175508849745, 0.09458671757950524, 0.17395899338897597, 0.4338161634926144]
printing an ep nov before normalisation:  54.52543404485476
maxi score, test score, baseline:  -0.9833561403508773 -1.0 -0.9833561403508773
probs:  [0.0914983949479186, 0.1443539464770288, 0.06204422008781844, 0.09465975316086463, 0.1737606532404847, 0.43368303208588477]
printing an ep nov before normalisation:  48.25875245248082
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.9833995625546808 -1.0 -0.9833995625546808
probs:  [0.09158381641351755, 0.14355722336834456, 0.06210188141694974, 0.09474785829358057, 0.17392240021428998, 0.4340868202933176]
maxi score, test score, baseline:  -0.9833995625546808 -1.0 -0.9833995625546808
probs:  [0.09158381641351755, 0.14355722336834456, 0.06210188141694974, 0.09474785829358057, 0.17392240021428998, 0.4340868202933176]
printing an ep nov before normalisation:  49.603328704833984
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.534]
 [0.518]
 [0.491]
 [0.506]
 [0.489]
 [0.49 ]] [[47.505]
 [46.625]
 [46.715]
 [42.661]
 [45.397]
 [46.21 ]
 [49.255]] [[2.369]
 [2.336]
 [2.327]
 [1.995]
 [2.216]
 [2.26 ]
 [2.49 ]]
printing an ep nov before normalisation:  38.27660083770752
maxi score, test score, baseline:  -0.9833995625546808 -1.0 -0.9833995625546808
probs:  [0.09160291860268857, 0.14369714655649896, 0.06205244784818318, 0.09477431583471277, 0.17413291240585568, 0.4337402587520609]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 36.974391562236924
printing an ep nov before normalisation:  33.37555590459503
actor:  1 policy actor:  1  step number:  55 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  40.95965945471433
maxi score, test score, baseline:  -0.9834427574171031 -1.0 -0.9834427574171031
probs:  [0.09021394761228956, 0.14162471134814072, 0.061050652352203215, 0.09334350152413286, 0.18704267198301816, 0.4267245151802154]
maxi score, test score, baseline:  -0.9834427574171031 -1.0 -0.9834427574171031
printing an ep nov before normalisation:  28.256278038024902
siam score:  -0.83348715
maxi score, test score, baseline:  -0.9834857267188861 -1.0 -0.9834857267188861
probs:  [0.08984126251557654, 0.14175280758928607, 0.06110581328868684, 0.09297733602992796, 0.1872119875729171, 0.4271107930036054]
printing an ep nov before normalisation:  50.55578107727236
actions average: 
K:  2  action  0 :  tensor([0.2680, 0.0440, 0.1185, 0.1337, 0.1668, 0.1567, 0.1122],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0399, 0.7751, 0.0284, 0.0460, 0.0312, 0.0322, 0.0471],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1297, 0.0229, 0.2012, 0.1658, 0.1209, 0.1927, 0.1669],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1289, 0.1012, 0.1100, 0.1992, 0.1540, 0.1993, 0.1075],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1589, 0.0048, 0.1018, 0.1923, 0.2873, 0.1581, 0.0968],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1437, 0.0024, 0.1252, 0.1686, 0.1440, 0.2994, 0.1167],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1263, 0.1330, 0.1091, 0.1799, 0.1017, 0.1372, 0.2128],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9834857267188861 -1.0 -0.9834857267188861
probs:  [0.08988111075331726, 0.14181571489383138, 0.06113289726286262, 0.09257473318408242, 0.18729508780885434, 0.4273004560970519]
maxi score, test score, baseline:  -0.9834857267188861 -1.0 -0.9834857267188861
probs:  [0.09006332686952431, 0.14195966370482893, 0.06112325813881155, 0.09275496454131453, 0.1868647373779644, 0.42723404936755627]
maxi score, test score, baseline:  -0.9834857267188861 -1.0 -0.9834857267188861
probs:  [0.09006332686952431, 0.14195966370482893, 0.06112325813881155, 0.09275496454131453, 0.1868647373779644, 0.42723404936755627]
actions average: 
K:  0  action  0 :  tensor([0.2116, 0.0103, 0.1467, 0.1601, 0.1578, 0.1717, 0.1418],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0033,     0.9657,     0.0082,     0.0041,     0.0009,     0.0015,
            0.0164], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.2106, 0.0083, 0.2419, 0.1368, 0.1204, 0.1515, 0.1307],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1377, 0.0487, 0.1180, 0.3038, 0.1355, 0.1304, 0.1260],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1670, 0.0064, 0.1481, 0.1349, 0.2580, 0.1492, 0.1364],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1831, 0.0051, 0.1574, 0.1505, 0.1535, 0.2101, 0.1403],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2215, 0.0300, 0.1346, 0.1354, 0.1379, 0.1500, 0.1905],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9834857267188861 -1.0 -0.9834857267188861
maxi score, test score, baseline:  -0.9834857267188861 -1.0 -0.9834857267188861
probs:  [0.09006332686952431, 0.14195966370482893, 0.06112325813881155, 0.09275496454131453, 0.1868647373779644, 0.42723404936755627]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.9834857267188861 -1.0 -0.9834857267188861
probs:  [0.09019336125567434, 0.14125765962356807, 0.06121144664613328, 0.0928888908996956, 0.186597025972926, 0.42785161560200263]
maxi score, test score, baseline:  -0.9834857267188861 -1.0 -0.9834857267188861
printing an ep nov before normalisation:  29.367701318722368
printing an ep nov before normalisation:  51.357354864778905
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]]
using explorer policy with actor:  1
from probs:  [0.09022443706274307, 0.14152118484893186, 0.061110594092147676, 0.0929322370074984, 0.18706694019550754, 0.4271446067931715]
printing an ep nov before normalisation:  40.53927934579198
maxi score, test score, baseline:  -0.9835709956709958 -1.0 -0.9835709956709958
maxi score, test score, baseline:  -0.9835709956709958 -1.0 -0.9835709956709958
probs:  [0.09030368295818993, 0.14164450417505423, 0.06116380031109263, 0.0921406315363905, 0.1872301865676413, 0.4275171944516314]
maxi score, test score, baseline:  -0.9835709956709958 -1.0 -0.9835709956709958
maxi score, test score, baseline:  -0.9835709956709958 -1.0 -0.9835709956709958
probs:  [0.09034218184519203, 0.14170492337340648, 0.06118985772783403, 0.09175329924041034, 0.18731006888184304, 0.4276996689313141]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.9836132987910191 -1.0 -0.9836132987910191
probs:  [0.0903424608933647, 0.1417048411453677, 0.06118983290160627, 0.09175331168220396, 0.1873100603047721, 0.4276994930726854]
siam score:  -0.82896906
maxi score, test score, baseline:  -0.9836553832902671 -1.0 -0.9836553832902671
printing an ep nov before normalisation:  45.0584941679484
printing an ep nov before normalisation:  29.083202342705697
Printing some Q and Qe and total Qs values:  [[0.612]
 [0.577]
 [0.563]
 [0.556]
 [0.574]
 [0.569]
 [0.579]] [[23.559]
 [23.481]
 [25.501]
 [26.147]
 [25.045]
 [24.564]
 [22.673]] [[1.837]
 [1.792]
 [2.026]
 [2.099]
 [1.981]
 [1.917]
 [1.695]]
printing an ep nov before normalisation:  16.77499532699585
maxi score, test score, baseline:  -0.9836553832902671 -1.0 -0.9836553832902671
probs:  [0.0905523023893399, 0.14025070241859355, 0.06133164752569336, 0.09196616486909626, 0.18720659444730683, 0.42869258834997015]
maxi score, test score, baseline:  -0.9836553832902671 -1.0 -0.9836553832902671
probs:  [0.09056861164567892, 0.14037981205524633, 0.061281634707686, 0.09198568316687997, 0.18744227979818165, 0.42834197862632706]
maxi score, test score, baseline:  -0.9836553832902671 -1.0 -0.9836553832902671
probs:  [0.09056861164567892, 0.14037981205524633, 0.061281634707686, 0.09198568316687997, 0.18744227979818165, 0.42834197862632706]
siam score:  -0.82798415
maxi score, test score, baseline:  -0.9836553832902671 -1.0 -0.9836553832902671
probs:  [0.09056861164567892, 0.14037981205524633, 0.061281634707686, 0.09198568316687997, 0.18744227979818165, 0.42834197862632706]
maxi score, test score, baseline:  -0.9836553832902671 -1.0 -0.9836553832902671
probs:  [0.09056861164567892, 0.14037981205524633, 0.061281634707686, 0.09198568316687997, 0.18744227979818165, 0.42834197862632706]
siam score:  -0.8276282
printing an ep nov before normalisation:  47.992052618142566
printing an ep nov before normalisation:  0.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.9836972508591066 -1.0 -0.9836972508591066
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.299623806194326
maxi score, test score, baseline:  -0.9836972508591066 -1.0 -0.9836972508591066
probs:  [0.0904000795769566, 0.1390882046224698, 0.06147425594054384, 0.0918516009259608, 0.18749499849518886, 0.42969086043888016]
maxi score, test score, baseline:  -0.9836972508591066 -1.0 -0.9836972508591066
probs:  [0.0904000795769566, 0.1390882046224698, 0.06147425594054384, 0.0918516009259608, 0.18749499849518886, 0.42969086043888016]
maxi score, test score, baseline:  -0.9836972508591066 -1.0 -0.9836972508591066
probs:  [0.0904000795769566, 0.1390882046224698, 0.06147425594054384, 0.0918516009259608, 0.18749499849518886, 0.42969086043888016]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  15.959108333627903
maxi score, test score, baseline:  -0.9836972508591066 -1.0 -0.9836972508591066
printing an ep nov before normalisation:  59.853521635373
maxi score, test score, baseline:  -0.9836972508591066 -1.0 -0.9836972508591066
probs:  [0.09000897266158986, 0.13927691989495306, 0.0614520339998935, 0.09191193775989207, 0.1878152725192837, 0.42953486316438766]
printing an ep nov before normalisation:  63.235845971123084
printing an ep nov before normalisation:  44.629230626825716
printing an ep nov before normalisation:  62.012744824992716
printing an ep nov before normalisation:  50.26813983917236
maxi score, test score, baseline:  -0.9837389031705228 -1.0 -0.9837389031705228
printing an ep nov before normalisation:  49.36589586976846
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.59284877159351
maxi score, test score, baseline:  -0.983780341880342 -1.0 -0.983780341880342
probs:  [0.09010303548030645, 0.13973094633239838, 0.06133645059853331, 0.09201941610344015, 0.18808584115623955, 0.4287243103290822]
printing an ep nov before normalisation:  13.27869100289604
maxi score, test score, baseline:  -0.983780341880342 -1.0 -0.983780341880342
probs:  [0.09015125040882982, 0.13980575691793515, 0.0613692494796788, 0.09206865802264043, 0.1876510923655395, 0.42895399280537627]
maxi score, test score, baseline:  -0.983780341880342 -1.0 -0.983780341880342
probs:  [0.09015125040882982, 0.13980575691793515, 0.0613692494796788, 0.09206865802264043, 0.1876510923655395, 0.42895399280537627]
printing an ep nov before normalisation:  32.32115395418015
maxi score, test score, baseline:  -0.983780341880342 -1.0 -0.983780341880342
probs:  [0.09022828340240217, 0.13907021507163728, 0.061421652253629506, 0.09214733183926566, 0.18781156092449802, 0.42932095650856744]
printing an ep nov before normalisation:  33.22174124794667
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.231]
 [0.457]
 [0.451]
 [0.461]
 [0.443]
 [0.449]] [[14.578]
 [22.749]
 [14.362]
 [14.726]
 [14.16 ]
 [16.228]
 [14.199]] [[0.762]
 [0.877]
 [0.76 ]
 [0.768]
 [0.756]
 [0.822]
 [0.745]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
from probs:  [0.09045439020550584, 0.13854119500141615, 0.061380619699353656, 0.09237687833469917, 0.18821257548123843, 0.42903434127778683]
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.653]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]] [[40.807]
 [41.289]
 [40.807]
 [40.807]
 [40.807]
 [40.807]
 [40.807]] [[2.139]
 [2.236]
 [2.139]
 [2.139]
 [2.139]
 [2.139]
 [2.139]]
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.613]
 [0.394]
 [0.421]
 [0.317]
 [0.317]
 [0.317]] [[53.946]
 [32.043]
 [39.866]
 [35.616]
 [53.946]
 [53.946]
 [53.946]] [[0.317]
 [0.613]
 [0.394]
 [0.421]
 [0.317]
 [0.317]
 [0.317]]
maxi score, test score, baseline:  -0.983780341880342 -1.0 -0.983780341880342
probs:  [0.0905293957661649, 0.13782636544819996, 0.0614314814782687, 0.09245348039051479, 0.18836876254150678, 0.4293905143753448]
maxi score, test score, baseline:  -0.983780341880342 -1.0 -0.983780341880342
probs:  [0.0905293957661649, 0.13782636544819996, 0.0614314814782687, 0.09245348039051479, 0.18836876254150678, 0.4293905143753448]
actor:  1 policy actor:  1  step number:  58 total reward:  0.5533333333333338  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.983780341880342 -1.0 -0.983780341880342
probs:  [0.09032218031912141, 0.13503344018529156, 0.06233183981531528, 0.09217302719635254, 0.18443742045706316, 0.4357020920268559]
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.315]
 [0.148]
 [0.363]
 [0.307]
 [0.335]
 [0.326]] [[65.172]
 [69.991]
 [72.353]
 [67.653]
 [62.94 ]
 [70.602]
 [64.927]] [[1.59 ]
 [1.733]
 [1.636]
 [1.712]
 [1.515]
 [1.771]
 [1.593]]
siam score:  -0.8264961
printing an ep nov before normalisation:  61.205919457548674
printing an ep nov before normalisation:  63.78183004081902
actions average: 
K:  4  action  0 :  tensor([0.2665, 0.0173, 0.1168, 0.1520, 0.1485, 0.1475, 0.1513],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0626, 0.7660, 0.0313, 0.0408, 0.0188, 0.0258, 0.0546],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1556, 0.0726, 0.1544, 0.1511, 0.1435, 0.1346, 0.1882],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1422, 0.0508, 0.1377, 0.1732, 0.1303, 0.1841, 0.1818],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2520, 0.0122, 0.1062, 0.1489, 0.1961, 0.1372, 0.1474],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1840, 0.0471, 0.1251, 0.1524, 0.1398, 0.2016, 0.1500],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1615, 0.0851, 0.1084, 0.1453, 0.1208, 0.1320, 0.2468],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  50.55421396089123
printing an ep nov before normalisation:  70.46990024900737
printing an ep nov before normalisation:  38.08206558227539
actions average: 
K:  4  action  0 :  tensor([0.3400, 0.0787, 0.0939, 0.1107, 0.1123, 0.1265, 0.1379],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0393, 0.7814, 0.0278, 0.0413, 0.0241, 0.0270, 0.0591],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1591, 0.0063, 0.1860, 0.1603, 0.1144, 0.1832, 0.1908],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1502, 0.0774, 0.1051, 0.2643, 0.1176, 0.1397, 0.1457],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1869, 0.0245, 0.1213, 0.1493, 0.1576, 0.1751, 0.1853],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1559, 0.0236, 0.1374, 0.1436, 0.1316, 0.2264, 0.1814],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1899, 0.0176, 0.1331, 0.1509, 0.1486, 0.1761, 0.1838],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  43.804659568849935
printing an ep nov before normalisation:  57.23237034350606
maxi score, test score, baseline:  -0.9838215686274511 -1.0 -0.9838215686274511
probs:  [0.09048653895595368, 0.13477880437580583, 0.06227020171898074, 0.09235206169431678, 0.18484303314073525, 0.4352693601142077]
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.458]
 [0.44 ]
 [0.431]
 [0.43 ]
 [0.423]
 [0.432]] [[51.151]
 [47.022]
 [50.052]
 [51.413]
 [51.338]
 [52.944]
 [50.853]] [[1.808]
 [1.608]
 [1.762]
 [1.829]
 [1.824]
 [1.907]
 [1.798]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9838215686274511 -1.0 -0.9838215686274511
probs:  [0.09048653895595368, 0.13477880437580583, 0.06227020171898074, 0.09235206169431678, 0.18484303314073525, 0.4352693601142077]
maxi score, test score, baseline:  -0.9838215686274511 -1.0 -0.9838215686274511
probs:  [0.0905236495862278, 0.1348341070772493, 0.06229572311103716, 0.09197953892129597, 0.18491889861999317, 0.43544808268419666]
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.739]
 [0.646]
 [0.638]
 [0.646]
 [0.646]
 [0.646]] [[47.102]
 [46.188]
 [47.102]
 [47.748]
 [47.102]
 [47.102]
 [47.102]] [[2.278]
 [2.321]
 [2.278]
 [2.305]
 [2.278]
 [2.278]
 [2.278]]
maxi score, test score, baseline:  -0.9838215686274511 -1.0 -0.9838215686274511
probs:  [0.09059310829350621, 0.13416979957838907, 0.0623434906344419, 0.0920501163775301, 0.18506089346290983, 0.43578259165322275]
maxi score, test score, baseline:  -0.9838215686274511 -1.0 -0.9838215686274511
probs:  [0.09059310829350621, 0.13416979957838907, 0.0623434906344419, 0.0920501163775301, 0.18506089346290983, 0.43578259165322275]
from probs:  [0.09059310829350621, 0.13416979957838907, 0.0623434906344419, 0.0920501163775301, 0.18506089346290983, 0.43578259165322275]
maxi score, test score, baseline:  -0.9838215686274511 -1.0 -0.9838215686274511
probs:  [0.09019981084085144, 0.13422780978888552, 0.06237041485688929, 0.09208989744625576, 0.1851409290281761, 0.4359711380389417]
printing an ep nov before normalisation:  66.90192953929535
maxi score, test score, baseline:  -0.9838215686274511 -1.0 -0.9838215686274511
probs:  [0.09019981084085144, 0.13422780978888552, 0.06237041485688929, 0.09208989744625576, 0.1851409290281761, 0.4359711380389417]
actor:  1 policy actor:  1  step number:  68 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9838215686274511 -1.0 -0.9838215686274511
probs:  [0.08888979222569211, 0.1322774038559037, 0.061465174852536816, 0.09075238751828171, 0.19698336665216962, 0.4296318748954161]
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.378]
 [0.396]
 [0.402]
 [0.396]
 [0.396]
 [0.4  ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.396]
 [0.378]
 [0.396]
 [0.402]
 [0.396]
 [0.396]
 [0.4  ]]
siam score:  -0.8283705
printing an ep nov before normalisation:  72.46239799798778
printing an ep nov before normalisation:  34.96608938421927
printing an ep nov before normalisation:  43.855396448233364
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.458]
 [0.322]
 [0.315]
 [0.316]
 [0.426]
 [0.313]] [[36.542]
 [43.718]
 [36.67 ]
 [36.166]
 [35.952]
 [43.04 ]
 [36.019]] [[1.237]
 [1.741]
 [1.26 ]
 [1.229]
 [1.219]
 [1.676]
 [1.22 ]]
Printing some Q and Qe and total Qs values:  [[0.347]
 [0.339]
 [0.347]
 [0.336]
 [0.347]
 [0.347]
 [0.347]] [[23.187]
 [34.263]
 [23.187]
 [26.953]
 [23.187]
 [23.187]
 [23.187]] [[0.347]
 [0.339]
 [0.347]
 [0.336]
 [0.347]
 [0.347]
 [0.347]]
printing an ep nov before normalisation:  21.686010360717773
printing an ep nov before normalisation:  42.318964514273375
actions average: 
K:  1  action  0 :  tensor([0.2678, 0.0011, 0.1339, 0.1377, 0.1403, 0.1563, 0.1630],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0213, 0.8163, 0.0327, 0.0290, 0.0202, 0.0489, 0.0315],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1604, 0.0693, 0.2775, 0.1140, 0.1074, 0.1352, 0.1362],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1252, 0.0277, 0.1315, 0.2747, 0.1415, 0.1554, 0.1440],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1867, 0.0171, 0.1307, 0.1362, 0.2221, 0.1488, 0.1584],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1902, 0.0034, 0.1372, 0.1416, 0.1425, 0.2265, 0.1585],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1793, 0.0351, 0.1283, 0.1545, 0.1468, 0.1614, 0.1946],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.393]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.405]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.41 ]
 [0.393]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.405]]
maxi score, test score, baseline:  -0.9838625850340137 -1.0 -0.9838625850340137
probs:  [0.0889022320275124, 0.13238522435536895, 0.06141682709539748, 0.09076869418686125, 0.19723407906698015, 0.4292929432678797]
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.462]
 [0.438]
 [0.433]
 [0.431]
 [0.425]
 [0.422]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.425]
 [0.462]
 [0.438]
 [0.433]
 [0.431]
 [0.425]
 [0.422]]
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]
 [0.399]] [[67.357]
 [67.357]
 [67.357]
 [67.357]
 [67.357]
 [67.357]
 [67.357]] [[1.998]
 [1.998]
 [1.998]
 [1.998]
 [1.998]
 [1.998]
 [1.998]]
printing an ep nov before normalisation:  72.19763199986639
maxi score, test score, baseline:  -0.9839439932318106 -1.0 -0.9839439932318106
probs:  [0.08901255694630962, 0.13272801510227958, 0.06137921590054498, 0.0904865917137697, 0.19736478123585224, 0.429028839101244]
printing an ep nov before normalisation:  27.209125066549543
printing an ep nov before normalisation:  54.22495100264503
printing an ep nov before normalisation:  45.90056120023146
printing an ep nov before normalisation:  86.038293489344
printing an ep nov before normalisation:  62.46851478468127
maxi score, test score, baseline:  -0.9839439932318106 -1.0 -0.9839439932318106
probs:  [0.08875522167813187, 0.13231274152187267, 0.06143749985290914, 0.09065918365198684, 0.19739871916646753, 0.42943663412863203]
Printing some Q and Qe and total Qs values:  [[0.673]
 [0.649]
 [0.635]
 [0.626]
 [0.639]
 [0.646]
 [0.658]] [[53.254]
 [47.511]
 [52.531]
 [52.278]
 [51.638]
 [51.008]
 [51.17 ]] [[2.673]
 [2.311]
 [2.592]
 [2.569]
 [2.544]
 [2.514]
 [2.535]]
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.316]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]] [[45.683]
 [49.303]
 [45.683]
 [45.683]
 [45.683]
 [45.683]
 [45.683]] [[1.92 ]
 [2.052]
 [1.92 ]
 [1.92 ]
 [1.92 ]
 [1.92 ]
 [1.92 ]]
maxi score, test score, baseline:  -0.9839439932318106 -1.0 -0.9839439932318106
probs:  [0.08875522167813187, 0.13231274152187267, 0.06143749985290914, 0.09065918365198684, 0.19739871916646753, 0.42943663412863203]
printing an ep nov before normalisation:  46.714600949105176
actor:  1 policy actor:  1  step number:  48 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  2.0
using another actor
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.605]
 [0.564]
 [0.566]
 [0.558]
 [0.558]
 [0.558]] [[56.118]
 [49.73 ]
 [62.954]
 [55.64 ]
 [56.118]
 [56.118]
 [56.118]] [[2.21 ]
 [1.939]
 [2.556]
 [2.195]
 [2.21 ]
 [2.21 ]
 [2.21 ]]
line 256 mcts: sample exp_bonus 38.64401392881818
maxi score, test score, baseline:  -0.9839439932318106 -1.0 -0.9839439932318106
probs:  [0.08862961596480619, 0.13117182468952523, 0.06194866103574726, 0.09048919722623344, 0.19474067011373605, 0.43302003096995184]
maxi score, test score, baseline:  -0.9839843881856541 -1.0 -0.9839843881856541
Printing some Q and Qe and total Qs values:  [[0.342]
 [0.382]
 [0.342]
 [0.342]
 [0.342]
 [0.342]
 [0.342]] [[51.273]
 [49.46 ]
 [51.273]
 [51.273]
 [51.273]
 [52.597]
 [51.273]] [[1.488]
 [1.458]
 [1.488]
 [1.488]
 [1.488]
 [1.539]
 [1.488]]
maxi score, test score, baseline:  -0.9839843881856541 -1.0 -0.9839843881856541
probs:  [0.08862986125631767, 0.1311717604997878, 0.0619486249194594, 0.09048921024270208, 0.19474076697418202, 0.43301977610755116]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9839843881856541 -1.0 -0.9839843881856541
maxi score, test score, baseline:  -0.9839843881856541 -1.0 -0.9839843881856541
probs:  [0.08862986125631767, 0.1311717604997878, 0.0619486249194594, 0.09048921024270208, 0.19474076697418202, 0.43301977610755116]
maxi score, test score, baseline:  -0.9839843881856541 -1.0 -0.9839843881856541
probs:  [0.08862986125631767, 0.1311717604997878, 0.0619486249194594, 0.09048921024270208, 0.19474076697418202, 0.43301977610755116]
actor:  1 policy actor:  1  step number:  51 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  58 total reward:  0.20666666666666644  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9839843881856541 -1.0 -0.9839843881856541
probs:  [0.06693185092679792, 0.11496369752514445, 0.0945706895910776, 0.06903114519609446, 0.18673615765704757, 0.46776645910383785]
printing an ep nov before normalisation:  65.26101112365723
maxi score, test score, baseline:  -0.9839843881856541 -1.0 -0.9839843881856541
probs:  [0.06693185092679792, 0.11496369752514445, 0.0945706895910776, 0.06903114519609446, 0.18673615765704757, 0.46776645910383785]
actor:  1 policy actor:  1  step number:  59 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9839843881856541 -1.0 -0.9839843881856541
maxi score, test score, baseline:  -0.9839843881856541 -1.0 -0.9839843881856541
probs:  [0.06455293419599914, 0.11101968695914781, 0.09129117504718162, 0.10157105243362319, 0.18045347717005544, 0.4511116741939928]
printing an ep nov before normalisation:  44.805047152939714
from probs:  [0.06455301275702889, 0.11101954684392905, 0.09129050976472447, 0.10157102249310306, 0.1804536878285709, 0.45111222031264353]
maxi score, test score, baseline:  -0.9840245791245792 -1.0 -0.9840245791245792
probs:  [0.0645074216895305, 0.1110865580600121, 0.09130971163432544, 0.10161513711404974, 0.18068895869618645, 0.45079221280589576]
printing an ep nov before normalisation:  31.490294221437786
maxi score, test score, baseline:  -0.9840245791245792 -1.0 -0.9840245791245792
probs:  [0.0645074216895305, 0.1110865580600121, 0.09130971163432544, 0.10161513711404974, 0.18068895869618645, 0.45079221280589576]
maxi score, test score, baseline:  -0.9840645675902604 -1.0 -0.9840645675902604
maxi score, test score, baseline:  -0.9840645675902604 -1.0 -0.9840645675902604
probs:  [0.06450749990001232, 0.11108641852789156, 0.09130904819835275, 0.10161510738716735, 0.18068916950832598, 0.45079275647825007]
maxi score, test score, baseline:  -0.9840645675902604 -1.0 -0.9840645675902604
probs:  [0.06454572771946737, 0.11115230662845957, 0.09136319176643469, 0.10167537107359309, 0.18020302807116342, 0.4510603747408818]
Printing some Q and Qe and total Qs values:  [[0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]] [[61.022]
 [61.022]
 [61.022]
 [61.022]
 [61.022]
 [61.022]
 [61.022]] [[2.377]
 [2.377]
 [2.377]
 [2.377]
 [2.377]
 [2.377]
 [2.377]]
printing an ep nov before normalisation:  43.97798478384259
maxi score, test score, baseline:  -0.9841043551088778 -1.0 -0.9841043551088778
probs:  [0.06454580580524859, 0.11115216811470088, 0.0913625316326883, 0.10167534183511955, 0.1802032350547148, 0.4510609175575278]
printing an ep nov before normalisation:  38.42767000198364
printing an ep nov before normalisation:  34.475821922529825
maxi score, test score, baseline:  -0.9841043551088778 -1.0 -0.9841043551088778
probs:  [0.06462756491376971, 0.11129308483948014, 0.09080007609364919, 0.1018042295916504, 0.1798417629323454, 0.45163328162910515]
maxi score, test score, baseline:  -0.9841043551088778 -1.0 -0.9841043551088778
probs:  [0.06462756491376971, 0.11129308483948014, 0.09080007609364919, 0.1018042295916504, 0.1798417629323454, 0.45163328162910515]
actor:  1 policy actor:  1  step number:  65 total reward:  0.09333333333333238  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]] [[29.132]
 [29.132]
 [29.132]
 [29.132]
 [29.132]
 [29.132]
 [29.132]] [[2.368]
 [2.368]
 [2.368]
 [2.368]
 [2.368]
 [2.368]
 [2.368]]
maxi score, test score, baseline:  -0.9841043551088778 -1.0 -0.9841043551088778
probs:  [0.06285560690243026, 0.1363405687656816, 0.08766014509713216, 0.09901085651213015, 0.1749043375789461, 0.4392284851436797]
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.711]
 [0.605]
 [0.641]
 [0.614]
 [0.619]
 [0.62 ]] [[53.409]
 [45.132]
 [56.306]
 [50.644]
 [56.851]
 [56.471]
 [55.959]] [[2.356]
 [1.894]
 [2.491]
 [2.171]
 [2.534]
 [2.516]
 [2.484]]
maxi score, test score, baseline:  -0.9841043551088778 -1.0 -0.9841043551088778
printing an ep nov before normalisation:  58.26457142657544
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.589]
 [0.533]
 [0.559]
 [0.597]
 [0.533]
 [0.538]] [[51.51 ]
 [46.444]
 [52.472]
 [54.201]
 [56.462]
 [52.472]
 [49.316]] [[1.947]
 [1.729]
 [1.958]
 [2.067]
 [2.212]
 [1.958]
 [1.814]]
printing an ep nov before normalisation:  66.20692955049887
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.928207192386516
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.39999999999999947  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.603]
 [0.507]
 [0.498]
 [0.497]
 [0.514]
 [0.497]] [[35.766]
 [40.167]
 [35.958]
 [36.267]
 [36.085]
 [36.417]
 [33.925]] [[1.503]
 [1.895]
 [1.546]
 [1.556]
 [1.544]
 [1.581]
 [1.414]]
printing an ep nov before normalisation:  73.1960070204203
maxi score, test score, baseline:  -0.9841043551088778 -1.0 -0.9841043551088778
probs:  [0.06333483438538619, 0.13510286710909705, 0.08755983040792803, 0.09864533970961213, 0.17276561812596933, 0.44259151026200727]
maxi score, test score, baseline:  -0.9841043551088778 -1.0 -0.9841043551088778
probs:  [0.06333483438538619, 0.13510286710909705, 0.08755983040792803, 0.09864533970961213, 0.17276561812596933, 0.44259151026200727]
maxi score, test score, baseline:  -0.9841043551088778 -1.0 -0.9841043551088778
probs:  [0.06333483438538619, 0.13510286710909705, 0.08755983040792803, 0.09864533970961213, 0.17276561812596933, 0.44259151026200727]
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.337]
 [0.422]
 [0.457]
 [0.484]
 [0.586]
 [0.639]] [[44.002]
 [38.906]
 [45.204]
 [45.881]
 [46.275]
 [46.395]
 [45.799]] [[1.968]
 [1.444]
 [1.992]
 [2.076]
 [2.132]
 [2.242]
 [2.252]]
maxi score, test score, baseline:  -0.9841043551088778 -1.0 -0.9841043551088778
probs:  [0.06328801816063466, 0.1352237794544672, 0.08756963025289247, 0.09868104742126423, 0.17297455182479118, 0.44226297288595035]
maxi score, test score, baseline:  -0.9841043551088778 -1.0 -0.9841043551088778
maxi score, test score, baseline:  -0.9841439431913117 -1.0 -0.9841439431913117
probs:  [0.063323348753498, 0.13529941631112258, 0.08761781883361386, 0.098736042468676, 0.17251306445053943, 0.44251030918255013]
printing an ep nov before normalisation:  50.557279898543754
maxi score, test score, baseline:  -0.9841833333333334 -1.0 -0.9841833333333334
probs:  [0.06327667236864942, 0.13542065479667734, 0.0876270744867301, 0.09877181516367729, 0.17272103596753366, 0.4421827472167322]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9842225270157939 -1.0 -0.9842225270157939
probs:  [0.06323002964864251, 0.1355418059358588, 0.08763632326376132, 0.09880756205021238, 0.17292885760149745, 0.44185542150002755]
printing an ep nov before normalisation:  41.66660431631701
maxi score, test score, baseline:  -0.9842225270157939 -1.0 -0.9842225270157939
probs:  [0.06323002964864251, 0.1355418059358588, 0.08763632326376132, 0.09880756205021238, 0.17292885760149745, 0.44185542150002755]
maxi score, test score, baseline:  -0.9842225270157939 -1.0 -0.9842225270157939
probs:  [0.06327111239290081, 0.13562997135428895, 0.08769329710666253, 0.09822122343788237, 0.17304136597569772, 0.4421430297325676]
printing an ep nov before normalisation:  62.9202204524238
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.664]
 [0.578]
 [0.576]
 [0.568]
 [0.598]
 [0.624]] [[41.42 ]
 [42.677]
 [43.358]
 [41.843]
 [43.334]
 [43.931]
 [41.501]] [[2.089]
 [2.372]
 [2.341]
 [2.217]
 [2.329]
 [2.408]
 [2.238]]
printing an ep nov before normalisation:  60.32238006591797
maxi score, test score, baseline:  -0.9842225270157939 -1.0 -0.9842225270157939
probs:  [0.06327111239290081, 0.13562997135428895, 0.08769329710666253, 0.09822122343788237, 0.17304136597569772, 0.4421430297325676]
actor:  1 policy actor:  1  step number:  53 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9842225270157939 -1.0 -0.9842225270157939
probs:  [0.06371264608528512, 0.13448165612147314, 0.08759823328112673, 0.09789484291384519, 0.17107105788142976, 0.44524156371684004]
printing an ep nov before normalisation:  52.8092318620566
maxi score, test score, baseline:  -0.9842225270157939 -1.0 -0.9842225270157939
probs:  [0.06357683698362941, 0.13475060608123315, 0.08787644867852736, 0.09817102342154102, 0.17133277678168568, 0.44429230805338343]
printing an ep nov before normalisation:  47.81506487919638
maxi score, test score, baseline:  -0.9842615257048094 -1.0 -0.9842615257048094
probs:  [0.06357688560897135, 0.13475077031283328, 0.08787581709275058, 0.09817096582051067, 0.17133291622589822, 0.444292644939036]
siam score:  -0.81402344
maxi score, test score, baseline:  -0.9842615257048094 -1.0 -0.9842615257048094
probs:  [0.06357033714208518, 0.13495247239288008, 0.08732385677114225, 0.09826563757161136, 0.17164165548574872, 0.44424604063653256]
maxi score, test score, baseline:  -0.9842615257048094 -1.0 -0.9842615257048094
printing an ep nov before normalisation:  51.78950971474943
maxi score, test score, baseline:  -0.9842615257048094 -1.0 -0.9842615257048094
probs:  [0.06352476649615858, 0.13507122171328936, 0.08733296615870152, 0.09829993467660819, 0.17184486242723543, 0.44392624852800683]
maxi score, test score, baseline:  -0.9842615257048094 -1.0 -0.9842615257048094
probs:  [0.06352476649615858, 0.13507122171328936, 0.08733296615870152, 0.09829993467660819, 0.17184486242723543, 0.44392624852800683]
Printing some Q and Qe and total Qs values:  [[0.61 ]
 [0.505]
 [0.622]
 [0.619]
 [0.62 ]
 [0.621]
 [0.569]] [[31.809]
 [34.085]
 [31.7  ]
 [31.853]
 [31.791]
 [31.738]
 [33.199]] [[1.94 ]
 [1.965]
 [1.946]
 [1.951]
 [1.949]
 [1.947]
 [1.977]]
maxi score, test score, baseline:  -0.9842615257048094 -1.0 -0.9842615257048094
probs:  [0.06347922834417322, 0.13518988636003748, 0.08734206905085358, 0.0983342073262108, 0.17204792447293962, 0.44360668444578527]
using explorer policy with actor:  1
siam score:  -0.8133708
maxi score, test score, baseline:  -0.9842615257048094 -1.0 -0.9842615257048094
probs:  [0.06347922834417322, 0.13518988636003748, 0.08734206905085358, 0.0983342073262108, 0.17204792447293962, 0.44360668444578527]
printing an ep nov before normalisation:  24.420242130264327
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9842615257048094 -1.0 -0.9842615257048094
probs:  [0.06344898112129967, 0.13459751690467417, 0.08744400286181231, 0.09849702878332715, 0.1726190609106503, 0.4433934094182365]
printing an ep nov before normalisation:  57.60579338272645
maxi score, test score, baseline:  -0.9842615257048094 -1.0 -0.9842615257048094
probs:  [0.06350889972639233, 0.1337791771985268, 0.08752662905755408, 0.0985901149649266, 0.17278229217408506, 0.44381288687851506]
UNIT TEST: sample policy line 217 mcts : [0.122 0.429 0.061 0.082 0.    0.184 0.122]
Printing some Q and Qe and total Qs values:  [[0.308]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]] [[20.616]
 [18.939]
 [18.939]
 [18.939]
 [18.939]
 [18.939]
 [18.939]] [[0.734]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9842615257048094 -1.0 -0.9842615257048094
probs:  [0.06356802142026378, 0.1329717213254325, 0.08760815633347871, 0.09868196310998392, 0.1729433524786785, 0.44422678533216253]
maxi score, test score, baseline:  -0.9842615257048094 -1.0 -0.9842615257048094
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9842615257048094 -1.0 -0.9842615257048094
probs:  [0.06352286115960377, 0.1330848694363511, 0.08761783111210107, 0.09871689700764978, 0.17314767503484801, 0.4439098662494462]
using another actor
maxi score, test score, baseline:  -0.9842615257048094 -1.0 -0.9842615257048094
probs:  [0.06356335078780276, 0.13316979124779613, 0.08767371121200665, 0.09814163503343785, 0.17325818664687243, 0.44419332507208414]
printing an ep nov before normalisation:  51.36176294543883
maxi score, test score, baseline:  -0.9843389438943896 -1.0 -0.9843389438943896
probs:  [0.06338328305964087, 0.13355065246929365, 0.08796125999801095, 0.09845144593460717, 0.17371987628735697, 0.44293348225109036]
maxi score, test score, baseline:  -0.9843389438943896 -1.0 -0.9843389438943896
probs:  [0.06338328305964087, 0.13355065246929365, 0.08796125999801095, 0.09845144593460717, 0.17371987628735697, 0.44293348225109036]
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.482]
 [0.444]
 [0.429]
 [0.407]
 [0.409]
 [0.401]] [[50.727]
 [51.585]
 [51.598]
 [51.629]
 [51.313]
 [52.349]
 [51.646]] [[2.114]
 [2.243]
 [2.206]
 [2.193]
 [2.149]
 [2.221]
 [2.166]]
maxi score, test score, baseline:  -0.9843389438943896 -1.0 -0.9843389438943896
probs:  [0.06338328305964087, 0.13355065246929365, 0.08796125999801095, 0.09845144593460717, 0.17371987628735697, 0.44293348225109036]
printing an ep nov before normalisation:  40.95028659803596
maxi score, test score, baseline:  -0.9843389438943896 -1.0 -0.9843389438943896
probs:  [0.06338328305964087, 0.13355065246929365, 0.08796125999801095, 0.09845144593460717, 0.17371987628735697, 0.44293348225109036]
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.63926356767061
maxi score, test score, baseline:  -0.9843389438943896 -1.0 -0.9843389438943896
probs:  [0.06333819082351627, 0.13366389206336002, 0.08797162767300722, 0.09848548458875457, 0.173923757254728, 0.4426170475966339]
actor:  1 policy actor:  1  step number:  51 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  65 total reward:  0.1599999999999997  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  32.69683619817325
from probs:  [0.06271424587673596, 0.13062488415155932, 0.08650174304859068, 0.09665454239919342, 0.18524941315068183, 0.4382551713732388]
printing an ep nov before normalisation:  43.158530889487494
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.82632541656494
maxi score, test score, baseline:  -0.9843389438943896 -1.0 -0.9843389438943896
probs:  [0.06270551339595831, 0.13080686558542243, 0.0865598131604612, 0.09674112468762279, 0.18499337073744646, 0.4381933124330888]
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.768]
 [0.756]
 [0.789]
 [0.511]
 [0.795]
 [0.833]] [[33.522]
 [31.624]
 [30.757]
 [32.377]
 [34.358]
 [35.352]
 [32.994]] [[1.715]
 [1.692]
 [1.625]
 [1.761]
 [1.608]
 [1.955]
 [1.843]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9843389438943896 -1.0 -0.9843389438943896
probs:  [0.06166688226980733, 0.12863782692727022, 0.08512522689894977, 0.0951375398546302, 0.19851052715409262, 0.43092199689524996]
siam score:  -0.82133514
maxi score, test score, baseline:  -0.9843389438943896 -1.0 -0.9843389438943896
probs:  [0.06166688226980733, 0.12863782692727022, 0.08512522689894977, 0.0951375398546302, 0.19851052715409262, 0.43092199689524996]
maxi score, test score, baseline:  -0.9843389438943896 -1.0 -0.9843389438943896
probs:  [0.06161972724913843, 0.12873620414606674, 0.08512904839422357, 0.09516311876028731, 0.198760742316844, 0.4305911591334399]
maxi score, test score, baseline:  -0.9843389438943896 -1.0 -0.9843389438943896
probs:  [0.06161972724913843, 0.12873620414606674, 0.08512904839422357, 0.09516311876028731, 0.198760742316844, 0.4305911591334399]
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.502]
 [0.436]
 [0.439]
 [0.433]
 [0.42 ]
 [0.425]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.443]
 [0.502]
 [0.436]
 [0.439]
 [0.433]
 [0.42 ]
 [0.425]]
printing an ep nov before normalisation:  62.04127497756193
printing an ep nov before normalisation:  55.614151184045475
maxi score, test score, baseline:  -0.9844155993431857 -1.0 -0.9844155993431857
printing an ep nov before normalisation:  45.589680671691895
printing an ep nov before normalisation:  42.63469219207764
maxi score, test score, baseline:  -0.9844155993431857 -1.0 -0.9844155993431857
probs:  [0.0615758223168255, 0.12896805076886997, 0.0848479373469546, 0.09545435573126551, 0.19886789848926822, 0.43028593534681614]
printing an ep nov before normalisation:  68.48486699623729
maxi score, test score, baseline:  -0.9844155993431857 -1.0 -0.9844155993431857
from probs:  [0.061668999692011, 0.1282811022690115, 0.084976406763005, 0.09559890972538564, 0.19853632186839212, 0.4309382596821948]
maxi score, test score, baseline:  -0.9844155993431857 -1.0 -0.9844155993431857
probs:  [0.06176138334647928, 0.12760302901136547, 0.08510378182716102, 0.09574223235298934, 0.19820454619399974, 0.431585027268005]
maxi score, test score, baseline:  -0.9844155993431857 -1.0 -0.9844155993431857
probs:  [0.06176138334647928, 0.12760302901136547, 0.08510378182716102, 0.09574223235298934, 0.19820454619399974, 0.431585027268005]
printing an ep nov before normalisation:  35.845950285251384
printing an ep nov before normalisation:  16.466938814520347
actor:  1 policy actor:  1  step number:  61 total reward:  0.10666666666666613  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9844155993431857 -1.0 -0.9844155993431857
probs:  [0.062109029880731456, 0.1268912294043226, 0.08507583000342565, 0.09554309912506624, 0.19635671020461282, 0.4340241013818411]
actor:  1 policy actor:  1  step number:  53 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9844155993431857 -1.0 -0.9844155993431857
probs:  [0.0625507926507252, 0.12598672861082527, 0.08504031095213907, 0.09529005576723631, 0.19400862079557665, 0.4371234912234974]
printing an ep nov before normalisation:  42.08185708151872
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.37 ]
 [0.319]
 [0.277]
 [0.249]
 [0.246]
 [0.241]] [[40.89 ]
 [40.967]
 [33.056]
 [30.249]
 [29.518]
 [29.584]
 [29.422]] [[0.758]
 [0.812]
 [0.63 ]
 [0.541]
 [0.501]
 [0.499]
 [0.492]]
maxi score, test score, baseline:  -0.9844155993431857 -1.0 -0.9844155993431857
maxi score, test score, baseline:  -0.9844536445536446 -1.0 -0.9844536445536446
probs:  [0.06250560742704901, 0.1260794650853442, 0.08504337556514592, 0.09531591637210143, 0.19424916590265479, 0.43680646964770464]
maxi score, test score, baseline:  -0.9844536445536446 -1.0 -0.9844536445536446
probs:  [0.06255733803868659, 0.12535520245823298, 0.08511379862555825, 0.09539485930642692, 0.19441016242310466, 0.43716863914799065]
maxi score, test score, baseline:  -0.9844536445536446 -1.0 -0.9844536445536446
actions average: 
K:  2  action  0 :  tensor([0.2158, 0.0116, 0.1156, 0.1896, 0.1881, 0.1507, 0.1287],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0250, 0.8359, 0.0233, 0.0485, 0.0173, 0.0158, 0.0341],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1444, 0.0731, 0.2045, 0.1618, 0.1508, 0.1443, 0.1211],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1710, 0.0141, 0.1068, 0.2764, 0.1603, 0.1539, 0.1175],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1679, 0.0054, 0.1323, 0.1855, 0.1750, 0.1744, 0.1594],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1095, 0.0152, 0.1244, 0.1472, 0.1198, 0.3797, 0.1043],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1553, 0.0360, 0.1342, 0.1849, 0.1555, 0.1656, 0.1685],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9844536445536446 -1.0 -0.9844536445536446
probs:  [0.06255004748211701, 0.12552226304884637, 0.08516913352315118, 0.09547873840278419, 0.19416289419035088, 0.4371169233527504]
from probs:  [0.06255004748211701, 0.12552226304884637, 0.08516913352315118, 0.09547873840278419, 0.19416289419035088, 0.4371169233527504]
maxi score, test score, baseline:  -0.9844536445536446 -1.0 -0.9844536445536446
probs:  [0.06258661118202383, 0.12559571579829967, 0.08521894745696122, 0.09494926113580915, 0.19427655654050333, 0.4373729078864028]
maxi score, test score, baseline:  -0.9844536445536446 -1.0 -0.9844536445536446
probs:  [0.06258661118202383, 0.12559571579829967, 0.08521894745696122, 0.09494926113580915, 0.19427655654050333, 0.4373729078864028]
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.766]
 [0.657]
 [0.657]
 [0.657]
 [0.657]
 [0.657]] [[32.472]
 [40.717]
 [32.472]
 [32.472]
 [32.472]
 [32.472]
 [32.472]] [[1.465]
 [1.973]
 [1.465]
 [1.465]
 [1.465]
 [1.465]
 [1.465]]
maxi score, test score, baseline:  -0.9844536445536446 -1.0 -0.9844536445536446
probs:  [0.06258661118202383, 0.12559571579829967, 0.08521894745696122, 0.09494926113580915, 0.19427655654050333, 0.4373729078864028]
maxi score, test score, baseline:  -0.9844536445536446 -1.0 -0.9844536445536446
probs:  [0.06258661118202383, 0.12559571579829967, 0.08521894745696122, 0.09494926113580915, 0.19427655654050333, 0.4373729078864028]
maxi score, test score, baseline:  -0.9844536445536446 -1.0 -0.9844536445536446
probs:  [0.062545303764625, 0.1258271929769085, 0.08551758940296378, 0.0952317171837878, 0.1937924376247202, 0.4370857590469947]
printing an ep nov before normalisation:  34.693305059639655
maxi score, test score, baseline:  -0.984491503267974 -1.0 -0.984491503267974
probs:  [0.06254535429138597, 0.12582730763014754, 0.08551702317071652, 0.0952316668927257, 0.1937925382944259, 0.43708610972059825]
line 256 mcts: sample exp_bonus 31.32809893168712
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.576]
 [0.459]
 [0.49 ]
 [0.498]
 [0.512]
 [0.496]] [[37.595]
 [36.198]
 [36.36 ]
 [38.866]
 [38.119]
 [34.764]
 [35.454]] [[2.084]
 [2.029]
 [1.924]
 [2.144]
 [2.096]
 [1.857]
 [1.893]]
maxi score, test score, baseline:  -0.984491503267974 -1.0 -0.984491503267974
probs:  [0.06254535429138597, 0.12582730763014754, 0.08551702317071652, 0.0952316668927257, 0.1937925382944259, 0.43708610972059825]
printing an ep nov before normalisation:  0.0
siam score:  -0.8169252
printing an ep nov before normalisation:  41.711932487771406
maxi score, test score, baseline:  -0.984491503267974 -1.0 -0.984491503267974
probs:  [0.06253774436025833, 0.12599447503707606, 0.08557285829660133, 0.09531433272594989, 0.19354842552404572, 0.43703216405606865]
maxi score, test score, baseline:  -0.984491503267974 -1.0 -0.984491503267974
probs:  [0.06253774436025833, 0.12599447503707606, 0.08557285829660133, 0.09531433272594989, 0.19354842552404572, 0.43703216405606865]
printing an ep nov before normalisation:  36.304476339579494
siam score:  -0.8156457
printing an ep nov before normalisation:  46.68129625339436
printing an ep nov before normalisation:  43.39181335779388
maxi score, test score, baseline:  -0.9845291768541158 -1.0 -0.9845291768541158
printing an ep nov before normalisation:  34.25355111692346
printing an ep nov before normalisation:  29.138095196485246
printing an ep nov before normalisation:  39.67154026031494
maxi score, test score, baseline:  -0.9845666666666668 -1.0 -0.9845666666666668
maxi score, test score, baseline:  -0.9845666666666668 -1.0 -0.9845666666666668
maxi score, test score, baseline:  -0.9845666666666668 -1.0 -0.9845666666666668
probs:  [0.0625750287041825, 0.1260696986343941, 0.08562263991282379, 0.09537094660545846, 0.19306849757667982, 0.4372931885664614]
printing an ep nov before normalisation:  30.153257846832275
actor:  1 policy actor:  1  step number:  52 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  58.617133372028874
printing an ep nov before normalisation:  50.09934577576985
Printing some Q and Qe and total Qs values:  [[-0.03 ]
 [-0.024]
 [-0.029]
 [-0.028]
 [-0.031]
 [-0.027]
 [-0.025]] [[14.258]
 [18.796]
 [12.114]
 [12.059]
 [23.444]
 [12.121]
 [12.399]] [[0.666]
 [0.894]
 [0.561]
 [0.559]
 [1.114]
 [0.564]
 [0.579]]
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([0.3389, 0.0083, 0.1269, 0.1370, 0.1235, 0.1498, 0.1156],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0393, 0.7651, 0.0486, 0.0412, 0.0250, 0.0309, 0.0500],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1664, 0.0205, 0.2173, 0.1456, 0.1226, 0.1721, 0.1554],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1767, 0.0289, 0.1118, 0.2235, 0.1815, 0.1349, 0.1426],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2094, 0.0219, 0.1279, 0.1576, 0.1803, 0.1556, 0.1473],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1504, 0.0099, 0.1705, 0.1510, 0.1227, 0.2579, 0.1374],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1204, 0.0753, 0.1334, 0.1964, 0.1299, 0.1472, 0.1974],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.46 ]
 [0.426]
 [0.39 ]
 [0.395]
 [0.377]
 [0.377]] [[45.93 ]
 [44.994]
 [44.985]
 [45.382]
 [43.867]
 [42.028]
 [42.028]] [[1.797]
 [1.817]
 [1.782]
 [1.769]
 [1.69 ]
 [1.57 ]
 [1.57 ]]
printing an ep nov before normalisation:  40.694459805092436
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.515]
 [0.437]
 [0.516]
 [0.516]
 [0.516]
 [0.579]] [[60.418]
 [78.431]
 [50.608]
 [60.418]
 [60.418]
 [60.418]
 [72.601]] [[1.492]
 [1.962]
 [1.156]
 [1.492]
 [1.492]
 [1.492]
 [1.873]]
from probs:  [0.06177203716167221, 0.1246305480784889, 0.0845887322064258, 0.09423936970612387, 0.20309859127607194, 0.4316707215712173]
maxi score, test score, baseline:  -0.9845666666666668 -1.0 -0.9845666666666668
probs:  [0.06172615253108533, 0.12471822634751639, 0.08459132890168242, 0.0942624722502908, 0.20335299986090993, 0.43134882010851505]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9845666666666668 -1.0 -0.9845666666666668
probs:  [0.06168932765470317, 0.12495058216065055, 0.08488756808968664, 0.09454331465171824, 0.20283619777452305, 0.43109300966871833]
maxi score, test score, baseline:  -0.9846039740470398 -1.0 -0.9846039740470398
probs:  [0.061689371330785205, 0.12495068832409884, 0.08488701107216338, 0.09454326198057933, 0.2028363548244575, 0.4310933124679157]
maxi score, test score, baseline:  -0.9846039740470398 -1.0 -0.9846039740470398
printing an ep nov before normalisation:  58.221498904977416
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9846411003236247 -1.0 -0.9846411003236247
probs:  [0.0616894147925948, 0.12495079396766223, 0.08488645679041014, 0.0945432095665737, 0.202836511101165, 0.4310936137815942]
actor:  1 policy actor:  1  step number:  72 total reward:  0.08666666666666634  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9846411003236247 -1.0 -0.9846411003236247
maxi score, test score, baseline:  -0.9846411003236247 -1.0 -0.9846411003236247
probs:  [0.06200459736545123, 0.12434616914860988, 0.08486435916200882, 0.09438070474659394, 0.20109944403571048, 0.4333047255416256]
printing an ep nov before normalisation:  49.56497429561741
maxi score, test score, baseline:  -0.9846780468119453 -1.0 -0.9846780468119453
probs:  [0.0620046438048591, 0.12434626542117161, 0.08486381534791233, 0.09438065137712233, 0.2010995762353541, 0.43330504781358065]
maxi score, test score, baseline:  -0.9846780468119453 -1.0 -0.9846780468119453
probs:  [0.06199734332331366, 0.12450917402302696, 0.08491892643884076, 0.09446174594258044, 0.2008595154026343, 0.4332532948696039]
printing an ep nov before normalisation:  63.05692406603699
maxi score, test score, baseline:  -0.9846780468119453 -1.0 -0.9846780468119453
maxi score, test score, baseline:  -0.9846780468119453 -1.0 -0.9846780468119453
probs:  [0.06199734332331366, 0.12450917402302696, 0.08491892643884076, 0.09446174594258044, 0.2008595154026343, 0.4332532948696039]
printing an ep nov before normalisation:  51.62021482706037
maxi score, test score, baseline:  -0.9846780468119453 -1.0 -0.9846780468119453
probs:  [0.06199734332331366, 0.12450917402302696, 0.08491892643884076, 0.09446174594258044, 0.2008595154026343, 0.4332532948696039]
maxi score, test score, baseline:  -0.9846780468119453 -1.0 -0.9846780468119453
probs:  [0.06199734332331366, 0.12450917402302696, 0.08491892643884076, 0.09446174594258044, 0.2008595154026343, 0.4332532948696039]
maxi score, test score, baseline:  -0.9846780468119453 -1.0 -0.9846780468119453
probs:  [0.06199734332331366, 0.12450917402302696, 0.08491892643884076, 0.09446174594258044, 0.2008595154026343, 0.4332532948696039]
maxi score, test score, baseline:  -0.9846780468119453 -1.0 -0.9846780468119453
probs:  [0.06199734332331366, 0.12450917402302696, 0.08491892643884076, 0.09446174594258044, 0.2008595154026343, 0.4332532948696039]
maxi score, test score, baseline:  -0.9846780468119453 -1.0 -0.9846780468119453
probs:  [0.06203504131023911, 0.12458496469286796, 0.08497059209537002, 0.09451922668392497, 0.20037295102078514, 0.4335172241968128]
printing an ep nov before normalisation:  51.96001821258028
maxi score, test score, baseline:  -0.9846780468119453 -1.0 -0.9846780468119453
probs:  [0.06203504131023911, 0.12458496469286796, 0.08497059209537002, 0.09451922668392497, 0.20037295102078514, 0.4335172241968128]
maxi score, test score, baseline:  -0.9846780468119453 -1.0 -0.9846780468119453
probs:  [0.06203504131023911, 0.12458496469286796, 0.08497059209537002, 0.09451922668392497, 0.20037295102078514, 0.4335172241968128]
actor:  1 policy actor:  1  step number:  60 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  0.027212260654323472
actor:  0 policy actor:  1  step number:  82 total reward:  0.13999999999999946  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.981961191626409 -1.0 -0.981961191626409
probs:  [0.06028461314805657, 0.12135598396589543, 0.08294263311396419, 0.11922057665795031, 0.19493189149621143, 0.4212643016179222]
maxi score, test score, baseline:  -0.981961191626409 -1.0 -0.981961191626409
maxi score, test score, baseline:  -0.981961191626409 -1.0 -0.981961191626409
probs:  [0.06028461314805657, 0.12135598396589543, 0.08294263311396419, 0.11922057665795031, 0.19493189149621143, 0.4212643016179222]
printing an ep nov before normalisation:  103.06189588867448
Printing some Q and Qe and total Qs values:  [[0.388]
 [1.002]
 [0.388]
 [0.513]
 [0.739]
 [0.388]
 [0.388]] [[42.632]
 [50.41 ]
 [42.632]
 [49.714]
 [52.168]
 [42.632]
 [42.632]] [[1.645]
 [2.62 ]
 [1.645]
 [2.099]
 [2.439]
 [1.645]
 [1.645]]
maxi score, test score, baseline:  -0.981961191626409 -1.0 -0.981961191626409
probs:  [0.06028461314805657, 0.12135598396589543, 0.08294263311396419, 0.11922057665795031, 0.19493189149621143, 0.4212643016179222]
maxi score, test score, baseline:  -0.981961191626409 -1.0 -0.981961191626409
maxi score, test score, baseline:  -0.981961191626409 -1.0 -0.981961191626409
probs:  [0.06028461314805657, 0.12135598396589543, 0.08294263311396419, 0.11922057665795031, 0.19493189149621143, 0.4212643016179222]
maxi score, test score, baseline:  -0.981961191626409 -1.0 -0.981961191626409
probs:  [0.06028461314805657, 0.12135598396589543, 0.08294263311396419, 0.11922057665795031, 0.19493189149621143, 0.4212643016179222]
using explorer policy with actor:  1
printing an ep nov before normalisation:  6.538006459777534e-05
printing an ep nov before normalisation:  43.857647913201355
maxi score, test score, baseline:  -0.981961191626409 -1.0 -0.981961191626409
probs:  [0.06023453051740644, 0.12160166669021069, 0.08300228205194084, 0.11871519220744825, 0.19553389837235421, 0.4209124301606397]
printing an ep nov before normalisation:  0.0
actions average: 
K:  2  action  0 :  tensor([0.2290, 0.0387, 0.1461, 0.1507, 0.1480, 0.1604, 0.1270],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0171, 0.9005, 0.0153, 0.0198, 0.0115, 0.0122, 0.0236],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2036, 0.0031, 0.1513, 0.1392, 0.1637, 0.1922, 0.1470],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1125, 0.1400, 0.0968, 0.3110, 0.0836, 0.1243, 0.1318],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2070, 0.0110, 0.1290, 0.1206, 0.2531, 0.1549, 0.1244],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1611, 0.0408, 0.1624, 0.1184, 0.1329, 0.2341, 0.1503],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.2021, 0.1001, 0.1268, 0.1289, 0.1483, 0.1393, 0.1544],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.981961191626409 -1.0 -0.981961191626409
probs:  [0.060187351497822304, 0.12167964874389636, 0.08300153890008204, 0.11878728716476998, 0.19576266858471347, 0.4205815051087157]
printing an ep nov before normalisation:  53.42691758185023
maxi score, test score, baseline:  -0.981961191626409 -1.0 -0.981961191626409
probs:  [0.060187351497822304, 0.12167964874389636, 0.08300153890008204, 0.11878728716476998, 0.19576266858471347, 0.4205815051087157]
maxi score, test score, baseline:  -0.981961191626409 -1.0 -0.981961191626409
probs:  [0.06026696732617717, 0.12184078703558587, 0.08311140028502373, 0.11821183857969252, 0.19543009885353446, 0.42113890791998626]
printing an ep nov before normalisation:  61.42709843430681
printing an ep nov before normalisation:  47.611410220184105
printing an ep nov before normalisation:  43.838851906230474
deleting a thread, now have 2 threads
Frames:  50015 train batches done:  5863 episodes:  1545
maxi score, test score, baseline:  -0.981961191626409 -1.0 -0.981961191626409
probs:  [0.060255388451714235, 0.12199099093655533, 0.08315984425443669, 0.11835250756215492, 0.19518404436930006, 0.42105722442583876]
Starting evaluation
maxi score, test score, baseline:  -0.9820044176706827 -1.0 -0.9820044176706827
probs:  [0.060255432919222474, 0.12199109052689258, 0.08315920863698847, 0.11835257038867793, 0.19518416505952582, 0.4210575324686926]
maxi score, test score, baseline:  -0.9820044176706827 -1.0 -0.9820044176706827
maxi score, test score, baseline:  -0.9820044176706827 -1.0 -0.9820044176706827
probs:  [0.060255432919222474, 0.12199109052689258, 0.08315920863698847, 0.11835257038867793, 0.19518416505952582, 0.4210575324686926]
siam score:  -0.8092877
maxi score, test score, baseline:  -0.9820474358974359 -1.0 -0.9820474358974359
probs:  [0.060255477170307846, 0.12199118963380055, 0.08315857611800527, 0.11835263290666813, 0.19518428515891276, 0.42105783901230553]
printing an ep nov before normalisation:  36.058807373046875
printing an ep nov before normalisation:  43.52206773013778
printing an ep nov before normalisation:  43.28845262266861
printing an ep nov before normalisation:  41.791175535700575
maxi score, test score, baseline:  -0.9820474358974359 -1.0 -0.9820474358974359
probs:  [0.060255477170307846, 0.12199118963380055, 0.08315857611800527, 0.11835263290666813, 0.19518428515891276, 0.42105783901230553]
printing an ep nov before normalisation:  43.69059085845947
printing an ep nov before normalisation:  42.61426663868195
maxi score, test score, baseline:  -0.9822174603174603 -1.0 -0.9822174603174603
printing an ep nov before normalisation:  58.42658414096415
line 256 mcts: sample exp_bonus 48.10941982269287
actor:  0 policy actor:  1  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  27 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  29 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  29 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  29 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  30 total reward:  0.6200000000000001  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  13.79809593622958
actor:  0 policy actor:  1  step number:  32 total reward:  0.6333333333333334  reward:  1.0 rdn_beta:  2
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  34 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.9446067901234568 -1.0 -0.9446067901234568
line 256 mcts: sample exp_bonus 13.290432344004213
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9446067901234568 -1.0 -0.9446067901234568
probs:  [0.06011877312972861, 0.12222573993812104, 0.08378844867289656, 0.1178691027644786, 0.19589609396505736, 0.42010184152971786]
maxi score, test score, baseline:  -0.9447344880677444 -1.0 -0.9447344880677444
probs:  [0.060118912383546, 0.12222605224909606, 0.08378644440334532, 0.11786929438927912, 0.19589649040394258, 0.420102806170791]
maxi score, test score, baseline:  -0.9448615975422427 -1.0 -0.9448615975422427
probs:  [0.06015433694169062, 0.12229818296045429, 0.08383365664809313, 0.11793874206889919, 0.19542427234883267, 0.4203508090320301]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.431]
 [0.507]
 [0.504]
 [0.503]
 [0.501]
 [0.498]] [[14.548]
 [28.426]
 [15.061]
 [14.146]
 [14.154]
 [13.918]
 [13.674]] [[0.511]
 [0.431]
 [0.507]
 [0.504]
 [0.503]
 [0.501]
 [0.498]]
maxi score, test score, baseline:  -0.9448615975422427 -1.0 -0.9448615975422427
probs:  [0.06015433694169062, 0.12229818296045429, 0.08383365664809313, 0.11793874206889919, 0.19542427234883267, 0.4203508090320301]
printing an ep nov before normalisation:  14.719114303588867
maxi score, test score, baseline:  -0.9448615975422427 -1.0 -0.9448615975422427
probs:  [0.060107642341469314, 0.12237649880711965, 0.08383459608612522, 0.11800828833194618, 0.19564969085648815, 0.42002328357685165]
printing an ep nov before normalisation:  65.24498462677002
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.944988122605364 -0.17699999999999996 -0.17699999999999996
probs:  [0.058381136626091394, 0.12025843075057756, 0.11420133899932944, 0.11229864203123072, 0.18679801857013845, 0.40806243302263245]
printing an ep nov before normalisation:  91.01922585800823
maxi score, test score, baseline:  -0.944988122605364 -0.17699999999999996 -0.17699999999999996
probs:  [0.058381136626091394, 0.12025843075057756, 0.11420133899932944, 0.11229864203123072, 0.18679801857013845, 0.40806243302263245]
maxi score, test score, baseline:  -0.944988122605364 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.944988122605364 -0.17699999999999996 -0.17699999999999996
probs:  [0.058291039752893424, 0.12039146117095134, 0.11431252776504629, 0.11240296974022253, 0.1871709883471959, 0.40743101322369063]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.944988122605364 -0.17699999999999996 -0.17699999999999996
probs:  [0.058246080842117236, 0.12045784419388886, 0.11436801166387361, 0.11245502992825747, 0.18735710262989796, 0.40711593074196484]
Printing some Q and Qe and total Qs values:  [[-0.158]
 [-0.148]
 [-0.156]
 [-0.158]
 [-0.158]
 [-0.158]
 [-0.158]] [[70.569]
 [69.174]
 [77.277]
 [70.569]
 [70.569]
 [70.569]
 [70.569]] [[1.128]
 [1.089]
 [1.369]
 [1.128]
 [1.128]
 [1.128]
 [1.128]]
printing an ep nov before normalisation:  40.530991554260254
maxi score, test score, baseline:  -0.944988122605364 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.944988122605364 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.944988122605364 -0.17699999999999996 -0.17699999999999996
probs:  [0.058201181457179355, 0.12052413932534901, 0.1144234221017222, 0.11250702118832502, 0.1875429704962944, 0.40680126543113]
printing an ep nov before normalisation:  54.60506359392268
maxi score, test score, baseline:  -0.944988122605364 -0.17699999999999996 -0.17699999999999996
probs:  [0.05822850551708047, 0.1205807896503588, 0.11447720171493055, 0.11255989903301719, 0.18716097972057177, 0.4069926243640413]
Printing some Q and Qe and total Qs values:  [[-0.133]
 [-0.133]
 [-0.133]
 [-0.133]
 [-0.133]
 [-0.133]
 [-0.133]] [[64.614]
 [64.614]
 [64.614]
 [64.614]
 [64.614]
 [64.614]
 [64.614]] [[0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]]
siam score:  -0.8044554
maxi score, test score, baseline:  -0.944988122605364 -0.17699999999999996 -0.17699999999999996
probs:  [0.05822850551708047, 0.1205807896503588, 0.11447720171493055, 0.11255989903301719, 0.18716097972057177, 0.4069926243640413]
Printing some Q and Qe and total Qs values:  [[0.189]
 [0.096]
 [0.189]
 [0.127]
 [0.189]
 [0.189]
 [0.189]] [[53.109]
 [55.93 ]
 [53.109]
 [66.211]
 [53.109]
 [53.109]
 [53.109]] [[1.288]
 [1.287]
 [1.288]
 [1.654]
 [1.288]
 [1.288]
 [1.288]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4  reward:  1.0 rdn_beta:  2.0
using another actor
from probs:  [0.05822850551708047, 0.1205807896503588, 0.11447720171493055, 0.11255989903301719, 0.18716097972057177, 0.4069926243640413]
maxi score, test score, baseline:  -0.944988122605364 -0.17699999999999996 -0.17699999999999996
probs:  [0.05902374563792214, 0.11940352812466594, 0.11349302593452756, 0.11163637672745555, 0.18387746761616142, 0.41256585595926737]
maxi score, test score, baseline:  -0.944988122605364 -0.17699999999999996 -0.17699999999999996
probs:  [0.05902374563792214, 0.11940352812466594, 0.11349302593452756, 0.11163637672745555, 0.18387746761616142, 0.41256585595926737]
printing an ep nov before normalisation:  20.6024432182312
maxi score, test score, baseline:  -0.944988122605364 -0.17699999999999996 -0.17699999999999996
probs:  [0.05905617867872081, 0.11946921439511606, 0.11300516281736965, 0.1116977853596216, 0.1839786619097965, 0.4127929968393755]
maxi score, test score, baseline:  -0.944988122605364 -0.17699999999999996 -0.17699999999999996
probs:  [0.05905617867872081, 0.11946921439511606, 0.11300516281736965, 0.1116977853596216, 0.1839786619097965, 0.4127929968393755]
printing an ep nov before normalisation:  65.4484748840332
from probs:  [0.059088139863251926, 0.11953394502040798, 0.11252439743954985, 0.11175830058040051, 0.1840783839652289, 0.4130168331311608]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9453642313546423 -0.17699999999999996 -0.17699999999999996
probs:  [0.059088139863251926, 0.11953394502040798, 0.11252439743954985, 0.11175830058040051, 0.1840783839652289, 0.4130168331311608]
maxi score, test score, baseline:  -0.9453642313546423 -0.17699999999999996 -0.17699999999999996
probs:  [0.059088139863251926, 0.11953394502040798, 0.11252439743954985, 0.11175830058040051, 0.1840783839652289, 0.4130168331311608]
using another actor
maxi score, test score, baseline:  -0.9456121212121212 -0.17699999999999996 -0.17699999999999996
siam score:  -0.8053757
maxi score, test score, baseline:  -0.9458577677224737 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9458577677224737 -0.17699999999999996 -0.17699999999999996
probs:  [0.059088139863251926, 0.11953394502040798, 0.11252439743954985, 0.11175830058040051, 0.1840783839652289, 0.4130168331311608]
line 256 mcts: sample exp_bonus 56.357045163339286
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  65.88826739920692
Printing some Q and Qe and total Qs values:  [[-0.135]
 [-0.135]
 [-0.135]
 [-0.135]
 [-0.135]
 [-0.135]
 [-0.135]] [[48.101]
 [48.101]
 [48.101]
 [48.101]
 [48.101]
 [48.101]
 [48.101]] [[0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9459797592174568 -0.17699999999999996 -0.17699999999999996
probs:  [0.059000736513004796, 0.119664148315826, 0.11262936616107583, 0.11186051133109064, 0.18444094910641282, 0.41240428857258987]
maxi score, test score, baseline:  -0.9459797592174568 -0.17699999999999996 -0.17699999999999996
siam score:  -0.8094418
maxi score, test score, baseline:  -0.9459797592174568 -0.17699999999999996 -0.17699999999999996
probs:  [0.05898871740621871, 0.11979337084449126, 0.1122054551701497, 0.11197156477863374, 0.18472099040723533, 0.41231990139327124]
maxi score, test score, baseline:  -0.9459797592174568 -0.17699999999999996 -0.17699999999999996
probs:  [0.05898871740621871, 0.11979337084449126, 0.1122054551701497, 0.11197156477863374, 0.18472099040723533, 0.41231990139327124]
actions average: 
K:  1  action  0 :  tensor([0.3598, 0.0230, 0.1278, 0.1148, 0.1165, 0.1352, 0.1228],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0159, 0.9250, 0.0129, 0.0099, 0.0049, 0.0062, 0.0252],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1456, 0.0229, 0.2426, 0.1304, 0.1336, 0.1758, 0.1490],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1164, 0.0842, 0.1216, 0.2957, 0.1193, 0.1432, 0.1196],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2134, 0.0183, 0.1053, 0.1198, 0.3312, 0.1151, 0.0969],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1534, 0.0175, 0.1361, 0.1466, 0.1388, 0.2835, 0.1241],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1303, 0.1672, 0.1488, 0.1157, 0.1088, 0.1142, 0.2150],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.059]
 [0.185]
 [0.059]
 [0.059]
 [0.059]
 [0.059]
 [0.059]] [[46.692]
 [48.496]
 [46.692]
 [46.692]
 [46.692]
 [46.692]
 [46.692]] [[1.282]
 [1.493]
 [1.282]
 [1.282]
 [1.282]
 [1.282]
 [1.282]]
printing an ep nov before normalisation:  29.572699069976807
printing an ep nov before normalisation:  0.017046882298927812
actor:  1 policy actor:  1  step number:  50 total reward:  0.33999999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9459797592174568 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9459797592174568 -0.17699999999999996 -0.17699999999999996
probs:  [0.05960805318182416, 0.118960317707672, 0.11155364810531103, 0.11132534445415376, 0.18189239911969507, 0.416660237431344]
maxi score, test score, baseline:  -0.9459797592174568 -0.17699999999999996 -0.17699999999999996
probs:  [0.05960805318182416, 0.118960317707672, 0.11155364810531103, 0.11132534445415376, 0.18189239911969507, 0.416660237431344]
Printing some Q and Qe and total Qs values:  [[-0.005]
 [ 0.139]
 [ 0.1  ]
 [-0.012]
 [ 0.065]
 [ 0.076]
 [-0.016]] [[47.066]
 [50.528]
 [48.555]
 [49.378]
 [48.283]
 [50.631]
 [49.572]] [[0.887]
 [1.166]
 [1.05 ]
 [0.97 ]
 [1.004]
 [1.108]
 [0.974]]
printing an ep nov before normalisation:  39.37414155649956
maxi score, test score, baseline:  -0.9459797592174568 -0.17699999999999996 -0.17699999999999996
probs:  [0.05963569234948561, 0.11901553908597264, 0.1116054274528359, 0.11091266874088565, 0.18197686632272259, 0.4168538060480976]
maxi score, test score, baseline:  -0.9459797592174568 -0.17699999999999996 -0.17699999999999996
probs:  [0.05963569234948561, 0.11901553908597264, 0.1116054274528359, 0.11091266874088565, 0.18197686632272259, 0.4168538060480976]
line 256 mcts: sample exp_bonus 49.13497484391883
actor:  1 policy actor:  1  step number:  59 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  50.749043769038416
maxi score, test score, baseline:  -0.9459797592174568 -0.17699999999999996 -0.17699999999999996
probs:  [0.05919083857296418, 0.11812674825903295, 0.11077203628221875, 0.11008445678975351, 0.18808761055034542, 0.41373830954568513]
maxi score, test score, baseline:  -0.9459797592174568 -0.17699999999999996 -0.17699999999999996
probs:  [0.05919083857296418, 0.11812674825903295, 0.11077203628221875, 0.11008445678975351, 0.18808761055034542, 0.41373830954568513]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9459797592174568 -0.17699999999999996 -0.17699999999999996
actor:  0 policy actor:  1  step number:  56 total reward:  0.16666666666666596  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.138]
 [-0.138]
 [-0.138]
 [-0.138]
 [-0.138]
 [-0.138]
 [-0.138]] [[55.913]
 [55.913]
 [55.913]
 [55.913]
 [55.913]
 [55.913]
 [55.913]] [[0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]]
maxi score, test score, baseline:  -0.9434735735735735 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9434735735735735 -0.17699999999999996 -0.17699999999999996
probs:  [0.05919083857296418, 0.11812674825903295, 0.11077203628221875, 0.11008445678975351, 0.18808761055034542, 0.41373830954568513]
maxi score, test score, baseline:  -0.9434735735735735 -0.17699999999999996 -0.17699999999999996
probs:  [0.05914768712815503, 0.11818879159995258, 0.11082095218768143, 0.11013214543364061, 0.1882745271299634, 0.41343589652060697]
Printing some Q and Qe and total Qs values:  [[-0.083]
 [-0.063]
 [-0.083]
 [-0.076]
 [-0.083]
 [-0.083]
 [-0.083]] [[41.213]
 [34.856]
 [41.213]
 [44.374]
 [41.213]
 [41.213]
 [41.213]] [[1.462]
 [1.238]
 [1.462]
 [1.591]
 [1.462]
 [1.462]
 [1.462]]
maxi score, test score, baseline:  -0.9434735735735735 -0.17699999999999996 -0.17699999999999996
probs:  [0.05914768712815503, 0.11818879159995258, 0.11082095218768143, 0.11013214543364061, 0.1882745271299634, 0.41343589652060697]
maxi score, test score, baseline:  -0.9434735735735735 -0.17699999999999996 -0.17699999999999996
probs:  [0.05914768712815503, 0.11818879159995258, 0.11082095218768143, 0.11013214543364061, 0.1882745271299634, 0.41343589652060697]
maxi score, test score, baseline:  -0.9434735735735735 -0.17699999999999996 -0.17699999999999996
probs:  [0.05914768712815503, 0.11818879159995258, 0.11082095218768143, 0.11013214543364061, 0.1882745271299634, 0.41343589652060697]
printing an ep nov before normalisation:  46.41196881098669
maxi score, test score, baseline:  -0.9434735735735735 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9434735735735735 -0.17699999999999996 -0.17699999999999996
probs:  [0.05914768712815503, 0.11818879159995258, 0.11082095218768143, 0.11013214543364061, 0.1882745271299634, 0.41343589652060697]
printing an ep nov before normalisation:  27.037896168679726
printing an ep nov before normalisation:  37.49562317944219
maxi score, test score, baseline:  -0.9434735735735735 -0.17699999999999996 -0.17699999999999996
probs:  [0.05914768712815503, 0.11818879159995258, 0.11082095218768143, 0.11013214543364061, 0.1882745271299634, 0.41343589652060697]
maxi score, test score, baseline:  -0.9434735735735735 -0.17699999999999996 -0.17699999999999996
probs:  [0.05914768712815503, 0.11818879159995258, 0.11082095218768143, 0.11013214543364061, 0.1882745271299634, 0.41343589652060697]
actor:  1 policy actor:  1  step number:  66 total reward:  0.1666666666666663  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9434735735735735 -0.17699999999999996 -0.17699999999999996
probs:  [0.058729101486445036, 0.11735143133173506, 0.11003585151429446, 0.10935193042073947, 0.19402731706927762, 0.41050436817750824]
actor:  1 policy actor:  1  step number:  63 total reward:  0.33333333333333337  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.144]
 [-0.144]
 [-0.144]
 [-0.144]
 [-0.144]
 [-0.144]
 [-0.144]] [[62.252]
 [62.252]
 [62.252]
 [62.252]
 [62.252]
 [62.252]
 [62.252]] [[0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]]
maxi score, test score, baseline:  -0.9434735735735735 -0.17699999999999996 -0.17699999999999996
probs:  [0.0592878794371935, 0.11658065894665742, 0.10943099596719835, 0.10876258615615318, 0.19151754096494902, 0.4144203385278485]
maxi score, test score, baseline:  -0.9434735735735735 -0.17699999999999996 -0.17699999999999996
probs:  [0.0592878794371935, 0.11658065894665742, 0.10943099596719835, 0.10876258615615318, 0.19151754096494902, 0.4144203385278485]
printing an ep nov before normalisation:  51.50340421372216
maxi score, test score, baseline:  -0.9434735735735735 -0.17699999999999996 -0.17699999999999996
probs:  [0.0592878794371935, 0.11658065894665742, 0.10943099596719835, 0.10876258615615318, 0.19151754096494902, 0.4144203385278485]
Printing some Q and Qe and total Qs values:  [[-0.144]
 [-0.116]
 [-0.144]
 [-0.143]
 [-0.144]
 [-0.144]
 [-0.144]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.144]
 [-0.116]
 [-0.144]
 [-0.143]
 [-0.144]
 [-0.144]
 [-0.144]]
maxi score, test score, baseline:  -0.9436003745318352 -0.17699999999999996 -0.17699999999999996
probs:  [0.05931737509772937, 0.11663872190154559, 0.1089872730852279, 0.1088167508678668, 0.19161296890692264, 0.4146269101407078]
actor:  1 policy actor:  1  step number:  54 total reward:  0.32666666666666655  reward:  1.0 rdn_beta:  2.0
from probs:  [0.0598339229766787, 0.11592408022821842, 0.10843697479124717, 0.1082701151740194, 0.1892879758592856, 0.41824693097055077]
maxi score, test score, baseline:  -0.9437266068759342 -0.17699999999999996 -0.17699999999999996
probs:  [0.0598339229766787, 0.11592408022821842, 0.10843697479124717, 0.1082701151740194, 0.1892879758592856, 0.41824693097055077]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  1.667
UNIT TEST: sample policy line 217 mcts : [0.102 0.449 0.122 0.102 0.082 0.082 0.061]
printing an ep nov before normalisation:  42.12621064406249
maxi score, test score, baseline:  -0.9438522744220731 -0.17699999999999996 -0.17699999999999996
probs:  [0.059043560947863435, 0.11439115515935075, 0.10700316956883557, 0.1068385189627909, 0.20001196324705148, 0.41271163211410783]
printing an ep nov before normalisation:  54.36093620167705
actions average: 
K:  3  action  0 :  tensor([0.2776, 0.0479, 0.1236, 0.1253, 0.1585, 0.1236, 0.1436],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0332, 0.6784, 0.0561, 0.0711, 0.0337, 0.0547, 0.0727],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1544, 0.0822, 0.1700, 0.1753, 0.1322, 0.1595, 0.1264],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1387, 0.0920, 0.1122, 0.2708, 0.1282, 0.1455, 0.1126],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1690, 0.0709, 0.1298, 0.1664, 0.1756, 0.1454, 0.1429],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1898, 0.1253, 0.1248, 0.1391, 0.1453, 0.1403, 0.1354],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2009, 0.1249, 0.1458, 0.1140, 0.1282, 0.1303, 0.1560],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9439773809523809 -0.17699999999999996 -0.17699999999999996
probs:  [0.05902337630063155, 0.11444280796785591, 0.10705958910899457, 0.10689504473559559, 0.20000837347134484, 0.4125708084155775]
actions average: 
K:  0  action  0 :  tensor([0.2648, 0.0028, 0.1329, 0.1502, 0.1686, 0.1484, 0.1322],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0033, 0.9651, 0.0046, 0.0044, 0.0034, 0.0037, 0.0156],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1605, 0.0144, 0.2705, 0.1306, 0.1274, 0.1709, 0.1257],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1761, 0.0667, 0.1505, 0.1623, 0.1433, 0.1675, 0.1336],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1987, 0.0110, 0.1132, 0.1291, 0.3060, 0.1239, 0.1180],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1642, 0.0261, 0.1511, 0.1731, 0.1536, 0.2075, 0.1244],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1751, 0.1492, 0.1215, 0.1321, 0.1075, 0.1188, 0.1958],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9439773809523809 -0.17699999999999996 -0.17699999999999996
probs:  [0.05902337630063155, 0.11444280796785591, 0.10705958910899457, 0.10689504473559559, 0.20000837347134484, 0.4125708084155775]
Printing some Q and Qe and total Qs values:  [[-0.005]
 [ 0.274]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[39.94 ]
 [46.726]
 [39.94 ]
 [39.94 ]
 [39.94 ]
 [39.94 ]
 [39.94 ]] [[1.109]
 [1.703]
 [1.109]
 [1.109]
 [1.109]
 [1.109]
 [1.109]]
printing an ep nov before normalisation:  47.137323566553405
Printing some Q and Qe and total Qs values:  [[-0.088]
 [-0.088]
 [-0.088]
 [-0.088]
 [-0.088]
 [-0.088]
 [-0.088]] [[34.661]
 [34.661]
 [34.661]
 [34.661]
 [34.661]
 [34.661]
 [34.661]] [[1.042]
 [1.042]
 [1.042]
 [1.042]
 [1.042]
 [1.042]
 [1.042]]
siam score:  -0.799341
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.149]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.149]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]]
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.581]
 [0.582]
 [0.583]
 [0.585]
 [0.586]
 [0.584]] [[7.078]
 [6.274]
 [6.934]
 [6.906]
 [6.678]
 [6.459]
 [5.082]] [[1.142]
 [1.083]
 [1.137]
 [1.136]
 [1.119]
 [1.102]
 [0.991]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3866666666666667  reward:  1.0 rdn_beta:  2.0
siam score:  -0.79614013
line 256 mcts: sample exp_bonus 0.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.9439773809523809 -0.17699999999999996 -0.17699999999999996
probs:  [0.05964638792416671, 0.11373883373429233, 0.1065324017299842, 0.1063717972753068, 0.19677387602279503, 0.41693670331345484]
printing an ep nov before normalisation:  53.01658822732311
printing an ep nov before normalisation:  15.483104751320601
maxi score, test score, baseline:  -0.9439773809523809 -0.17699999999999996 -0.17699999999999996
probs:  [0.059702471159941976, 0.11384589190287703, 0.10617015358092201, 0.10647191299226304, 0.19648008523241023, 0.4173294851315858]
maxi score, test score, baseline:  -0.9439773809523809 -0.17699999999999996 -0.17699999999999996
probs:  [0.059702471159941976, 0.11384589190287703, 0.10617015358092201, 0.10647191299226304, 0.19648008523241023, 0.4173294851315858]
siam score:  -0.79530686
maxi score, test score, baseline:  -0.9439773809523809 -0.17699999999999996 -0.17699999999999996
probs:  [0.059702471159941976, 0.11384589190287703, 0.10617015358092201, 0.10647191299226304, 0.19648008523241023, 0.4173294851315858]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9439773809523809 -0.17699999999999996 -0.17699999999999996
probs:  [0.059702471159941976, 0.11384589190287703, 0.10617015358092201, 0.10647191299226304, 0.19648008523241023, 0.4173294851315858]
printing an ep nov before normalisation:  13.6021096698615
printing an ep nov before normalisation:  14.716414202439262
printing an ep nov before normalisation:  43.64231381574964
from probs:  [0.05966054127632212, 0.11389989718884748, 0.10621055844697914, 0.10651285253713996, 0.19668050766828082, 0.41703564288243034]
maxi score, test score, baseline:  -0.9441019302152932 -0.17699999999999996 -0.17699999999999996
probs:  [0.05968773860459985, 0.11395187583944945, 0.10580250093609322, 0.1065614561341216, 0.19677030780396454, 0.4172261206817713]
siam score:  -0.79488695
printing an ep nov before normalisation:  47.76801574968104
printing an ep nov before normalisation:  41.001710375036815
maxi score, test score, baseline:  -0.9442259259259259 -0.17699999999999996 -0.17699999999999996
probs:  [0.05971264358896622, 0.1139994734368093, 0.10584669056236494, 0.10618810939304806, 0.19685253907875971, 0.41740054394005177]
printing an ep nov before normalisation:  51.97493969712814
maxi score, test score, baseline:  -0.9442259259259259 -0.17699999999999996 -0.17699999999999996
probs:  [0.05962904853600538, 0.11410773213144677, 0.10592613670795134, 0.10626876213851057, 0.19725360675097006, 0.41681471373511597]
printing an ep nov before normalisation:  57.28209413655117
maxi score, test score, baseline:  -0.9442259259259259 -0.17699999999999996 -0.17699999999999996
probs:  [0.05962904853600538, 0.11410773213144677, 0.10592613670795134, 0.10626876213851057, 0.19725360675097006, 0.41681471373511597]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9442259259259259 -0.17699999999999996 -0.17699999999999996
probs:  [0.05962904853600538, 0.11410773213144677, 0.10592613670795134, 0.10626876213851057, 0.19725360675097006, 0.41681471373511597]
maxi score, test score, baseline:  -0.9442259259259259 -0.17699999999999996 -0.17699999999999996
Printing some Q and Qe and total Qs values:  [[0.23 ]
 [0.508]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]] [[39.723]
 [48.781]
 [39.723]
 [39.723]
 [39.723]
 [39.723]
 [39.723]] [[1.031]
 [1.627]
 [1.031]
 [1.031]
 [1.031]
 [1.031]
 [1.031]]
maxi score, test score, baseline:  -0.944349371766445 -0.17699999999999996 -0.17699999999999996
probs:  [0.05958732757825336, 0.1141617623194218, 0.10596578701219986, 0.10630901463751427, 0.19745377322991012, 0.4165223352227005]
printing an ep nov before normalisation:  78.87601061191437
actor:  1 policy actor:  1  step number:  66 total reward:  0.2199999999999993  reward:  1.0 rdn_beta:  1.333
from probs:  [0.05844466555649231, 0.11197022169936224, 0.10393176672437462, 0.1234722644089008, 0.19366142431806346, 0.4085196572928066]
maxi score, test score, baseline:  -0.944349371766445 -0.17699999999999996 -0.17699999999999996
probs:  [0.05840183395240991, 0.11201938056767072, 0.10396711048596159, 0.12354119099974328, 0.1938509799026436, 0.4082195040915709]
maxi score, test score, baseline:  -0.944349371766445 -0.17699999999999996 -0.17699999999999996
using another actor
maxi score, test score, baseline:  -0.944349371766445 -0.17699999999999996 -0.17699999999999996
probs:  [0.058430638828001494, 0.1115807398553147, 0.10401844039149474, 0.12360219728130649, 0.19394674357449848, 0.4084212400693841]
maxi score, test score, baseline:  -0.944349371766445 -0.17699999999999996 -0.17699999999999996
printing an ep nov before normalisation:  53.1626857415636
printing an ep nov before normalisation:  47.76785820719659
maxi score, test score, baseline:  -0.9445946284032377 -0.17699999999999996 -0.17699999999999996
probs:  [0.05843929203665913, 0.11168430329120702, 0.10412308520206225, 0.12370404170307067, 0.19356692111284912, 0.4084823566541517]
printing an ep nov before normalisation:  47.741807696802304
maxi score, test score, baseline:  -0.9445946284032377 -0.17699999999999996 -0.17699999999999996
probs:  [0.05843929203665913, 0.11168430329120702, 0.10412308520206225, 0.12370404170307067, 0.19356692111284912, 0.4084823566541517]
printing an ep nov before normalisation:  47.273483147051834
maxi score, test score, baseline:  -0.9445946284032377 -0.17699999999999996 -0.17699999999999996
probs:  [0.05843929203665913, 0.11168430329120702, 0.10412308520206225, 0.12370404170307067, 0.19356692111284912, 0.4084823566541517]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.145]
 [-0.14 ]
 [-0.139]
 [-0.137]
 [-0.14 ]
 [-0.14 ]
 [-0.14 ]] [[35.1  ]
 [30.879]
 [33.17 ]
 [32.498]
 [30.879]
 [30.879]
 [30.879]] [[1.178]
 [1.024]
 [1.112]
 [1.089]
 [1.024]
 [1.024]
 [1.024]]
printing an ep nov before normalisation:  42.73541755200427
maxi score, test score, baseline:  -0.9445946284032377 -0.17699999999999996 -0.17699999999999996
probs:  [0.05849508991079418, 0.11130391930495187, 0.10422260165896577, 0.12382229678932527, 0.1932829527805439, 0.40887313955541904]
maxi score, test score, baseline:  -0.9445946284032377 -0.17699999999999996 -0.17699999999999996
probs:  [0.05849508991079418, 0.11130391930495187, 0.10422260165896577, 0.12382229678932527, 0.1932829527805439, 0.40887313955541904]
maxi score, test score, baseline:  -0.9445946284032377 -0.17699999999999996 -0.17699999999999996
probs:  [0.05852314396011632, 0.110877050891532, 0.10427263653276003, 0.12388175306922491, 0.1933757982047178, 0.40906961734164904]
maxi score, test score, baseline:  -0.9445946284032377 -0.17699999999999996 -0.17699999999999996
printing an ep nov before normalisation:  29.27323341369629
maxi score, test score, baseline:  -0.9445946284032377 -0.17699999999999996 -0.17699999999999996
probs:  [0.058507790822228865, 0.11097609592630303, 0.10435725030578949, 0.12400921459193914, 0.19318773684291762, 0.40896191151082173]
maxi score, test score, baseline:  -0.9445946284032377 -0.17699999999999996 -0.17699999999999996
probs:  [0.058507790822228865, 0.11097609592630303, 0.10435725030578949, 0.12400921459193914, 0.19318773684291762, 0.40896191151082173]
maxi score, test score, baseline:  -0.9445946284032377 -0.17699999999999996 -0.17699999999999996
probs:  [0.058507790822228865, 0.11097609592630303, 0.10435725030578949, 0.12400921459193914, 0.19318773684291762, 0.40896191151082173]
printing an ep nov before normalisation:  28.573190553505434
maxi score, test score, baseline:  -0.9445946284032377 -0.17699999999999996 -0.17699999999999996
probs:  [0.058507790822228865, 0.11097609592630303, 0.10435725030578949, 0.12400921459193914, 0.19318773684291762, 0.40896191151082173]
maxi score, test score, baseline:  -0.9445946284032377 -0.17699999999999996 -0.17699999999999996
probs:  [0.058507790822228865, 0.11097609592630303, 0.10435725030578949, 0.12400921459193914, 0.19318773684291762, 0.40896191151082173]
printing an ep nov before normalisation:  25.441399308843238
printing an ep nov before normalisation:  50.245729689923415
printing an ep nov before normalisation:  49.98149460040667
printing an ep nov before normalisation:  43.005818221819396
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
probs:  [0.05846522724007668, 0.11102336907795984, 0.10439319060667183, 0.12407880317402474, 0.19337577355242863, 0.40866363634883823]
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
probs:  [0.05846522724007668, 0.11102336907795984, 0.10439319060667183, 0.12407880317402474, 0.19337577355242863, 0.40866363634883823]
Printing some Q and Qe and total Qs values:  [[-0.157]
 [-0.114]
 [-0.127]
 [-0.15 ]
 [-0.137]
 [-0.096]
 [-0.137]] [[38.664]
 [20.801]
 [40.201]
 [40.24 ]
 [39.086]
 [34.505]
 [38.318]] [[0.475]
 [0.209]
 [0.532]
 [0.51 ]
 [0.502]
 [0.464]
 [0.489]]
siam score:  -0.78380674
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actions average: 
K:  1  action  0 :  tensor([0.3176, 0.0068, 0.1307, 0.1256, 0.1420, 0.1445, 0.1327],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0218, 0.8100, 0.0175, 0.0272, 0.0185, 0.0091, 0.0960],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1196, 0.0030, 0.4311, 0.0850, 0.0935, 0.1746, 0.0932],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1768, 0.0935, 0.1415, 0.1461, 0.1559, 0.1588, 0.1274],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1893, 0.0052, 0.1291, 0.1384, 0.2728, 0.1441, 0.1210],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1260, 0.0742, 0.1417, 0.1394, 0.1240, 0.2701, 0.1247],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1757, 0.0197, 0.1454, 0.1521, 0.1735, 0.1629, 0.1706],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
probs:  [0.058520371089063015, 0.11018360943490671, 0.10449175221357739, 0.12419597447093805, 0.19355845437477873, 0.40904983841673603]
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
probs:  [0.058520371089063015, 0.11018360943490671, 0.10449175221357739, 0.12419597447093805, 0.19355845437477873, 0.40904983841673603]
Printing some Q and Qe and total Qs values:  [[-0.16 ]
 [-0.142]
 [-0.16 ]
 [-0.16 ]
 [-0.16 ]
 [-0.16 ]
 [-0.16 ]] [[38.796]
 [46.371]
 [38.796]
 [38.796]
 [38.796]
 [38.796]
 [38.796]] [[1.156]
 [1.661]
 [1.156]
 [1.156]
 [1.156]
 [1.156]
 [1.156]]
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
probs:  [0.058520371089063015, 0.11018360943490671, 0.10449175221357739, 0.12419597447093805, 0.19355845437477873, 0.40904983841673603]
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
probs:  [0.058520371089063015, 0.11018360943490671, 0.10449175221357739, 0.12419597447093805, 0.19355845437477873, 0.40904983841673603]
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
probs:  [0.058520371089063015, 0.11018360943490671, 0.10449175221357739, 0.12419597447093805, 0.19355845437477873, 0.40904983841673603]
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
probs:  [0.058520371089063015, 0.11018360943490671, 0.10449175221357739, 0.12419597447093805, 0.19355845437477873, 0.40904983841673603]
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
probs:  [0.05847791585841838, 0.11022943198674025, 0.10452784900005495, 0.12426574016981891, 0.19374674082669688, 0.40875232215827056]
printing an ep nov before normalisation:  17.570673359705324
Printing some Q and Qe and total Qs values:  [[-0.134]
 [-0.121]
 [-0.134]
 [-0.14 ]
 [-0.134]
 [-0.134]
 [-0.134]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.134]
 [-0.121]
 [-0.134]
 [-0.14 ]
 [-0.134]
 [-0.134]
 [-0.134]]
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
probs:  [0.0584355146988111, 0.1102751961789118, 0.10456389981361802, 0.12433541701501434, 0.19393478747674348, 0.4084551848169014]
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
probs:  [0.05846707173274457, 0.11033481225898523, 0.10462042456435343, 0.12386181028830928, 0.19403968563157617, 0.4086761955240314]
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
probs:  [0.05846707173274457, 0.11033481225898523, 0.10462042456435343, 0.12386181028830928, 0.19403968563157617, 0.4086761955240314]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
probs:  [0.05849823581029802, 0.11039368598415086, 0.10467624545421661, 0.12339410103553257, 0.19414327756734912, 0.4088944541484529]
Printing some Q and Qe and total Qs values:  [[-0.14]
 [-0.14]
 [-0.14]
 [-0.14]
 [-0.14]
 [-0.14]
 [-0.14]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.14]
 [-0.14]
 [-0.14]
 [-0.14]
 [-0.14]
 [-0.14]
 [-0.14]]
printing an ep nov before normalisation:  31.13785982131958
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
probs:  [0.0584137170927816, 0.1104855410503152, 0.10474866901692821, 0.12353013978994609, 0.19451976707255889, 0.40830216597747004]
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
printing an ep nov before normalisation:  52.934916923186144
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.1141710341323
printing an ep nov before normalisation:  49.10484995367861
printing an ep nov before normalisation:  39.5415109701009
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
probs:  [0.058368531235037366, 0.1103082312660295, 0.10454946217051235, 0.12390611484825997, 0.19488249256639245, 0.4079851679137684]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
probs:  [0.05832658966621821, 0.11035343354322667, 0.10458500244952694, 0.1239741315019809, 0.19506959259038653, 0.40769125024866076]
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
probs:  [0.05832658966621821, 0.11035343354322667, 0.10458500244952694, 0.1239741315019809, 0.19506959259038653, 0.40769125024866076]
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
probs:  [0.05832658966621821, 0.11035343354322667, 0.10458500244952694, 0.1239741315019809, 0.19506959259038653, 0.40769125024866076]
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
actions average: 
K:  1  action  0 :  tensor([0.3287, 0.0256, 0.1501, 0.1317, 0.1146, 0.1441, 0.1052],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0132, 0.9005, 0.0126, 0.0168, 0.0056, 0.0078, 0.0436],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1367, 0.0110, 0.3371, 0.1202, 0.0932, 0.1980, 0.1038],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1273, 0.0220, 0.1157, 0.3594, 0.1273, 0.1245, 0.1239],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1896, 0.0067, 0.1280, 0.1651, 0.1866, 0.1605, 0.1636],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1323, 0.0063, 0.1744, 0.1505, 0.1165, 0.3099, 0.1101],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1704, 0.0466, 0.1468, 0.1521, 0.1399, 0.1217, 0.2225],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  29.807522296905518
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
printing an ep nov before normalisation:  74.11071259426937
Printing some Q and Qe and total Qs values:  [[-0.152]
 [-0.135]
 [-0.151]
 [-0.153]
 [-0.15 ]
 [-0.152]
 [-0.152]] [[46.914]
 [50.201]
 [48.568]
 [47.909]
 [38.59 ]
 [46.238]
 [49.952]] [[0.315]
 [0.386]
 [0.343]
 [0.33 ]
 [0.179]
 [0.303]
 [0.365]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
probs:  [0.05831081060003153, 0.1104480873772875, 0.10421877690721043, 0.1240976968184014, 0.19534406611836386, 0.40758056217870525]
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
probs:  [0.05831081060003153, 0.1104480873772875, 0.10421877690721043, 0.1240976968184014, 0.19534406611836386, 0.40758056217870525]
printing an ep nov before normalisation:  63.14284113575681
Printing some Q and Qe and total Qs values:  [[-0.164]
 [-0.161]
 [-0.165]
 [-0.165]
 [-0.166]
 [-0.167]
 [-0.166]] [[41.233]
 [36.722]
 [42.483]
 [46.861]
 [42.451]
 [45.941]
 [42.174]] [[0.91 ]
 [0.705]
 [0.967]
 [1.168]
 [0.964]
 [1.124]
 [0.951]]
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
probs:  [0.05836361290122863, 0.11008371959028505, 0.10387085560768138, 0.12421020766627844, 0.19552124057036718, 0.4079503636641595]
siam score:  -0.7694975
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
probs:  [0.05836361290122863, 0.11008371959028505, 0.10387085560768138, 0.12421020766627844, 0.19552124057036718, 0.4079503636641595]
from probs:  [0.05836361290122863, 0.11008371959028505, 0.10387085560768138, 0.12421020766627844, 0.19552124057036718, 0.4079503636641595]
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
probs:  [0.05838902599755608, 0.11013170454540089, 0.10348005470025594, 0.12426435774970587, 0.19560651244893013, 0.40812834455815117]
actions average: 
K:  1  action  0 :  tensor([0.3414, 0.0591, 0.1175, 0.1322, 0.1458, 0.0909, 0.1131],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0198, 0.8744, 0.0239, 0.0190, 0.0135, 0.0195, 0.0300],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1182, 0.0699, 0.3445, 0.1139, 0.0994, 0.1429, 0.1111],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1252, 0.0600, 0.1346, 0.3084, 0.1090, 0.1068, 0.1560],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1419, 0.0118, 0.1570, 0.1652, 0.2370, 0.1400, 0.1471],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0866, 0.0309, 0.1821, 0.1252, 0.1021, 0.3720, 0.1011],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1517, 0.1256, 0.1294, 0.1355, 0.1090, 0.0841, 0.2646],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  72.16864004377278
siam score:  -0.7747284
maxi score, test score, baseline:  -0.9448377289377289 -0.17699999999999996 -0.17699999999999996
probs:  [0.05838902599755608, 0.11013170454540089, 0.10348005470025594, 0.12426435774970587, 0.19560651244893013, 0.40812834455815117]
printing an ep nov before normalisation:  62.01457977294922
maxi score, test score, baseline:  -0.9449584795321637 -0.17699999999999996 -0.17699999999999996
probs:  [0.058420210736411196, 0.11019058750381656, 0.10353537698375058, 0.12379592689036245, 0.19571115067287606, 0.40834674721278313]
Printing some Q and Qe and total Qs values:  [[-0.155]
 [-0.162]
 [-0.151]
 [-0.154]
 [-0.159]
 [-0.159]
 [-0.152]] [[45.671]
 [47.232]
 [44.07 ]
 [45.011]
 [45.447]
 [45.198]
 [44.801]] [[1.267]
 [1.352]
 [1.176]
 [1.228]
 [1.249]
 [1.234]
 [1.218]]
printing an ep nov before normalisation:  40.41976166183073
maxi score, test score, baseline:  -0.9450787016776075 -0.17699999999999996 -0.17699999999999996
probs:  [0.05837850363285315, 0.11023543352125868, 0.1035690963825086, 0.12386351920906223, 0.19589897559281366, 0.4080544716615037]
maxi score, test score, baseline:  -0.9450787016776075 -0.17699999999999996 -0.17699999999999996
probs:  [0.05837850363285315, 0.11023543352125868, 0.1035690963825086, 0.12386351920906223, 0.19589897559281366, 0.4080544716615037]
actions average: 
K:  1  action  0 :  tensor([0.3477, 0.0098, 0.1086, 0.1265, 0.1524, 0.1237, 0.1313],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0149, 0.8798, 0.0152, 0.0201, 0.0144, 0.0122, 0.0435],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1925, 0.0544, 0.2399, 0.1185, 0.1146, 0.1556, 0.1245],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1369, 0.0607, 0.1187, 0.2315, 0.1374, 0.1549, 0.1600],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1276, 0.0219, 0.1029, 0.1352, 0.3495, 0.1302, 0.1328],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1248, 0.0088, 0.1491, 0.1129, 0.1076, 0.3981, 0.0986],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1559, 0.0313, 0.1281, 0.1526, 0.1429, 0.1427, 0.2465],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9450787016776075 -0.17699999999999996 -0.17699999999999996
probs:  [0.05833684867616202, 0.11028022346721411, 0.10360277362151804, 0.12393102701631559, 0.1960865656731131, 0.40776256154567714]
maxi score, test score, baseline:  -0.9451983988355167 -0.17699999999999996 -0.17699999999999996
printing an ep nov before normalisation:  50.01505157729201
printing an ep nov before normalisation:  16.73119234201368
printing an ep nov before normalisation:  34.80163335800171
maxi score, test score, baseline:  -0.9451983988355167 -0.17699999999999996 -0.17699999999999996
probs:  [0.05833684867616202, 0.11028022346721411, 0.10360277362151804, 0.12393102701631559, 0.1960865656731131, 0.40776256154567714]
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9451983988355167 -0.17699999999999996 -0.17699999999999996
probs:  [0.05839011202331297, 0.10946663195443006, 0.10369746075974527, 0.12404431696278301, 0.19626588665643496, 0.40813559164329366]
using explorer policy with actor:  1
using another actor
from probs:  [0.058265609922205905, 0.10959673412251988, 0.1037988049854797, 0.12424708541360197, 0.19682866240604607, 0.4072631031501464]
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.23259777425392
printing an ep nov before normalisation:  41.1341780871749
maxi score, test score, baseline:  -0.9453175744371822 -0.17699999999999996 -0.17699999999999996
probs:  [0.05829086599760946, 0.1096442914123697, 0.10340973266812217, 0.12430100766488492, 0.1969141182997063, 0.40743998395730746]
from probs:  [0.05829086599760946, 0.1096442914123697, 0.10340973266812217, 0.12430100766488492, 0.1969141182997063, 0.40743998395730746]
maxi score, test score, baseline:  -0.945436231884058 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.945436231884058 -0.17699999999999996 -0.17699999999999996
probs:  [0.058317079157794584, 0.10924328518786525, 0.10345628207466714, 0.12435697331227226, 0.1970028125640956, 0.4076235677033052]
printing an ep nov before normalisation:  47.710745127994336
maxi score, test score, baseline:  -0.945436231884058 -0.17699999999999996 -0.17699999999999996
probs:  [0.058317079157794584, 0.10924328518786525, 0.10345628207466714, 0.12435697331227226, 0.1970028125640956, 0.4076235677033052]
maxi score, test score, baseline:  -0.945436231884058 -0.17699999999999996 -0.17699999999999996
probs:  [0.058317079157794584, 0.10924328518786525, 0.10345628207466714, 0.12435697331227226, 0.1970028125640956, 0.4076235677033052]
maxi score, test score, baseline:  -0.945436231884058 -0.17699999999999996 -0.17699999999999996
probs:  [0.058317079157794584, 0.10924328518786525, 0.10345628207466714, 0.12435697331227226, 0.1970028125640956, 0.4076235677033052]
actions average: 
K:  4  action  0 :  tensor([0.2984, 0.0247, 0.1245, 0.1724, 0.1512, 0.1211, 0.1076],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0356, 0.7823, 0.0287, 0.0693, 0.0286, 0.0132, 0.0423],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1197, 0.0889, 0.2369, 0.1288, 0.1540, 0.1523, 0.1193],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2061, 0.0142, 0.1678, 0.1910, 0.1335, 0.1825, 0.1049],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1577, 0.0551, 0.1235, 0.1403, 0.2939, 0.1057, 0.1238],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1609, 0.0070, 0.1886, 0.1391, 0.1443, 0.2188, 0.1414],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1875, 0.0966, 0.1503, 0.1385, 0.1567, 0.1354, 0.1350],
       grad_fn=<DivBackward0>)
siam score:  -0.78298
actor:  0 policy actor:  1  step number:  68 total reward:  0.20666666666666578  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.149]
 [-0.137]
 [-0.15 ]
 [-0.147]
 [-0.15 ]
 [-0.151]
 [-0.152]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.149]
 [-0.137]
 [-0.15 ]
 [-0.147]
 [-0.15 ]
 [-0.151]
 [-0.152]]
printing an ep nov before normalisation:  50.995131215407056
maxi score, test score, baseline:  -0.9429368763557484 -0.17699999999999996 -0.17699999999999996
probs:  [0.05834295639046488, 0.10884741794788146, 0.10350223494014844, 0.12441222174736201, 0.19709037019163508, 0.4078047987825081]
Printing some Q and Qe and total Qs values:  [[-0.175]
 [-0.169]
 [-0.175]
 [-0.175]
 [-0.175]
 [-0.175]
 [-0.175]] [[51.33]
 [60.83]
 [51.33]
 [51.33]
 [51.33]
 [51.33]
 [51.33]] [[0.959]
 [1.385]
 [0.959]
 [0.959]
 [0.959]
 [0.959]
 [0.959]]
using another actor
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  53 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9429368763557484 -0.17699999999999996 -0.17699999999999996
probs:  [0.0586529193611463, 0.10853179338170102, 0.10325281996751218, 0.1239037994169764, 0.19568169979470498, 0.40997696807795914]
maxi score, test score, baseline:  -0.9429368763557484 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9429368763557484 -0.17699999999999996 -0.17699999999999996
probs:  [0.0586529193611463, 0.10853179338170102, 0.10325281996751218, 0.1239037994169764, 0.19568169979470498, 0.40997696807795914]
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]]
maxi score, test score, baseline:  -0.9429368763557484 -0.17699999999999996 -0.17699999999999996
probs:  [0.05863375641650963, 0.10858508620042835, 0.10330938726607952, 0.12394755717653333, 0.19568093463866723, 0.409843278301782]
printing an ep nov before normalisation:  58.80602597136844
printing an ep nov before normalisation:  53.20592600520704
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.9430601731601732 -0.17699999999999996 -0.17699999999999996
probs:  [0.05863375641650963, 0.10858508620042835, 0.10330938726607952, 0.12394755717653333, 0.19568093463866723, 0.409843278301782]
maxi score, test score, baseline:  -0.9430601731601732 -0.17699999999999996 -0.17699999999999996
probs:  [0.058620543048219266, 0.10867795306408666, 0.10339105027652579, 0.12407304888704758, 0.19548684155026505, 0.40975056317385566]
maxi score, test score, baseline:  -0.9430601731601732 -0.17699999999999996 -0.17699999999999996
probs:  [0.058620543048219266, 0.10867795306408666, 0.10339105027652579, 0.12407304888704758, 0.19548684155026505, 0.40975056317385566]
actor:  0 policy actor:  1  step number:  67 total reward:  0.03999999999999926  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9409367170626349 -0.17699999999999996 -0.17699999999999996
probs:  [0.058645932595266265, 0.10829131624020288, 0.10343587485618452, 0.12412685159410214, 0.19557164523263262, 0.4099283794816115]
printing an ep nov before normalisation:  78.50154276351263
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.77841604
maxi score, test score, baseline:  -0.9409367170626349 -0.17699999999999996 -0.17699999999999996
probs:  [0.058673420819996075, 0.108342126979979, 0.10348440459627, 0.1241851015857168, 0.1951940520886829, 0.4101208939293552]
maxi score, test score, baseline:  -0.9409367170626349 -0.17699999999999996 -0.17699999999999996
probs:  [0.058673420819996075, 0.108342126979979, 0.10348440459627, 0.1241851015857168, 0.1951940520886829, 0.4101208939293552]
Printing some Q and Qe and total Qs values:  [[-0.135]
 [-0.114]
 [-0.135]
 [-0.129]
 [-0.133]
 [-0.127]
 [-0.134]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.135]
 [-0.114]
 [-0.135]
 [-0.129]
 [-0.133]
 [-0.127]
 [-0.134]]
Printing some Q and Qe and total Qs values:  [[-0.163]
 [-0.112]
 [-0.161]
 [-0.136]
 [-0.134]
 [-0.161]
 [-0.121]] [[52.956]
 [57.661]
 [72.405]
 [55.172]
 [52.877]
 [44.535]
 [62.093]] [[0.624]
 [0.756]
 [0.96 ]
 [0.689]
 [0.652]
 [0.482]
 [0.823]]
using explorer policy with actor:  1
siam score:  -0.7762105
maxi score, test score, baseline:  -0.9410637931034482 -0.17699999999999996 -0.17699999999999996
probs:  [0.058673420819996075, 0.108342126979979, 0.10348440459627, 0.1241851015857168, 0.1951940520886829, 0.4101208939293552]
Printing some Q and Qe and total Qs values:  [[-0.13 ]
 [-0.113]
 [-0.126]
 [-0.122]
 [-0.125]
 [-0.131]
 [-0.128]] [[58.821]
 [58.608]
 [56.785]
 [56.437]
 [57.756]
 [61.148]
 [59.419]] [[1.039]
 [1.051]
 [0.994]
 [0.989]
 [1.019]
 [1.096]
 [1.056]]
maxi score, test score, baseline:  -0.9410637931034482 -0.17699999999999996 -0.17699999999999996
probs:  [0.05873494449716742, 0.10845585071374313, 0.10359302303531345, 0.1232653511316799, 0.19539905412938066, 0.41055177649271557]
maxi score, test score, baseline:  -0.9410637931034482 -0.17699999999999996 -0.17699999999999996
probs:  [0.05873494449716742, 0.10845585071374313, 0.10359302303531345, 0.1232653511316799, 0.19539905412938066, 0.41055177649271557]
maxi score, test score, baseline:  -0.9411903225806452 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9411903225806452 -0.17699999999999996 -0.17699999999999996
probs:  [0.05873494449716742, 0.10845585071374313, 0.10359302303531345, 0.1232653511316799, 0.19539905412938066, 0.41055177649271557]
printing an ep nov before normalisation:  44.32524778945863
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.145]
 [-0.065]
 [-0.086]
 [-0.077]
 [-0.043]
 [-0.062]
 [-0.077]] [[55.301]
 [40.86 ]
 [39.091]
 [44.94 ]
 [42.107]
 [43.447]
 [40.701]] [[0.43 ]
 [0.337]
 [0.295]
 [0.374]
 [0.374]
 [0.371]
 [0.323]]
maxi score, test score, baseline:  -0.9411903225806452 -0.17699999999999996 -0.17699999999999996
probs:  [0.05873494449716742, 0.10845585071374313, 0.10359302303531345, 0.1232653511316799, 0.19539905412938066, 0.41055177649271557]
actions average: 
K:  1  action  0 :  tensor([0.3727, 0.0167, 0.0908, 0.1203, 0.1838, 0.1032, 0.1125],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0367, 0.8239, 0.0185, 0.0300, 0.0136, 0.0153, 0.0621],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1734, 0.0059, 0.2822, 0.1320, 0.1304, 0.1640, 0.1121],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1535, 0.0427, 0.1060, 0.2695, 0.1427, 0.1225, 0.1630],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1603, 0.0379, 0.1057, 0.1436, 0.3108, 0.1226, 0.1192],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1598, 0.0120, 0.1786, 0.1609, 0.1370, 0.2237, 0.1279],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1518, 0.1554, 0.1371, 0.1288, 0.1393, 0.1584, 0.1292],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9411903225806452 -0.17699999999999996 -0.17699999999999996
probs:  [0.05873494449716742, 0.10845585071374313, 0.10359302303531345, 0.1232653511316799, 0.19539905412938066, 0.41055177649271557]
printing an ep nov before normalisation:  42.01648558576514
actor:  0 policy actor:  1  step number:  59 total reward:  0.11999999999999955  reward:  1.0 rdn_beta:  1.667
using another actor
from probs:  [0.05873494449716742, 0.10845585071374313, 0.10359302303531345, 0.1232653511316799, 0.19539905412938066, 0.41055177649271557]
printing an ep nov before normalisation:  52.680612121514685
maxi score, test score, baseline:  -0.9390434689507494 -0.17699999999999996 -0.17699999999999996
Printing some Q and Qe and total Qs values:  [[-0.142]
 [-0.127]
 [-0.142]
 [-0.142]
 [-0.142]
 [-0.142]
 [-0.142]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.142]
 [-0.127]
 [-0.142]
 [-0.142]
 [-0.142]
 [-0.142]
 [-0.142]]
maxi score, test score, baseline:  -0.9390434689507494 -0.17699999999999996 -0.17699999999999996
probs:  [0.058760088893526255, 0.10807359929191339, 0.10363741480785947, 0.12331818376285583, 0.1954828373621565, 0.4107278758816885]
maxi score, test score, baseline:  -0.9390434689507494 -0.17699999999999996 -0.17699999999999996
actor:  1 policy actor:  1  step number:  77 total reward:  0.06666666666666488  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  48.983779368634444
maxi score, test score, baseline:  -0.9390434689507494 -0.17699999999999996 -0.17699999999999996
probs:  [0.05835000052679994, 0.11430786552840763, 0.10291341454793299, 0.1224565187175076, 0.19411638860761993, 0.4078558120717318]
printing an ep nov before normalisation:  44.146569198248145
printing an ep nov before normalisation:  37.525506892353974
maxi score, test score, baseline:  -0.9390434689507494 -0.17699999999999996 -0.17699999999999996
probs:  [0.058308997972328275, 0.11435857849739554, 0.10294545189310685, 0.12252058741392917, 0.19429790857702747, 0.40756847564621274]
printing an ep nov before normalisation:  46.43823806648223
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.166]
 [-0.181]
 [-0.169]
 [-0.168]
 [-0.168]
 [-0.169]
 [-0.171]] [[65.   ]
 [58.779]
 [61.915]
 [62.641]
 [65.197]
 [65.939]
 [66.366]] [[1.039]
 [0.804]
 [0.927]
 [0.954]
 [1.044]
 [1.069]
 [1.082]]
maxi score, test score, baseline:  -0.9390434689507494 -0.17699999999999996 -0.17699999999999996
probs:  [0.05833606496388645, 0.1144117232441959, 0.10299328654267995, 0.12257752963555119, 0.19392335571742633, 0.4077580398962602]
printing an ep nov before normalisation:  47.43743419647217
maxi score, test score, baseline:  -0.9390434689507494 -0.17699999999999996 -0.17699999999999996
probs:  [0.05833606496388645, 0.1144117232441959, 0.10299328654267995, 0.12257752963555119, 0.19392335571742633, 0.4077580398962602]
maxi score, test score, baseline:  -0.9390434689507494 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9390434689507494 -0.17699999999999996 -0.17699999999999996
probs:  [0.05836592695571943, 0.1144703558407114, 0.10304606070902257, 0.1221276943462846, 0.19402278315818802, 0.40796717899007395]
maxi score, test score, baseline:  -0.9390434689507494 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9390434689507494 -0.17699999999999996 -0.17699999999999996
probs:  [0.058347234923505575, 0.11451919968655715, 0.10310188157299753, 0.12217186172071928, 0.1940230429363764, 0.40783677915984407]
maxi score, test score, baseline:  -0.9390434689507494 -0.17699999999999996 -0.17699999999999996
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9390434689507494 -0.17699999999999996 -0.17699999999999996
probs:  [0.058347234923505575, 0.11451919968655715, 0.10310188157299753, 0.12217186172071928, 0.1940230429363764, 0.40783677915984407]
printing an ep nov before normalisation:  11.244111061096191
siam score:  -0.78787863
maxi score, test score, baseline:  -0.9390434689507494 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9390434689507494 -0.17699999999999996 -0.17699999999999996
probs:  [0.05837416181003319, 0.11457210891253733, 0.10314950971906328, 0.12222831068513527, 0.19365054645589608, 0.4080253624173349]
actor:  1 policy actor:  1  step number:  65 total reward:  0.013333333333332753  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  34.71216774020061
printing an ep nov before normalisation:  38.566711254226476
maxi score, test score, baseline:  -0.9390434689507494 -0.17699999999999996 -0.17699999999999996
probs:  [0.05828827912873208, 0.11402336071214293, 0.10308500023916366, 0.12213022945576596, 0.19504874812385625, 0.40742438234033906]
actor:  1 policy actor:  1  step number:  60 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  76.52657592626504
maxi score, test score, baseline:  -0.9390434689507494 -0.17699999999999996 -0.17699999999999996
probs:  [0.05782834151495132, 0.11312262458852239, 0.10227077359681773, 0.12116537759821024, 0.20140969120458926, 0.4042031914969091]
siam score:  -0.78294516
maxi score, test score, baseline:  -0.9390434689507494 -0.17699999999999996 -0.17699999999999996
actor:  1 policy actor:  1  step number:  44 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  57.93249612259201
maxi score, test score, baseline:  -0.9390434689507494 -0.17699999999999996 -0.17699999999999996
probs:  [0.05838946576580945, 0.11246883050881233, 0.1018554148843386, 0.1203348692631277, 0.19881606592313927, 0.4081353536547726]
printing an ep nov before normalisation:  3.910971362146043
siam score:  -0.78128624
maxi score, test score, baseline:  -0.9390434689507494 -0.17699999999999996 -0.17699999999999996
probs:  [0.05834884244621283, 0.11251616278113855, 0.1018854853182906, 0.12039499499369832, 0.19900383479707612, 0.40785067966358374]
maxi score, test score, baseline:  -0.9390434689507494 -0.17699999999999996 -0.17699999999999996
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9390434689507494 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9390434689507494 -0.17699999999999996 -0.17699999999999996
probs:  [0.05830826901050028, 0.11256343693124306, 0.10191551882689878, 0.12045504689216939, 0.19919137309801105, 0.40756635524117746]
Printing some Q and Qe and total Qs values:  [[-0.14 ]
 [-0.14 ]
 [-0.14 ]
 [-0.151]
 [-0.163]
 [-0.14 ]
 [-0.151]] [[63.003]
 [77.271]
 [63.003]
 [69.365]
 [67.461]
 [63.003]
 [75.74 ]] [[0.883]
 [1.419]
 [0.883]
 [1.111]
 [1.028]
 [0.883]
 [1.35 ]]
printing an ep nov before normalisation:  39.79854844760898
maxi score, test score, baseline:  -0.9391735042735042 -0.17699999999999996 -0.17699999999999996
probs:  [0.058358178639873984, 0.11221368007971862, 0.10159181734524361, 0.12055827358897311, 0.19936214939523816, 0.40791590095095254]
printing an ep nov before normalisation:  42.72724593887401
printing an ep nov before normalisation:  47.80972242408122
maxi score, test score, baseline:  -0.9391735042735042 -0.17699999999999996 -0.17699999999999996
probs:  [0.05831770095550064, 0.11226037068590702, 0.1016213158430607, 0.12061847040960919, 0.19954989491489322, 0.4076322471910292]
maxi score, test score, baseline:  -0.9391735042735042 -0.17699999999999996 -0.17699999999999996
probs:  [0.058277272803104276, 0.11230700415745279, 0.10165077824397802, 0.1206785935682367, 0.19973741069297843, 0.40734894053424975]
maxi score, test score, baseline:  -0.9391735042735042 -0.17699999999999996 -0.17699999999999996
probs:  [0.058277272803104276, 0.11230700415745279, 0.10165077824397802, 0.1206785935682367, 0.19973741069297843, 0.40734894053424975]
maxi score, test score, baseline:  -0.9391735042735042 -0.17699999999999996 -0.17699999999999996
probs:  [0.05830093219372036, 0.11235264878311547, 0.10128550206746861, 0.12072764467689114, 0.19981863162060892, 0.4075146406581955]
UNIT TEST: sample policy line 217 mcts : [0.02  0.041 0.061 0.041 0.714 0.102 0.02 ]
maxi score, test score, baseline:  -0.9391735042735042 -0.17699999999999996 -0.17699999999999996
probs:  [0.05828326099598995, 0.11240014865964802, 0.10133940531694445, 0.12077029883993637, 0.19981552429707913, 0.40739136189040204]
maxi score, test score, baseline:  -0.9393029850746268 -0.17699999999999996 -0.17699999999999996
probs:  [0.05828326099598995, 0.11240014865964802, 0.10133940531694445, 0.12077029883993637, 0.19981552429707913, 0.40739136189040204]
printing an ep nov before normalisation:  30.76344799580382
maxi score, test score, baseline:  -0.9393029850746268 -0.17699999999999996 -0.17699999999999996
probs:  [0.05828326099598995, 0.11240014865964802, 0.10133940531694445, 0.12077029883993637, 0.19981552429707913, 0.40739136189040204]
printing an ep nov before normalisation:  50.63483307192129
printing an ep nov before normalisation:  39.830462531771396
maxi score, test score, baseline:  -0.9393029850746268 -0.17699999999999996 -0.17699999999999996
probs:  [0.058242898873428844, 0.11244686696156325, 0.10136832558451424, 0.12083048569597764, 0.20000290419918793, 0.40710851868532816]
maxi score, test score, baseline:  -0.9393029850746268 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9393029850746268 -0.17699999999999996 -0.17699999999999996
printing an ep nov before normalisation:  69.06413140546277
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9393029850746268 -0.17699999999999996 -0.17699999999999996
using explorer policy with actor:  1
siam score:  -0.7746866
maxi score, test score, baseline:  -0.9393029850746268 -0.17699999999999996 -0.17699999999999996
probs:  [0.05823595497231952, 0.11259312266339373, 0.10152187304999667, 0.12097122340006104, 0.1996169943078514, 0.4070608316063777]
using another actor
printing an ep nov before normalisation:  59.702546996376405
maxi score, test score, baseline:  -0.9393029850746268 -0.17699999999999996 -0.17699999999999996
probs:  [0.058195657643568405, 0.11263998760226031, 0.10155098512824419, 0.12103152270811122, 0.19980340290418944, 0.4067784440136264]
printing an ep nov before normalisation:  49.19046907447126
printing an ep nov before normalisation:  63.849092491959766
actor:  1 policy actor:  1  step number:  51 total reward:  0.3066666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.939431914893617 -0.17699999999999996 -0.17699999999999996
probs:  [0.05862225004342024, 0.1121438696867658, 0.1012428011441148, 0.12039318693430834, 0.19783005893997826, 0.4097678332514126]
maxi score, test score, baseline:  -0.939431914893617 -0.17699999999999996 -0.17699999999999996
probs:  [0.05862225004342024, 0.1121438696867658, 0.1012428011441148, 0.12039318693430834, 0.19783005893997826, 0.4097678332514126]
Printing some Q and Qe and total Qs values:  [[-0.143]
 [-0.137]
 [-0.143]
 [-0.143]
 [-0.143]
 [-0.143]
 [-0.143]] [[39.817]
 [51.048]
 [39.817]
 [39.817]
 [39.817]
 [39.817]
 [39.817]] [[0.882]
 [1.452]
 [0.882]
 [0.882]
 [0.882]
 [0.882]
 [0.882]]
Printing some Q and Qe and total Qs values:  [[-0.114]
 [-0.073]
 [-0.114]
 [-0.114]
 [-0.114]
 [-0.114]
 [-0.114]] [[42.798]
 [49.154]
 [42.798]
 [42.798]
 [42.798]
 [42.798]
 [42.798]] [[1.363]
 [1.833]
 [1.363]
 [1.363]
 [1.363]
 [1.363]
 [1.363]]
maxi score, test score, baseline:  -0.939431914893617 -0.17699999999999996 -0.17699999999999996
probs:  [0.05862225004342024, 0.1121438696867658, 0.1012428011441148, 0.12039318693430834, 0.19783005893997826, 0.4097678332514126]
printing an ep nov before normalisation:  2.412146500319068e-05
maxi score, test score, baseline:  -0.939431914893617 -0.17699999999999996 -0.17699999999999996
probs:  [0.058542827496002814, 0.11223623642532392, 0.10130017853082235, 0.12051203165184364, 0.198197454150542, 0.4092112717454653]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.939431914893617 -0.17699999999999996 -0.17699999999999996
probs:  [0.05853175139820458, 0.11233721764709637, 0.10137833636777413, 0.1201413186785057, 0.19847783511312803, 0.40913354079529124]
Printing some Q and Qe and total Qs values:  [[-0.122]
 [-0.119]
 [-0.119]
 [-0.117]
 [-0.119]
 [-0.119]
 [-0.119]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.122]
 [-0.119]
 [-0.119]
 [-0.117]
 [-0.119]
 [-0.119]
 [-0.119]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.318]
 [0.439]
 [0.401]
 [0.339]
 [0.312]
 [0.431]
 [0.391]] [[40.848]
 [36.961]
 [38.574]
 [41.165]
 [43.198]
 [41.54 ]
 [39.233]] [[1.497]
 [1.397]
 [1.45 ]
 [1.537]
 [1.625]
 [1.65 ]
 [1.479]]
Printing some Q and Qe and total Qs values:  [[-0.162]
 [-0.148]
 [-0.158]
 [-0.152]
 [-0.16 ]
 [-0.161]
 [-0.161]] [[31.998]
 [43.917]
 [31.092]
 [41.191]
 [31.271]
 [31.1  ]
 [31.249]] [[0.731]
 [1.382]
 [0.687]
 [1.232]
 [0.694]
 [0.684]
 [0.691]]
maxi score, test score, baseline:  -0.939431914893617 -0.17699999999999996 -0.17699999999999996
probs:  [0.05853175139820458, 0.11233721764709637, 0.10137833636777413, 0.1201413186785057, 0.19847783511312803, 0.40913354079529124]
maxi score, test score, baseline:  -0.939431914893617 -0.17699999999999996 -0.17699999999999996
probs:  [0.05853175139820458, 0.11233721764709637, 0.10137833636777413, 0.1201413186785057, 0.19847783511312803, 0.40913354079529124]
actor:  1 policy actor:  1  step number:  55 total reward:  0.14666666666666617  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  4  action  0 :  tensor([0.2491, 0.0106, 0.1578, 0.1408, 0.1355, 0.1715, 0.1348],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0392, 0.7747, 0.0371, 0.0271, 0.0493, 0.0332, 0.0394],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1284, 0.0617, 0.2756, 0.1251, 0.1144, 0.1618, 0.1329],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1378, 0.0273, 0.1163, 0.3329, 0.1124, 0.1537, 0.1196],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2673, 0.0245, 0.0945, 0.1244, 0.2378, 0.1343, 0.1172],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1676, 0.0574, 0.1634, 0.1417, 0.1666, 0.1739, 0.1294],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1607, 0.1879, 0.1165, 0.1726, 0.1177, 0.1195, 0.1252],
       grad_fn=<DivBackward0>)
actions average: 
K:  4  action  0 :  tensor([0.1735, 0.0377, 0.1575, 0.1893, 0.1316, 0.1564, 0.1539],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0292, 0.8080, 0.0458, 0.0265, 0.0250, 0.0308, 0.0346],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1553, 0.0430, 0.2043, 0.1517, 0.1114, 0.1838, 0.1505],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1294, 0.1172, 0.1245, 0.2438, 0.1003, 0.1346, 0.1502],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1522, 0.1696, 0.1141, 0.1815, 0.1179, 0.1267, 0.1380],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1592, 0.1547, 0.1440, 0.1272, 0.1341, 0.1393, 0.1414],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1793, 0.0095, 0.1729, 0.1860, 0.0812, 0.1613, 0.2099],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.148]
 [-0.13 ]
 [-0.143]
 [-0.144]
 [-0.144]
 [-0.147]
 [-0.148]] [[37.982]
 [42.527]
 [37.938]
 [37.198]
 [37.2  ]
 [37.483]
 [37.982]] [[0.935]
 [1.206]
 [0.938]
 [0.896]
 [0.896]
 [0.909]
 [0.935]]
maxi score, test score, baseline:  -0.939431914893617 -0.17699999999999996 -0.17699999999999996
probs:  [0.05818348587833264, 0.11178960710328156, 0.10087132761785345, 0.11956479455368478, 0.20289651623809798, 0.4066942686087497]
printing an ep nov before normalisation:  39.977283048529664
line 256 mcts: sample exp_bonus 37.61999502392558
maxi score, test score, baseline:  -0.939431914893617 -0.17699999999999996 -0.17699999999999996
probs:  [0.058211374906651424, 0.11184325006716243, 0.10091972512356848, 0.11962217294314774, 0.2025138846333021, 0.4068895923261679]
printing an ep nov before normalisation:  27.20179557800293
printing an ep nov before normalisation:  55.27111684133273
maxi score, test score, baseline:  -0.939431914893617 -0.17699999999999996 -0.17699999999999996
probs:  [0.05822478610348164, 0.11155337973795514, 0.10103985927005514, 0.11978904222192031, 0.20240957112748273, 0.406983361539105]
maxi score, test score, baseline:  -0.939431914893617 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.939431914893617 -0.17699999999999996 -0.17699999999999996
probs:  [0.058247850697788546, 0.11159761784509016, 0.10068320183013857, 0.11983655020537062, 0.20248988265419993, 0.40714489676741217]
Printing some Q and Qe and total Qs values:  [[-0.136]
 [-0.14 ]
 [-0.136]
 [-0.136]
 [-0.136]
 [-0.136]
 [-0.136]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.136]
 [-0.14 ]
 [-0.136]
 [-0.136]
 [-0.136]
 [-0.136]
 [-0.136]]
siam score:  -0.7697462
Printing some Q and Qe and total Qs values:  [[-0.14 ]
 [-0.137]
 [-0.139]
 [-0.138]
 [-0.139]
 [-0.139]
 [-0.138]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.14 ]
 [-0.137]
 [-0.139]
 [-0.138]
 [-0.139]
 [-0.139]
 [-0.138]]
maxi score, test score, baseline:  -0.939431914893617 -0.17699999999999996 -0.17699999999999996
probs:  [0.058303626066667176, 0.11170459552313289, 0.10077970442397707, 0.11946852016389449, 0.20220802859530215, 0.4075355252270263]
maxi score, test score, baseline:  -0.939431914893617 -0.17699999999999996 -0.17699999999999996
printing an ep nov before normalisation:  39.606985847946284
printing an ep nov before normalisation:  45.97262661200765
printing an ep nov before normalisation:  40.80023765563965
siam score:  -0.7739339
maxi score, test score, baseline:  -0.939431914893617 -0.17699999999999996 -0.17699999999999996
probs:  [0.058303626066667176, 0.11170459552313289, 0.10077970442397707, 0.11946852016389449, 0.20220802859530215, 0.4075355252270263]
printing an ep nov before normalisation:  49.1492441910509
maxi score, test score, baseline:  -0.939431914893617 -0.17699999999999996 -0.17699999999999996
probs:  [0.058303626066667176, 0.11170459552313289, 0.10077970442397707, 0.11946852016389449, 0.20220802859530215, 0.4075355252270263]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.939431914893617 -0.17699999999999996 -0.17699999999999996
probs:  [0.05826401993195131, 0.11174937687949292, 0.10080722159656817, 0.11952557055072352, 0.20239582903405803, 0.40725798200720603]
printing an ep nov before normalisation:  45.425608175221235
siam score:  -0.7766438
maxi score, test score, baseline:  -0.939431914893617 -0.17699999999999996 -0.17699999999999996
probs:  [0.05826401993195131, 0.11174937687949292, 0.10080722159656817, 0.11952557055072352, 0.20239582903405803, 0.40725798200720603]
actor:  1 policy actor:  1  step number:  66 total reward:  0.27333333333333265  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  41 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9395602972399151 -0.17699999999999996 -0.17699999999999996
probs:  [0.05934654336615022, 0.11060661535416949, 0.10011971390080657, 0.11760170348010689, 0.19748167664354033, 0.4148437472552266]
printing an ep nov before normalisation:  26.1029314994812
maxi score, test score, baseline:  -0.9396881355932204 -0.17699999999999996 -0.17699999999999996
probs:  [0.05934654336615022, 0.11060661535416949, 0.10011971390080657, 0.11760170348010689, 0.19748167664354033, 0.4148437472552266]
maxi score, test score, baseline:  -0.9396881355932204 -0.17699999999999996 -0.17699999999999996
probs:  [0.05934654336615022, 0.11060661535416949, 0.10011971390080657, 0.11760170348010689, 0.19748167664354033, 0.4148437472552266]
maxi score, test score, baseline:  -0.9396881355932204 -0.17699999999999996 -0.17699999999999996
probs:  [0.059308231590884206, 0.11065006275321083, 0.10014643482306754, 0.11765630795730288, 0.1976636886798023, 0.4145752741957323]
maxi score, test score, baseline:  -0.9398154334038056 -0.17699999999999996 -0.17699999999999996
probs:  [0.059308231590884206, 0.11065006275321083, 0.10014643482306754, 0.11765630795730288, 0.1976636886798023, 0.4145752741957323]
printing an ep nov before normalisation:  44.10806806008674
printing an ep nov before normalisation:  47.77346520787023
maxi score, test score, baseline:  -0.9398154334038056 -0.17699999999999996 -0.17699999999999996
probs:  [0.05926996369324466, 0.1106934603929097, 0.10017312514244885, 0.11771084989720222, 0.19784549226171597, 0.41430710861247855]
printing an ep nov before normalisation:  41.80882791344585
actor:  1 policy actor:  1  step number:  68 total reward:  0.12666666666666637  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9398154334038056 -0.17699999999999996 -0.17699999999999996
probs:  [0.059397460463569855, 0.11054887291247217, 0.10008420127335832, 0.11752913306368441, 0.19723977909330812, 0.41520055319360705]
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.21821364544324
maxi score, test score, baseline:  -0.9398154334038056 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9399421940928271 -0.17699999999999996 -0.17699999999999996
probs:  [0.05941970940028331, 0.1105903250762773, 0.09974660560000868, 0.11757320575181968, 0.1973137766958113, 0.4153563774757998]
printing an ep nov before normalisation:  45.11899372366216
siam score:  -0.7703108
maxi score, test score, baseline:  -0.9399421940928271 -0.17699999999999996 -0.17699999999999996
probs:  [0.05941970940028331, 0.1105903250762773, 0.09974660560000868, 0.11757320575181968, 0.1973137766958113, 0.4153563774757998]
Printing some Q and Qe and total Qs values:  [[-0.151]
 [-0.144]
 [-0.152]
 [-0.152]
 [-0.144]
 [-0.15 ]
 [-0.154]] [[30.459]
 [39.642]
 [29.916]
 [29.604]
 [34.273]
 [41.818]
 [29.736]] [[0.662]
 [0.914]
 [0.646]
 [0.638]
 [0.771]
 [0.965]
 [0.639]]
maxi score, test score, baseline:  -0.9399421940928271 -0.17699999999999996 -0.17699999999999996
probs:  [0.05941970940028331, 0.1105903250762773, 0.09974660560000868, 0.11757320575181968, 0.1973137766958113, 0.4153563774757998]
maxi score, test score, baseline:  -0.9399421940928271 -0.17699999999999996 -0.17699999999999996
probs:  [0.05941970940028331, 0.1105903250762773, 0.09974660560000868, 0.11757320575181968, 0.1973137766958113, 0.4153563774757998]
printing an ep nov before normalisation:  47.729330427337224
Printing some Q and Qe and total Qs values:  [[-0.081]
 [-0.03 ]
 [-0.058]
 [-0.035]
 [-0.041]
 [-0.122]
 [-0.062]] [[34.205]
 [40.538]
 [26.522]
 [34.517]
 [39.036]
 [25.219]
 [35.584]] [[1.092]
 [1.361]
 [0.851]
 [1.149]
 [1.299]
 [0.742]
 [1.159]]
maxi score, test score, baseline:  -0.9399421940928271 -0.17699999999999996 -0.17699999999999996
probs:  [0.05941970940028331, 0.1105903250762773, 0.09974660560000868, 0.11757320575181968, 0.1973137766958113, 0.4153563774757998]
printing an ep nov before normalisation:  41.322019321161314
maxi score, test score, baseline:  -0.9400684210526317 -0.17699999999999996 -0.17699999999999996
probs:  [0.05944662512623659, 0.11064047197073426, 0.09979182950749292, 0.11762652283439091, 0.19694966413633894, 0.4155448864248064]
maxi score, test score, baseline:  -0.9400684210526317 -0.17699999999999996 -0.17699999999999996
probs:  [0.059473402938976884, 0.11069036191803153, 0.09983682169266003, 0.11767956672610795, 0.19658741724948664, 0.4157324294747369]
maxi score, test score, baseline:  -0.9400684210526317 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9400684210526317 -0.17699999999999996 -0.17699999999999996
probs:  [0.059473402938976884, 0.11069036191803153, 0.09983682169266003, 0.11767956672610795, 0.19658741724948664, 0.4157324294747369]
maxi score, test score, baseline:  -0.9400684210526317 -0.17699999999999996 -0.17699999999999996
probs:  [0.059473402938976884, 0.11069036191803153, 0.09983682169266003, 0.11767956672610795, 0.19658741724948664, 0.4157324294747369]
Printing some Q and Qe and total Qs values:  [[-0.151]
 [-0.151]
 [-0.151]
 [-0.151]
 [-0.151]
 [-0.151]
 [-0.151]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.151]
 [-0.151]
 [-0.151]
 [-0.151]
 [-0.151]
 [-0.151]
 [-0.151]]
maxi score, test score, baseline:  -0.9400684210526317 -0.17699999999999996 -0.17699999999999996
probs:  [0.05941876437463292, 0.11077787933691605, 0.09991283495130543, 0.11777449232060948, 0.19676598085865393, 0.4153500481578822]
printing an ep nov before normalisation:  36.37135028839111
printing an ep nov before normalisation:  41.75838968580653
maxi score, test score, baseline:  -0.9400684210526317 -0.17699999999999996 -0.17699999999999996
probs:  [0.05941876437463292, 0.11077787933691605, 0.09991283495130543, 0.11777449232060948, 0.19676598085865393, 0.4153500481578822]
printing an ep nov before normalisation:  46.29349246200549
maxi score, test score, baseline:  -0.9400684210526317 -0.17699999999999996 -0.17699999999999996
probs:  [0.059440759141558316, 0.1108189281903588, 0.09957915229033508, 0.11781813689769775, 0.19683893105477093, 0.4155040924252792]
siam score:  -0.7752596
maxi score, test score, baseline:  -0.9400684210526317 -0.17699999999999996 -0.17699999999999996
probs:  [0.0594654069582354, 0.11044966728157522, 0.09962048135669799, 0.11786704596496217, 0.19692068063941293, 0.41567671779911636]
printing an ep nov before normalisation:  54.25739466163959
printing an ep nov before normalisation:  57.66782760620117
maxi score, test score, baseline:  -0.9400684210526317 -0.17699999999999996 -0.17699999999999996
probs:  [0.0594654069582354, 0.11044966728157522, 0.09962048135669799, 0.11786704596496217, 0.19692068063941293, 0.41567671779911636]
maxi score, test score, baseline:  -0.9400684210526317 -0.17699999999999996 -0.17699999999999996
probs:  [0.0594654069582354, 0.11044966728157522, 0.09962048135669799, 0.11786704596496217, 0.19692068063941293, 0.41567671779911636]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  53.049516449637444
maxi score, test score, baseline:  -0.9400684210526317 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9400684210526317 -0.17699999999999996 -0.17699999999999996
probs:  [0.060007544493339504, 0.10991400518097437, 0.09931374662644228, 0.11673197947003296, 0.19455703425297446, 0.41947568997623635]
actor:  1 policy actor:  1  step number:  54 total reward:  0.39333333333333276  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9400684210526317 -0.17699999999999996 -0.17699999999999996
probs:  [0.060511267707482636, 0.10934152968605958, 0.09896985846100112, 0.11601247902028917, 0.19215928969744267, 0.4230055754277249]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9400684210526317 -0.17699999999999996 -0.17699999999999996
probs:  [0.060511267707482636, 0.10934152968605958, 0.09896985846100112, 0.11601247902028917, 0.19215928969744267, 0.4230055754277249]
using another actor
maxi score, test score, baseline:  -0.9400684210526317 -0.17699999999999996 -0.17699999999999996
probs:  [0.06047465354963376, 0.10938314124484207, 0.09899485467959557, 0.11606477739030586, 0.19233357469200427, 0.4227489984436185]
maxi score, test score, baseline:  -0.9400684210526317 -0.17699999999999996 -0.17699999999999996
probs:  [0.06047465354963376, 0.10938314124484207, 0.09899485467959557, 0.11606477739030586, 0.19233357469200427, 0.4227489984436185]
maxi score, test score, baseline:  -0.9400684210526317 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9400684210526317 -0.17699999999999996 -0.17699999999999996
probs:  [0.06047465354963376, 0.10938314124484207, 0.09899485467959557, 0.11606477739030586, 0.19233357469200427, 0.4227489984436185]
maxi score, test score, baseline:  -0.9400684210526317 -0.17699999999999996 -0.17699999999999996
probs:  [0.06047465354963376, 0.10938314124484207, 0.09899485467959557, 0.11606477739030586, 0.19233357469200427, 0.4227489984436185]
maxi score, test score, baseline:  -0.9400684210526317 -0.17699999999999996 -0.17699999999999996
actor:  1 policy actor:  1  step number:  55 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  39.826850885822665
line 256 mcts: sample exp_bonus 52.105349201271196
printing an ep nov before normalisation:  51.54074941068593
printing an ep nov before normalisation:  44.708929998187635
maxi score, test score, baseline:  -0.940194117647059 -0.17699999999999996 -0.17699999999999996
actor:  1 policy actor:  1  step number:  51 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.940194117647059 -0.17699999999999996 -0.17699999999999996
probs:  [0.057283872664750116, 0.10370828809700391, 0.12076025907752794, 0.135809513328163, 0.18203660713579986, 0.40040145969675534]
Printing some Q and Qe and total Qs values:  [[0.16 ]
 [0.239]
 [0.11 ]
 [0.223]
 [0.157]
 [0.152]
 [0.217]] [[52.079]
 [54.62 ]
 [53.762]
 [51.834]
 [52.83 ]
 [56.199]
 [59.143]] [[0.16 ]
 [0.239]
 [0.11 ]
 [0.223]
 [0.157]
 [0.152]
 [0.217]]
maxi score, test score, baseline:  -0.940194117647059 -0.17699999999999996 -0.17699999999999996
siam score:  -0.7722704
siam score:  -0.7744734
maxi score, test score, baseline:  -0.940194117647059 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.940194117647059 -0.17699999999999996 -0.17699999999999996
probs:  [0.057283872664750116, 0.10370828809700391, 0.12076025907752794, 0.135809513328163, 0.18203660713579986, 0.40040145969675534]
printing an ep nov before normalisation:  64.82047176667409
maxi score, test score, baseline:  -0.940194117647059 -0.17699999999999996 -0.17699999999999996
probs:  [0.057283872664750116, 0.10370828809700391, 0.12076025907752794, 0.135809513328163, 0.18203660713579986, 0.40040145969675534]
printing an ep nov before normalisation:  50.82737445831299
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.9403192872117402 -0.17699999999999996 -0.17699999999999996
probs:  [0.05731631769448252, 0.10376708876130442, 0.12082874032717654, 0.13531928136564703, 0.1821398756201838, 0.4006286962312057]
maxi score, test score, baseline:  -0.9403192872117402 -0.17699999999999996 -0.17699999999999996
printing an ep nov before normalisation:  28.98247718811035
maxi score, test score, baseline:  -0.9403192872117402 -0.17699999999999996 -0.17699999999999996
probs:  [0.0573396093624169, 0.10380930063654396, 0.12087790170997392, 0.13537434499794673, 0.18180701828851886, 0.4007918250045997]
maxi score, test score, baseline:  -0.9403192872117402 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9403192872117402 -0.17699999999999996 -0.17699999999999996
probs:  [0.05737170858596613, 0.10386747459057614, 0.12094565307157021, 0.13488956640254615, 0.1819089577466957, 0.4010166396026456]
using another actor
maxi score, test score, baseline:  -0.9403192872117402 -0.17699999999999996 -0.17699999999999996
probs:  [0.05735383409373712, 0.10356287792371588, 0.12104778436395709, 0.13501801901687463, 0.182126166831872, 0.40089131776984327]
printing an ep nov before normalisation:  58.94674767916955
printing an ep nov before normalisation:  67.96668599172003
printing an ep nov before normalisation:  49.62587764950284
maxi score, test score, baseline:  -0.9403192872117402 -0.17699999999999996 -0.17699999999999996
probs:  [0.05735383409373712, 0.10356287792371588, 0.12104778436395709, 0.13501801901687463, 0.182126166831872, 0.40089131776984327]
printing an ep nov before normalisation:  32.64337061817014
maxi score, test score, baseline:  -0.9403192872117402 -0.17699999999999996 -0.17699999999999996
probs:  [0.05735383409373712, 0.10356287792371588, 0.12104778436395709, 0.13501801901687463, 0.182126166831872, 0.40089131776984327]
maxi score, test score, baseline:  -0.9403192872117402 -0.17699999999999996 -0.17699999999999996
probs:  [0.05731458814611276, 0.10359334828391836, 0.1211046344762314, 0.1350959462408732, 0.182275166855955, 0.4006163159969093]
maxi score, test score, baseline:  -0.9403192872117402 -0.17699999999999996 -0.17699999999999996
probs:  [0.05731458814611276, 0.10359334828391836, 0.1211046344762314, 0.1350959462408732, 0.182275166855955, 0.4006163159969093]
actor:  0 policy actor:  1  step number:  51 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.937710320781032 -0.17699999999999996 -0.17699999999999996
Printing some Q and Qe and total Qs values:  [[-0.15 ]
 [-0.133]
 [-0.149]
 [-0.15 ]
 [-0.145]
 [-0.145]
 [-0.147]] [[57.139]
 [52.415]
 [55.607]
 [56.699]
 [56.603]
 [54.397]
 [55.856]] [[0.169]
 [0.137]
 [0.154]
 [0.164]
 [0.168]
 [0.145]
 [0.159]]
maxi score, test score, baseline:  -0.937710320781032 -0.17699999999999996 -0.17699999999999996
probs:  [0.05733784124463621, 0.10363542088136227, 0.12115382812943849, 0.13515082953390353, 0.18194290567900848, 0.4007791745316509]
maxi score, test score, baseline:  -0.937710320781032 -0.17699999999999996 -0.17699999999999996
probs:  [0.05733784124463621, 0.10363542088136227, 0.12115382812943849, 0.13515082953390353, 0.18194290567900848, 0.4007791745316509]
printing an ep nov before normalisation:  35.87330981040877
from probs:  [0.05733784124463621, 0.10363542088136227, 0.12115382812943849, 0.13515082953390353, 0.18194290567900848, 0.4007791745316509]
maxi score, test score, baseline:  -0.937710320781032 -0.17699999999999996 -0.17699999999999996
probs:  [0.057284586232979905, 0.10370919021826142, 0.12124560540847812, 0.13525699494697058, 0.182097170727386, 0.400406452465924]
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.2519268473424
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.129]
 [-0.117]
 [-0.129]
 [-0.129]
 [-0.129]
 [-0.129]
 [-0.129]] [[39.449]
 [49.794]
 [39.449]
 [39.449]
 [39.449]
 [39.449]
 [39.449]] [[1.227]
 [1.664]
 [1.227]
 [1.227]
 [1.227]
 [1.227]
 [1.227]]
Printing some Q and Qe and total Qs values:  [[-0.137]
 [-0.126]
 [-0.137]
 [-0.137]
 [-0.137]
 [-0.137]
 [-0.137]] [[32.053]
 [51.616]
 [32.053]
 [32.053]
 [32.053]
 [32.053]
 [32.053]] [[0.606]
 [1.503]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]]
printing an ep nov before normalisation:  38.01733458054419
maxi score, test score, baseline:  -0.9378401530967293 -0.17699999999999996 -0.17699999999999996
probs:  [0.057284586232979905, 0.10370919021826142, 0.12124560540847812, 0.13525699494697058, 0.182097170727386, 0.400406452465924]
siam score:  -0.7672977
maxi score, test score, baseline:  -0.9378401530967293 -0.17699999999999996 -0.17699999999999996
probs:  [0.05724545361388561, 0.10373971144742225, 0.12130243766019469, 0.1353348493963362, 0.18224530253435897, 0.4001322453478023]
maxi score, test score, baseline:  -0.9378401530967293 -0.17699999999999996 -0.17699999999999996
probs:  [0.05724545361388561, 0.10373971144742225, 0.12130243766019469, 0.1353348493963362, 0.18224530253435897, 0.4001322453478023]
printing an ep nov before normalisation:  46.37066498835362
maxi score, test score, baseline:  -0.9378401530967293 -0.17699999999999996 -0.17699999999999996
printing an ep nov before normalisation:  46.42596372937427
actor:  1 policy actor:  1  step number:  60 total reward:  0.24666666666666648  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9378401530967293 -0.17699999999999996 -0.17699999999999996
probs:  [0.05751818092056881, 0.10352699957639626, 0.12090635614809205, 0.1347922576971292, 0.18121292622822655, 0.4020432794295871]
line 256 mcts: sample exp_bonus 64.51609261797662
siam score:  -0.76469404
maxi score, test score, baseline:  -0.9378401530967293 -0.17699999999999996 -0.17699999999999996
probs:  [0.05757955602141182, 0.10363758262600527, 0.12051879103054085, 0.1343843378992902, 0.18140659693820602, 0.40247313548454583]
maxi score, test score, baseline:  -0.9378401530967293 -0.17699999999999996 -0.17699999999999996
probs:  [0.05757955602141182, 0.10363758262600527, 0.12051879103054085, 0.1343843378992902, 0.18140659693820602, 0.40247313548454583]
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]] [[25.637]
 [25.637]
 [25.637]
 [25.637]
 [25.637]
 [25.637]
 [25.637]] [[0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]]
from probs:  [0.05757955602141182, 0.10363758262600527, 0.12051879103054085, 0.1343843378992902, 0.18140659693820602, 0.40247313548454583]
maxi score, test score, baseline:  -0.9380981981981982 -0.17699999999999996 -0.17699999999999996
probs:  [0.05757955602141182, 0.10363758262600527, 0.12051879103054085, 0.1343843378992902, 0.18140659693820602, 0.40247313548454583]
maxi score, test score, baseline:  -0.9380981981981982 -0.17699999999999996 -0.17699999999999996
probs:  [0.05757955602141182, 0.10363758262600527, 0.12051879103054085, 0.1343843378992902, 0.18140659693820602, 0.40247313548454583]
printing an ep nov before normalisation:  35.84940238222292
maxi score, test score, baseline:  -0.938226417704011 -0.17699999999999996 -0.17699999999999996
probs:  [0.05757955602141182, 0.10363758262600527, 0.12051879103054085, 0.1343843378992902, 0.18140659693820602, 0.40247313548454583]
maxi score, test score, baseline:  -0.938226417704011 -0.17699999999999996 -0.17699999999999996
probs:  [0.05757955602141182, 0.10363758262600527, 0.12051879103054085, 0.1343843378992902, 0.18140659693820602, 0.40247313548454583]
printing an ep nov before normalisation:  38.3032532238743
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.803]
 [0.725]
 [0.736]
 [0.572]
 [0.73 ]
 [0.737]] [[25.157]
 [31.89 ]
 [29.462]
 [29.16 ]
 [27.626]
 [29.783]
 [30.075]] [[0.663]
 [0.803]
 [0.725]
 [0.736]
 [0.572]
 [0.73 ]
 [0.737]]
maxi score, test score, baseline:  -0.938226417704011 -0.17699999999999996 -0.17699999999999996
probs:  [0.05757955602141182, 0.10363758262600527, 0.12051879103054085, 0.1343843378992902, 0.18140659693820602, 0.40247313548454583]
maxi score, test score, baseline:  -0.9383541062801932 -0.17699999999999996 -0.17699999999999996
probs:  [0.05757955602141182, 0.10363758262600527, 0.12051879103054085, 0.1343843378992902, 0.18140659693820602, 0.40247313548454583]
printing an ep nov before normalisation:  59.279544352888976
printing an ep nov before normalisation:  50.932369232177734
maxi score, test score, baseline:  -0.9384812672176308 -0.17699999999999996 -0.17699999999999996
probs:  [0.05757955602141182, 0.10363758262600527, 0.12051879103054085, 0.1343843378992902, 0.18140659693820602, 0.40247313548454583]
maxi score, test score, baseline:  -0.9384812672176308 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9384812672176308 -0.17699999999999996 -0.17699999999999996
probs:  [0.05757955602141182, 0.10363758262600527, 0.12051879103054085, 0.1343843378992902, 0.18140659693820602, 0.40247313548454583]
maxi score, test score, baseline:  -0.9384812672176308 -0.17699999999999996 -0.17699999999999996
probs:  [0.05757955602141182, 0.10363758262600527, 0.12051879103054085, 0.1343843378992902, 0.18140659693820602, 0.40247313548454583]
maxi score, test score, baseline:  -0.9384812672176308 -0.17699999999999996 -0.17699999999999996
probs:  [0.05757955602141182, 0.10363758262600527, 0.12051879103054085, 0.1343843378992902, 0.18140659693820602, 0.40247313548454583]
Printing some Q and Qe and total Qs values:  [[-0.005]
 [-0.055]
 [-0.005]
 [-0.046]
 [-0.03 ]
 [-0.005]
 [-0.005]] [[41.431]
 [43.772]
 [41.431]
 [45.086]
 [43.74 ]
 [41.431]
 [41.431]] [[1.114]
 [1.171]
 [1.114]
 [1.24 ]
 [1.194]
 [1.114]
 [1.114]]
maxi score, test score, baseline:  -0.9384812672176308 -0.17699999999999996 -0.17699999999999996
probs:  [0.05757955602141182, 0.10363758262600527, 0.12051879103054085, 0.1343843378992902, 0.18140659693820602, 0.40247313548454583]
maxi score, test score, baseline:  -0.9384812672176308 -0.17699999999999996 -0.17699999999999996
probs:  [0.05760262739850762, 0.10367915165323112, 0.12056713982472286, 0.1344382553218688, 0.1810781040874533, 0.40263472171421616]
printing an ep nov before normalisation:  0.344606342820839
printing an ep nov before normalisation:  42.781745148231636
maxi score, test score, baseline:  -0.9384812672176308 -0.17699999999999996 -0.17699999999999996
probs:  [0.05763400792126398, 0.1037356917461525, 0.12063290141449581, 0.13396598875748952, 0.18117690701173983, 0.40285450314885846]
Printing some Q and Qe and total Qs values:  [[0.984]
 [0.984]
 [0.984]
 [0.984]
 [0.984]
 [0.984]
 [0.984]] [[41.326]
 [41.326]
 [41.326]
 [41.326]
 [41.326]
 [41.326]
 [41.326]] [[0.984]
 [0.984]
 [0.984]
 [0.984]
 [0.984]
 [0.984]
 [0.984]]
actor:  0 policy actor:  0  step number:  53 total reward:  0.1866666666666661  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9362923182441701 -0.17699999999999996 -0.17699999999999996
probs:  [0.05762007762130414, 0.10377799006452483, 0.1206673207316966, 0.13399419098838686, 0.1811830952600012, 0.40275732533408626]
maxi score, test score, baseline:  -0.9362923182441701 -0.17699999999999996 -0.17699999999999996
probs:  [0.05762007762130414, 0.10377799006452483, 0.1206673207316966, 0.13399419098838686, 0.1811830952600012, 0.40275732533408626]
printing an ep nov before normalisation:  54.12741549980535
maxi score, test score, baseline:  -0.9362923182441701 -0.17699999999999996 -0.17699999999999996
probs:  [0.05762007762130414, 0.10377799006452483, 0.1206673207316966, 0.13399419098838686, 0.1811830952600012, 0.40275732533408626]
Printing some Q and Qe and total Qs values:  [[-0.13 ]
 [-0.112]
 [-0.13 ]
 [-0.129]
 [-0.128]
 [-0.13 ]
 [-0.13 ]] [[54.738]
 [55.135]
 [54.738]
 [54.063]
 [55.477]
 [54.738]
 [54.738]] [[1.039]
 [1.073]
 [1.039]
 [1.015]
 [1.071]
 [1.039]
 [1.039]]
maxi score, test score, baseline:  -0.9362923182441701 -0.17699999999999996 -0.17699999999999996
probs:  [0.05762007762130414, 0.10377799006452483, 0.1206673207316966, 0.13399419098838686, 0.1811830952600012, 0.40275732533408626]
using explorer policy with actor:  0
printing an ep nov before normalisation:  41.40938218701937
maxi score, test score, baseline:  -0.9362923182441701 -0.17699999999999996 -0.17699999999999996
probs:  [0.05762007762130414, 0.10377799006452483, 0.1206673207316966, 0.13399419098838686, 0.1811830952600012, 0.40275732533408626]
actor:  0 policy actor:  1  step number:  57 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  0.0017177052438910323
siam score:  -0.7603273
Printing some Q and Qe and total Qs values:  [[-0.177]
 [-0.212]
 [-0.171]
 [-0.176]
 [-0.176]
 [-0.176]
 [-0.175]] [[28.207]
 [28.65 ]
 [32.3  ]
 [29.704]
 [29.602]
 [29.521]
 [29.505]] [[1.049]
 [1.033]
 [1.234]
 [1.116]
 [1.111]
 [1.108]
 [1.107]]
maxi score, test score, baseline:  -0.9337124572210814 -0.17699999999999996 -0.17699999999999996
probs:  [0.05765108239789936, 0.10383388950384312, 0.12073232920927772, 0.13352748142081752, 0.18128074212429793, 0.4029744753438644]
actions average: 
K:  2  action  0 :  tensor([0.3849, 0.0268, 0.1202, 0.1130, 0.1334, 0.1380, 0.0836],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0295, 0.8809, 0.0138, 0.0235, 0.0098, 0.0096, 0.0330],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1300, 0.1604, 0.1955, 0.1270, 0.1190, 0.1592, 0.1089],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1384, 0.0628, 0.1268, 0.2751, 0.1443, 0.1403, 0.1123],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1784, 0.0671, 0.1094, 0.1045, 0.3425, 0.1242, 0.0740],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1231, 0.0127, 0.1189, 0.1187, 0.1322, 0.4093, 0.0853],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1333, 0.0371, 0.1599, 0.2197, 0.1235, 0.1325, 0.1938],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.128]
 [-0.111]
 [-0.128]
 [-0.127]
 [-0.128]
 [-0.128]
 [-0.127]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.128]
 [-0.111]
 [-0.128]
 [-0.127]
 [-0.128]
 [-0.128]
 [-0.127]]
actions average: 
K:  1  action  0 :  tensor([0.4267, 0.1020, 0.0981, 0.0813, 0.1327, 0.0656, 0.0935],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0157, 0.9007, 0.0182, 0.0159, 0.0108, 0.0112, 0.0276],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1480, 0.0044, 0.2529, 0.1566, 0.1379, 0.1856, 0.1146],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1790, 0.0780, 0.1045, 0.3134, 0.1220, 0.0973, 0.1057],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2408, 0.0056, 0.0957, 0.1329, 0.3281, 0.1103, 0.0866],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1247, 0.0095, 0.2153, 0.1048, 0.0948, 0.3835, 0.0674],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1529, 0.1609, 0.1293, 0.1114, 0.1258, 0.1036, 0.2161],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  32.08526231564496
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9337124572210814 -0.17699999999999996 -0.17699999999999996
probs:  [0.05764312253981613, 0.10392001770739917, 0.12085288454137462, 0.13314072645256145, 0.18152465251347144, 0.4029185962453773]
actor:  0 policy actor:  1  step number:  48 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9310475409836065 -0.17699999999999996 -0.17699999999999996
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.9310475409836065 -0.17699999999999996 -0.17699999999999996
probs:  [0.05766615260558727, 0.10396157953930889, 0.12090122720693978, 0.13319398983313616, 0.18119715757156, 0.40307989324346793]
printing an ep nov before normalisation:  35.377732092839295
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.980269493905894
maxi score, test score, baseline:  -0.9311883435582822 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9313285714285714 -0.17699999999999996 -0.17699999999999996
probs:  [0.057689064687632914, 0.10400292844869304, 0.1209493222115629, 0.13324698034379653, 0.1808713403961699, 0.4032403639121448]
actions average: 
K:  4  action  0 :  tensor([0.2844, 0.1057, 0.1352, 0.1517, 0.1086, 0.1170, 0.0974],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0178, 0.8593, 0.0344, 0.0229, 0.0115, 0.0171, 0.0370],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1330, 0.2085, 0.2120, 0.1265, 0.1069, 0.1152, 0.0979],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1369, 0.1058, 0.1391, 0.2359, 0.1364, 0.1241, 0.1217],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2058, 0.0382, 0.1358, 0.1055, 0.2491, 0.1468, 0.1188],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2187, 0.0285, 0.1440, 0.1650, 0.1523, 0.1481, 0.1433],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1266, 0.2091, 0.1054, 0.1241, 0.1108, 0.0744, 0.2497],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.9313285714285714 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9313285714285714 -0.17699999999999996 -0.17699999999999996
probs:  [0.057689064687632914, 0.10400292844869304, 0.1209493222115629, 0.13324698034379653, 0.1808713403961699, 0.4032403639121448]
siam score:  -0.7612332
maxi score, test score, baseline:  -0.9313285714285714 -0.17699999999999996 -0.17699999999999996
printing an ep nov before normalisation:  0.004260360074113123
printing an ep nov before normalisation:  26.236828168233238
printing an ep nov before normalisation:  69.24308158667435
siam score:  -0.75764376
maxi score, test score, baseline:  -0.9313285714285714 -0.17699999999999996 -0.17699999999999996
probs:  [0.0576629123316371, 0.1037861243515744, 0.1206554994913188, 0.13351452388427065, 0.18132400126019535, 0.4030569386810037]
maxi score, test score, baseline:  -0.9313285714285714 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9313285714285714 -0.17699999999999996 -0.17699999999999996
probs:  [0.057586163474093205, 0.10384709489556886, 0.12076684034013002, 0.13366426052266822, 0.18161649232361493, 0.4025191484439246]
maxi score, test score, baseline:  -0.9313285714285714 -0.17699999999999996 -0.17699999999999996
probs:  [0.05760907443767331, 0.10388845369794447, 0.12081494636368846, 0.13371750975283953, 0.18129040467620328, 0.40267961107165096]
printing an ep nov before normalisation:  51.41776109513843
line 256 mcts: sample exp_bonus 42.9472972190535
siam score:  -0.7542373
maxi score, test score, baseline:  -0.9313285714285714 -0.17699999999999996 -0.17699999999999996
probs:  [0.05760907443767331, 0.10388845369794447, 0.12081494636368846, 0.13371750975283953, 0.18129040467620328, 0.40267961107165096]
siam score:  -0.75341827
actions average: 
K:  3  action  0 :  tensor([0.2662, 0.0521, 0.1365, 0.1356, 0.1539, 0.1247, 0.1309],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0239, 0.8326, 0.0307, 0.0283, 0.0153, 0.0238, 0.0453],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1347, 0.0252, 0.2514, 0.1506, 0.1433, 0.1539, 0.1408],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0746, 0.1530, 0.0967, 0.3560, 0.1135, 0.1029, 0.1033],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2118, 0.0185, 0.0870, 0.1180, 0.4094, 0.0707, 0.0846],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1025, 0.0196, 0.2072, 0.2034, 0.1361, 0.2133, 0.1179],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1082, 0.1749, 0.1157, 0.1562, 0.0966, 0.0838, 0.2646],
       grad_fn=<DivBackward0>)
siam score:  -0.74995023
maxi score, test score, baseline:  -0.9313285714285714 -0.17699999999999996 -0.17699999999999996
probs:  [0.057591932815258574, 0.10358932209244356, 0.12091505810693808, 0.13384155707259457, 0.18150270458822834, 0.4025594253245368]
Printing some Q and Qe and total Qs values:  [[-0.155]
 [-0.132]
 [-0.153]
 [-0.147]
 [-0.151]
 [-0.148]
 [-0.148]] [[41.993]
 [47.183]
 [43.439]
 [47.096]
 [47.248]
 [41.673]
 [41.673]] [[0.43 ]
 [0.614]
 [0.477]
 [0.595]
 [0.596]
 [0.427]
 [0.427]]
printing an ep nov before normalisation:  39.58745712625655
printing an ep nov before normalisation:  28.467016220092773
actor:  1 policy actor:  1  step number:  78 total reward:  0.019999999999998352  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.9313285714285714 -0.17699999999999996 -0.17699999999999996
probs:  [0.058378629642593494, 0.1011933469368481, 0.11897596793015615, 0.13224334264280058, 0.18116132925344838, 0.4080473835941534]
printing an ep nov before normalisation:  55.82099843915893
printing an ep nov before normalisation:  36.00162148085211
printing an ep nov before normalisation:  30.67057356582037
maxi score, test score, baseline:  -0.9314682281059062 -0.17699999999999996 -0.17699999999999996
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9314682281059062 -0.17699999999999996 -0.17699999999999996
probs:  [0.05840122395823869, 0.10132577579299454, 0.11811323246103868, 0.13245542546436948, 0.18149890360973187, 0.40820543871362663]
actions average: 
K:  1  action  0 :  tensor([0.3307, 0.0597, 0.1092, 0.1333, 0.1425, 0.1081, 0.1163],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0092, 0.9482, 0.0065, 0.0113, 0.0046, 0.0043, 0.0159],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1614, 0.0653, 0.1872, 0.1760, 0.1386, 0.1180, 0.1533],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1158, 0.0269, 0.1261, 0.3571, 0.1372, 0.1164, 0.1206],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1670, 0.0927, 0.1215, 0.1322, 0.2458, 0.1226, 0.1182],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0197, 0.0153, 0.1766, 0.0339, 0.0364, 0.6900, 0.0281],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1498, 0.0975, 0.1301, 0.1477, 0.1250, 0.1313, 0.2186],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  41 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.9314682281059062 -0.17699999999999996 -0.17699999999999996
probs:  [0.057652551662740414, 0.1000255756555219, 0.1160940419824837, 0.13075524751068657, 0.19251037187774145, 0.402962211310826]
maxi score, test score, baseline:  -0.9314682281059062 -0.17699999999999996 -0.17699999999999996
probs:  [0.05768368367447131, 0.10007964182863356, 0.11615680512279662, 0.13028513420894966, 0.19261449492765723, 0.40318024023749155]
maxi score, test score, baseline:  -0.9314682281059062 -0.17699999999999996 -0.17699999999999996
probs:  [0.05764513428547851, 0.10010472753729403, 0.1162060221850284, 0.13035555748140096, 0.19277847275409643, 0.4029100857567016]
printing an ep nov before normalisation:  19.865212355171682
maxi score, test score, baseline:  -0.9314682281059062 -0.17699999999999996 -0.17699999999999996
printing an ep nov before normalisation:  35.51399021365748
siam score:  -0.756255
maxi score, test score, baseline:  -0.9314682281059062 -0.17699999999999996 -0.17699999999999996
probs:  [0.05764513428547851, 0.10010472753729403, 0.1162060221850284, 0.13035555748140096, 0.19277847275409643, 0.4029100857567016]
maxi score, test score, baseline:  -0.9314682281059062 -0.17699999999999996 -0.17699999999999996
printing an ep nov before normalisation:  41.943169637260304
printing an ep nov before normalisation:  26.05547030498542
siam score:  -0.75570786
actor:  0 policy actor:  0  step number:  73 total reward:  0.09333333333333282  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.9293850948509486 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9293850948509486 -0.17699999999999996 -0.17699999999999996
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.9293850948509486 -0.17699999999999996 -0.17699999999999996
probs:  [0.057625620321007556, 0.10016632901336828, 0.11625711033112805, 0.13039740669753797, 0.1927795629678926, 0.4027739706690657]
maxi score, test score, baseline:  -0.9295281271129141 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9295281271129141 -0.17699999999999996 -0.17699999999999996
probs:  [0.057625620321007556, 0.10016632901336828, 0.11625711033112805, 0.13039740669753797, 0.1927795629678926, 0.4027739706690657]
printing an ep nov before normalisation:  26.07298849570333
maxi score, test score, baseline:  -0.9295281271129141 -0.17699999999999996 -0.17699999999999996
printing an ep nov before normalisation:  52.25951139070528
maxi score, test score, baseline:  -0.9295281271129141 -0.17699999999999996 -0.17699999999999996
probs:  [0.057625620321007556, 0.10016632901336828, 0.11625711033112805, 0.13039740669753797, 0.1927795629678926, 0.4027739706690657]
maxi score, test score, baseline:  -0.9295281271129141 -0.17699999999999996 -0.17699999999999996
probs:  [0.057625620321007556, 0.10016632901336828, 0.11625711033112805, 0.13039740669753797, 0.1927795629678926, 0.4027739706690657]
printing an ep nov before normalisation:  44.40114367548836
printing an ep nov before normalisation:  42.61472612895177
maxi score, test score, baseline:  -0.9295281271129141 -0.17699999999999996 -0.17699999999999996
probs:  [0.0575871113656353, 0.10019146156964064, 0.1163063149275258, 0.13046776538053942, 0.19294324625329795, 0.40250410050336083]
maxi score, test score, baseline:  -0.9295281271129141 -0.17699999999999996 -0.17699999999999996
probs:  [0.0575871113656353, 0.10019146156964064, 0.1163063149275258, 0.13046776538053942, 0.19294324625329795, 0.40250410050336083]
printing an ep nov before normalisation:  38.82360219955444
actor:  0 policy actor:  0  step number:  69 total reward:  0.026666666666665617  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.196]
 [0.348]
 [0.208]
 [0.304]
 [0.304]
 [0.304]
 [0.304]] [[46.551]
 [47.491]
 [45.059]
 [44.782]
 [44.782]
 [44.782]
 [44.782]] [[1.295]
 [1.484]
 [1.248]
 [1.333]
 [1.333]
 [1.333]
 [1.333]]
maxi score, test score, baseline:  -0.9275923076923076 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9275923076923076 -0.17699999999999996 -0.17699999999999996
probs:  [0.0575871113656353, 0.10019146156964064, 0.1163063149275258, 0.13046776538053942, 0.19294324625329795, 0.40250410050336083]
maxi score, test score, baseline:  -0.9275923076923076 -0.17699999999999996 -0.17699999999999996
probs:  [0.0575871113656353, 0.10019146156964064, 0.1163063149275258, 0.13046776538053942, 0.19294324625329795, 0.40250410050336083]
using another actor
maxi score, test score, baseline:  -0.9275923076923076 -0.17699999999999996 -0.17699999999999996
probs:  [0.057579480388640206, 0.10027031239152664, 0.11641787699839039, 0.13007147759023144, 0.19321037224310617, 0.40245048038810516]
actor:  1 policy actor:  1  step number:  62 total reward:  0.20666666666666633  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9277383838383838 -0.17699999999999996 -0.17699999999999996
probs:  [0.05682482984468791, 0.0989548479118825, 0.11489028783449552, 0.14149066148654912, 0.1906739879944288, 0.39716538492795617]
maxi score, test score, baseline:  -0.9277383838383838 -0.17699999999999996 -0.17699999999999996
probs:  [0.05682482984468791, 0.0989548479118825, 0.11489028783449552, 0.14149066148654912, 0.1906739879944288, 0.39716538492795617]
printing an ep nov before normalisation:  45.5993704240289
maxi score, test score, baseline:  -0.9277383838383838 -0.17699999999999996 -0.17699999999999996
probs:  [0.05682482984468791, 0.0989548479118825, 0.11489028783449552, 0.14149066148654912, 0.1906739879944288, 0.39716538492795617]
printing an ep nov before normalisation:  42.808982869173335
maxi score, test score, baseline:  -0.9277383838383838 -0.17699999999999996 -0.17699999999999996
probs:  [0.05682482984468791, 0.0989548479118825, 0.11489028783449552, 0.14149066148654912, 0.1906739879944288, 0.39716538492795617]
actor:  1 policy actor:  1  step number:  44 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  55.74912541678235
maxi score, test score, baseline:  -0.927883870967742 -0.17699999999999996 -0.17699999999999996
actor:  1 policy actor:  1  step number:  62 total reward:  0.2866666666666665  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  52.87014124250988
maxi score, test score, baseline:  -0.927883870967742 -0.17699999999999996 -0.17699999999999996
probs:  [0.056301993736161106, 0.09672801872042062, 0.1120189329426565, 0.15670726720801112, 0.18473748129881434, 0.3935063060939363]
maxi score, test score, baseline:  -0.927883870967742 -0.17699999999999996 -0.17699999999999996
probs:  [0.056301993736161106, 0.09672801872042062, 0.1120189329426565, 0.15670726720801112, 0.18473748129881434, 0.3935063060939363]
maxi score, test score, baseline:  -0.927883870967742 -0.17699999999999996 -0.17699999999999996
probs:  [0.056301993736161106, 0.09672801872042062, 0.1120189329426565, 0.15670726720801112, 0.18473748129881434, 0.3935063060939363]
maxi score, test score, baseline:  -0.927883870967742 -0.17699999999999996 -0.17699999999999996
probs:  [0.056301993736161106, 0.09672801872042062, 0.1120189329426565, 0.15670726720801112, 0.18473748129881434, 0.3935063060939363]
printing an ep nov before normalisation:  50.14968435588658
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.927883870967742 -0.17699999999999996 -0.17699999999999996
printing an ep nov before normalisation:  55.35813448329586
actions average: 
K:  0  action  0 :  tensor([0.4100, 0.0029, 0.0921, 0.0969, 0.2228, 0.0825, 0.0928],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0098, 0.9396, 0.0091, 0.0067, 0.0040, 0.0033, 0.0276],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1297, 0.0091, 0.4712, 0.0728, 0.0503, 0.1643, 0.1027],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1823, 0.0746, 0.1211, 0.2324, 0.1257, 0.1297, 0.1342],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1608, 0.0093, 0.1148, 0.1546, 0.3028, 0.1261, 0.1316],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0899, 0.0099, 0.1492, 0.1023, 0.0975, 0.4799, 0.0713],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1501, 0.1239, 0.1314, 0.1338, 0.1300, 0.1250, 0.2059],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  63 total reward:  0.1066666666666658  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.927883870967742 -0.17699999999999996 -0.17699999999999996
probs:  [0.05580149299273197, 0.10355759643053203, 0.1112595763967787, 0.15574029274932089, 0.18364028102972213, 0.39000076040091436]
printing an ep nov before normalisation:  51.020632228432014
Printing some Q and Qe and total Qs values:  [[-0.18 ]
 [-0.131]
 [-0.224]
 [-0.136]
 [-0.14 ]
 [-0.209]
 [-0.119]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.18 ]
 [-0.131]
 [-0.224]
 [-0.136]
 [-0.14 ]
 [-0.209]
 [-0.119]]
actions average: 
K:  3  action  0 :  tensor([0.2431, 0.1318, 0.1059, 0.1296, 0.1265, 0.0948, 0.1684],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0187, 0.8821, 0.0156, 0.0356, 0.0070, 0.0060, 0.0350],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1033, 0.0923, 0.2290, 0.1313, 0.0929, 0.2620, 0.0891],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1355, 0.0352, 0.0979, 0.3308, 0.1457, 0.1441, 0.1110],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1691, 0.0914, 0.0691, 0.1806, 0.2990, 0.0998, 0.0910],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1145, 0.0790, 0.1545, 0.1681, 0.1302, 0.2602, 0.0935],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1132, 0.2521, 0.0968, 0.1370, 0.0918, 0.0859, 0.2232],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.927883870967742 -0.17699999999999996 -0.17699999999999996
probs:  [0.05572258415987207, 0.10361539561376638, 0.11133942349274856, 0.15594747164267928, 0.18392732724416203, 0.3894477978467717]
maxi score, test score, baseline:  -0.927883870967742 -0.17699999999999996 -0.17699999999999996
probs:  [0.055745902272937006, 0.10365880336349116, 0.11138607124674389, 0.15601283117099377, 0.18358528799858018, 0.38961110394725407]
printing an ep nov before normalisation:  45.79270400445337
maxi score, test score, baseline:  -0.927883870967742 -0.17699999999999996 -0.17699999999999996
probs:  [0.05578377940971079, 0.1037293134115722, 0.11146184424575761, 0.1554384551696544, 0.18371023503179112, 0.3898763727315139]
maxi score, test score, baseline:  -0.927883870967742 -0.17699999999999996 -0.17699999999999996
printing an ep nov before normalisation:  41.9017487532178
siam score:  -0.75618285
maxi score, test score, baseline:  -0.927883870967742 -0.17699999999999996 -0.17699999999999996
probs:  [0.05580701218032207, 0.10377256229235854, 0.11150832127265455, 0.1555032913765606, 0.18336973173354837, 0.390039081144556]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.927883870967742 -0.17699999999999996 -0.17699999999999996
probs:  [0.05580701218032207, 0.10377256229235854, 0.11150832127265455, 0.1555032913765606, 0.18336973173354837, 0.390039081144556]
actions average: 
K:  4  action  0 :  tensor([0.2961, 0.0310, 0.1527, 0.1234, 0.1483, 0.1268, 0.1217],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0437, 0.7918, 0.0357, 0.0294, 0.0212, 0.0137, 0.0644],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1429, 0.1136, 0.1628, 0.1560, 0.1544, 0.1340, 0.1363],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0950, 0.0672, 0.1348, 0.3050, 0.1235, 0.1532, 0.1214],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1701, 0.0555, 0.1251, 0.1020, 0.3536, 0.1027, 0.0909],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1492, 0.0204, 0.1906, 0.1444, 0.1531, 0.2145, 0.1277],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0538, 0.0691, 0.1110, 0.0858, 0.0995, 0.0583, 0.5224],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.927883870967742 -0.17699999999999996 -0.17699999999999996
probs:  [0.05580701218032207, 0.10377256229235854, 0.11150832127265455, 0.1555032913765606, 0.18336973173354837, 0.390039081144556]
printing an ep nov before normalisation:  48.454327521029086
maxi score, test score, baseline:  -0.927883870967742 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.927883870967742 -0.17699999999999996 -0.17699999999999996
probs:  [0.05580701218032207, 0.10377256229235854, 0.11150832127265455, 0.1555032913765606, 0.18336973173354837, 0.390039081144556]
actor:  1 policy actor:  1  step number:  59 total reward:  0.19999999999999962  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  54.128104794142125
from probs:  [0.05603581702194191, 0.10366284829281497, 0.11134401183618987, 0.15502848563051944, 0.1822864742937934, 0.3916423629247404]
printing an ep nov before normalisation:  55.98567941456928
maxi score, test score, baseline:  -0.9280287726358148 -0.17699999999999996 -0.17699999999999996
probs:  [0.05599676048599499, 0.10369185212784682, 0.11138399227056793, 0.15513089241004474, 0.18242783350791425, 0.3913686691976313]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9280287726358148 -0.17699999999999996 -0.17699999999999996
probs:  [0.05599676048599499, 0.10369185212784682, 0.11138399227056793, 0.15513089241004474, 0.18242783350791425, 0.3913686691976313]
maxi score, test score, baseline:  -0.9280287726358148 -0.17699999999999996 -0.17699999999999996
printing an ep nov before normalisation:  22.79822689805401
maxi score, test score, baseline:  -0.9280287726358148 -0.17699999999999996 -0.17699999999999996
probs:  [0.05599676048599499, 0.10369185212784682, 0.11138399227056793, 0.15513089241004474, 0.18242783350791425, 0.3913686691976313]
siam score:  -0.7656925
maxi score, test score, baseline:  -0.9280287726358148 -0.17699999999999996 -0.17699999999999996
printing an ep nov before normalisation:  60.882345826125935
printing an ep nov before normalisation:  51.81153175920385
maxi score, test score, baseline:  -0.9280287726358148 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9280287726358148 -0.17699999999999996 -0.17699999999999996
probs:  [0.05603418951220838, 0.10376123725701301, 0.11145853119677834, 0.15456526948709579, 0.18254997239098794, 0.39163080015591645]
maxi score, test score, baseline:  -0.9280287726358148 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9280287726358148 -0.17699999999999996 -0.17699999999999996
probs:  [0.05603418951220838, 0.10376123725701301, 0.11145853119677834, 0.15456526948709579, 0.18254997239098794, 0.39163080015591645]
printing an ep nov before normalisation:  0.0
using another actor
maxi score, test score, baseline:  -0.9281730923694779 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9281730923694779 -0.17699999999999996 -0.17699999999999996
probs:  [0.05603418951220838, 0.10376123725701301, 0.11145853119677834, 0.15456526948709579, 0.18254997239098794, 0.39163080015591645]
printing an ep nov before normalisation:  39.7775928359886
maxi score, test score, baseline:  -0.9281730923694779 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9281730923694779 -0.17699999999999996 -0.17699999999999996
probs:  [0.05603418951220838, 0.10376123725701301, 0.11145853119677834, 0.15456526948709579, 0.18254997239098794, 0.39163080015591645]
actor:  0 policy actor:  1  step number:  49 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.9257249832999332 -0.17699999999999996 -0.17699999999999996
probs:  [0.05603418951220838, 0.10376123725701301, 0.11145853119677834, 0.15456526948709579, 0.18254997239098794, 0.39163080015591645]
printing an ep nov before normalisation:  33.96852970123291
maxi score, test score, baseline:  -0.9258733333333333 -0.17699999999999996 -0.17699999999999996
probs:  [0.05603418951220838, 0.10376123725701301, 0.11145853119677834, 0.15456526948709579, 0.18254997239098794, 0.39163080015591645]
maxi score, test score, baseline:  -0.9258733333333333 -0.17699999999999996 -0.17699999999999996
probs:  [0.05603418951220838, 0.10376123725701301, 0.11145853119677834, 0.15456526948709579, 0.18254997239098794, 0.39163080015591645]
printing an ep nov before normalisation:  44.162361341785505
using another actor
from probs:  [0.05603418951220838, 0.10376123725701301, 0.11145853119677834, 0.15456526948709579, 0.18254997239098794, 0.39163080015591645]
using explorer policy with actor:  0
printing an ep nov before normalisation:  39.109078988896506
maxi score, test score, baseline:  -0.9278733333333333 -0.17699999999999996 -0.17699999999999996
probs:  [0.056017305291057064, 0.10343607199556522, 0.11154264898183901, 0.15472794241446836, 0.1827636429675997, 0.39151238834947066]
printing an ep nov before normalisation:  75.37305435387766
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9278733333333333 -0.17699999999999996 -0.17699999999999996
probs:  [0.056017305291057064, 0.10343607199556522, 0.11154264898183901, 0.15472794241446836, 0.1827636429675997, 0.39151238834947066]
using another actor
maxi score, test score, baseline:  -0.9278733333333333 -0.17699999999999996 -0.17699999999999996
probs:  [0.05604030127866519, 0.10347858054202795, 0.1115884933396597, 0.15479155728043004, 0.18242762886078384, 0.3916734386984332]
maxi score, test score, baseline:  -0.9278733333333333 -0.17699999999999996 -0.17699999999999996
probs:  [0.05604030127866519, 0.10347858054202795, 0.1115884933396597, 0.15479155728043004, 0.18242762886078384, 0.3916734386984332]
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]] [[29.565]
 [29.565]
 [29.565]
 [29.565]
 [29.565]
 [29.565]
 [29.565]] [[0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]]
siam score:  -0.76797974
actor:  1 policy actor:  1  step number:  55 total reward:  0.3333333333333328  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.148]
 [-0.137]
 [-0.148]
 [-0.148]
 [-0.148]
 [-0.149]
 [-0.148]] [[ 0.   ]
 [57.763]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [37.21 ]
 [ 0.   ]] [[-0.586]
 [ 0.683]
 [-0.586]
 [-0.586]
 [-0.586]
 [ 0.223]
 [-0.586]]
printing an ep nov before normalisation:  49.460711192871955
maxi score, test score, baseline:  -0.9278733333333333 -0.17699999999999996 -0.17699999999999996
probs:  [0.056439612502210953, 0.10318469139642211, 0.11117609663786923, 0.15374784805345376, 0.18098008257818746, 0.3944716688318566]
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  53 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  51.91999548678091
Printing some Q and Qe and total Qs values:  [[-0.128]
 [-0.128]
 [-0.128]
 [-0.128]
 [-0.128]
 [-0.128]
 [-0.128]] [[59.674]
 [59.674]
 [59.674]
 [59.674]
 [59.674]
 [59.674]
 [59.674]] [[1.872]
 [1.872]
 [1.872]
 [1.872]
 [1.872]
 [1.872]
 [1.872]]
maxi score, test score, baseline:  -0.9254733333333333 -0.17699999999999996 -0.17699999999999996
probs:  [0.05649919610008522, 0.10329374078660965, 0.11129360255869734, 0.1532562810120888, 0.18076822050731914, 0.39488895903519977]
maxi score, test score, baseline:  -0.9254733333333333 -0.17699999999999996 -0.17699999999999996
probs:  [0.05649919610008522, 0.10329374078660965, 0.11129360255869734, 0.1532562810120888, 0.18076822050731914, 0.39488895903519977]
maxi score, test score, baseline:  -0.9254733333333333 -0.17699999999999996 -0.17699999999999996
probs:  [0.056535690392752115, 0.10336053232828604, 0.11136557363157285, 0.15270848589082514, 0.18088517310130856, 0.3951445446552554]
maxi score, test score, baseline:  -0.9254733333333333 -0.17699999999999996 -0.17699999999999996
probs:  [0.056535690392752115, 0.10336053232828604, 0.11136557363157285, 0.15270848589082514, 0.18088517310130856, 0.3951445446552554]
maxi score, test score, baseline:  -0.9254733333333333 -0.17699999999999996 -0.17699999999999996
probs:  [0.056535690392752115, 0.10336053232828604, 0.11136557363157285, 0.15270848589082514, 0.18088517310130856, 0.3951445446552554]
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.532]
 [0.508]
 [0.562]
 [0.559]
 [0.508]
 [0.508]] [[43.137]
 [41.852]
 [43.137]
 [41.365]
 [39.178]
 [43.137]
 [43.137]] [[2.362]
 [2.295]
 [2.362]
 [2.291]
 [2.134]
 [2.362]
 [2.362]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.43333333333333346  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.9254733333333333 -0.17699999999999996 -0.17699999999999996
probs:  [0.05706169072964643, 0.10296856774969207, 0.11081667639160289, 0.1513490927390836, 0.1789733977262693, 0.39883057466370575]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.9254733333333333 -0.17699999999999996 -0.17699999999999996
printing an ep nov before normalisation:  39.25417539865231
maxi score, test score, baseline:  -0.9254733333333333 -0.17699999999999996 -0.17699999999999996
maxi score, test score, baseline:  -0.9254733333333333 -0.17699999999999996 -0.17699999999999996
probs:  [0.057023867480077685, 0.10299675285346177, 0.11085614609502087, 0.15144684299290267, 0.17911086827915218, 0.39856552229938474]
Starting evaluation
printing an ep nov before normalisation:  58.46621853996908
maxi score, test score, baseline:  -0.9254733333333333 -0.17699999999999996 -0.17699999999999996
probs:  [0.057081214679768115, 0.10272167672376092, 0.11096775758831053, 0.15097091666037, 0.17929128225265317, 0.3989671520951372]
printing an ep nov before normalisation:  53.86347697916982
using explorer policy with actor:  0
maxi score, test score, baseline:  -0.9254733333333333 -0.17699999999999996 -0.17699999999999996
probs:  [0.057081214679768115, 0.10272167672376092, 0.11096775758831053, 0.15097091666037, 0.17929128225265317, 0.3989671520951372]
maxi score, test score, baseline:  -0.9254733333333333 -0.17699999999999996 -0.17699999999999996
printing an ep nov before normalisation:  38.09343529400864
printing an ep nov before normalisation:  55.093618710618664
printing an ep nov before normalisation:  39.23891179074184
printing an ep nov before normalisation:  31.657539220808175
printing an ep nov before normalisation:  31.33903980255127
maxi score, test score, baseline:  -0.9254733333333333 -0.17699999999999996 -0.17699999999999996
probs:  [0.057081214679768115, 0.10272167672376092, 0.11096775758831053, 0.15097091666037, 0.17929128225265317, 0.3989671520951372]
printing an ep nov before normalisation:  41.089017435526614
printing an ep nov before normalisation:  34.390875786080045
using explorer policy with actor:  0
printing an ep nov before normalisation:  46.988926350211145
Printing some Q and Qe and total Qs values:  [[0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]] [[52.659]
 [52.659]
 [52.659]
 [52.659]
 [52.659]
 [52.659]
 [52.659]] [[0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]]
Printing some Q and Qe and total Qs values:  [[0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]] [[42.631]
 [42.631]
 [42.631]
 [42.631]
 [42.631]
 [42.631]
 [42.631]] [[0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]]
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.819]
 [0.788]
 [0.765]
 [0.759]
 [0.708]
 [0.756]] [[42.466]
 [47.314]
 [44.235]
 [44.848]
 [44.169]
 [46.48 ]
 [46.092]] [[0.713]
 [0.819]
 [0.788]
 [0.765]
 [0.759]
 [0.708]
 [0.756]]
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.282]
 [0.313]
 [0.313]
 [0.447]
 [0.313]
 [0.313]] [[30.889]
 [35.761]
 [30.889]
 [30.889]
 [36.323]
 [30.889]
 [30.889]] [[1.474]
 [1.627]
 [1.474]
 [1.474]
 [1.813]
 [1.474]
 [1.474]]
printing an ep nov before normalisation:  48.70086026793565
printing an ep nov before normalisation:  41.08376055657555
printing an ep nov before normalisation:  51.29923434645892
actor:  0 policy actor:  1  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  1 policy actor:  1  step number:  52 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  28 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  28 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  28 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  28 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.8922066666666667 -0.17699999999999996 -0.17699999999999996
actor:  0 policy actor:  1  step number:  29 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  29 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  29 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  29 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  35.93863696862627
actor:  0 policy actor:  1  step number:  30 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  26.337944662147258
actor:  0 policy actor:  1  step number:  31 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  31 total reward:  0.5733333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  31 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  1  step number:  32 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  22.056094817298693
actor:  0 policy actor:  1  step number:  36 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  44.81549722707059
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([0.3028, 0.0204, 0.1395, 0.1250, 0.1462, 0.1192, 0.1470],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0112, 0.9233, 0.0148, 0.0095, 0.0058, 0.0059, 0.0295],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1783, 0.0143, 0.2478, 0.1685, 0.1018, 0.1487, 0.1407],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0990, 0.0301, 0.1371, 0.3476, 0.0924, 0.1596, 0.1342],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1394, 0.0092, 0.1095, 0.1174, 0.4298, 0.0982, 0.0965],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1229, 0.0158, 0.2195, 0.1323, 0.1006, 0.3025, 0.1063],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1542, 0.1060, 0.1507, 0.1632, 0.1205, 0.1363, 0.1691],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
UNIT TEST: sample policy line 217 mcts : [0.122 0.204 0.061 0.041 0.51  0.041 0.02 ]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
siam score:  -0.77337646
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.88549225732858
Printing some Q and Qe and total Qs values:  [[-0.124]
 [-0.082]
 [-0.124]
 [-0.124]
 [-0.124]
 [-0.124]
 [-0.124]] [[35.888]
 [44.748]
 [35.888]
 [35.888]
 [35.888]
 [35.888]
 [35.888]] [[0.747]
 [1.107]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.466533667039386
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.01024176695942
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.64380492970332
printing an ep nov before normalisation:  43.608694462643484
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.538503185110855
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.179]
 [-0.173]
 [-0.177]
 [-0.178]
 [-0.178]
 [-0.179]
 [-0.181]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.179]
 [-0.173]
 [-0.177]
 [-0.178]
 [-0.178]
 [-0.179]
 [-0.181]]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.56025567339739
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  39.0130219463354
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.377458656051612
printing an ep nov before normalisation:  34.562558983564
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.62170760239891
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.41572666168213
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
printing an ep nov before normalisation:  33.81337156978326
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.627901653746974
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.168]
 [-0.168]
 [-0.168]
 [-0.174]
 [-0.168]
 [-0.168]
 [-0.168]] [[64.828]
 [64.828]
 [64.828]
 [70.565]
 [64.828]
 [64.828]
 [64.828]] [[1.611]
 [1.611]
 [1.611]
 [1.798]
 [1.611]
 [1.611]
 [1.611]]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.62542873805954
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
line 256 mcts: sample exp_bonus 47.02820720344278
printing an ep nov before normalisation:  51.144483373479204
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.85240650177002
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
printing an ep nov before normalisation:  44.45241451263428
printing an ep nov before normalisation:  45.80146231237137
actor:  1 policy actor:  1  step number:  49 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.163]
 [-0.163]
 [-0.163]
 [-0.163]
 [-0.163]
 [-0.163]
 [-0.163]] [[65.236]
 [65.236]
 [65.236]
 [65.236]
 [65.236]
 [65.236]
 [65.236]] [[1.808]
 [1.808]
 [1.808]
 [1.808]
 [1.808]
 [1.808]
 [1.808]]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7787775
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.61587429046631
printing an ep nov before normalisation:  39.48000907897949
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  66 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  13.648556674796232
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  13.47171271325307
using explorer policy with actor:  1
siam score:  -0.77758074
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.774081630830025
line 256 mcts: sample exp_bonus 40.973573612913675
printing an ep nov before normalisation:  41.91288948059082
printing an ep nov before normalisation:  42.87608505093944
printing an ep nov before normalisation:  40.77943801879883
printing an ep nov before normalisation:  40.25126143986755
printing an ep nov before normalisation:  23.78710980471842
actor:  1 policy actor:  1  step number:  65 total reward:  0.17333333333333256  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
Printing some Q and Qe and total Qs values:  [[-0.127]
 [-0.096]
 [-0.106]
 [-0.119]
 [-0.14 ]
 [-0.115]
 [-0.117]] [[49.886]
 [43.754]
 [42.618]
 [44.145]
 [33.041]
 [40.504]
 [44.034]] [[1.169]
 [1.035]
 [0.995]
 [1.023]
 [0.703]
 [0.929]
 [1.022]]
Printing some Q and Qe and total Qs values:  [[-0.096]
 [-0.052]
 [-0.09 ]
 [-0.09 ]
 [-0.088]
 [-0.091]
 [-0.092]] [[41.397]
 [40.118]
 [40.994]
 [40.639]
 [40.783]
 [40.934]
 [41.282]] [[1.034]
 [1.01 ]
 [1.018]
 [1.   ]
 [1.009]
 [1.015]
 [1.032]]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  12.984148045889723
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.14947025718759
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  4  action  0 :  tensor([0.2369, 0.0625, 0.1346, 0.1791, 0.1530, 0.1310, 0.1029],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0227, 0.8688, 0.0203, 0.0250, 0.0168, 0.0169, 0.0295],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0629, 0.2526, 0.3255, 0.0825, 0.0514, 0.1508, 0.0744],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1211, 0.2143, 0.1307, 0.2013, 0.0826, 0.1197, 0.1303],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1783, 0.0274, 0.1237, 0.1329, 0.3188, 0.1237, 0.0951],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1388, 0.1723, 0.1267, 0.1496, 0.1356, 0.1616, 0.1154],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1876, 0.1618, 0.1244, 0.1327, 0.1136, 0.1380, 0.1419],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
printing an ep nov before normalisation:  28.029690889842275
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
printing an ep nov before normalisation:  47.53847751765657
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8600066666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.02272039928654
printing an ep nov before normalisation:  51.94608816598341
printing an ep nov before normalisation:  34.542211232186254
actor:  0 policy actor:  0  step number:  56 total reward:  0.08666666666666578  reward:  1.0 rdn_beta:  2.0
siam score:  -0.78785914
maxi score, test score, baseline:  -0.8578333333333334 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.786399
maxi score, test score, baseline:  -0.8578333333333334 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.78612655
maxi score, test score, baseline:  -0.8578333333333334 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8578333333333334 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8578333333333334 0.6366666666666668 0.6366666666666668
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.8578333333333334 0.6366666666666668 0.6366666666666668
Printing some Q and Qe and total Qs values:  [[-0.149]
 [-0.144]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]] [[41.667]
 [46.404]
 [41.667]
 [41.667]
 [41.667]
 [41.667]
 [41.667]] [[0.796]
 [1.023]
 [0.796]
 [0.796]
 [0.796]
 [0.796]
 [0.796]]
actor:  0 policy actor:  0  step number:  60 total reward:  0.20666666666666567  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.346482276916504
printing an ep nov before normalisation:  28.657941016439658
siam score:  -0.7915766
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.508]
 [0.285]
 [0.285]
 [0.285]
 [0.285]
 [0.285]] [[44.566]
 [47.37 ]
 [44.566]
 [44.566]
 [44.566]
 [44.566]
 [44.566]] [[0.557]
 [0.812]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]]
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.708435495671544
printing an ep nov before normalisation:  47.23240482208843
Printing some Q and Qe and total Qs values:  [[0.256]
 [0.389]
 [0.256]
 [0.256]
 [0.25 ]
 [0.256]
 [0.256]] [[39.273]
 [40.23 ]
 [39.273]
 [39.273]
 [37.372]
 [39.273]
 [39.273]] [[0.575]
 [0.722]
 [0.575]
 [0.575]
 [0.54 ]
 [0.575]
 [0.575]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.7969178879307
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.585]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]] [[35.504]
 [33.811]
 [35.504]
 [35.504]
 [35.504]
 [35.504]
 [35.504]] [[0.532]
 [0.585]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]]
printing an ep nov before normalisation:  36.182277403080036
siam score:  -0.79346496
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.67486543695447
Printing some Q and Qe and total Qs values:  [[-0.161]
 [-0.16 ]
 [-0.16 ]
 [-0.162]
 [-0.162]
 [-0.163]
 [-0.162]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.161]
 [-0.16 ]
 [-0.16 ]
 [-0.162]
 [-0.162]
 [-0.163]
 [-0.162]]
printing an ep nov before normalisation:  26.041971207170054
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.56390864195174
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.493]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]] [[43.537]
 [39.379]
 [43.537]
 [43.537]
 [43.537]
 [43.537]
 [43.537]] [[1.663]
 [1.594]
 [1.663]
 [1.663]
 [1.663]
 [1.663]
 [1.663]]
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
printing an ep nov before normalisation:  30.199284553527832
printing an ep nov before normalisation:  25.615290233067107
siam score:  -0.78894395
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  66 total reward:  0.13999999999999924  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.54096330203426
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.63981938948253
siam score:  -0.7828746
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.4640, 0.0426, 0.0743, 0.0840, 0.1933, 0.0602, 0.0815],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0136, 0.9033, 0.0141, 0.0113, 0.0093, 0.0068, 0.0416],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.2262, 0.0617, 0.1680, 0.1483, 0.1504, 0.1129, 0.1327],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1301, 0.0513, 0.1341, 0.2680, 0.1382, 0.1337, 0.1446],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1765, 0.0215, 0.0919, 0.1095, 0.4219, 0.0816, 0.0971],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1812, 0.0373, 0.1907, 0.1315, 0.1512, 0.1393, 0.1689],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1135, 0.1948, 0.1243, 0.1352, 0.1004, 0.1078, 0.2240],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.603995499992386
Printing some Q and Qe and total Qs values:  [[-0.107]
 [-0.106]
 [-0.108]
 [-0.118]
 [-0.111]
 [-0.11 ]
 [-0.118]] [[39.753]
 [40.476]
 [41.673]
 [36.496]
 [41.812]
 [42.645]
 [36.496]] [[0.656]
 [0.686]
 [0.734]
 [0.511]
 [0.736]
 [0.772]
 [0.511]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.082 0.347 0.143 0.224 0.061 0.082 0.061]
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.03793013776584
maxi score, test score, baseline:  -0.85542 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0
actor:  1 policy actor:  1  step number:  66 total reward:  0.16666666666666652  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.18 ]
 [-0.203]
 [-0.18 ]
 [-0.18 ]
 [-0.18 ]
 [-0.18 ]
 [-0.18 ]] [[33.564]
 [47.44 ]
 [33.564]
 [33.564]
 [33.564]
 [33.564]
 [33.564]] [[-0.02 ]
 [ 0.065]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]
 [-0.02 ]]
line 256 mcts: sample exp_bonus 43.620805954531
actor:  0 policy actor:  0  step number:  71 total reward:  0.039999999999998925  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8533400000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.87714218356523
maxi score, test score, baseline:  -0.8533400000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.319]
 [0.319]
 [0.321]
 [0.319]
 [0.319]
 [0.319]] [[42.531]
 [42.344]
 [42.344]
 [42.684]
 [42.344]
 [42.344]
 [42.344]] [[0.316]
 [0.319]
 [0.319]
 [0.321]
 [0.319]
 [0.319]
 [0.319]]
maxi score, test score, baseline:  -0.8533400000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8533400000000001 0.6366666666666668 0.6366666666666668
Printing some Q and Qe and total Qs values:  [[-0.174]
 [-0.174]
 [-0.174]
 [-0.174]
 [-0.174]
 [-0.174]
 [-0.174]] [[66.217]
 [66.217]
 [66.217]
 [66.217]
 [66.217]
 [66.217]
 [66.217]] [[1.259]
 [1.259]
 [1.259]
 [1.259]
 [1.259]
 [1.259]
 [1.259]]
maxi score, test score, baseline:  -0.8533400000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8533400000000001 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8533400000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.98707354984048
printing an ep nov before normalisation:  41.83922134476453
maxi score, test score, baseline:  -0.8533400000000001 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8533400000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.107]
 [-0.107]
 [-0.107]
 [-0.107]
 [-0.107]
 [-0.107]
 [-0.107]] [[42.437]
 [42.437]
 [42.437]
 [42.437]
 [42.437]
 [42.437]
 [42.437]] [[1.893]
 [1.893]
 [1.893]
 [1.893]
 [1.893]
 [1.893]
 [1.893]]
printing an ep nov before normalisation:  45.81535214672793
maxi score, test score, baseline:  -0.8533400000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8533400000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8533400000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.284596812515254
maxi score, test score, baseline:  -0.8533400000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8533400000000001 0.6366666666666668 0.6366666666666668
main train batch thing paused
add a thread
Adding thread: now have 3 threads
printing an ep nov before normalisation:  45.4529506525789
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.577]
 [0.609]
 [0.609]
 [0.607]
 [0.605]
 [0.606]] [[32.775]
 [36.458]
 [32.746]
 [32.208]
 [32.738]
 [32.426]
 [32.651]] [[0.611]
 [0.577]
 [0.609]
 [0.609]
 [0.607]
 [0.605]
 [0.606]]
printing an ep nov before normalisation:  0.8108343705077914
Printing some Q and Qe and total Qs values:  [[0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]] [[42.104]
 [42.104]
 [42.104]
 [42.104]
 [42.104]
 [42.104]
 [42.104]] [[0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]
 [0.327]]
printing an ep nov before normalisation:  50.64208418093566
printing an ep nov before normalisation:  49.464836155156576
printing an ep nov before normalisation:  46.753636098528276
maxi score, test score, baseline:  -0.8533400000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]] [[41.996]
 [41.996]
 [41.996]
 [41.996]
 [41.996]
 [41.996]
 [41.996]] [[1.197]
 [1.197]
 [1.197]
 [1.197]
 [1.197]
 [1.197]
 [1.197]]
maxi score, test score, baseline:  -0.8533400000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7947287
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.442]
 [0.264]
 [0.211]
 [0.264]
 [0.17 ]
 [0.26 ]] [[29.3  ]
 [38.155]
 [29.3  ]
 [32.113]
 [29.3  ]
 [32.171]
 [32.71 ]] [[0.778]
 [1.246]
 [0.778]
 [0.817]
 [0.778]
 [0.778]
 [0.886]]
main train batch thing paused
add a thread
Adding thread: now have 4 threads
printing an ep nov before normalisation:  44.08097267150879
actor:  1 policy actor:  1  step number:  63 total reward:  0.2666666666666664  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  60.4525417041193
printing an ep nov before normalisation:  52.50582811614597
maxi score, test score, baseline:  -0.8533400000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8533400000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.8533400000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.60370720408142
maxi score, test score, baseline:  -0.8533400000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.739740983106906
maxi score, test score, baseline:  -0.8533400000000001 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8533400000000001 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8533400000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8533400000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.4913],
        [-0.0000],
        [-0.5592],
        [-0.1436],
        [-0.2268],
        [ 0.6826],
        [ 0.3817],
        [-0.0000],
        [-0.0000],
        [-0.3586]], dtype=torch.float64)
-0.057834381198 -0.5491820552883377
-0.7572925800000002 -0.7572925800000002
-0.032346567066 -0.5915111594658384
-0.071551887066 -0.2151350022153964
-0.07129183386599999 -0.29809121213532197
-0.09703970119800001 0.5855390156940801
-0.08397170119799999 0.29775390727772655
-0.4619999999999998 -0.4619999999999998
-0.07161461999999955 -0.07161461999999955
-0.032346567066 -0.3909327809191706
printing an ep nov before normalisation:  31.462948765823462
actor:  0 policy actor:  0  step number:  63 total reward:  0.13333333333333275  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.3399999999999993  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
printing an ep nov before normalisation:  60.72450780299752
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.658527387060914
Printing some Q and Qe and total Qs values:  [[-0.04]
 [-0.04]
 [-0.04]
 [-0.04]
 [-0.04]
 [-0.04]
 [-0.04]] [[39.671]
 [39.671]
 [39.671]
 [39.671]
 [39.671]
 [39.671]
 [39.671]] [[1.293]
 [1.293]
 [1.293]
 [1.293]
 [1.293]
 [1.293]
 [1.293]]
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.60727396560753
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.500533791194606
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.00913767009151
actor:  1 policy actor:  1  step number:  45 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.182]
 [-0.166]
 [-0.19 ]
 [-0.178]
 [-0.191]
 [-0.181]
 [-0.191]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.182]
 [-0.166]
 [-0.19 ]
 [-0.178]
 [-0.191]
 [-0.181]
 [-0.191]]
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.31377654487374
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.101]
 [ 0.181]
 [-0.072]
 [-0.025]
 [-0.08 ]
 [-0.048]
 [ 0.143]] [[40.186]
 [41.343]
 [38.334]
 [30.071]
 [31.465]
 [32.045]
 [39.504]] [[0.133]
 [0.426]
 [0.145]
 [0.115]
 [0.074]
 [0.11 ]
 [0.37 ]]
printing an ep nov before normalisation:  35.403818148109956
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  24.692762642886958
printing an ep nov before normalisation:  41.234463609646355
siam score:  -0.8002635
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.039376870635493
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.119436117801754
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.82302683201368
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.141]
 [-0.14 ]
 [-0.151]
 [-0.15 ]
 [-0.141]
 [-0.148]
 [-0.135]] [[40.173]
 [47.332]
 [39.977]
 [39.882]
 [39.838]
 [39.599]
 [40.17 ]] [[0.748]
 [1.074]
 [0.728]
 [0.726]
 [0.732]
 [0.714]
 [0.753]]
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
printing an ep nov before normalisation:  52.210128877112574
actor:  1 policy actor:  1  step number:  59 total reward:  0.17333333333333312  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.0
actor:  1 policy actor:  1  step number:  71 total reward:  0.13333333333333253  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.85142560545877
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.214454174041748
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.47100397494538
Printing some Q and Qe and total Qs values:  [[-0.16 ]
 [-0.156]
 [-0.158]
 [-0.161]
 [-0.157]
 [-0.158]
 [-0.16 ]] [[32.786]
 [32.054]
 [33.364]
 [33.698]
 [33.097]
 [33.885]
 [34.076]] [[0.844]
 [0.801]
 [0.882]
 [0.9  ]
 [0.866]
 [0.915]
 [0.925]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.2586, 0.0102, 0.1393, 0.1589, 0.1727, 0.1285, 0.1318],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0055, 0.9542, 0.0061, 0.0085, 0.0027, 0.0029, 0.0201],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1102, 0.0670, 0.1944, 0.1358, 0.1570, 0.1908, 0.1448],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0807, 0.1086, 0.1048, 0.3446, 0.1215, 0.0989, 0.1409],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1923, 0.0021, 0.1085, 0.0791, 0.4465, 0.0873, 0.0841],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0800, 0.0090, 0.1324, 0.1410, 0.1294, 0.3895, 0.1185],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0673, 0.1505, 0.1394, 0.1390, 0.0899, 0.1489, 0.2649],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.92211390859738
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.3  ]
 [0.315]
 [0.316]
 [0.316]
 [0.316]
 [0.312]] [[35.724]
 [40.953]
 [35.863]
 [35.973]
 [34.445]
 [34.344]
 [34.6  ]] [[0.317]
 [0.3  ]
 [0.315]
 [0.316]
 [0.316]
 [0.316]
 [0.312]]
printing an ep nov before normalisation:  50.28659112933248
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.34562110900879
Printing some Q and Qe and total Qs values:  [[-0.198]
 [-0.184]
 [-0.198]
 [-0.198]
 [-0.198]
 [-0.198]
 [-0.198]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.198]
 [-0.184]
 [-0.198]
 [-0.198]
 [-0.198]
 [-0.198]
 [-0.198]]
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.027]
 [ 0.165]
 [-0.027]
 [-0.027]
 [-0.05 ]
 [-0.027]
 [-0.027]] [[33.442]
 [43.85 ]
 [33.442]
 [33.442]
 [33.937]
 [33.442]
 [33.442]] [[0.747]
 [1.41 ]
 [0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]]
printing an ep nov before normalisation:  36.121413768793055
printing an ep nov before normalisation:  16.017636915094986
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.861]
 [0.925]
 [0.822]
 [0.944]
 [0.938]
 [0.822]
 [0.822]] [[52.513]
 [52.089]
 [45.726]
 [55.71 ]
 [54.937]
 [45.726]
 [45.726]] [[0.861]
 [0.925]
 [0.822]
 [0.944]
 [0.938]
 [0.822]
 [0.822]]
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
line 256 mcts: sample exp_bonus 52.44057643356735
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.592]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]] [[26.021]
 [44.869]
 [26.021]
 [26.021]
 [26.021]
 [26.021]
 [26.021]] [[0.518]
 [0.592]
 [0.518]
 [0.518]
 [0.518]
 [0.518]
 [0.518]]
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.12666666666666626  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  53.123235702514656
Printing some Q and Qe and total Qs values:  [[0.958]
 [0.958]
 [0.958]
 [0.958]
 [1.007]
 [0.958]
 [0.958]] [[56.486]
 [56.486]
 [56.486]
 [56.486]
 [56.846]
 [56.486]
 [56.486]] [[0.958]
 [0.958]
 [0.958]
 [0.958]
 [1.007]
 [0.958]
 [0.958]]
Printing some Q and Qe and total Qs values:  [[-0.193]
 [-0.193]
 [-0.193]
 [-0.193]
 [-0.193]
 [-0.193]
 [-0.193]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.193]
 [-0.193]
 [-0.193]
 [-0.193]
 [-0.193]
 [-0.193]
 [-0.193]]
maxi score, test score, baseline:  -0.8510733333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  45 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.16]
 [-0.16]
 [-0.16]
 [-0.16]
 [-0.16]
 [-0.16]
 [-0.16]] [[93.943]
 [93.943]
 [93.943]
 [93.943]
 [93.943]
 [93.943]
 [93.943]] [[1.507]
 [1.507]
 [1.507]
 [1.507]
 [1.507]
 [1.507]
 [1.507]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8482466666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]] [[36.03]
 [36.03]
 [36.03]
 [36.03]
 [36.03]
 [36.03]
 [36.03]] [[0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.446]]
printing an ep nov before normalisation:  45.11955977462783
printing an ep nov before normalisation:  42.688299847223014
printing an ep nov before normalisation:  59.45152270222155
maxi score, test score, baseline:  -0.8482466666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.711340593253766
maxi score, test score, baseline:  -0.8482466666666667 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8482466666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8482466666666667 0.6366666666666668 0.6366666666666668
printing an ep nov before normalisation:  30.416698455810547
maxi score, test score, baseline:  -0.8482466666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.88231220624213
actions average: 
K:  1  action  0 :  tensor([0.3640, 0.0515, 0.1227, 0.1016, 0.1408, 0.1001, 0.1193],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0156, 0.9134, 0.0084, 0.0111, 0.0063, 0.0045, 0.0407],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0942, 0.0361, 0.3534, 0.1219, 0.1001, 0.1630, 0.1313],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1057, 0.0074, 0.1462, 0.3411, 0.1216, 0.1231, 0.1549],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1332, 0.0039, 0.1277, 0.1220, 0.3595, 0.1481, 0.1055],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1134, 0.0238, 0.1934, 0.1554, 0.1385, 0.2536, 0.1219],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1447, 0.1069, 0.1532, 0.1724, 0.1231, 0.1385, 0.1611],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8482466666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.52341921965251
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.531]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]] [[37.072]
 [46.71 ]
 [37.072]
 [37.072]
 [37.072]
 [37.072]
 [37.072]] [[0.618]
 [0.783]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]]
maxi score, test score, baseline:  -0.8482466666666667 0.6366666666666668 0.6366666666666668
printing an ep nov before normalisation:  40.29024554911132
actor:  1 policy actor:  1  step number:  62 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  28.61127967662319
printing an ep nov before normalisation:  3.5494694299548732
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8482466666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.9417547927269
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.37722199526171
maxi score, test score, baseline:  -0.8482466666666667 0.6366666666666668 0.6366666666666668
printing an ep nov before normalisation:  37.54743576049805
maxi score, test score, baseline:  -0.8482466666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.79092044
Printing some Q and Qe and total Qs values:  [[-0.117]
 [-0.129]
 [-0.116]
 [-0.118]
 [-0.116]
 [-0.116]
 [-0.118]] [[25.201]
 [27.283]
 [24.22 ]
 [24.208]
 [24.106]
 [24.341]
 [24.677]] [[1.309]
 [1.416]
 [1.254]
 [1.252]
 [1.248]
 [1.261]
 [1.278]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.4718554655678
maxi score, test score, baseline:  -0.8482466666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8482466666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.62395766906553
printing an ep nov before normalisation:  50.84595249033394
printing an ep nov before normalisation:  36.59873425084502
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  55.09559639929949
printing an ep nov before normalisation:  70.56744382270044
maxi score, test score, baseline:  -0.8482466666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0587385928747608
siam score:  -0.80013245
actor:  1 policy actor:  1  step number:  65 total reward:  0.09333333333333238  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  62 total reward:  0.08666666666666611  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8005336
maxi score, test score, baseline:  -0.8482466666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.26292195362581
maxi score, test score, baseline:  -0.8482466666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8482466666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.351632299137066
line 256 mcts: sample exp_bonus 15.88883175796299
printing an ep nov before normalisation:  59.920361771767745
Printing some Q and Qe and total Qs values:  [[0.85 ]
 [0.898]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]] [[52.403]
 [49.486]
 [52.403]
 [52.403]
 [52.403]
 [52.403]
 [52.403]] [[0.85 ]
 [0.898]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]
 [0.85 ]]
maxi score, test score, baseline:  -0.8482466666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  60 total reward:  0.24666666666666648  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.974289386827486
printing an ep nov before normalisation:  47.38340721042349
printing an ep nov before normalisation:  0.0017943795063501966
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.159]
 [-0.161]
 [-0.161]
 [-0.161]
 [-0.161]
 [-0.161]
 [-0.161]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.159]
 [-0.161]
 [-0.161]
 [-0.161]
 [-0.161]
 [-0.161]
 [-0.161]]
maxi score, test score, baseline:  -0.8457533333333334 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.356917894940125
Printing some Q and Qe and total Qs values:  [[-0.125]
 [-0.125]
 [-0.125]
 [-0.125]
 [-0.125]
 [-0.125]
 [-0.125]] [[51.7]
 [51.7]
 [51.7]
 [51.7]
 [51.7]
 [51.7]
 [51.7]] [[1.082]
 [1.082]
 [1.082]
 [1.082]
 [1.082]
 [1.082]
 [1.082]]
maxi score, test score, baseline:  -0.8457533333333334 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8457533333333334 0.6366666666666668 0.6366666666666668
Printing some Q and Qe and total Qs values:  [[-0.168]
 [-0.148]
 [-0.169]
 [-0.169]
 [-0.169]
 [-0.169]
 [-0.171]] [[16.019]
 [42.326]
 [15.839]
 [15.858]
 [15.842]
 [15.853]
 [15.915]] [[-0.043]
 [ 0.376]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.047]]
printing an ep nov before normalisation:  33.34404095738389
printing an ep nov before normalisation:  26.132768006653905
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  65 total reward:  0.1333333333333323  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.3533333333333327  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.121]
 [-0.117]
 [-0.122]
 [-0.122]
 [-0.12 ]
 [-0.12 ]
 [-0.119]] [[31.073]
 [24.441]
 [33.757]
 [32.325]
 [33.482]
 [29.909]
 [26.116]] [[0.356]
 [0.259]
 [0.397]
 [0.374]
 [0.394]
 [0.339]
 [0.282]]
Printing some Q and Qe and total Qs values:  [[-0.158]
 [-0.154]
 [-0.17 ]
 [-0.16 ]
 [-0.152]
 [-0.168]
 [-0.172]] [[68.796]
 [54.859]
 [69.036]
 [66.854]
 [68.834]
 [71.251]
 [65.373]] [[1.409]
 [0.747]
 [1.409]
 [1.314]
 [1.417]
 [1.516]
 [1.231]]
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.51371383666992
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
Printing some Q and Qe and total Qs values:  [[-0.016]
 [ 0.234]
 [-0.016]
 [ 0.139]
 [-0.028]
 [ 0.017]
 [-0.016]] [[38.364]
 [43.15 ]
 [38.364]
 [41.676]
 [40.613]
 [50.915]
 [38.364]] [[0.636]
 [1.054]
 [0.636]
 [0.907]
 [0.702]
 [1.11 ]
 [0.636]]
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
Printing some Q and Qe and total Qs values:  [[0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]
 [0.094]] [[40.761]
 [40.761]
 [40.761]
 [40.761]
 [40.761]
 [40.761]
 [40.761]] [[1.093]
 [1.093]
 [1.093]
 [1.093]
 [1.093]
 [1.093]
 [1.093]]
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.98962747856649
printing an ep nov before normalisation:  62.25992712362851
siam score:  -0.7995302
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.97216118706597
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  0.17333333333333312  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
printing an ep nov before normalisation:  40.218586921691895
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  67 total reward:  0.2533333333333334  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.321372509002686
printing an ep nov before normalisation:  65.03382682800293
printing an ep nov before normalisation:  50.456533432006836
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.25059084786737
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.894570793613347
printing an ep nov before normalisation:  59.99699831008911
printing an ep nov before normalisation:  32.98235174724777
printing an ep nov before normalisation:  64.68679426125466
printing an ep nov before normalisation:  57.29612977651377
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.79435160030127
printing an ep nov before normalisation:  16.594814396062763
printing an ep nov before normalisation:  58.90576185065791
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.168]
 [-0.149]
 [-0.168]
 [-0.165]
 [-0.189]
 [-0.168]
 [-0.15 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.168]
 [-0.149]
 [-0.168]
 [-0.165]
 [-0.189]
 [-0.168]
 [-0.15 ]]
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8434866666666668 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.508]
 [0.422]
 [0.454]
 [0.394]
 [0.381]
 [0.369]] [[33.219]
 [28.508]
 [36.357]
 [30.612]
 [33.817]
 [34.538]
 [34.157]] [[1.988]
 [1.856]
 [2.143]
 [1.901]
 [1.993]
 [2.015]
 [1.985]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.23333333333333306  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8028442
maxi score, test score, baseline:  -0.8434866666666668 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.10711833408901
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  0.0
printing an ep nov before normalisation:  34.775858431916895
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8009343
printing an ep nov before normalisation:  39.283921417795604
printing an ep nov before normalisation:  39.010075810214886
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.617]
 [0.593]
 [0.597]
 [0.592]
 [0.591]
 [0.586]] [[44.63 ]
 [41.822]
 [44.826]
 [46.01 ]
 [44.535]
 [44.171]
 [46.467]] [[0.596]
 [0.617]
 [0.593]
 [0.597]
 [0.592]
 [0.591]
 [0.586]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.1501],
        [-0.0000],
        [-0.0000],
        [ 0.3624],
        [-0.0000],
        [ 0.4898],
        [-0.0000],
        [-0.4333],
        [-0.4019]], dtype=torch.float64)
-0.9389570813039999 -0.9389570813039999
-0.058483887065999995 -0.20857480012721258
-0.95755836 -0.95755836
-0.5050148400000001 -0.5050148400000001
-0.083839701198 0.278525018039795
-0.6279801330000001 -0.6279801330000001
-0.083839701198 0.4059553909257515
0.9734999999999999 0.9734999999999999
-0.032346567066 -0.46566300603991156
-0.09703970119800001 -0.4989254554693109
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.4666666666666668  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  66.22276623686871
Printing some Q and Qe and total Qs values:  [[-0.101]
 [-0.084]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]
 [-0.101]] [[49.61 ]
 [44.089]
 [49.61 ]
 [49.61 ]
 [49.61 ]
 [49.61 ]
 [49.61 ]] [[1.346]
 [1.07 ]
 [1.346]
 [1.346]
 [1.346]
 [1.346]
 [1.346]]
printing an ep nov before normalisation:  41.642843798618166
line 256 mcts: sample exp_bonus 38.91902967553225
printing an ep nov before normalisation:  34.086899757385254
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 23.84903344262733
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.95865732215712
printing an ep nov before normalisation:  36.90348148345947
printing an ep nov before normalisation:  45.44570276888846
printing an ep nov before normalisation:  47.27308521724052
printing an ep nov before normalisation:  36.53270254384209
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.10655975341797
maxi score, test score, baseline:  -0.8434866666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  69 total reward:  0.013333333333332753  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  0.010546840271672409
actor:  0 policy actor:  0  step number:  56 total reward:  0.15333333333333288  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.84118 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.69520217391248
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.11368751525879
siam score:  -0.80319333
printing an ep nov before normalisation:  44.57588195800781
actor:  1 policy actor:  1  step number:  55 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  30.243842100893264
Printing some Q and Qe and total Qs values:  [[-0.174]
 [-0.174]
 [-0.174]
 [-0.174]
 [-0.174]
 [-0.174]
 [-0.174]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.174]
 [-0.174]
 [-0.174]
 [-0.174]
 [-0.174]
 [-0.174]
 [-0.174]]
printing an ep nov before normalisation:  66.33991925897796
printing an ep nov before normalisation:  55.62931482690801
printing an ep nov before normalisation:  32.85891056060791
maxi score, test score, baseline:  -0.84118 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.84118 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.5642858668782
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.407]
 [0.28 ]
 [0.255]
 [0.165]
 [0.28 ]
 [0.294]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.28 ]
 [0.407]
 [0.28 ]
 [0.255]
 [0.165]
 [0.28 ]
 [0.294]]
Printing some Q and Qe and total Qs values:  [[-0.165]
 [-0.144]
 [-0.168]
 [-0.155]
 [-0.155]
 [-0.172]
 [-0.155]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.165]
 [-0.144]
 [-0.168]
 [-0.155]
 [-0.155]
 [-0.172]
 [-0.155]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  58 total reward:  0.09999999999999964  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.84118 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.26599068958644
maxi score, test score, baseline:  -0.84118 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.84118 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.84118 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.241]] [[29.848]
 [29.848]
 [29.848]
 [29.848]
 [29.848]
 [29.848]
 [29.848]] [[1.417]
 [1.417]
 [1.417]
 [1.417]
 [1.417]
 [1.417]
 [1.417]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actor:  1 policy actor:  1  step number:  52 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.169]
 [-0.169]
 [-0.169]
 [-0.169]
 [-0.169]
 [-0.169]
 [-0.169]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.169]
 [-0.169]
 [-0.169]
 [-0.169]
 [-0.169]
 [-0.169]
 [-0.169]]
printing an ep nov before normalisation:  61.97938620541148
Printing some Q and Qe and total Qs values:  [[-0.115]
 [-0.089]
 [-0.115]
 [-0.115]
 [-0.115]
 [-0.115]
 [-0.115]] [[36.098]
 [44.313]
 [36.098]
 [36.098]
 [36.098]
 [36.098]
 [36.098]] [[0.281]
 [0.454]
 [0.281]
 [0.281]
 [0.281]
 [0.281]
 [0.281]]
maxi score, test score, baseline:  -0.84118 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.832869400162267
printing an ep nov before normalisation:  33.81115272011039
maxi score, test score, baseline:  -0.84118 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.84118 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.098]
 [-0.096]
 [-0.106]
 [-0.115]
 [-0.114]
 [-0.119]
 [-0.113]] [[39.669]
 [43.789]
 [41.996]
 [43.147]
 [45.302]
 [43.826]
 [41.298]] [[0.635]
 [0.797]
 [0.717]
 [0.753]
 [0.838]
 [0.776]
 [0.683]]
printing an ep nov before normalisation:  32.98691440502231
maxi score, test score, baseline:  -0.84118 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.0426141110216
printing an ep nov before normalisation:  49.313394413776564
maxi score, test score, baseline:  -0.84118 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.84118 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  70 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.84118 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.84118 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.84118 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.84118 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.63540334608012
printing an ep nov before normalisation:  49.425300001440746
Printing some Q and Qe and total Qs values:  [[-0.117]
 [-0.101]
 [-0.117]
 [-0.117]
 [-0.117]
 [-0.117]
 [-0.117]] [[32.46]
 [34.08]
 [32.46]
 [32.46]
 [32.46]
 [32.46]
 [32.46]] [[1.413]
 [1.563]
 [1.413]
 [1.413]
 [1.413]
 [1.413]
 [1.413]]
maxi score, test score, baseline:  -0.84118 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.84118 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.032]
 [ 0.318]
 [ 0.125]
 [ 0.338]
 [ 0.065]
 [ 0.014]
 [ 0.099]] [[43.505]
 [40.707]
 [43.526]
 [39.651]
 [46.747]
 [43.866]
 [37.718]] [[1.358]
 [1.545]
 [1.517]
 [1.503]
 [1.645]
 [1.425]
 [1.152]]
Printing some Q and Qe and total Qs values:  [[-0.086]
 [-0.091]
 [-0.096]
 [-0.095]
 [-0.095]
 [-0.105]
 [-0.1  ]] [[31.489]
 [29.448]
 [31.631]
 [33.434]
 [33.821]
 [32.261]
 [32.49 ]] [[0.15 ]
 [0.114]
 [0.142]
 [0.171]
 [0.177]
 [0.143]
 [0.151]]
maxi score, test score, baseline:  -0.84118 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.31999999999999984  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.84118 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.301407376841276
maxi score, test score, baseline:  -0.84118 0.6366666666666668 0.6366666666666668
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.73442857280566
printing an ep nov before normalisation:  24.19170712143807
printing an ep nov before normalisation:  39.6100768468364
actor:  1 policy actor:  1  step number:  61 total reward:  0.14666666666666628  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  59 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  43.140411050426266
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]] [[35.941]
 [35.941]
 [35.941]
 [35.941]
 [35.941]
 [35.941]
 [35.941]] [[48.503]
 [48.503]
 [48.503]
 [48.503]
 [48.503]
 [48.503]
 [48.503]]
Printing some Q and Qe and total Qs values:  [[-0.149]
 [-0.154]
 [-0.151]
 [-0.149]
 [-0.151]
 [-0.149]
 [-0.149]] [[30.588]
 [35.046]
 [32.084]
 [30.588]
 [31.875]
 [30.588]
 [30.588]] [[0.229]
 [0.332]
 [0.263]
 [0.229]
 [0.258]
 [0.229]
 [0.229]]
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.10671733277756
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
Printing some Q and Qe and total Qs values:  [[-0.129]
 [-0.129]
 [-0.129]
 [-0.129]
 [-0.129]
 [-0.129]
 [-0.129]] [[62.866]
 [62.866]
 [62.866]
 [62.866]
 [62.866]
 [62.866]
 [62.866]] [[1.173]
 [1.173]
 [1.173]
 [1.173]
 [1.173]
 [1.173]
 [1.173]]
printing an ep nov before normalisation:  37.95971870422363
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
actor:  1 policy actor:  1  step number:  52 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  1.0
UNIT TEST: sample policy line 217 mcts : [0.245 0.306 0.02  0.02  0.02  0.367 0.02 ]
siam score:  -0.8164265
Printing some Q and Qe and total Qs values:  [[-0.109]
 [-0.095]
 [-0.109]
 [-0.109]
 [-0.109]
 [-0.109]
 [-0.109]] [[32.303]
 [39.968]
 [32.303]
 [32.303]
 [32.303]
 [32.303]
 [32.303]] [[0.204]
 [0.362]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]]
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.043]
 [ 0.059]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]] [[35.045]
 [44.716]
 [35.045]
 [35.045]
 [35.045]
 [35.045]
 [35.045]] [[0.309]
 [0.614]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]]
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.664]
 [0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]] [[44.251]
 [41.891]
 [44.251]
 [44.251]
 [44.251]
 [44.251]
 [44.251]] [[1.087]
 [1.235]
 [1.087]
 [1.087]
 [1.087]
 [1.087]
 [1.087]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
using explorer policy with actor:  1
siam score:  -0.8131032
printing an ep nov before normalisation:  66.1308702486151
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  77.16581648496081
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.474241505163782
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 45.30809308214587
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.82911396026611
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  73 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.31067811188596
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.623629902158264
printing an ep nov before normalisation:  35.39380388558784
using explorer policy with actor:  1
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([0.2844, 0.0179, 0.1356, 0.1311, 0.1891, 0.1138, 0.1281],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0123, 0.9281, 0.0136, 0.0104, 0.0045, 0.0069, 0.0242],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1241, 0.1728, 0.1534, 0.1502, 0.1142, 0.1501, 0.1352],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1239, 0.0900, 0.1422, 0.2313, 0.1392, 0.1500, 0.1234],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1535, 0.0202, 0.0969, 0.0966, 0.4528, 0.0901, 0.0898],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1396, 0.0521, 0.1540, 0.2173, 0.1465, 0.1638, 0.1267],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1472, 0.1113, 0.1124, 0.1431, 0.1482, 0.1106, 0.2271],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.197]
 [-0.197]
 [-0.197]
 [-0.167]
 [-0.197]
 [-0.197]
 [-0.197]] [[40.392]
 [40.392]
 [40.392]
 [42.003]
 [40.392]
 [40.392]
 [40.392]] [[0.513]
 [0.513]
 [0.513]
 [0.599]
 [0.513]
 [0.513]
 [0.513]]
printing an ep nov before normalisation:  20.55044624217571
line 256 mcts: sample exp_bonus 48.766055801580436
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  0.13333333333333264  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.24928697372647
printing an ep nov before normalisation:  46.35382219094568
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.63201904296875
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.104]
 [-0.104]
 [-0.104]
 [-0.104]
 [-0.104]
 [-0.104]
 [-0.104]] [[34.371]
 [46.378]
 [34.371]
 [34.371]
 [34.371]
 [34.371]
 [34.371]] [[0.467]
 [0.855]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]]
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.35018505455478
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.363]
 [0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.282]] [[39.535]
 [44.922]
 [39.535]
 [39.535]
 [39.535]
 [39.535]
 [39.535]] [[0.768]
 [0.978]
 [0.768]
 [0.768]
 [0.768]
 [0.768]
 [0.768]]
printing an ep nov before normalisation:  44.51227676450085
printing an ep nov before normalisation:  38.11734736500287
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.0933333333333326  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  66 total reward:  0.16666666666666585  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  48.15583409705114
actor:  1 policy actor:  1  step number:  78 total reward:  0.08666666666666578  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  52.97005122265525
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8390200000000001 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.133]
 [-0.124]
 [-0.157]
 [-0.133]
 [-0.157]
 [-0.133]
 [-0.133]] [[41.921]
 [40.503]
 [36.889]
 [41.921]
 [37.104]
 [41.921]
 [41.921]] [[1.713]
 [1.602]
 [1.264]
 [1.713]
 [1.282]
 [1.713]
 [1.713]]
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actor:  0 policy actor:  0  step number:  48 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.8363133333333335 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8363133333333335 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8363133333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.6819486618042
Printing some Q and Qe and total Qs values:  [[0.267]
 [0.313]
 [0.267]
 [0.22 ]
 [0.267]
 [0.267]
 [0.267]] [[47.056]
 [46.806]
 [47.056]
 [54.485]
 [47.056]
 [47.056]
 [47.056]] [[1.58 ]
 [1.614]
 [1.58 ]
 [1.915]
 [1.58 ]
 [1.58 ]
 [1.58 ]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.8363133333333335 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8363133333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.8363133333333335 0.6366666666666668 0.6366666666666668
maxi score, test score, baseline:  -0.8363133333333335 0.6366666666666668 0.6366666666666668
Starting evaluation
actions average: 
K:  2  action  0 :  tensor([0.1629, 0.0703, 0.1267, 0.1602, 0.1941, 0.1548, 0.1310],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0172, 0.8996, 0.0138, 0.0135, 0.0103, 0.0098, 0.0358],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1220, 0.0918, 0.2814, 0.1291, 0.1141, 0.1528, 0.1089],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0547, 0.1219, 0.1353, 0.3686, 0.0615, 0.0843, 0.1736],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1661, 0.0231, 0.1205, 0.1485, 0.2700, 0.1555, 0.1162],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1065, 0.0054, 0.1517, 0.1053, 0.1183, 0.4398, 0.0730],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0616, 0.3022, 0.0608, 0.1944, 0.0501, 0.0601, 0.2709],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.622]
 [0.384]
 [0.392]
 [0.41 ]
 [0.447]
 [0.447]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.411]
 [0.622]
 [0.384]
 [0.392]
 [0.41 ]
 [0.447]
 [0.447]]
printing an ep nov before normalisation:  42.80931615185463
printing an ep nov before normalisation:  42.64897172289474
Printing some Q and Qe and total Qs values:  [[0.702]
 [0.82 ]
 [0.702]
 [0.754]
 [0.762]
 [0.702]
 [0.702]] [[50.295]
 [49.207]
 [50.295]
 [57.449]
 [58.725]
 [50.295]
 [50.295]] [[0.702]
 [0.82 ]
 [0.702]
 [0.754]
 [0.762]
 [0.702]
 [0.702]]
printing an ep nov before normalisation:  37.81160212417011
printing an ep nov before normalisation:  56.69167627411097
printing an ep nov before normalisation:  41.85820357090266
printing an ep nov before normalisation:  32.87320613861084
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  48.933968028308904
maxi score, test score, baseline:  -0.8363133333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.51468992148696
maxi score, test score, baseline:  -0.8363133333333335 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.895]
 [0.954]
 [0.888]
 [0.838]
 [0.895]
 [0.895]
 [0.868]] [[41.097]
 [40.239]
 [39.229]
 [47.376]
 [41.097]
 [41.097]
 [42.32 ]] [[0.895]
 [0.954]
 [0.888]
 [0.838]
 [0.895]
 [0.895]
 [0.868]]
printing an ep nov before normalisation:  36.370951408787896
printing an ep nov before normalisation:  47.92579826364903
printing an ep nov before normalisation:  29.52580451965332
printing an ep nov before normalisation:  34.64417383456167
printing an ep nov before normalisation:  47.039549120921244
printing an ep nov before normalisation:  31.703965663909912
Printing some Q and Qe and total Qs values:  [[0.929]
 [0.955]
 [0.924]
 [0.925]
 [0.913]
 [0.912]
 [0.934]] [[26.847]
 [31.814]
 [27.486]
 [27.467]
 [27.51 ]
 [28.343]
 [27.897]] [[0.929]
 [0.955]
 [0.924]
 [0.925]
 [0.913]
 [0.912]
 [0.934]]
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.8229266666666667 0.6366666666666668 0.6366666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  28 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  28 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  46.087511189244246
actor:  0 policy actor:  0  step number:  28 total reward:  0.6466666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  29 total reward:  0.6533333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  30 total reward:  0.6333333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  30 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  30 total reward:  0.6333333333333334  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  30 total reward:  0.6066666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  31 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  31 total reward:  0.6133333333333334  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  22.43109586879114
printing an ep nov before normalisation:  32.201223279253604
maxi score, test score, baseline:  -0.7702733333333335 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.16628042103572
actor:  1 policy actor:  1  step number:  57 total reward:  0.1733333333333329  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  28.868425778114783
Printing some Q and Qe and total Qs values:  [[0.454]
 [0.482]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]] [[33.038]
 [38.274]
 [33.038]
 [33.038]
 [33.038]
 [33.038]
 [33.038]] [[0.454]
 [0.482]
 [0.454]
 [0.454]
 [0.454]
 [0.454]
 [0.454]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  56 total reward:  0.15333333333333288  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.7702733333333335 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.29999999999999916  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.039]
 [0.029]
 [0.038]
 [0.051]
 [0.044]
 [0.038]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.035]
 [0.039]
 [0.029]
 [0.038]
 [0.051]
 [0.044]
 [0.038]]
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.504]
 [0.504]
 [0.489]
 [0.487]
 [0.475]
 [0.501]] [[51.9  ]
 [49.803]
 [48.892]
 [52.721]
 [50.492]
 [51.61 ]
 [49.402]] [[0.47 ]
 [0.504]
 [0.504]
 [0.489]
 [0.487]
 [0.475]
 [0.501]]
printing an ep nov before normalisation:  43.83078347690396
printing an ep nov before normalisation:  41.33254703791114
maxi score, test score, baseline:  -0.7702733333333335 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7702733333333335 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7702733333333335 0.651 0.651
printing an ep nov before normalisation:  64.645848250235
printing an ep nov before normalisation:  48.65992233287635
maxi score, test score, baseline:  -0.7702733333333335 0.651 0.651
printing an ep nov before normalisation:  47.195280144095044
printing an ep nov before normalisation:  44.94369834804054
maxi score, test score, baseline:  -0.7702733333333335 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.798916682440435
Printing some Q and Qe and total Qs values:  [[-0.097]
 [-0.097]
 [-0.097]
 [-0.097]
 [-0.097]
 [-0.097]
 [-0.097]] [[40.638]
 [40.638]
 [40.638]
 [40.638]
 [40.638]
 [40.638]
 [40.638]] [[0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]]
maxi score, test score, baseline:  -0.7702733333333335 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  6.004624234871395e-06
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7702733333333335 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.13206789881223813
printing an ep nov before normalisation:  57.99516469723682
using explorer policy with actor:  1
siam score:  -0.80808777
printing an ep nov before normalisation:  67.26504520417167
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7702733333333335 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.157]
 [-0.132]
 [-0.157]
 [-0.157]
 [-0.157]
 [-0.157]
 [-0.157]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.157]
 [-0.132]
 [-0.157]
 [-0.157]
 [-0.157]
 [-0.157]
 [-0.157]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.703401264236234
printing an ep nov before normalisation:  43.06526710288303
maxi score, test score, baseline:  -0.7702733333333335 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  98.47822389373177
maxi score, test score, baseline:  -0.7702733333333335 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.58223681225246
printing an ep nov before normalisation:  60.70985907191117
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.182844829383733
actor:  1 policy actor:  1  step number:  52 total reward:  0.3399999999999994  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.166]
 [-0.152]
 [-0.169]
 [-0.169]
 [-0.167]
 [-0.167]
 [-0.167]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.166]
 [-0.152]
 [-0.169]
 [-0.169]
 [-0.167]
 [-0.167]
 [-0.167]]
printing an ep nov before normalisation:  42.64305647603726
printing an ep nov before normalisation:  39.133338304387465
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.701]
 [0.716]
 [0.426]
 [0.443]
 [0.404]
 [0.462]] [[39.655]
 [38.796]
 [42.039]
 [42.65 ]
 [42.596]
 [42.576]
 [38.772]] [[0.426]
 [0.701]
 [0.716]
 [0.426]
 [0.443]
 [0.404]
 [0.462]]
maxi score, test score, baseline:  -0.7702733333333335 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.48440346878199
maxi score, test score, baseline:  -0.7702733333333335 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.375]
 [0.375]
 [0.375]
 [0.351]
 [0.375]
 [0.375]] [[45.277]
 [45.277]
 [45.277]
 [45.277]
 [58.417]
 [45.277]
 [45.277]] [[1.095]
 [1.095]
 [1.095]
 [1.095]
 [1.412]
 [1.095]
 [1.095]]
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.27 ]
 [0.417]
 [0.463]
 [0.432]
 [0.47 ]
 [0.348]] [[56.55 ]
 [56.545]
 [58.11 ]
 [57.744]
 [60.625]
 [56.97 ]
 [58.551]] [[1.307]
 [1.338]
 [1.53 ]
 [1.566]
 [1.618]
 [1.55 ]
 [1.474]]
Printing some Q and Qe and total Qs values:  [[0.61 ]
 [0.697]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]] [[29.773]
 [29.8  ]
 [29.773]
 [29.773]
 [29.773]
 [29.773]
 [29.773]] [[0.61 ]
 [0.697]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]]
printing an ep nov before normalisation:  30.539389746315017
actor:  1 policy actor:  1  step number:  47 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  61 total reward:  0.13333333333333275  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.7680066666666667 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7680066666666667 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7680066666666667 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.48654088544939
printing an ep nov before normalisation:  25.672075748443604
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.7680066666666667 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.40228603723699
printing an ep nov before normalisation:  61.232959323670954
maxi score, test score, baseline:  -0.7680066666666667 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7680066666666667 0.651 0.651
maxi score, test score, baseline:  -0.7680066666666667 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.501680065489005
siam score:  -0.8204618
Printing some Q and Qe and total Qs values:  [[-0.032]
 [-0.015]
 [-0.027]
 [-0.027]
 [-0.039]
 [-0.027]
 [-0.027]] [[47.38 ]
 [46.104]
 [41.593]
 [41.593]
 [41.685]
 [44.024]
 [41.593]] [[0.234]
 [0.237]
 [0.176]
 [0.176]
 [0.166]
 [0.203]
 [0.176]]
maxi score, test score, baseline:  -0.7680066666666667 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.75431390670815
Printing some Q and Qe and total Qs values:  [[0.146]
 [0.218]
 [0.146]
 [0.146]
 [0.146]
 [0.146]
 [0.146]] [[40.533]
 [42.381]
 [40.533]
 [40.533]
 [40.533]
 [40.533]
 [40.533]] [[0.393]
 [0.488]
 [0.393]
 [0.393]
 [0.393]
 [0.393]
 [0.393]]
maxi score, test score, baseline:  -0.7680066666666667 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7680066666666667 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7680066666666667 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.87754253042226
printing an ep nov before normalisation:  0.0
Printing some Q and Qe and total Qs values:  [[-0.194]
 [-0.149]
 [-0.18 ]
 [-0.159]
 [-0.172]
 [-0.193]
 [-0.16 ]] [[60.061]
 [60.123]
 [58.475]
 [52.31 ]
 [55.469]
 [63.826]
 [57.983]] [[0.549]
 [0.595]
 [0.526]
 [0.405]
 [0.465]
 [0.636]
 [0.534]]
printing an ep nov before normalisation:  63.12454490440508
actor:  1 policy actor:  1  step number:  53 total reward:  0.42666666666666675  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  36.877531178080474
printing an ep nov before normalisation:  40.93778786764081
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([0.4273, 0.0216, 0.1034, 0.1338, 0.1256, 0.0897, 0.0986],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0131, 0.8824, 0.0153, 0.0270, 0.0062, 0.0071, 0.0489],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1493, 0.0628, 0.1949, 0.1681, 0.1581, 0.1271, 0.1397],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0999, 0.1268, 0.1150, 0.3235, 0.1215, 0.0950, 0.1183],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1728, 0.0059, 0.1292, 0.1452, 0.3402, 0.1074, 0.0993],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0944, 0.0618, 0.1360, 0.2007, 0.1102, 0.2883, 0.1086],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0939, 0.0916, 0.1316, 0.1461, 0.1001, 0.1142, 0.3226],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.7680066666666668 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.8500917404364
maxi score, test score, baseline:  -0.7680066666666668 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.6549480726205
maxi score, test score, baseline:  -0.7680066666666668 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.27461471911292
actor:  1 policy actor:  1  step number:  66 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  0.03999999999999926  reward:  1.0 rdn_beta:  0.667
siam score:  -0.81199133
maxi score, test score, baseline:  -0.7680066666666668 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7680066666666668 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7680066666666668 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.586515312750876
maxi score, test score, baseline:  -0.7680066666666667 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.70029236474693
printing an ep nov before normalisation:  56.1003978565825
maxi score, test score, baseline:  -0.7680066666666667 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7680066666666667 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.143]
 [-0.114]
 [-0.143]
 [-0.143]
 [-0.143]
 [-0.143]
 [-0.143]] [[35.951]
 [48.694]
 [35.951]
 [35.951]
 [35.951]
 [35.951]
 [35.951]] [[0.996]
 [1.439]
 [0.996]
 [0.996]
 [0.996]
 [0.996]
 [0.996]]
maxi score, test score, baseline:  -0.7680066666666667 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.775556564331055
printing an ep nov before normalisation:  30.36679744720459
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.654]
 [0.537]
 [0.52 ]
 [0.558]
 [0.534]
 [0.566]] [[36.338]
 [23.043]
 [34.135]
 [35.135]
 [34.156]
 [34.662]
 [34.291]] [[2.364]
 [1.818]
 [2.261]
 [2.294]
 [2.283]
 [2.285]
 [2.299]]
maxi score, test score, baseline:  -0.7680066666666667 0.651 0.651
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.687]
 [0.68 ]
 [0.683]
 [0.681]
 [0.682]
 [0.684]] [[16.443]
 [13.825]
 [15.439]
 [15.602]
 [15.685]
 [15.726]
 [14.849]] [[1.685]
 [1.529]
 [1.62 ]
 [1.633]
 [1.637]
 [1.64 ]
 [1.588]]
actor:  1 policy actor:  1  step number:  57 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.7680066666666667 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  69 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  2  action  0 :  tensor([0.3906, 0.0218, 0.1240, 0.1253, 0.1250, 0.1074, 0.1059],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0290, 0.8742, 0.0221, 0.0171, 0.0162, 0.0158, 0.0256],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1171, 0.0159, 0.2649, 0.1646, 0.1471, 0.1652, 0.1252],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1343, 0.0088, 0.1521, 0.1834, 0.2028, 0.1852, 0.1333],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1617, 0.0427, 0.1179, 0.1274, 0.3091, 0.1113, 0.1299],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0956, 0.0152, 0.1380, 0.1668, 0.1249, 0.3775, 0.0821],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1075, 0.2786, 0.1343, 0.1145, 0.1009, 0.0998, 0.1645],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.76558 0.651 0.651
printing an ep nov before normalisation:  58.55429466222692
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.76463591226506
actions average: 
K:  1  action  0 :  tensor([0.1890, 0.0037, 0.1808, 0.1348, 0.1657, 0.1507, 0.1753],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0215, 0.8770, 0.0205, 0.0238, 0.0098, 0.0096, 0.0379],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0978, 0.0159, 0.3212, 0.1402, 0.1291, 0.1699, 0.1259],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0737, 0.1645, 0.1018, 0.2760, 0.1358, 0.1483, 0.0999],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1387, 0.0091, 0.0919, 0.0832, 0.5186, 0.0664, 0.0921],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1666, 0.0129, 0.1740, 0.1556, 0.1703, 0.1434, 0.1772],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1456, 0.0099, 0.1452, 0.1382, 0.1247, 0.1003, 0.3363],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.235]
 [0.228]
 [0.235]
 [0.203]
 [0.235]
 [0.235]
 [0.235]] [[47.639]
 [47.135]
 [47.639]
 [47.309]
 [47.639]
 [47.639]
 [47.639]] [[1.786]
 [1.746]
 [1.786]
 [1.733]
 [1.786]
 [1.786]
 [1.786]]
printing an ep nov before normalisation:  59.27475341348795
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.68565264924507
maxi score, test score, baseline:  -0.76558 0.651 0.651
actor:  1 policy actor:  1  step number:  39 total reward:  0.5600000000000002  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.76558 0.651 0.651
printing an ep nov before normalisation:  38.14623043231256
actor:  1 policy actor:  1  step number:  51 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  66 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.816]
 [0.507]
 [0.506]
 [0.462]
 [0.507]
 [0.507]] [[33.091]
 [51.304]
 [33.091]
 [33.36 ]
 [34.046]
 [33.091]
 [33.091]] [[0.507]
 [0.816]
 [0.507]
 [0.506]
 [0.462]
 [0.507]
 [0.507]]
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.72221800871654
Printing some Q and Qe and total Qs values:  [[-0.14 ]
 [-0.126]
 [-0.14 ]
 [-0.14 ]
 [-0.14 ]
 [-0.14 ]
 [-0.14 ]] [[61.398]
 [48.981]
 [61.398]
 [61.398]
 [61.398]
 [61.398]
 [61.398]] [[0.489]
 [0.266]
 [0.489]
 [0.489]
 [0.489]
 [0.489]
 [0.489]]
printing an ep nov before normalisation:  70.2753933178504
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.40439640013254
printing an ep nov before normalisation:  48.079923931580566
Printing some Q and Qe and total Qs values:  [[-0.163]
 [-0.163]
 [-0.163]
 [-0.163]
 [-0.163]
 [-0.163]
 [-0.163]] [[52.464]
 [52.464]
 [52.464]
 [52.464]
 [52.464]
 [52.464]
 [52.464]] [[0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]]
printing an ep nov before normalisation:  19.036346082425574
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.978628158569336
printing an ep nov before normalisation:  54.7526529129043
maxi score, test score, baseline:  -0.76558 0.651 0.651
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.148]
 [-0.147]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]
 [-0.149]] [[64.282]
 [56.975]
 [66.65 ]
 [66.65 ]
 [67.801]
 [67.496]
 [66.65 ]] [[1.354]
 [1.013]
 [1.465]
 [1.465]
 [1.518]
 [1.504]
 [1.465]]
printing an ep nov before normalisation:  41.943583488464355
printing an ep nov before normalisation:  26.319348812103275
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.745]
 [0.547]
 [0.547]
 [0.547]
 [0.443]
 [0.547]] [[52.165]
 [46.129]
 [52.165]
 [52.165]
 [52.165]
 [61.2  ]
 [52.165]] [[1.081]
 [1.163]
 [1.081]
 [1.081]
 [1.081]
 [1.151]
 [1.081]]
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.393346669353793
printing an ep nov before normalisation:  59.38507858444443
actor:  1 policy actor:  1  step number:  64 total reward:  0.08666666666666589  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.61031175221053
printing an ep nov before normalisation:  55.92430962456597
printing an ep nov before normalisation:  33.66377592086792
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  64 total reward:  0.059999999999999276  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  58.313110909974895
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.693827586455818
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  36.10206780240957
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.76558 0.651 0.651
siam score:  -0.82043546
actor:  1 policy actor:  1  step number:  52 total reward:  0.3266666666666661  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.76558 0.651 0.651
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.81974304
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.515003852537674
actor:  1 policy actor:  1  step number:  58 total reward:  0.3266666666666661  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.76558 0.651 0.651
printing an ep nov before normalisation:  46.01337390074969
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.76558 0.651 0.651
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.66383778480482
printing an ep nov before normalisation:  35.02623697872497
maxi score, test score, baseline:  -0.76558 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.36390998641821
printing an ep nov before normalisation:  61.3728012791329
actor:  0 policy actor:  0  step number:  55 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.7627266666666668 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  66 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7627266666666668 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7627266666666668 0.651 0.651
maxi score, test score, baseline:  -0.7627266666666668 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7627266666666668 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.84475924378886
printing an ep nov before normalisation:  49.15792988270355
printing an ep nov before normalisation:  13.484350364495938
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]
 [0.433]] [[45.737]
 [45.737]
 [45.737]
 [45.737]
 [45.737]
 [45.737]
 [45.737]] [[0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]]
maxi score, test score, baseline:  -0.7627266666666668 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7627266666666668 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.42212232516019
maxi score, test score, baseline:  -0.7627266666666668 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.061 0.265 0.082 0.061 0.082 0.122 0.327]
actor:  1 policy actor:  1  step number:  59 total reward:  0.19999999999999962  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.121]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]
 [-0.121]] [[36.618]
 [36.618]
 [36.618]
 [36.618]
 [36.618]
 [36.618]
 [36.618]] [[0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]]
UNIT TEST: sample policy line 217 mcts : [0.041 0.429 0.224 0.02  0.224 0.02  0.041]
line 256 mcts: sample exp_bonus 52.42762991906553
printing an ep nov before normalisation:  39.743143986181
actor:  1 policy actor:  1  step number:  55 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.332]]
printing an ep nov before normalisation:  44.94460827301163
maxi score, test score, baseline:  -0.7627266666666668 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.13834323246582
Printing some Q and Qe and total Qs values:  [[-0.019]
 [ 0.341]
 [-0.055]
 [-0.028]
 [-0.018]
 [-0.07 ]
 [ 0.01 ]] [[30.675]
 [41.206]
 [26.931]
 [25.402]
 [25.171]
 [26.332]
 [27.774]] [[0.41 ]
 [1.036]
 [0.279]
 [0.268]
 [0.272]
 [0.25 ]
 [0.366]]
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.434]
 [0.388]
 [0.452]
 [0.383]
 [0.366]
 [0.452]] [[43.349]
 [47.658]
 [45.068]
 [43.349]
 [46.593]
 [45.924]
 [43.349]] [[1.133]
 [1.241]
 [1.119]
 [1.133]
 [1.159]
 [1.122]
 [1.133]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.7627266666666668 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  24.220636800553322
actions average: 
K:  0  action  0 :  tensor([0.2808, 0.0550, 0.0996, 0.1367, 0.2027, 0.1104, 0.1149],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0043, 0.9649, 0.0026, 0.0085, 0.0017, 0.0016, 0.0165],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1136, 0.0614, 0.3566, 0.1251, 0.1075, 0.1188, 0.1170],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1161, 0.0031, 0.1251, 0.3563, 0.1241, 0.1363, 0.1391],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1480, 0.0012, 0.0760, 0.0841, 0.5348, 0.0879, 0.0681],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0502, 0.0036, 0.3213, 0.0843, 0.0699, 0.4056, 0.0652],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1199, 0.1688, 0.1255, 0.1743, 0.1289, 0.1301, 0.1524],
       grad_fn=<DivBackward0>)
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  61 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  49.949362751996105
maxi score, test score, baseline:  -0.7627266666666668 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.2574, 0.0076, 0.1413, 0.1653, 0.1624, 0.1393, 0.1268],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0057, 0.9631, 0.0072, 0.0049, 0.0031, 0.0030, 0.0131],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1044, 0.0222, 0.3538, 0.1274, 0.1320, 0.1342, 0.1260],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1161, 0.0243, 0.1341, 0.3429, 0.1302, 0.1216, 0.1308],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1494, 0.0204, 0.1189, 0.1225, 0.3776, 0.0989, 0.1123],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0644, 0.0074, 0.1700, 0.0787, 0.0686, 0.5509, 0.0599],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1414, 0.1006, 0.1695, 0.1638, 0.1430, 0.1404, 0.1414],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.119]
 [-0.112]
 [-0.119]
 [-0.119]
 [-0.141]
 [-0.142]
 [-0.119]] [[46.684]
 [41.224]
 [46.684]
 [46.684]
 [57.89 ]
 [55.364]
 [46.684]] [[0.364]
 [0.3  ]
 [0.364]
 [0.364]
 [0.488]
 [0.454]
 [0.364]]
printing an ep nov before normalisation:  40.836124438279136
maxi score, test score, baseline:  -0.7627266666666668 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7627266666666668 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  39 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  49.11515104831248
maxi score, test score, baseline:  -0.7597133333333332 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.20452110868505
printing an ep nov before normalisation:  77.72109841169485
printing an ep nov before normalisation:  65.00656327716976
maxi score, test score, baseline:  -0.7597133333333332 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7597133333333332 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7597133333333332 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.096551869198265
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.827]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]] [[43.983]
 [46.437]
 [43.983]
 [43.983]
 [43.983]
 [43.983]
 [43.983]] [[0.536]
 [0.827]
 [0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.536]]
actions average: 
K:  2  action  0 :  tensor([0.2349, 0.0861, 0.1208, 0.1670, 0.1426, 0.1092, 0.1394],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0106, 0.9207, 0.0067, 0.0157, 0.0052, 0.0039, 0.0371],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1473, 0.0140, 0.2189, 0.1705, 0.1734, 0.1428, 0.1333],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0834, 0.0839, 0.0744, 0.4264, 0.1060, 0.0969, 0.1290],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0873, 0.0985, 0.0544, 0.0853, 0.5580, 0.0625, 0.0540],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1516, 0.0476, 0.1614, 0.1325, 0.1378, 0.2409, 0.1281],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0834, 0.2700, 0.0830, 0.1681, 0.0981, 0.0877, 0.2097],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.7597133333333332 0.651 0.651
maxi score, test score, baseline:  -0.7597133333333332 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.12208624363111653
maxi score, test score, baseline:  -0.7597133333333332 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8298083
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7597133333333332 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7597133333333332 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.51485818307743
maxi score, test score, baseline:  -0.7597133333333332 0.651 0.651
maxi score, test score, baseline:  -0.7597133333333332 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.7597133333333332 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.800769422282116
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.693]
 [0.585]
 [0.62 ]
 [0.606]
 [0.654]
 [0.623]] [[37.83 ]
 [42.425]
 [38.684]
 [39.543]
 [38.598]
 [39.964]
 [39.139]] [[0.593]
 [0.693]
 [0.585]
 [0.62 ]
 [0.606]
 [0.654]
 [0.623]]
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.716]
 [0.585]
 [0.62 ]
 [0.622]
 [0.652]
 [0.611]] [[34.87 ]
 [40.787]
 [35.629]
 [36.076]
 [35.98 ]
 [36.532]
 [35.717]] [[0.593]
 [0.716]
 [0.585]
 [0.62 ]
 [0.622]
 [0.652]
 [0.611]]
printing an ep nov before normalisation:  48.71828096373638
maxi score, test score, baseline:  -0.7597133333333332 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7597133333333332 0.651 0.651
maxi score, test score, baseline:  -0.7597133333333332 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.48443645647285
maxi score, test score, baseline:  -0.7597133333333332 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7597133333333332 0.651 0.651
Printing some Q and Qe and total Qs values:  [[0.916]
 [0.974]
 [0.876]
 [0.892]
 [0.9  ]
 [0.911]
 [0.908]] [[31.226]
 [40.666]
 [30.131]
 [30.149]
 [29.758]
 [29.661]
 [30.144]] [[0.916]
 [0.974]
 [0.876]
 [0.892]
 [0.9  ]
 [0.911]
 [0.908]]
printing an ep nov before normalisation:  32.40597415211395
actions average: 
K:  3  action  0 :  tensor([0.2875, 0.1132, 0.1042, 0.1233, 0.1105, 0.0779, 0.1833],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0066, 0.9339, 0.0102, 0.0143, 0.0039, 0.0038, 0.0274],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1613, 0.0398, 0.1581, 0.1743, 0.1600, 0.1668, 0.1396],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0857, 0.0769, 0.1064, 0.3159, 0.1238, 0.1543, 0.1370],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1179, 0.0393, 0.1015, 0.1250, 0.4005, 0.1105, 0.1054],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1436, 0.0193, 0.1615, 0.1753, 0.1485, 0.2238, 0.1280],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1459, 0.0084, 0.1569, 0.1789, 0.1757, 0.1717, 0.1624],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  55 total reward:  0.34666666666666657  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.406309665582626
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  13.140417039529353
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.30209065947837
printing an ep nov before normalisation:  45.67277908325195
printing an ep nov before normalisation:  23.49875087948686
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.361]
 [0.399]
 [0.399]
 [0.341]
 [0.343]
 [0.343]] [[38.294]
 [34.777]
 [38.294]
 [38.294]
 [33.791]
 [33.512]
 [34.147]] [[2.794]
 [2.361]
 [2.794]
 [2.794]
 [2.23 ]
 [2.201]
 [2.272]]
Printing some Q and Qe and total Qs values:  [[-0.172]
 [-0.139]
 [-0.131]
 [-0.142]
 [-0.139]
 [-0.138]
 [-0.155]] [[54.154]
 [52.135]
 [61.58 ]
 [63.714]
 [52.135]
 [68.281]
 [64.319]] [[0.187]
 [0.189]
 [0.342]
 [0.363]
 [0.189]
 [0.436]
 [0.359]]
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.13955782071084
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.24666666666666592  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.756330916513484
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 18.922582262852725
printing an ep nov before normalisation:  0.0
actor:  1 policy actor:  1  step number:  64 total reward:  0.08666666666666623  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84551287
printing an ep nov before normalisation:  43.030382372456444
printing an ep nov before normalisation:  41.11231274838334
maxi score, test score, baseline:  -0.75702 0.651 0.651
Printing some Q and Qe and total Qs values:  [[-0.112]
 [-0.099]
 [-0.103]
 [-0.11 ]
 [-0.11 ]
 [-0.12 ]
 [-0.119]] [[57.529]
 [52.863]
 [53.835]
 [56.087]
 [57.428]
 [57.834]
 [55.006]] [[0.78 ]
 [0.65 ]
 [0.676]
 [0.738]
 [0.779]
 [0.782]
 [0.696]]
printing an ep nov before normalisation:  37.37499713897705
maxi score, test score, baseline:  -0.75702 0.651 0.651
maxi score, test score, baseline:  -0.75702 0.651 0.651
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.365]
 [0.3  ]
 [0.348]
 [0.381]
 [0.365]
 [0.335]] [[55.053]
 [46.929]
 [56.891]
 [57.051]
 [53.432]
 [54.027]
 [56.622]] [[1.251]
 [1.108]
 [1.285]
 [1.337]
 [1.282]
 [1.28 ]
 [1.314]]
printing an ep nov before normalisation:  47.283814409827784
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84057724
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.56504268490997
siam score:  -0.8412675
printing an ep nov before normalisation:  36.2075052781385
printing an ep nov before normalisation:  61.017619342999055
printing an ep nov before normalisation:  46.140604493760414
actor:  1 policy actor:  1  step number:  45 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  62.091104579625025
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.14518120960939
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.84378105
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.283887590842085
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.108]
 [-0.081]
 [-0.108]
 [-0.108]
 [-0.108]
 [-0.108]
 [-0.108]] [[44.666]
 [50.165]
 [44.666]
 [44.666]
 [44.666]
 [44.666]
 [44.666]] [[0.368]
 [0.509]
 [0.368]
 [0.368]
 [0.368]
 [0.368]
 [0.368]]
printing an ep nov before normalisation:  50.46018590106128
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  0.19999999999999973  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [ 0.261]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]
 [-0.01 ]] [[32.177]
 [44.666]
 [32.177]
 [32.177]
 [32.177]
 [32.177]
 [32.177]] [[0.249]
 [0.717]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]]
printing an ep nov before normalisation:  35.677339112013705
printing an ep nov before normalisation:  34.44486050910889
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84732574
actor:  1 policy actor:  1  step number:  52 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8497217
actor:  1 policy actor:  1  step number:  66 total reward:  0.09999999999999931  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.164]
 [ 0.387]
 [-0.061]
 [ 0.141]
 [ 0.141]
 [-0.068]
 [-0.001]] [[41.887]
 [44.005]
 [41.444]
 [41.176]
 [41.176]
 [40.046]
 [38.347]] [[0.605]
 [0.868]
 [0.371]
 [0.568]
 [0.568]
 [0.337]
 [0.372]]
actions average: 
K:  2  action  0 :  tensor([0.5151, 0.0669, 0.0532, 0.0599, 0.1964, 0.0427, 0.0657],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0226, 0.8241, 0.0181, 0.0449, 0.0188, 0.0182, 0.0532],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1230, 0.0709, 0.1942, 0.1551, 0.1320, 0.1844, 0.1404],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1159, 0.0753, 0.1548, 0.1940, 0.1614, 0.1717, 0.1270],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1310, 0.0379, 0.1375, 0.1443, 0.2906, 0.1472, 0.1114],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0947, 0.0050, 0.1197, 0.1057, 0.1288, 0.4259, 0.1202],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0954, 0.2081, 0.0983, 0.1366, 0.1135, 0.0975, 0.2506],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  57.393031813259256
printing an ep nov before normalisation:  38.589066699989964
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]
 [0.467]] [[52.78]
 [52.78]
 [52.78]
 [52.78]
 [52.78]
 [52.78]
 [52.78]] [[1.048]
 [1.048]
 [1.048]
 [1.048]
 [1.048]
 [1.048]
 [1.048]]
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  50.53503092318398
printing an ep nov before normalisation:  51.2772338267629
printing an ep nov before normalisation:  56.38526750929477
printing an ep nov before normalisation:  35.17041946010107
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.121]
 [-0.114]
 [-0.113]
 [-0.12 ]
 [-0.12 ]
 [-0.12 ]
 [-0.12 ]] [[31.12 ]
 [36.671]
 [33.346]
 [31.286]
 [31.58 ]
 [31.939]
 [31.303]] [[0.489]
 [0.711]
 [0.583]
 [0.497]
 [0.508]
 [0.522]
 [0.497]]
maxi score, test score, baseline:  -0.75702 0.651 0.651
printing an ep nov before normalisation:  28.46492290496826
maxi score, test score, baseline:  -0.75702 0.651 0.651
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.37342557747108
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.860905553962716
siam score:  -0.8496921
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
UNIT TEST: sample policy line 217 mcts : [0.041 0.49  0.041 0.041 0.02  0.306 0.061]
maxi score, test score, baseline:  -0.75702 0.651 0.651
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8480606
printing an ep nov before normalisation:  49.8025554453909
maxi score, test score, baseline:  -0.75702 0.651 0.651
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.75702 0.651 0.651
printing an ep nov before normalisation:  22.094266187932494
printing an ep nov before normalisation:  46.894452936101644
siam score:  -0.8491913
printing an ep nov before normalisation:  45.93158646630847
printing an ep nov before normalisation:  37.111822346813206
maxi score, test score, baseline:  -0.75702 0.651 0.651
printing an ep nov before normalisation:  52.53278613221873
maxi score, test score, baseline:  -0.75702 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  38.26946614447362
actor:  1 policy actor:  1  step number:  52 total reward:  0.3399999999999994  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  74.32873077342745
actor:  1 policy actor:  1  step number:  47 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.75702 0.651 0.651
printing an ep nov before normalisation:  47.73698265347362
printing an ep nov before normalisation:  50.950897270058384
printing an ep nov before normalisation:  51.044627287208236
printing an ep nov before normalisation:  51.09616831327746
printing an ep nov before normalisation:  40.337037934478566
maxi score, test score, baseline:  -0.7570200000000001 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7570200000000001 0.651 0.651
actor:  0 policy actor:  0  step number:  54 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  55.75239642879179
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.82555103302002
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.037126541137695
using explorer policy with actor:  1
printing an ep nov before normalisation:  2.328675824416706
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  48.17778478972048
printing an ep nov before normalisation:  35.683431531629815
printing an ep nov before normalisation:  40.955238342285156
actor:  1 policy actor:  1  step number:  67 total reward:  0.17333333333333256  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.144]
 [0.281]
 [0.12 ]
 [0.241]
 [0.187]
 [0.008]
 [0.205]] [[45.382]
 [45.012]
 [35.512]
 [37.808]
 [35.756]
 [31.829]
 [46.067]] [[0.144]
 [0.281]
 [0.12 ]
 [0.241]
 [0.187]
 [0.008]
 [0.205]]
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.747]
 [0.624]
 [0.551]
 [0.582]
 [0.459]
 [0.441]] [[49.707]
 [46.819]
 [51.245]
 [51.581]
 [49.431]
 [54.16 ]
 [53.403]] [[2.22 ]
 [2.185]
 [2.313]
 [2.26 ]
 [2.169]
 [2.314]
 [2.254]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  62 total reward:  0.13999999999999901  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  59 total reward:  0.18666666666666598  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.75466 0.651 0.651
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.14666666666666606  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  42.10841729406084
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.003825438175692
printing an ep nov before normalisation:  29.913625670565015
printing an ep nov before normalisation:  43.42199880573348
Printing some Q and Qe and total Qs values:  [[-0.136]
 [-0.136]
 [-0.136]
 [-0.136]
 [-0.136]
 [-0.136]
 [-0.136]] [[55.418]
 [55.418]
 [55.418]
 [55.418]
 [55.418]
 [55.418]
 [55.418]] [[36.828]
 [36.828]
 [36.828]
 [36.828]
 [36.828]
 [36.828]
 [36.828]]
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.334]
 [0.355]
 [0.357]
 [0.358]
 [0.335]
 [0.347]] [[23.658]
 [44.835]
 [19.04 ]
 [18.846]
 [18.613]
 [24.439]
 [18.624]] [[0.331]
 [0.334]
 [0.355]
 [0.357]
 [0.358]
 [0.335]
 [0.347]]
printing an ep nov before normalisation:  16.686276249468758
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.95495521606536
actor:  1 policy actor:  1  step number:  58 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.148]
 [-0.145]
 [-0.148]
 [-0.142]
 [-0.142]
 [-0.147]
 [-0.148]] [[64.216]
 [67.035]
 [65.126]
 [66.204]
 [66.104]
 [65.493]
 [64.201]] [[1.049]
 [1.155]
 [1.082]
 [1.128]
 [1.124]
 [1.097]
 [1.049]]
printing an ep nov before normalisation:  59.7338336693396
printing an ep nov before normalisation:  38.671972753754844
printing an ep nov before normalisation:  61.05666413578348
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.083390593241454
actor:  1 policy actor:  1  step number:  50 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.148]
 [-0.084]
 [-0.152]
 [-0.152]
 [-0.107]
 [-0.108]
 [-0.154]] [[31.417]
 [33.686]
 [32.015]
 [31.908]
 [35.699]
 [35.718]
 [31.939]] [[1.023]
 [1.178]
 [1.042]
 [1.038]
 [1.235]
 [1.234]
 [1.038]]
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  71 total reward:  0.05333333333333279  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.117]
 [-0.117]
 [-0.117]
 [-0.117]
 [-0.117]
 [-0.117]
 [-0.117]] [[54.941]
 [54.941]
 [54.941]
 [54.941]
 [54.941]
 [54.941]
 [54.941]] [[0.883]
 [0.883]
 [0.883]
 [0.883]
 [0.883]
 [0.883]
 [0.883]]
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.4446, 0.0316, 0.0803, 0.0885, 0.1704, 0.0999, 0.0847],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0038, 0.9544, 0.0042, 0.0075, 0.0013, 0.0018, 0.0271],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1160, 0.0017, 0.3619, 0.1251, 0.1428, 0.1582, 0.0943],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1568, 0.0034, 0.1443, 0.1763, 0.1926, 0.1936, 0.1331],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1172, 0.0599, 0.0960, 0.1045, 0.4298, 0.1053, 0.0873],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1260, 0.0025, 0.1414, 0.1244, 0.1438, 0.3602, 0.1017],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1479, 0.1045, 0.1219, 0.1273, 0.1629, 0.1196, 0.2159],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.80487076276178
printing an ep nov before normalisation:  52.12812843606627
printing an ep nov before normalisation:  43.993004042237565
printing an ep nov before normalisation:  41.152571326967816
printing an ep nov before normalisation:  45.531369273204746
printing an ep nov before normalisation:  41.13680124998953
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  55.62799886109236
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.12816100533225
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.164]
 [-0.164]
 [-0.164]
 [-0.164]
 [-0.164]
 [-0.164]
 [-0.164]] [[46.078]
 [46.078]
 [46.078]
 [46.078]
 [46.078]
 [46.078]
 [46.078]] [[0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]]
printing an ep nov before normalisation:  40.61992747610304
Printing some Q and Qe and total Qs values:  [[-0.161]
 [-0.169]
 [-0.164]
 [-0.161]
 [-0.161]
 [-0.161]
 [-0.161]] [[56.881]
 [71.634]
 [63.013]
 [56.881]
 [56.881]
 [56.881]
 [56.881]] [[0.229]
 [0.424]
 [0.31 ]
 [0.229]
 [0.229]
 [0.229]
 [0.229]]
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.47 ]
 [0.34 ]
 [0.351]
 [0.352]
 [0.358]
 [0.311]] [[30.336]
 [32.21 ]
 [29.429]
 [29.331]
 [29.284]
 [28.923]
 [29.04 ]] [[1.911]
 [2.109]
 [1.838]
 [1.843]
 [1.842]
 [1.829]
 [1.788]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.02767300003239
line 256 mcts: sample exp_bonus 52.967075730025726
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.854825817332305
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.642484188079834
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.180450439453125
printing an ep nov before normalisation:  32.072181701660156
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.937395572662354
Printing some Q and Qe and total Qs values:  [[0.78 ]
 [0.746]
 [0.78 ]
 [0.78 ]
 [0.781]
 [0.781]
 [0.781]] [[25.245]
 [25.135]
 [24.066]
 [24.293]
 [24.614]
 [24.356]
 [23.628]] [[2.774]
 [2.73 ]
 [2.68 ]
 [2.698]
 [2.724]
 [2.703]
 [2.647]]
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.44646805463044
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.144]
 [-0.145]
 [-0.137]
 [-0.134]
 [-0.144]
 [-0.136]
 [-0.16 ]] [[48.602]
 [42.44 ]
 [47.711]
 [48.878]
 [48.602]
 [49.287]
 [48.237]] [[0.953]
 [0.682]
 [0.921]
 [0.976]
 [0.953]
 [0.991]
 [0.921]]
siam score:  -0.843414
UNIT TEST: sample policy line 217 mcts : [0.041 0.49  0.061 0.041 0.143 0.041 0.184]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.680990408233335
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.9669006633496
Printing some Q and Qe and total Qs values:  [[-0.184]
 [-0.137]
 [-0.127]
 [-0.151]
 [-0.161]
 [-0.24 ]
 [-0.166]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.184]
 [-0.137]
 [-0.127]
 [-0.151]
 [-0.161]
 [-0.24 ]
 [-0.166]]
line 256 mcts: sample exp_bonus 58.517145928333214
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
line 256 mcts: sample exp_bonus 34.40771386039328
Printing some Q and Qe and total Qs values:  [[-0.113]
 [-0.104]
 [-0.128]
 [-0.13 ]
 [-0.13 ]
 [-0.127]
 [-0.113]] [[51.   ]
 [43.207]
 [52.328]
 [52.882]
 [51.967]
 [52.484]
 [51.   ]] [[1.224]
 [1.026]
 [1.243]
 [1.256]
 [1.232]
 [1.249]
 [1.224]]
printing an ep nov before normalisation:  45.855124423209006
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.15 ]
 [-0.145]
 [-0.15 ]
 [-0.15 ]
 [-0.15 ]
 [-0.15 ]
 [-0.15 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.15 ]
 [-0.145]
 [-0.15 ]
 [-0.15 ]
 [-0.15 ]
 [-0.15 ]
 [-0.15 ]]
Printing some Q and Qe and total Qs values:  [[-0.132]
 [-0.134]
 [-0.132]
 [-0.132]
 [-0.132]
 [-0.132]
 [-0.132]] [[48.211]
 [37.586]
 [48.211]
 [48.211]
 [49.099]
 [48.211]
 [48.211]] [[0.823]
 [0.412]
 [0.823]
 [0.823]
 [0.857]
 [0.823]
 [0.823]]
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.75466 0.651 0.651
printing an ep nov before normalisation:  45.468926429748535
printing an ep nov before normalisation:  63.20224185790913
siam score:  -0.83376527
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.415419967042716
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.043]
 [-0.045]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]] [[45.577]
 [56.442]
 [45.577]
 [45.577]
 [45.577]
 [45.577]
 [45.577]] [[0.673]
 [0.916]
 [0.673]
 [0.673]
 [0.673]
 [0.673]
 [0.673]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.42704438871474
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.743]
 [0.825]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]] [[36.021]
 [43.41 ]
 [36.021]
 [36.021]
 [36.021]
 [36.021]
 [36.021]] [[0.743]
 [0.825]
 [0.743]
 [0.743]
 [0.743]
 [0.743]
 [0.743]]
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.257]
 [0.309]
 [0.257]
 [0.257]
 [0.257]
 [0.257]
 [0.257]] [[31.443]
 [33.909]
 [31.443]
 [31.443]
 [31.443]
 [31.443]
 [31.443]] [[0.653]
 [0.771]
 [0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  37.98791899633025
siam score:  -0.8332459
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3599999999999999  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.141]
 [-0.18 ]
 [-0.187]
 [-0.162]
 [-0.161]
 [-0.22 ]
 [-0.167]] [[52.85 ]
 [62.936]
 [50.001]
 [61.328]
 [59.922]
 [59.856]
 [61.214]] [[1.172]
 [1.488]
 [1.025]
 [1.45 ]
 [1.401]
 [1.34 ]
 [1.44 ]]
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.088]
 [0.052]
 [0.049]
 [0.041]
 [0.052]
 [0.052]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.041]
 [0.088]
 [0.052]
 [0.049]
 [0.041]
 [0.052]
 [0.052]]
printing an ep nov before normalisation:  58.40821387306487
printing an ep nov before normalisation:  45.66772937774658
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.18819754630027
printing an ep nov before normalisation:  46.263437271118164
printing an ep nov before normalisation:  82.69343564583687
printing an ep nov before normalisation:  39.93280993971253
printing an ep nov before normalisation:  42.20238372506261
Printing some Q and Qe and total Qs values:  [[-0.005]
 [ 0.07 ]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]] [[26.958]
 [32.841]
 [26.958]
 [26.958]
 [26.958]
 [26.958]
 [26.958]] [[0.351]
 [0.578]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]]
actions average: 
K:  0  action  0 :  tensor([0.2829, 0.0038, 0.1445, 0.1443, 0.1948, 0.1070, 0.1226],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0051, 0.9514, 0.0047, 0.0079, 0.0029, 0.0024, 0.0256],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1134, 0.0141, 0.3299, 0.1521, 0.1150, 0.1179, 0.1576],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1178, 0.0134, 0.1183, 0.3747, 0.1540, 0.1140, 0.1077],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0754, 0.0046, 0.0714, 0.1082, 0.6071, 0.0800, 0.0532],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1330, 0.0067, 0.1928, 0.1626, 0.1345, 0.2437, 0.1266],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0881, 0.1989, 0.1035, 0.1214, 0.0845, 0.0740, 0.3296],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.776580810546875
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  69 total reward:  0.1066666666666658  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.158]
 [-0.113]
 [-0.138]
 [-0.122]
 [-0.13 ]
 [-0.133]
 [-0.13 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.158]
 [-0.113]
 [-0.138]
 [-0.122]
 [-0.13 ]
 [-0.133]
 [-0.13 ]]
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.83555209111499
printing an ep nov before normalisation:  33.63029596781117
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.83986986
Printing some Q and Qe and total Qs values:  [[-0.18 ]
 [-0.18 ]
 [-0.18 ]
 [-0.18 ]
 [-0.18 ]
 [-0.156]
 [-0.18 ]] [[70.019]
 [70.019]
 [70.019]
 [70.019]
 [70.019]
 [73.005]
 [70.019]] [[0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.791]
 [0.699]]
actor:  1 policy actor:  1  step number:  57 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.10859814168486
maxi score, test score, baseline:  -0.75466 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  56 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  38.54658662823046
printing an ep nov before normalisation:  45.77122939487311
line 256 mcts: sample exp_bonus 53.548228735370174
line 256 mcts: sample exp_bonus 49.457146230898175
maxi score, test score, baseline:  -0.7524066666666667 0.651 0.651
maxi score, test score, baseline:  -0.7524066666666667 0.651 0.651
actor:  1 policy actor:  1  step number:  45 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  1.0
siam score:  -0.83651656
printing an ep nov before normalisation:  44.901323760145644
printing an ep nov before normalisation:  55.91349612235215
printing an ep nov before normalisation:  54.13337958637215
maxi score, test score, baseline:  -0.7524066666666667 0.651 0.651
printing an ep nov before normalisation:  38.2229469573611
maxi score, test score, baseline:  -0.7524066666666667 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.133673409669726
maxi score, test score, baseline:  -0.7524066666666667 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.2279888134140151
maxi score, test score, baseline:  -0.7524066666666667 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  63 total reward:  0.1466666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.7501133333333334 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.7501133333333334 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.66220441414934
siam score:  -0.83226067
actor:  1 policy actor:  1  step number:  79 total reward:  0.10666666666666558  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.43579639866801
maxi score, test score, baseline:  -0.7501133333333334 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.83258957
siam score:  -0.8338457
line 256 mcts: sample exp_bonus 35.3898476522367
maxi score, test score, baseline:  -0.7501133333333334 0.651 0.651
printing an ep nov before normalisation:  40.0653427428835
UNIT TEST: sample policy line 217 mcts : [0.02  0.265 0.041 0.082 0.388 0.163 0.041]
maxi score, test score, baseline:  -0.7501133333333334 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.461]
 [0.442]
 [0.444]
 [0.459]
 [0.446]
 [0.46 ]
 [0.405]] [[22.299]
 [27.771]
 [25.4  ]
 [21.851]
 [25.296]
 [23.258]
 [25.287]] [[1.645]
 [1.916]
 [1.792]
 [1.619]
 [1.789]
 [1.695]
 [1.748]]
printing an ep nov before normalisation:  0.08190052905305834
actor:  1 policy actor:  1  step number:  55 total reward:  0.3066666666666661  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.7501133333333334 0.651 0.651
Printing some Q and Qe and total Qs values:  [[-0.154]
 [-0.154]
 [-0.146]
 [-0.175]
 [-0.154]
 [-0.153]
 [-0.154]] [[41.293]
 [41.293]
 [46.252]
 [45.692]
 [41.293]
 [43.994]
 [41.293]] [[0.843]
 [0.843]
 [1.118]
 [1.058]
 [0.843]
 [0.989]
 [0.843]]
maxi score, test score, baseline:  -0.7501133333333334 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.42678735692208
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.33028256859711
maxi score, test score, baseline:  -0.7501133333333334 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.27333333333333254  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  53 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.7501133333333334 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.725951469665745
actor:  1 policy actor:  1  step number:  54 total reward:  0.5133333333333336  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  61 total reward:  0.14666666666666606  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.7501133333333334 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.608360337202136
printing an ep nov before normalisation:  44.62934567666167
Starting evaluation
actor:  1 policy actor:  1  step number:  48 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.696]
 [0.471]
 [0.463]
 [0.471]
 [0.471]
 [0.471]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.471]
 [0.696]
 [0.471]
 [0.463]
 [0.471]
 [0.471]
 [0.471]]
maxi score, test score, baseline:  -0.7501133333333334 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.94697492058732
Printing some Q and Qe and total Qs values:  [[0.624]
 [0.843]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]] [[38.023]
 [33.247]
 [38.023]
 [38.023]
 [38.023]
 [38.023]
 [38.023]] [[0.624]
 [0.843]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]]
printing an ep nov before normalisation:  47.99118952867917
printing an ep nov before normalisation:  44.2113733291626
printing an ep nov before normalisation:  51.450117007336424
Printing some Q and Qe and total Qs values:  [[0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]] [[29.999]
 [29.999]
 [29.999]
 [29.999]
 [29.999]
 [29.999]
 [29.999]] [[0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]]
printing an ep nov before normalisation:  58.059937675786486
maxi score, test score, baseline:  -0.7501133333333334 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.59600444450808
printing an ep nov before normalisation:  47.188364885614796
maxi score, test score, baseline:  -0.7501133333333334 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.671461867053466
printing an ep nov before normalisation:  44.03003692626953
line 256 mcts: sample exp_bonus 54.79025670250501
maxi score, test score, baseline:  -0.7501133333333334 0.651 0.651
actor:  1 policy actor:  1  step number:  44 total reward:  0.5400000000000003  reward:  1.0 rdn_beta:  2.0
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
siam score:  -0.83765525
actor:  0 policy actor:  0  step number:  29 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  30 total reward:  0.6600000000000001  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.6861933333333333 0.651 0.651
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  35 total reward:  0.6266666666666667  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.68294 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  60 total reward:  0.0999999999999992  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 54.52163436921173
maxi score, test score, baseline:  -0.68074 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]
 [-0.047]] [[31.915]
 [31.915]
 [31.915]
 [31.915]
 [31.915]
 [31.915]
 [37.004]] [[0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [0.951]
 [1.253]]
printing an ep nov before normalisation:  48.252787684965895
printing an ep nov before normalisation:  41.785824052411606
printing an ep nov before normalisation:  55.2016019821167
maxi score, test score, baseline:  -0.68074 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  0.09999999999999953  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  51.87751452128092
printing an ep nov before normalisation:  44.305465171237614
maxi score, test score, baseline:  -0.68074 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.158]
 [-0.123]
 [-0.197]
 [-0.14 ]
 [-0.148]
 [-0.252]
 [-0.131]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.158]
 [-0.123]
 [-0.197]
 [-0.14 ]
 [-0.148]
 [-0.252]
 [-0.131]]
maxi score, test score, baseline:  -0.68074 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.90627670288086
printing an ep nov before normalisation:  40.86701693788407
printing an ep nov before normalisation:  29.4030256898883
printing an ep nov before normalisation:  52.63720587804896
printing an ep nov before normalisation:  23.543193340301514
maxi score, test score, baseline:  -0.68074 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  58 total reward:  0.13999999999999946  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6784600000000001 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.6784600000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.886278754678074
actions average: 
K:  0  action  0 :  tensor([0.1503, 0.0025, 0.1541, 0.1617, 0.2418, 0.1442, 0.1454],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0053, 0.9672, 0.0025, 0.0052, 0.0018, 0.0015, 0.0166],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1214, 0.0218, 0.3135, 0.1720, 0.1215, 0.1124, 0.1372],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1434, 0.0116, 0.1258, 0.2600, 0.1412, 0.1473, 0.1707],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1295, 0.0130, 0.1129, 0.1328, 0.3764, 0.1057, 0.1297],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0730, 0.0087, 0.1596, 0.1438, 0.1030, 0.4135, 0.0983],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1145, 0.0335, 0.1504, 0.1399, 0.1078, 0.1318, 0.3222],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.132]
 [-0.132]
 [-0.132]
 [-0.132]
 [-0.132]
 [-0.132]
 [-0.132]] [[46.867]
 [46.867]
 [46.867]
 [46.867]
 [46.867]
 [46.867]
 [46.867]] [[1.005]
 [1.005]
 [1.005]
 [1.005]
 [1.005]
 [1.005]
 [1.005]]
line 256 mcts: sample exp_bonus 44.87580630469472
maxi score, test score, baseline:  -0.6784600000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.643]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]
 [0.54 ]] [[46.019]
 [41.579]
 [46.019]
 [46.019]
 [46.019]
 [46.019]
 [46.019]] [[0.798]
 [0.853]
 [0.798]
 [0.798]
 [0.798]
 [0.798]
 [0.798]]
using explorer policy with actor:  1
Sims:  50 1 epoch:  80514 pick best:  False frame count:  80514
printing an ep nov before normalisation:  47.00324460170811
maxi score, test score, baseline:  -0.6784600000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6784600000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6784600000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  55 total reward:  0.09333333333333282  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.189750058361305
Printing some Q and Qe and total Qs values:  [[-0.095]
 [-0.095]
 [-0.095]
 [-0.095]
 [-0.095]
 [-0.095]
 [-0.095]] [[55.464]
 [55.464]
 [55.464]
 [55.464]
 [55.464]
 [55.464]
 [55.464]] [[1.238]
 [1.238]
 [1.238]
 [1.238]
 [1.238]
 [1.238]
 [1.238]]
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.47560524254146
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.096]
 [-0.105]
 [-0.127]
 [-0.105]
 [-0.105]
 [-0.105]
 [-0.105]] [[35.36 ]
 [31.361]
 [39.495]
 [31.361]
 [31.361]
 [31.361]
 [31.361]] [[0.892]
 [0.686]
 [1.064]
 [0.686]
 [0.686]
 [0.686]
 [0.686]]
printing an ep nov before normalisation:  46.36459484337315
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.378508193233365
actor:  1 policy actor:  1  step number:  54 total reward:  0.2999999999999994  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.2790, 0.0324, 0.1116, 0.1355, 0.1653, 0.1329, 0.1433],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0171, 0.8451, 0.0235, 0.0419, 0.0173, 0.0222, 0.0328],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0821, 0.0118, 0.4186, 0.1046, 0.0941, 0.1980, 0.0908],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1576, 0.0123, 0.1710, 0.1885, 0.1553, 0.1818, 0.1334],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1390, 0.0131, 0.0884, 0.1488, 0.3865, 0.0977, 0.1266],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0884, 0.0149, 0.2329, 0.1219, 0.0869, 0.3826, 0.0723],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1328, 0.1213, 0.1291, 0.1435, 0.1062, 0.1562, 0.2109],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  57.02885493142086
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.522]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]] [[25.606]
 [32.554]
 [25.606]
 [25.606]
 [25.606]
 [25.606]
 [25.606]] [[0.939]
 [1.074]
 [0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]] [[31.848]
 [31.848]
 [31.848]
 [31.848]
 [31.848]
 [31.848]
 [31.848]] [[1.265]
 [1.265]
 [1.265]
 [1.265]
 [1.265]
 [1.265]
 [1.265]]
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.61976105826242
printing an ep nov before normalisation:  53.33991949129204
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 50.19111550204414
Printing some Q and Qe and total Qs values:  [[0.147]
 [0.378]
 [0.147]
 [0.147]
 [0.147]
 [0.147]
 [0.147]] [[35.062]
 [51.026]
 [35.062]
 [35.062]
 [35.062]
 [35.062]
 [35.062]] [[0.542]
 [1.124]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.23951638768006
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
actor:  1 policy actor:  1  step number:  54 total reward:  0.4199999999999995  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  49.418055597532785
siam score:  -0.8464269
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]] [[40.176]
 [40.176]
 [40.176]
 [40.176]
 [40.176]
 [40.176]
 [40.176]] [[1.305]
 [1.305]
 [1.305]
 [1.305]
 [1.305]
 [1.305]
 [1.305]]
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.694382667541504
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  66 total reward:  0.09999999999999931  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
printing an ep nov before normalisation:  0.09582385234352842
printing an ep nov before normalisation:  41.18775704659326
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.58 ]
 [0.577]] [[21.947]
 [21.947]
 [21.947]
 [21.947]
 [21.947]
 [24.724]
 [21.947]] [[2.022]
 [2.022]
 [2.022]
 [2.022]
 [2.022]
 [2.388]
 [2.022]]
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.8926850636561
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.3666666666666658  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.634]
 [0.484]
 [0.449]
 [0.554]
 [0.449]
 [0.449]] [[37.285]
 [34.433]
 [38.456]
 [37.285]
 [36.339]
 [37.285]
 [37.285]] [[1.898]
 [1.866]
 [2.022]
 [1.898]
 [1.931]
 [1.898]
 [1.898]]
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.71802066045203
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6762733333333334 0.6793333333333333 0.6793333333333333
actor:  1 policy actor:  1  step number:  52 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  54 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.49630109827261
maxi score, test score, baseline:  -0.67362 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.67362 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.67362 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.94929617921656
printing an ep nov before normalisation:  40.79423569791932
maxi score, test score, baseline:  -0.67362 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  53.19745341262638
printing an ep nov before normalisation:  51.59917053858932
siam score:  -0.83694094
deleting a thread, now have 3 threads
Frames:  81580 train batches done:  9554 episodes:  2446
maxi score, test score, baseline:  -0.67362 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.67362 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3533333333333333  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.67362 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.67362 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.67885774430478
printing an ep nov before normalisation:  71.36707866724618
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 56.66595204701625
Printing some Q and Qe and total Qs values:  [[0.794]
 [0.84 ]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]] [[49.668]
 [51.322]
 [49.668]
 [49.668]
 [49.668]
 [49.668]
 [49.668]] [[0.794]
 [0.84 ]
 [0.794]
 [0.794]
 [0.794]
 [0.794]
 [0.794]]
printing an ep nov before normalisation:  65.3891480831778
printing an ep nov before normalisation:  31.017915921886296
printing an ep nov before normalisation:  39.65725555166245
printing an ep nov before normalisation:  32.61894702911377
maxi score, test score, baseline:  -0.67362 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  56 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6709933333333333 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  38.977227392170725
actions average: 
K:  2  action  0 :  tensor([0.1982, 0.0535, 0.1325, 0.1439, 0.1636, 0.1569, 0.1515],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0188, 0.9090, 0.0179, 0.0144, 0.0083, 0.0091, 0.0226],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1421, 0.0096, 0.2385, 0.1370, 0.1528, 0.1872, 0.1327],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1451, 0.0146, 0.0935, 0.3966, 0.1238, 0.1036, 0.1229],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1540, 0.0260, 0.0922, 0.1298, 0.3812, 0.1114, 0.1054],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0921, 0.0302, 0.2123, 0.1191, 0.0819, 0.3736, 0.0909],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1212, 0.1672, 0.1043, 0.1853, 0.1105, 0.0966, 0.2149],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.72181987762451
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6709933333333333 0.6793333333333333 0.6793333333333333
actor:  0 policy actor:  0  step number:  93 total reward:  0.03999999999999937  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  31.249868535005287
Printing some Q and Qe and total Qs values:  [[-0.109]
 [-0.093]
 [-0.109]
 [-0.1  ]
 [-0.109]
 [-0.11 ]
 [-0.11 ]] [[39.208]
 [47.242]
 [38.958]
 [45.651]
 [38.817]
 [38.636]
 [38.851]] [[0.773]
 [1.14 ]
 [0.762]
 [1.064]
 [0.756]
 [0.748]
 [0.757]]
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  71 total reward:  0.0533333333333319  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.2338, 0.0273, 0.1456, 0.1502, 0.1339, 0.1674, 0.1418],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0130, 0.9259, 0.0107, 0.0102, 0.0049, 0.0076, 0.0278],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1225, 0.0085, 0.3288, 0.1325, 0.1376, 0.1609, 0.1091],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1141, 0.0258, 0.1209, 0.3235, 0.1061, 0.1905, 0.1192],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1156, 0.0203, 0.1121, 0.1587, 0.3355, 0.1492, 0.1086],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1180, 0.0331, 0.1608, 0.1535, 0.1289, 0.3027, 0.1031],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0851, 0.2109, 0.0910, 0.2179, 0.0717, 0.1205, 0.2029],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  52 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]] [[66.319]
 [66.319]
 [66.319]
 [66.319]
 [66.319]
 [66.319]
 [66.319]] [[2.573]
 [2.573]
 [2.573]
 [2.573]
 [2.573]
 [2.573]
 [2.573]]
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.21159517314637
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
actor:  1 policy actor:  1  step number:  42 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
printing an ep nov before normalisation:  47.22640042357836
actor:  1 policy actor:  1  step number:  57 total reward:  0.2533333333333331  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.142]
 [-0.141]
 [-0.14 ]
 [-0.141]
 [-0.14 ]
 [-0.144]
 [-0.143]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.142]
 [-0.141]
 [-0.14 ]
 [-0.141]
 [-0.14 ]
 [-0.144]
 [-0.143]]
printing an ep nov before normalisation:  55.55233351293264
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
Printing some Q and Qe and total Qs values:  [[-0.114]
 [-0.066]
 [-0.114]
 [-0.114]
 [-0.114]
 [-0.114]
 [-0.114]] [[37.526]
 [46.112]
 [37.526]
 [37.526]
 [37.526]
 [37.526]
 [37.526]] [[0.32 ]
 [0.562]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]
 [0.32 ]]
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
Printing some Q and Qe and total Qs values:  [[0.144]
 [0.158]
 [0.146]
 [0.145]
 [0.146]
 [0.145]
 [0.15 ]] [[38.403]
 [38.55 ]
 [39.118]
 [39.227]
 [39.237]
 [37.941]
 [39.192]] [[0.144]
 [0.158]
 [0.146]
 [0.145]
 [0.146]
 [0.145]
 [0.15 ]]
printing an ep nov before normalisation:  51.83582401017801
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.532372956517875
siam score:  -0.83522797
printing an ep nov before normalisation:  36.40125823227099
actor:  1 policy actor:  1  step number:  60 total reward:  0.09999999999999953  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  55.814368115302464
printing an ep nov before normalisation:  39.13806915283203
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.068911478750714
actor:  1 policy actor:  1  step number:  42 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  59.288321114003416
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.26821606205866
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.032]
 [ 0.34 ]
 [-0.07 ]
 [-0.028]
 [-0.038]
 [-0.108]
 [-0.031]] [[39.041]
 [47.355]
 [37.89 ]
 [38.841]
 [43.359]
 [44.649]
 [38.05 ]] [[0.299]
 [0.797]
 [0.244]
 [0.301]
 [0.359]
 [0.309]
 [0.286]]
printing an ep nov before normalisation:  58.419970127487
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]] [[44.707]
 [44.707]
 [44.707]
 [44.707]
 [44.707]
 [44.707]
 [44.707]] [[0.842]
 [0.842]
 [0.842]
 [0.842]
 [0.842]
 [0.842]
 [0.842]]
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.102]
 [-0.097]
 [-0.106]
 [-0.107]
 [-0.105]
 [-0.107]
 [-0.107]] [[21.901]
 [20.916]
 [21.788]
 [21.613]
 [21.64 ]
 [21.613]
 [21.613]] [[0.681]
 [0.651]
 [0.673]
 [0.666]
 [0.668]
 [0.666]
 [0.666]]
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8352139
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.90171957015991
actions average: 
K:  1  action  0 :  tensor([0.1621, 0.0286, 0.1829, 0.1449, 0.1906, 0.1678, 0.1230],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0085, 0.9494, 0.0075, 0.0089, 0.0053, 0.0030, 0.0173],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0901, 0.1193, 0.4061, 0.0829, 0.0767, 0.1192, 0.1056],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0994, 0.0323, 0.1493, 0.2721, 0.1350, 0.1895, 0.1224],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1241, 0.0159, 0.1246, 0.1084, 0.4140, 0.1177, 0.0953],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0710, 0.0197, 0.1422, 0.1250, 0.1021, 0.4285, 0.1114],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0878, 0.2696, 0.1337, 0.1171, 0.1034, 0.1184, 0.1699],
       grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  39.10704372102213
printing an ep nov before normalisation:  40.39355360453215
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
printing an ep nov before normalisation:  37.070248299533645
actor:  1 policy actor:  1  step number:  46 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  51 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.148]
 [-0.127]
 [-0.187]
 [-0.136]
 [-0.144]
 [-0.135]
 [-0.128]] [[38.487]
 [44.055]
 [56.647]
 [52.702]
 [40.668]
 [39.59 ]
 [45.123]] [[0.759]
 [0.928]
 [1.205]
 [1.15 ]
 [0.821]
 [0.801]
 [0.955]]
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  48.85525578498753
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.517680644989014
printing an ep nov before normalisation:  47.176252606165804
Printing some Q and Qe and total Qs values:  [[0.892]
 [0.918]
 [0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]] [[29.042]
 [42.785]
 [29.042]
 [29.042]
 [29.042]
 [29.042]
 [29.042]] [[0.892]
 [0.918]
 [0.892]
 [0.892]
 [0.892]
 [0.892]
 [0.892]]
Printing some Q and Qe and total Qs values:  [[-0.147]
 [-0.077]
 [-0.147]
 [-0.147]
 [-0.147]
 [-0.147]
 [-0.147]] [[42.907]
 [48.351]
 [42.907]
 [42.907]
 [42.907]
 [42.907]
 [42.907]] [[1.126]
 [1.475]
 [1.126]
 [1.126]
 [1.126]
 [1.126]
 [1.126]]
printing an ep nov before normalisation:  40.88974823707725
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  36.14627095946107
maxi score, test score, baseline:  -0.6689133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.8051075587735
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  49 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  49 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  1.333
siam score:  -0.83324534
printing an ep nov before normalisation:  58.31955038098348
Printing some Q and Qe and total Qs values:  [[-0.008]
 [-0.065]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.008]
 [-0.008]] [[16.545]
 [26.975]
 [16.656]
 [16.728]
 [16.715]
 [16.527]
 [16.545]] [[0.711]
 [1.109]
 [0.714]
 [0.718]
 [0.717]
 [0.71 ]
 [0.711]]
maxi score, test score, baseline:  -0.6637933333333335 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.084216041978834
maxi score, test score, baseline:  -0.6637933333333335 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  0.333
UNIT TEST: sample policy line 217 mcts : [0.    0.388 0.061 0.51  0.02  0.02  0.   ]
printing an ep nov before normalisation:  44.38634927214231
printing an ep nov before normalisation:  57.1101074437479
maxi score, test score, baseline:  -0.6637933333333335 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6637933333333335 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.6637933333333335 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6637933333333335 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6637933333333335 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.28440809790182
printing an ep nov before normalisation:  0.00046119099465613544
actor:  1 policy actor:  1  step number:  59 total reward:  0.26666666666666594  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  44.807442506661076
printing an ep nov before normalisation:  52.139543489248275
maxi score, test score, baseline:  -0.6637933333333335 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6637933333333335 0.6793333333333333 0.6793333333333333
printing an ep nov before normalisation:  39.432404041212266
maxi score, test score, baseline:  -0.6637933333333335 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  0.333
deleting a thread, now have 2 threads
Frames:  83359 train batches done:  9768 episodes:  2496
printing an ep nov before normalisation:  31.78813328614441
printing an ep nov before normalisation:  34.26523685455322
maxi score, test score, baseline:  -0.6637933333333335 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([0.1710, 0.0967, 0.1651, 0.1658, 0.1465, 0.1225, 0.1323],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0423, 0.8675, 0.0126, 0.0108, 0.0100, 0.0068, 0.0498],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1199, 0.0137, 0.3476, 0.1385, 0.1263, 0.1258, 0.1283],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1370, 0.0228, 0.1135, 0.3258, 0.1717, 0.0917, 0.1374],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1339, 0.0185, 0.1143, 0.1432, 0.3858, 0.0934, 0.1110],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1196, 0.0117, 0.1743, 0.1379, 0.1439, 0.3060, 0.1066],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1241, 0.1220, 0.1236, 0.1503, 0.1217, 0.1130, 0.2452],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  36.071522077845025
printing an ep nov before normalisation:  22.996842861175537
actor:  1 policy actor:  1  step number:  60 total reward:  0.27333333333333265  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6637933333333335 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.35192222129181
printing an ep nov before normalisation:  36.684974075521524
maxi score, test score, baseline:  -0.6637933333333335 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.096540747927484
maxi score, test score, baseline:  -0.6637933333333335 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.999428272247314
printing an ep nov before normalisation:  49.52167934488703
Printing some Q and Qe and total Qs values:  [[0.36]
 [0.36]
 [0.36]
 [0.36]
 [0.36]
 [0.36]
 [0.36]] [[34.924]
 [34.924]
 [34.924]
 [34.924]
 [34.924]
 [34.924]
 [34.924]] [[46.914]
 [46.914]
 [46.914]
 [46.914]
 [46.914]
 [46.914]
 [46.914]]
maxi score, test score, baseline:  -0.6637933333333335 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6637933333333335 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.378602177741314
maxi score, test score, baseline:  -0.6637933333333335 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  62 total reward:  0.24666666666666648  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6613000000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.815066191111185
maxi score, test score, baseline:  -0.6613000000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6613000000000001 0.6793333333333333 0.6793333333333333
printing an ep nov before normalisation:  27.98626109546175
printing an ep nov before normalisation:  54.87382705839695
maxi score, test score, baseline:  -0.6613000000000001 0.6793333333333333 0.6793333333333333
printing an ep nov before normalisation:  49.254137760464786
maxi score, test score, baseline:  -0.6613000000000001 0.6793333333333333 0.6793333333333333
Printing some Q and Qe and total Qs values:  [[-0.173]
 [-0.111]
 [-0.127]
 [-0.122]
 [-0.159]
 [-0.159]
 [-0.159]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.173]
 [-0.111]
 [-0.127]
 [-0.122]
 [-0.159]
 [-0.159]
 [-0.159]]
printing an ep nov before normalisation:  41.85354282933127
actor:  0 policy actor:  0  step number:  49 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6585533333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6585533333333334 0.6793333333333333 0.6793333333333333
Printing some Q and Qe and total Qs values:  [[-0.096]
 [-0.096]
 [-0.096]
 [-0.096]
 [-0.096]
 [-0.096]
 [-0.096]] [[28.173]
 [28.173]
 [28.173]
 [28.173]
 [28.173]
 [28.173]
 [28.173]] [[0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6585533333333334 0.6793333333333333 0.6793333333333333
printing an ep nov before normalisation:  42.16882008585819
printing an ep nov before normalisation:  52.02039467423979
maxi score, test score, baseline:  -0.6585533333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6585533333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6585533333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  67 total reward:  0.03999999999999937  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.839293479919434
actor:  1 policy actor:  1  step number:  75 total reward:  0.05333333333333257  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.063]
 [-0.11 ]
 [-0.096]
 [-0.096]
 [-0.069]
 [-0.142]
 [-0.096]] [[46.072]
 [40.496]
 [41.554]
 [41.554]
 [53.374]
 [54.313]
 [41.554]] [[0.125]
 [0.023]
 [0.047]
 [0.047]
 [0.191]
 [0.127]
 [0.047]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6585533333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 32.59576731920242
maxi score, test score, baseline:  -0.6585533333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  0  action  0 :  tensor([0.3661, 0.0123, 0.1526, 0.1327, 0.1278, 0.1045, 0.1040],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0056, 0.9270, 0.0119, 0.0122, 0.0061, 0.0091, 0.0280],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1182, 0.0141, 0.3899, 0.1275, 0.1136, 0.1215, 0.1153],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0867, 0.0683, 0.0935, 0.4450, 0.0950, 0.1090, 0.1025],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1375, 0.0034, 0.1060, 0.1066, 0.4584, 0.1022, 0.0859],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1198, 0.0015, 0.1254, 0.1400, 0.1023, 0.3787, 0.1322],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1229, 0.1196, 0.1411, 0.1681, 0.1270, 0.1342, 0.1871],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  33.07972952443802
actor:  1 policy actor:  1  step number:  55 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6585533333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6585533333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6585533333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.39643096923828
printing an ep nov before normalisation:  0.0002972101265186211
printing an ep nov before normalisation:  0.004653677360551001
actor:  1 policy actor:  1  step number:  56 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6585533333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.90446200863859
maxi score, test score, baseline:  -0.6585533333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 44.62099710294907
maxi score, test score, baseline:  -0.6585533333333334 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.6585533333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6585533333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  61 total reward:  0.27999999999999914  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8351245
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6559933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6559933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  4.7247517613868695e-05
siam score:  -0.8352453
maxi score, test score, baseline:  -0.6559933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.447]
 [0.379]
 [0.405]
 [0.378]
 [0.415]
 [0.394]] [[45.378]
 [42.876]
 [45.975]
 [44.055]
 [45.843]
 [42.807]
 [43.89 ]] [[0.38 ]
 [0.447]
 [0.379]
 [0.405]
 [0.378]
 [0.415]
 [0.394]]
printing an ep nov before normalisation:  39.22912740340143
maxi score, test score, baseline:  -0.6559933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6559933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.6559933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6559933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6559933333333334 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.6559933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  55 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  1.0
siam score:  -0.836474
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.5912618524521
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8346395
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  0.05999999999999939  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  42 total reward:  0.4333333333333329  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
printing an ep nov before normalisation:  37.80708312988281
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.88784293859721
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.30035215468623
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.63778774194381
printing an ep nov before normalisation:  27.11308021628213
actor:  1 policy actor:  1  step number:  60 total reward:  0.20666666666666644  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  67 total reward:  0.13333333333333275  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.155]
 [-0.155]
 [-0.155]
 [-0.155]
 [-0.155]
 [-0.155]
 [-0.155]] [[50.931]
 [50.931]
 [50.931]
 [50.931]
 [50.931]
 [50.931]
 [50.931]] [[1.435]
 [1.435]
 [1.435]
 [1.435]
 [1.435]
 [1.435]
 [1.435]]
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.797324657440186
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.85714486669622
printing an ep nov before normalisation:  50.745859146118164
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.98600220490434
printing an ep nov before normalisation:  38.89950550950991
actor:  1 policy actor:  1  step number:  48 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.1055793762207
Printing some Q and Qe and total Qs values:  [[-0.18 ]
 [-0.194]
 [-0.18 ]
 [-0.18 ]
 [-0.18 ]
 [-0.182]
 [-0.18 ]] [[38.877]
 [34.659]
 [39.133]
 [38.877]
 [38.877]
 [39.408]
 [38.877]] [[0.47 ]
 [0.319]
 [0.478]
 [0.47 ]
 [0.47 ]
 [0.485]
 [0.47 ]]
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.391102741183765
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.46762631884219
siam score:  -0.8246814
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8262813
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.2270, 0.2027, 0.1158, 0.1091, 0.1309, 0.1157, 0.0987],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0208, 0.8316, 0.0167, 0.0536, 0.0222, 0.0145, 0.0407],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1188, 0.0228, 0.2679, 0.1516, 0.1231, 0.1984, 0.1174],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1250, 0.0763, 0.1337, 0.2908, 0.1163, 0.1221, 0.1359],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1618, 0.0781, 0.1310, 0.0955, 0.3130, 0.1324, 0.0882],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1281, 0.0714, 0.1541, 0.1135, 0.1174, 0.3034, 0.1121],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1286, 0.2907, 0.1029, 0.1283, 0.0982, 0.0869, 0.1644],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.88171195983887
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]] [[44.593]
 [44.593]
 [44.593]
 [44.593]
 [44.593]
 [44.593]
 [44.593]] [[1.285]
 [1.285]
 [1.285]
 [1.285]
 [1.285]
 [1.285]
 [1.285]]
actions average: 
K:  4  action  0 :  tensor([0.2853, 0.1527, 0.1058, 0.0991, 0.1316, 0.1052, 0.1205],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0159, 0.8861, 0.0217, 0.0104, 0.0081, 0.0129, 0.0449],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1831, 0.0398, 0.3746, 0.0924, 0.1257, 0.0642, 0.1202],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1975, 0.1304, 0.0975, 0.1574, 0.1333, 0.1481, 0.1359],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1940, 0.0064, 0.0934, 0.0928, 0.3760, 0.1177, 0.1197],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1200, 0.0353, 0.1519, 0.1284, 0.1478, 0.2932, 0.1234],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1137, 0.1720, 0.1458, 0.1288, 0.1312, 0.1827, 0.1258],
       grad_fn=<DivBackward0>)
siam score:  -0.82826644
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
printing an ep nov before normalisation:  33.517024517059326
actions average: 
K:  4  action  0 :  tensor([0.1019, 0.3212, 0.0800, 0.1049, 0.2145, 0.0997, 0.0778],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0308, 0.8467, 0.0209, 0.0233, 0.0164, 0.0166, 0.0452],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1277, 0.0286, 0.2762, 0.1361, 0.1668, 0.1519, 0.1126],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1090, 0.0828, 0.1074, 0.2814, 0.1252, 0.1351, 0.1592],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1159, 0.0488, 0.0763, 0.1111, 0.4569, 0.0991, 0.0918],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1230, 0.0047, 0.1269, 0.1863, 0.1989, 0.2328, 0.1275],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1221, 0.0784, 0.1206, 0.1631, 0.1466, 0.1585, 0.2106],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.39 ]
 [0.288]
 [0.292]
 [0.304]
 [0.31 ]
 [0.318]] [[50.347]
 [45.373]
 [49.041]
 [50.192]
 [49.556]
 [48.79 ]
 [49.226]] [[1.933]
 [1.699]
 [1.844]
 [1.925]
 [1.895]
 [1.849]
 [1.886]]
printing an ep nov before normalisation:  0.10749082407528476
actor:  1 policy actor:  1  step number:  76 total reward:  0.09999999999999953  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8345307
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.61534221097894
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.83553237
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.667
siam score:  -0.83532417
printing an ep nov before normalisation:  58.18758147989925
printing an ep nov before normalisation:  53.55572700231124
Printing some Q and Qe and total Qs values:  [[-0.037]
 [-0.021]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]] [[63.561]
 [60.58 ]
 [63.561]
 [63.561]
 [63.561]
 [63.561]
 [63.561]] [[1.573]
 [1.467]
 [1.573]
 [1.573]
 [1.573]
 [1.573]
 [1.573]]
printing an ep nov before normalisation:  62.51623493234834
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.4  ]
 [0.604]
 [0.538]
 [0.351]
 [0.346]
 [0.493]
 [0.343]] [[44.912]
 [42.98 ]
 [41.461]
 [45.502]
 [45.807]
 [43.041]
 [46.267]] [[1.483]
 [1.596]
 [1.459]
 [1.462]
 [1.472]
 [1.489]
 [1.49 ]]
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.206666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
printing an ep nov before normalisation:  54.925033781263565
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  90 total reward:  0.03333333333333155  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.47271267415664
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.557]
 [0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]] [[48.66]
 [41.6 ]
 [48.66]
 [48.66]
 [48.66]
 [48.66]
 [48.66]] [[0.733]
 [0.789]
 [0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.733]]
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.05999999999999939  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.3533333333333334  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
printing an ep nov before normalisation:  36.23856646453942
printing an ep nov before normalisation:  23.340048789978027
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.204540076692677
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.355]
 [0.353]
 [0.352]
 [0.355]
 [0.355]
 [0.355]] [[21.382]
 [21.382]
 [21.804]
 [21.8  ]
 [21.382]
 [21.382]
 [21.382]] [[0.355]
 [0.355]
 [0.353]
 [0.352]
 [0.355]
 [0.355]
 [0.355]]
printing an ep nov before normalisation:  21.666162942178655
printing an ep nov before normalisation:  11.415222410521814
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  11.825007779445661
printing an ep nov before normalisation:  12.58843567385117
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.67857646942139
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.163]
 [0.263]
 [0.259]
 [0.259]
 [0.255]
 [0.246]] [[25.062]
 [45.996]
 [24.8  ]
 [24.871]
 [24.755]
 [24.766]
 [25.564]] [[0.262]
 [0.163]
 [0.263]
 [0.259]
 [0.259]
 [0.255]
 [0.246]]
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  11.075644493103027
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.6535933333333334 0.6793333333333333 0.6793333333333333
actor:  0 policy actor:  0  step number:  64 total reward:  0.19333333333333258  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6512066666666667 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.6512066666666667 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.60678394832584
Printing some Q and Qe and total Qs values:  [[-0.142]
 [-0.157]
 [-0.125]
 [-0.12 ]
 [-0.164]
 [-0.119]
 [-0.15 ]] [[44.785]
 [37.411]
 [44.668]
 [47.359]
 [49.715]
 [54.427]
 [46.   ]] [[0.293]
 [0.15 ]
 [0.308]
 [0.359]
 [0.357]
 [0.483]
 [0.306]]
actor:  0 policy actor:  0  step number:  55 total reward:  0.19999999999999962  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
siam score:  -0.83916223
maxi score, test score, baseline:  -0.6488066666666668 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6488066666666668 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.759697914123535
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([0.3338, 0.0013, 0.0963, 0.1258, 0.2805, 0.0731, 0.0892],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0073, 0.9368, 0.0097, 0.0140, 0.0084, 0.0104, 0.0134],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1378, 0.0645, 0.2928, 0.1010, 0.1530, 0.1584, 0.0926],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1061, 0.0236, 0.1192, 0.3196, 0.1490, 0.1559, 0.1266],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1369, 0.0067, 0.1424, 0.1848, 0.2451, 0.1681, 0.1160],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1158, 0.0118, 0.1431, 0.1538, 0.1778, 0.3076, 0.0901],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1255, 0.1246, 0.1184, 0.1579, 0.2402, 0.1175, 0.1158],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.6488066666666668 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6488066666666668 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6488066666666668 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.6488066666666668 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6488066666666668 0.6793333333333333 0.6793333333333333
printing an ep nov before normalisation:  54.032306572077026
printing an ep nov before normalisation:  39.40880001541625
printing an ep nov before normalisation:  51.25847858864839
maxi score, test score, baseline:  -0.6488066666666668 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6488066666666668 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.6488066666666668 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.068]
 [-0.057]
 [-0.064]
 [-0.062]
 [-0.067]
 [-0.067]
 [-0.066]] [[32.337]
 [42.258]
 [37.075]
 [38.817]
 [31.699]
 [31.546]
 [31.484]] [[0.102]
 [0.213]
 [0.154]
 [0.174]
 [0.096]
 [0.095]
 [0.095]]
maxi score, test score, baseline:  -0.6488066666666668 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.99041154839079
printing an ep nov before normalisation:  38.30232858657837
maxi score, test score, baseline:  -0.6488066666666668 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  66 total reward:  0.07333333333333236  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  4  action  0 :  tensor([0.2915, 0.0247, 0.1083, 0.1449, 0.2012, 0.1153, 0.1141],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0083, 0.9198, 0.0122, 0.0133, 0.0035, 0.0064, 0.0366],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1361, 0.0428, 0.1792, 0.2164, 0.1245, 0.1448, 0.1562],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1215, 0.1028, 0.1240, 0.2489, 0.1301, 0.1389, 0.1337],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1735, 0.0592, 0.0842, 0.2187, 0.2709, 0.0924, 0.1010],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0686, 0.0854, 0.1186, 0.1159, 0.0926, 0.4274, 0.0915],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1005, 0.1738, 0.1071, 0.1677, 0.0886, 0.1073, 0.2551],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.6488066666666668 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6488066666666668 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  70 total reward:  0.019999999999999574  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8379128
actor:  1 policy actor:  1  step number:  46 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.93013127407322
printing an ep nov before normalisation:  62.08825747813159
maxi score, test score, baseline:  -0.6488066666666668 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.21722668594282
Printing some Q and Qe and total Qs values:  [[0.121]
 [0.287]
 [0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]] [[43.586]
 [50.493]
 [43.586]
 [43.586]
 [43.586]
 [43.586]
 [43.586]] [[0.357]
 [0.566]
 [0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]]
maxi score, test score, baseline:  -0.6488066666666668 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6488066666666668 0.6793333333333333 0.6793333333333333
actions average: 
K:  0  action  0 :  tensor([0.1973, 0.0042, 0.1417, 0.1748, 0.2057, 0.1380, 0.1383],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0022,     0.9432,     0.0037,     0.0128,     0.0007,     0.0008,
            0.0366], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1492, 0.0093, 0.2008, 0.1477, 0.1374, 0.2235, 0.1322],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1224, 0.0337, 0.1245, 0.3082, 0.1360, 0.1140, 0.1611],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1035, 0.0063, 0.0968, 0.1262, 0.4830, 0.0966, 0.0877],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1449, 0.0036, 0.1713, 0.1345, 0.1338, 0.2838, 0.1281],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1381, 0.0114, 0.1502, 0.1631, 0.1380, 0.1415, 0.2577],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  30.86083342267912
actor:  0 policy actor:  0  step number:  50 total reward:  0.27333333333333254  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6462600000000002 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6462600000000002 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.6462600000000002 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.036156965623057
printing an ep nov before normalisation:  50.241941738301634
maxi score, test score, baseline:  -0.6462600000000002 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6462600000000002 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.6462600000000001 0.6793333333333333 0.6793333333333333
printing an ep nov before normalisation:  52.674059180878984
maxi score, test score, baseline:  -0.6462600000000001 0.6793333333333333 0.6793333333333333
Printing some Q and Qe and total Qs values:  [[-0.153]
 [-0.165]
 [-0.153]
 [-0.153]
 [-0.153]
 [-0.153]
 [-0.153]] [[44.049]
 [41.96 ]
 [44.049]
 [44.049]
 [44.049]
 [44.049]
 [44.049]] [[1.806]
 [1.616]
 [1.806]
 [1.806]
 [1.806]
 [1.806]
 [1.806]]
printing an ep nov before normalisation:  34.231931160537684
printing an ep nov before normalisation:  44.165952394070935
maxi score, test score, baseline:  -0.6462600000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.083]
 [-0.066]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]
 [-0.079]] [[31.099]
 [48.168]
 [35.07 ]
 [35.07 ]
 [35.07 ]
 [35.07 ]
 [35.07 ]] [[0.301]
 [0.821]
 [0.422]
 [0.422]
 [0.422]
 [0.422]
 [0.422]]
printing an ep nov before normalisation:  43.800596455015416
maxi score, test score, baseline:  -0.6433800000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.381076869819985
maxi score, test score, baseline:  -0.6433800000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6433800000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6433800000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6433800000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.09685294443558
maxi score, test score, baseline:  -0.6433800000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  33.69674124977011
maxi score, test score, baseline:  -0.6433800000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6433800000000001 0.6793333333333333 0.6793333333333333
Printing some Q and Qe and total Qs values:  [[-0.107]
 [-0.107]
 [-0.107]
 [-0.107]
 [-0.107]
 [-0.107]
 [-0.107]] [[42.871]
 [42.871]
 [42.871]
 [42.871]
 [42.871]
 [42.871]
 [42.871]] [[0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.409706822885184
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.106]
 [0.021]
 [0.033]
 [0.03 ]
 [0.021]
 [0.059]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.021]
 [0.106]
 [0.021]
 [0.033]
 [0.03 ]
 [0.021]
 [0.059]]
siam score:  -0.84335035
maxi score, test score, baseline:  -0.6433800000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.2737, 0.0479, 0.1253, 0.1322, 0.1306, 0.1399, 0.1503],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0063, 0.9478, 0.0054, 0.0056, 0.0024, 0.0026, 0.0299],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1326, 0.0057, 0.3459, 0.1395, 0.1214, 0.1268, 0.1283],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1315, 0.0138, 0.1242, 0.2996, 0.1297, 0.1393, 0.1620],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2098, 0.0206, 0.0913, 0.1021, 0.3772, 0.0873, 0.1116],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0950, 0.0510, 0.1358, 0.1081, 0.0744, 0.4375, 0.0982],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1493, 0.0088, 0.1344, 0.2127, 0.1640, 0.1868, 0.1439],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.6433800000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6433800000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6433800000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6433800000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3599999999999993  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6433800000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.181]
 [0.194]
 [0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.181]
 [0.194]
 [0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]]
printing an ep nov before normalisation:  37.78951644897461
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.917]
 [0.979]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]] [[35.773]
 [44.815]
 [35.773]
 [35.773]
 [35.773]
 [35.773]
 [35.773]] [[0.917]
 [0.979]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]]
maxi score, test score, baseline:  -0.6433800000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.74 ]
 [0.886]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.744]] [[26.69 ]
 [35.035]
 [25.342]
 [25.342]
 [25.342]
 [25.342]
 [25.116]] [[0.74 ]
 [0.886]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.744]]
actor:  0 policy actor:  0  step number:  54 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6407533333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.92391531346339
printing an ep nov before normalisation:  31.56158447265625
actor:  0 policy actor:  0  step number:  40 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6376200000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  39.614596366882324
maxi score, test score, baseline:  -0.6376200000000001 0.6793333333333333 0.6793333333333333
printing an ep nov before normalisation:  43.24308427400645
printing an ep nov before normalisation:  39.45999622344971
printing an ep nov before normalisation:  42.101297214849154
printing an ep nov before normalisation:  65.8322019562996
printing an ep nov before normalisation:  33.473944664001465
maxi score, test score, baseline:  -0.6376200000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.37918281555176
printing an ep nov before normalisation:  49.90677757577642
maxi score, test score, baseline:  -0.6376200000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.80835946050577
maxi score, test score, baseline:  -0.6376200000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6376200000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.028626656749104
actor:  1 policy actor:  1  step number:  65 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  38.874300850698816
Printing some Q and Qe and total Qs values:  [[0.887]
 [0.887]
 [0.887]
 [0.887]
 [0.887]
 [0.887]
 [0.887]] [[46.229]
 [46.229]
 [46.229]
 [46.229]
 [46.229]
 [46.229]
 [46.229]] [[0.887]
 [0.887]
 [0.887]
 [0.887]
 [0.887]
 [0.887]
 [0.887]]
maxi score, test score, baseline:  -0.6376200000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6376200000000001 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  47 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  53.41359609491426
actions average: 
K:  3  action  0 :  tensor([0.3256, 0.0222, 0.1372, 0.0927, 0.1302, 0.1487, 0.1433],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0135, 0.8713, 0.0117, 0.0369, 0.0213, 0.0124, 0.0329],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1060, 0.0156, 0.4328, 0.0820, 0.1005, 0.1562, 0.1069],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1197, 0.2907, 0.0849, 0.1069, 0.1261, 0.1144, 0.1573],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1598, 0.0053, 0.0954, 0.0944, 0.4417, 0.1071, 0.0961],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0597, 0.0033, 0.1022, 0.1120, 0.0764, 0.5487, 0.0978],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1156, 0.0734, 0.1144, 0.1210, 0.1138, 0.1341, 0.3278],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  70.6092982988013
siam score:  -0.8438201
maxi score, test score, baseline:  -0.6351133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.071]
 [-0.057]
 [-0.119]
 [-0.06 ]
 [-0.066]
 [-0.115]
 [-0.066]] [[44.808]
 [42.316]
 [42.173]
 [43.523]
 [45.292]
 [44.207]
 [43.123]] [[0.573]
 [0.52 ]
 [0.454]
 [0.549]
 [0.592]
 [0.513]
 [0.533]]
Printing some Q and Qe and total Qs values:  [[0.051]
 [0.414]
 [0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]] [[39.037]
 [40.215]
 [39.037]
 [39.037]
 [39.037]
 [39.037]
 [39.037]] [[0.438]
 [0.824]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.1098],
        [-0.1640],
        [-0.4107],
        [-0.0000],
        [ 0.4564],
        [-0.2365],
        [ 0.4553],
        [-0.0000],
        [-0.0000],
        [-0.2873]], dtype=torch.float64)
-0.045546567066 -0.1553139260241442
-0.045026434398 -0.20898889093018938
-0.032346567066 -0.4430528398501497
0.99 0.99
-0.08410238119800001 0.3722566463460549
-0.09703970119800001 -0.33350260356695144
-0.08410238119800001 0.37124490770578417
-0.957 -0.957
0.99 0.99
-0.045026434398 -0.33234092879636357
actor:  1 policy actor:  1  step number:  56 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  54.58238210008641
maxi score, test score, baseline:  -0.6351133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  58 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  77.57922286200888
printing an ep nov before normalisation:  44.804033817139015
printing an ep nov before normalisation:  44.50993418463599
printing an ep nov before normalisation:  59.756795584007364
maxi score, test score, baseline:  -0.6351133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.77360330294961
Printing some Q and Qe and total Qs values:  [[-0.099]
 [-0.091]
 [-0.103]
 [-0.096]
 [-0.106]
 [-0.097]
 [-0.106]] [[37.653]
 [43.15 ]
 [37.138]
 [41.979]
 [36.211]
 [42.333]
 [36.023]] [[0.637]
 [0.899]
 [0.609]
 [0.84 ]
 [0.562]
 [0.855]
 [0.554]]
maxi score, test score, baseline:  -0.6351133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.40325805410174
actor:  1 policy actor:  1  step number:  61 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6351133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.403965494916285
maxi score, test score, baseline:  -0.6351133333333334 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.6351133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6351133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.92 ]
 [0.984]
 [0.92 ]
 [0.92 ]
 [0.92 ]
 [0.92 ]
 [0.92 ]] [[52.467]
 [40.967]
 [52.467]
 [52.467]
 [52.467]
 [52.467]
 [52.467]] [[0.92 ]
 [0.984]
 [0.92 ]
 [0.92 ]
 [0.92 ]
 [0.92 ]
 [0.92 ]]
maxi score, test score, baseline:  -0.6351133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.35294349953739
maxi score, test score, baseline:  -0.6351133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6351133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  64 total reward:  0.13999999999999957  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.661]
 [0.658]
 [0.661]
 [0.662]
 [0.662]
 [0.662]] [[13.545]
 [ 9.467]
 [13.833]
 [ 5.876]
 [ 9.961]
 [14.201]
 [13.434]] [[0.955]
 [0.873]
 [0.967]
 [0.792]
 [0.884]
 [0.979]
 [0.961]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  41.88587959414514
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[-0.141]
 [-0.141]
 [-0.141]
 [-0.141]
 [-0.141]
 [-0.157]
 [-0.141]] [[ 0.  ]
 [ 0.  ]
 [ 0.  ]
 [ 0.  ]
 [ 0.  ]
 [47.82]
 [ 0.  ]] [[-0.684]
 [-0.684]
 [-0.684]
 [-0.684]
 [-0.684]
 [ 0.407]
 [-0.684]]
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.41688955748751
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.17333333333333256  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  44.224894086384865
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
printing an ep nov before normalisation:  27.87623643875122
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.082]
 [-0.047]
 [-0.082]
 [-0.082]
 [-0.082]
 [-0.082]
 [-0.082]] [[40.457]
 [40.004]
 [40.457]
 [40.457]
 [40.457]
 [40.457]
 [40.457]] [[1.32 ]
 [1.325]
 [1.32 ]
 [1.32 ]
 [1.32 ]
 [1.32 ]
 [1.32 ]]
siam score:  -0.8374596
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  70 total reward:  0.0599999999999995  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.043]
 [ 0.002]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.052]
 [-0.043]] [[43.571]
 [46.233]
 [43.571]
 [43.571]
 [43.571]
 [48.464]
 [43.571]] [[1.233]
 [1.432]
 [1.233]
 [1.233]
 [1.233]
 [1.505]
 [1.233]]
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
actor:  1 policy actor:  1  step number:  64 total reward:  0.0066666666666660435  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  31.278395652770996
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.134]
 [-0.126]
 [-0.135]
 [-0.135]
 [-0.135]
 [-0.136]
 [-0.136]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.134]
 [-0.126]
 [-0.135]
 [-0.135]
 [-0.135]
 [-0.136]
 [-0.136]]
printing an ep nov before normalisation:  36.399419152308994
printing an ep nov before normalisation:  30.868722942225457
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.103608127915013
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.8958363812513
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
printing an ep nov before normalisation:  36.22819444046891
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.888336300849915
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8361156
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.1280274041205
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.85316672734921
actions average: 
K:  4  action  0 :  tensor([0.1544, 0.0951, 0.1400, 0.1651, 0.1443, 0.1459, 0.1553],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0153, 0.8566, 0.0471, 0.0128, 0.0061, 0.0203, 0.0418],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0787, 0.0125, 0.2339, 0.1800, 0.1110, 0.2444, 0.1396],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1278, 0.0289, 0.1993, 0.1303, 0.1642, 0.2450, 0.1045],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0847, 0.0232, 0.0922, 0.1330, 0.4588, 0.1153, 0.0927],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1037, 0.0255, 0.1133, 0.1804, 0.1598, 0.2983, 0.1190],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1184, 0.0881, 0.1054, 0.1641, 0.1093, 0.1582, 0.2565],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.6328333333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.537]
 [0.468]
 [0.47 ]
 [0.467]
 [0.47 ]
 [0.472]] [[26.588]
 [35.726]
 [26.138]
 [27.495]
 [29.538]
 [27.255]
 [24.81 ]] [[0.462]
 [0.537]
 [0.468]
 [0.47 ]
 [0.467]
 [0.47 ]
 [0.472]]
actor:  1 policy actor:  1  step number:  57 total reward:  0.18666666666666631  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]] [[46.041]
 [46.041]
 [46.041]
 [46.041]
 [46.041]
 [46.041]
 [46.041]] [[0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]]
actor:  0 policy actor:  0  step number:  63 total reward:  0.1599999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6305133333333334 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.6305133333333334 0.6793333333333333 0.6793333333333333
printing an ep nov before normalisation:  65.92352953360559
maxi score, test score, baseline:  -0.6305133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6305133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6305133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.07009944977918
maxi score, test score, baseline:  -0.6305133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6305133333333334 0.6793333333333333 0.6793333333333333
printing an ep nov before normalisation:  45.8763944122652
maxi score, test score, baseline:  -0.6305133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6305133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6305133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6305133333333334 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.6305133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6305133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6305133333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  50 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6276466666666667 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.682721475745296
siam score:  -0.84215945
actions average: 
K:  2  action  0 :  tensor([0.3143, 0.0557, 0.1033, 0.1187, 0.1287, 0.1382, 0.1410],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0148, 0.9032, 0.0103, 0.0148, 0.0034, 0.0044, 0.0490],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1485, 0.0499, 0.2431, 0.1181, 0.1166, 0.1284, 0.1953],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1611, 0.0319, 0.1163, 0.1894, 0.1393, 0.1688, 0.1932],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1240, 0.0597, 0.0597, 0.0749, 0.5082, 0.0923, 0.0812],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0798, 0.0853, 0.1894, 0.0844, 0.0823, 0.3842, 0.0945],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1323, 0.2183, 0.0963, 0.1306, 0.1234, 0.1434, 0.1557],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  8.234748065660824e-06
maxi score, test score, baseline:  -0.6276466666666667 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.6276466666666667 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6276466666666667 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6276466666666667 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.24053773476562
Printing some Q and Qe and total Qs values:  [[0.781]
 [0.868]
 [0.853]
 [0.898]
 [0.821]
 [0.808]
 [0.94 ]] [[47.746]
 [49.242]
 [49.951]
 [45.912]
 [44.556]
 [44.998]
 [46.122]] [[0.781]
 [0.868]
 [0.853]
 [0.898]
 [0.821]
 [0.808]
 [0.94 ]]
actor:  0 policy actor:  0  step number:  53 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.117]
 [-0.12 ]
 [-0.117]
 [-0.122]
 [-0.117]
 [-0.117]
 [-0.12 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.117]
 [-0.12 ]
 [-0.117]
 [-0.122]
 [-0.117]
 [-0.117]
 [-0.12 ]]
printing an ep nov before normalisation:  57.593969339338564
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.28156151838673
maxi score, test score, baseline:  -0.6252466666666667 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6252466666666667 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6252466666666667 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6252466666666667 0.6793333333333333 0.6793333333333333
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.27222428286385
printing an ep nov before normalisation:  58.83724171192651
actor:  1 policy actor:  1  step number:  55 total reward:  0.18666666666666631  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6252466666666667 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.6252466666666667 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6252466666666667 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6252466666666667 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.104]
 [-0.075]
 [-0.103]
 [-0.099]
 [-0.099]
 [-0.099]
 [-0.099]] [[38.149]
 [35.91 ]
 [37.149]
 [36.951]
 [36.843]
 [36.934]
 [36.492]] [[1.051]
 [0.943]
 [0.991]
 [0.982]
 [0.976]
 [0.982]
 [0.955]]
Printing some Q and Qe and total Qs values:  [[-0.104]
 [-0.07 ]
 [-0.104]
 [-0.104]
 [-0.104]
 [-0.104]
 [-0.104]] [[38.723]
 [37.733]
 [38.723]
 [38.723]
 [38.723]
 [38.723]
 [38.723]] [[0.977]
 [0.959]
 [0.977]
 [0.977]
 [0.977]
 [0.977]
 [0.977]]
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 0.37966479405417886
actor:  0 policy actor:  0  step number:  43 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.459]
 [0.422]
 [0.416]
 [0.42 ]
 [0.412]
 [0.417]] [[42.274]
 [39.752]
 [42.994]
 [44.482]
 [44.125]
 [43.103]
 [41.809]] [[1.47 ]
 [1.363]
 [1.503]
 [1.577]
 [1.561]
 [1.498]
 [1.433]]
printing an ep nov before normalisation:  46.73694288199804
siam score:  -0.845489
actor:  1 policy actor:  1  step number:  49 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8459018
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.2662, 0.0794, 0.1287, 0.1144, 0.1540, 0.1507, 0.1067],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0117, 0.9150, 0.0110, 0.0194, 0.0110, 0.0147, 0.0172],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1267, 0.0064, 0.1627, 0.1551, 0.1751, 0.2563, 0.1178],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1013, 0.0194, 0.1491, 0.2547, 0.1247, 0.1801, 0.1707],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0985, 0.0490, 0.0990, 0.0942, 0.4398, 0.1459, 0.0737],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0955, 0.1611, 0.1446, 0.1253, 0.1340, 0.2463, 0.0932],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1400, 0.0699, 0.1311, 0.1069, 0.1582, 0.2139, 0.1800],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
printing an ep nov before normalisation:  36.84897519697988
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.2871],
        [-0.0000],
        [ 0.3483],
        [ 0.0957],
        [-0.2167],
        [ 0.1981],
        [ 0.7360],
        [-0.0000],
        [-0.0000],
        [-0.0000]], dtype=torch.float64)
-0.032346567066 -0.31943464999329085
-0.9481788029999999 -0.9481788029999999
-0.09703970119800001 0.25121776652632677
-0.032346567066 0.06330560219056447
-0.083839701198 -0.3005481144400992
-0.045026434398 0.15308285104879818
-0.05809183386599999 0.677861652606696
-0.6729373200000001 -0.6729373200000001
0.9734999999999999 0.9734999999999999
-0.7722 -0.7722
printing an ep nov before normalisation:  66.41149957286333
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.756795628460985
printing an ep nov before normalisation:  43.626755847217126
printing an ep nov before normalisation:  55.430318862264436
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.504943799676624
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.897398948669434
printing an ep nov before normalisation:  15.114980146946966
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  61 total reward:  0.11999999999999944  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.059]
 [-0.034]
 [-0.059]
 [-0.059]
 [-0.059]
 [-0.059]
 [-0.059]] [[45.457]
 [47.986]
 [45.457]
 [45.457]
 [45.457]
 [45.457]
 [45.457]] [[0.553]
 [0.647]
 [0.553]
 [0.553]
 [0.553]
 [0.553]
 [0.553]]
printing an ep nov before normalisation:  49.33539526940032
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]] [[49.926]
 [49.926]
 [49.926]
 [49.926]
 [49.926]
 [49.926]
 [49.926]] [[1.304]
 [1.304]
 [1.304]
 [1.304]
 [1.304]
 [1.304]
 [1.304]]
printing an ep nov before normalisation:  41.79478456978598
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.629]
 [0.482]
 [0.478]
 [0.511]
 [0.485]
 [0.491]] [[29.395]
 [28.071]
 [31.565]
 [27.934]
 [29.395]
 [31.04 ]
 [28.518]] [[1.302]
 [1.362]
 [1.371]
 [1.205]
 [1.302]
 [1.35 ]
 [1.243]]
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  49 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.48010842845618
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.24897491962396
siam score:  -0.8484133
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.22062533031288
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
printing an ep nov before normalisation:  42.0196677549065
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  72 total reward:  0.09999999999999931  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8477466
actions average: 
K:  4  action  0 :  tensor([0.1344, 0.0055, 0.1390, 0.2348, 0.1866, 0.1520, 0.1478],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0276, 0.8058, 0.0311, 0.0344, 0.0133, 0.0225, 0.0653],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1201, 0.0187, 0.2368, 0.1465, 0.1220, 0.2479, 0.1079],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0807, 0.2435, 0.0908, 0.1835, 0.1758, 0.1249, 0.1008],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1746, 0.0491, 0.0863, 0.1237, 0.3700, 0.0967, 0.0995],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0626, 0.0604, 0.1987, 0.1149, 0.0799, 0.4056, 0.0779],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1472, 0.0186, 0.1381, 0.2004, 0.1559, 0.1578, 0.1821],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.07 ]
 [-0.046]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]
 [-0.07 ]] [[33.974]
 [42.137]
 [33.974]
 [33.974]
 [33.974]
 [33.974]
 [33.974]] [[0.636]
 [0.991]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]]
maxi score, test score, baseline:  -0.6223933333333334 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  58 total reward:  0.09999999999999953  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6201933333333333 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.55305365535733
actor:  1 policy actor:  1  step number:  52 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.6201933333333333 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.6201933333333333 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.6201933333333333 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.70350646972656
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actions average: 
K:  2  action  0 :  tensor([0.3670, 0.0093, 0.1090, 0.1147, 0.1643, 0.1154, 0.1203],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0090, 0.9051, 0.0184, 0.0138, 0.0049, 0.0058, 0.0429],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1058, 0.0111, 0.1946, 0.1651, 0.1359, 0.2690, 0.1186],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0631, 0.0083, 0.0928, 0.5125, 0.0887, 0.1469, 0.0878],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1322, 0.0039, 0.1261, 0.1301, 0.3415, 0.1374, 0.1287],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1101, 0.0084, 0.1754, 0.1188, 0.1058, 0.3568, 0.1246],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1232, 0.0743, 0.1302, 0.1149, 0.1031, 0.1142, 0.3401],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  57 total reward:  0.27999999999999936  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.6201933333333333 0.6793333333333333 0.6793333333333333
Printing some Q and Qe and total Qs values:  [[-0.15 ]
 [-0.145]
 [-0.151]
 [-0.152]
 [-0.152]
 [-0.151]
 [-0.152]] [[46.193]
 [42.591]
 [46.395]
 [46.656]
 [47.263]
 [47.321]
 [46.657]] [[0.694]
 [0.565]
 [0.7  ]
 [0.709]
 [0.732]
 [0.735]
 [0.709]]
siam score:  -0.8539219
Starting evaluation
printing an ep nov before normalisation:  52.63062053256564
Printing some Q and Qe and total Qs values:  [[-0.094]
 [-0.085]
 [-0.09 ]
 [-0.097]
 [-0.093]
 [-0.093]
 [-0.093]] [[45.205]
 [43.741]
 [46.317]
 [46.848]
 [45.644]
 [45.595]
 [45.275]] [[1.591]
 [1.486]
 [1.681]
 [1.716]
 [1.626]
 [1.623]
 [1.598]]
maxi score, test score, baseline:  -0.6201933333333333 0.6793333333333333 0.6793333333333333
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.654]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]] [[30.105]
 [26.228]
 [30.105]
 [30.105]
 [30.105]
 [30.105]
 [30.105]] [[0.527]
 [0.654]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]]
printing an ep nov before normalisation:  31.514110094818495
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.351]
 [0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[35.304]
 [47.064]
 [35.304]
 [35.304]
 [35.304]
 [35.304]
 [35.304]] [[0.57 ]
 [1.279]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]
 [0.57 ]]
printing an ep nov before normalisation:  44.172722540380505
Printing some Q and Qe and total Qs values:  [[0.879]
 [0.944]
 [0.879]
 [0.862]
 [0.882]
 [0.879]
 [0.879]] [[55.091]
 [43.099]
 [55.091]
 [63.87 ]
 [62.196]
 [55.091]
 [55.091]] [[0.879]
 [0.944]
 [0.879]
 [0.862]
 [0.882]
 [0.879]
 [0.879]]
printing an ep nov before normalisation:  40.74212370035571
maxi score, test score, baseline:  -0.6201933333333333 0.6793333333333333 0.6793333333333333
printing an ep nov before normalisation:  38.18258589808233
maxi score, test score, baseline:  -0.6201933333333333 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.1956313729947
printing an ep nov before normalisation:  42.60858858952041
printing an ep nov before normalisation:  44.058518240552914
printing an ep nov before normalisation:  42.205675480519744
maxi score, test score, baseline:  -0.6201933333333333 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  0.22573766096130043
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  37.32884199539464
maxi score, test score, baseline:  -0.5596333333333333 0.6793333333333333 0.6793333333333333
actor:  0 policy actor:  0  step number:  30 total reward:  0.6333333333333334  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.5563666666666667 0.6793333333333333 0.6793333333333333
maxi score, test score, baseline:  -0.5563666666666667 0.6793333333333333 0.6793333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  32 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  30.035247802734375
actions average: 
K:  3  action  0 :  tensor([0.2687, 0.0813, 0.1208, 0.0887, 0.1755, 0.1787, 0.0864],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0079, 0.9202, 0.0066, 0.0272, 0.0060, 0.0064, 0.0257],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1035, 0.1201, 0.2742, 0.1131, 0.1275, 0.1368, 0.1248],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1247, 0.1409, 0.1238, 0.2399, 0.1314, 0.1351, 0.1043],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1044, 0.0182, 0.1544, 0.1606, 0.2412, 0.1840, 0.1371],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0923, 0.0152, 0.1596, 0.1021, 0.1144, 0.3971, 0.1192],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1365, 0.0229, 0.1575, 0.1849, 0.2032, 0.1517, 0.1433],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  33.38673664555515
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.4496589421177
line 256 mcts: sample exp_bonus 51.559226378387564
printing an ep nov before normalisation:  30.89381373370596
printing an ep nov before normalisation:  31.995601654052734
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84519386
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.448709643262234
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.75137996673584
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]] [[41.313]
 [41.313]
 [41.313]
 [41.313]
 [41.313]
 [41.313]
 [41.313]] [[0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]]
printing an ep nov before normalisation:  43.07583505385539
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  12.917777837928256
printing an ep nov before normalisation:  34.46440564230034
siam score:  -0.84587884
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.18922627075175
printing an ep nov before normalisation:  21.41726337456609
printing an ep nov before normalisation:  49.35504469284571
actions average: 
K:  2  action  0 :  tensor([0.1458, 0.0564, 0.1377, 0.1340, 0.2119, 0.1492, 0.1649],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0114, 0.9239, 0.0109, 0.0128, 0.0034, 0.0046, 0.0331],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1166, 0.0614, 0.3612, 0.1069, 0.1027, 0.1130, 0.1381],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1001, 0.0290, 0.0957, 0.3816, 0.1114, 0.1184, 0.1639],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1770, 0.0147, 0.1008, 0.1240, 0.3288, 0.1366, 0.1183],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0949, 0.0351, 0.1546, 0.1119, 0.1123, 0.3734, 0.1178],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1339, 0.1033, 0.1302, 0.1441, 0.1661, 0.1734, 0.1491],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.751672883358891
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.533035500741356
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.89715821828354
siam score:  -0.848804
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  1  action  0 :  tensor([0.3462, 0.0211, 0.1162, 0.1071, 0.1800, 0.1261, 0.1034],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0088, 0.9198, 0.0101, 0.0149, 0.0076, 0.0094, 0.0294],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1568, 0.0022, 0.2728, 0.1339, 0.1373, 0.1482, 0.1489],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1138, 0.0341, 0.1271, 0.2831, 0.1363, 0.1280, 0.1777],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1392, 0.0108, 0.0650, 0.0700, 0.5872, 0.0578, 0.0700],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0877, 0.0058, 0.1666, 0.1270, 0.1025, 0.3870, 0.1234],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1730, 0.0039, 0.1714, 0.1592, 0.1640, 0.1954, 0.1331],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.03082108796866
actor:  1 policy actor:  1  step number:  69 total reward:  0.10666666666666558  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.56416496019823
printing an ep nov before normalisation:  40.23064513144291
maxi score, test score, baseline:  -0.5531 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([0.4118, 0.0304, 0.1011, 0.1102, 0.1241, 0.1010, 0.1214],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0129, 0.9136, 0.0127, 0.0202, 0.0093, 0.0104, 0.0211],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0956, 0.0791, 0.1063, 0.1912, 0.1259, 0.2581, 0.1437],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0800, 0.0674, 0.0838, 0.4458, 0.1002, 0.1030, 0.1198],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1626, 0.0205, 0.0728, 0.0842, 0.4796, 0.0835, 0.0969],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1023, 0.0466, 0.1391, 0.1681, 0.1328, 0.2901, 0.1211],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1400, 0.1170, 0.1463, 0.1362, 0.1465, 0.1402, 0.1737],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  33.48626686296627
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  0.11908718590234457
siam score:  -0.8451013
actor:  0 policy actor:  0  step number:  50 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.133]
 [-0.085]
 [-0.131]
 [-0.084]
 [-0.14 ]
 [-0.14 ]
 [-0.099]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.133]
 [-0.085]
 [-0.131]
 [-0.084]
 [-0.14 ]
 [-0.14 ]
 [-0.099]]
printing an ep nov before normalisation:  58.60201956685748
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.72127976699419
printing an ep nov before normalisation:  31.46922928329235
actor:  1 policy actor:  1  step number:  53 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.605]
 [0.612]
 [0.53 ]
 [0.447]
 [0.411]
 [0.428]
 [0.603]] [[46.76 ]
 [46.851]
 [51.518]
 [49.28 ]
 [52.472]
 [51.696]
 [47.303]] [[1.015]
 [1.024]
 [1.017]
 [0.898]
 [0.914]
 [0.918]
 [1.022]]
printing an ep nov before normalisation:  57.3945490106677
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.082 0.245 0.041 0.388 0.061 0.102 0.082]
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.84828812322812
siam score:  -0.8534355
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8550464
actor:  1 policy actor:  1  step number:  44 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.93125596598606
actions average: 
K:  3  action  0 :  tensor([0.5894, 0.0458, 0.0625, 0.0676, 0.0809, 0.0647, 0.0891],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0238, 0.8936, 0.0083, 0.0261, 0.0083, 0.0073, 0.0327],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0948, 0.0651, 0.3009, 0.1305, 0.0981, 0.1884, 0.1222],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1212, 0.0730, 0.1423, 0.2312, 0.1459, 0.1412, 0.1453],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1330, 0.1516, 0.1300, 0.1308, 0.1747, 0.1605, 0.1195],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1002, 0.0036, 0.1523, 0.1329, 0.1181, 0.3841, 0.1088],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1060, 0.1868, 0.1123, 0.1446, 0.1114, 0.1273, 0.2115],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.04 ]
 [-0.042]
 [-0.04 ]
 [-0.031]
 [-0.04 ]
 [-0.04 ]
 [-0.04 ]] [[39.755]
 [46.868]
 [39.755]
 [38.145]
 [39.755]
 [39.755]
 [39.755]] [[0.784]
 [1.091]
 [0.784]
 [0.723]
 [0.784]
 [0.784]
 [0.784]]
printing an ep nov before normalisation:  36.93734944882337
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.546]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]] [[25.738]
 [41.518]
 [25.738]
 [25.738]
 [25.738]
 [25.738]
 [25.738]] [[0.527]
 [0.789]
 [0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]]
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
actor:  1 policy actor:  1  step number:  60 total reward:  0.0599999999999995  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  35.672969818115234
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85381866
printing an ep nov before normalisation:  47.61589192936033
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.88237563472716
actor:  1 policy actor:  1  step number:  69 total reward:  0.06666666666666599  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.452]
 [0.494]
 [0.487]
 [0.49 ]
 [0.501]
 [0.493]] [[36.166]
 [37.633]
 [35.807]
 [34.61 ]
 [35.994]
 [36.661]
 [36.335]] [[1.589]
 [1.656]
 [1.566]
 [1.473]
 [1.576]
 [1.634]
 [1.604]]
actor:  1 policy actor:  1  step number:  65 total reward:  0.11999999999999922  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.105]
 [-0.115]
 [-0.105]
 [-0.105]
 [-0.105]
 [-0.105]
 [-0.105]] [[44.102]
 [47.393]
 [44.102]
 [44.102]
 [44.102]
 [44.102]
 [44.102]] [[0.784]
 [0.869]
 [0.784]
 [0.784]
 [0.784]
 [0.784]
 [0.784]]
Printing some Q and Qe and total Qs values:  [[-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.061]
 [-0.025]
 [-0.08 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.08 ]
 [-0.061]
 [-0.025]
 [-0.08 ]]
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 42.84813605731694
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
Printing some Q and Qe and total Qs values:  [[ 0.187]
 [-0.008]
 [ 0.095]
 [ 0.086]
 [ 0.093]
 [ 0.121]
 [ 0.094]] [[29.184]
 [29.996]
 [28.942]
 [29.1  ]
 [29.147]
 [29.071]
 [29.429]] [[1.679]
 [1.565]
 [1.563]
 [1.57 ]
 [1.581]
 [1.602]
 [1.61 ]]
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.506]
 [0.475]] [[39.335]
 [39.335]
 [39.335]
 [39.335]
 [39.335]
 [42.183]
 [39.335]] [[0.986]
 [0.986]
 [0.986]
 [0.986]
 [0.986]
 [1.083]
 [0.986]]
printing an ep nov before normalisation:  41.57326280591481
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  71 total reward:  0.09333333333333249  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.77453947067261
actions average: 
K:  1  action  0 :  tensor([0.2284, 0.0069, 0.1214, 0.1838, 0.1884, 0.1483, 0.1228],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0151, 0.8582, 0.0063, 0.0537, 0.0206, 0.0082, 0.0379],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1165, 0.0071, 0.3148, 0.1361, 0.1332, 0.1748, 0.1176],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1307, 0.0250, 0.1152, 0.2416, 0.1720, 0.1731, 0.1424],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1197, 0.0222, 0.1108, 0.1280, 0.3481, 0.1672, 0.1040],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0851, 0.0052, 0.1101, 0.1186, 0.1005, 0.4938, 0.0867],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1404, 0.0114, 0.1277, 0.1710, 0.1570, 0.1632, 0.2293],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.023]
 [-0.064]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]
 [-0.023]] [[38.19 ]
 [51.675]
 [38.19 ]
 [38.19 ]
 [38.19 ]
 [38.19 ]
 [38.19 ]] [[0.593]
 [0.971]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]]
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.7585123188797
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.053194973292957
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 34.21421511589147
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.266248166816414
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 33.033083329536574
printing an ep nov before normalisation:  31.14031697685702
actor:  1 policy actor:  1  step number:  59 total reward:  0.1866666666666661  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  30.80807322843806
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5506333333333333 0.6773333333333333 0.6773333333333333
actor:  0 policy actor:  0  step number:  41 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.067]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.067]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]]
maxi score, test score, baseline:  -0.5478333333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.11776826635294
printing an ep nov before normalisation:  12.73093581199646
siam score:  -0.86066175
Printing some Q and Qe and total Qs values:  [[-0.077]
 [-0.033]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]
 [-0.077]] [[30.119]
 [42.453]
 [30.119]
 [30.119]
 [30.119]
 [30.119]
 [30.119]] [[0.169]
 [0.477]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]]
maxi score, test score, baseline:  -0.5478333333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.36678389423264
printing an ep nov before normalisation:  45.28004928488749
printing an ep nov before normalisation:  37.864102022854695
printing an ep nov before normalisation:  49.47690469103609
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.87325859069824
actor:  1 policy actor:  1  step number:  46 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.138]
 [-0.062]
 [-0.091]
 [-0.075]
 [-0.074]
 [-0.1  ]
 [-0.076]] [[48.787]
 [50.875]
 [72.748]
 [54.203]
 [50.02 ]
 [48.692]
 [46.342]] [[0.285]
 [0.403]
 [0.81 ]
 [0.457]
 [0.374]
 [0.321]
 [0.299]]
Printing some Q and Qe and total Qs values:  [[0.481]
 [0.877]
 [0.877]
 [0.485]
 [0.877]
 [0.877]
 [0.461]] [[33.979]
 [36.759]
 [36.759]
 [33.571]
 [36.759]
 [36.759]
 [32.851]] [[2.022]
 [2.544]
 [2.544]
 [2.007]
 [2.544]
 [2.544]
 [1.95 ]]
Printing some Q and Qe and total Qs values:  [[-0.088]
 [-0.088]
 [-0.088]
 [-0.099]
 [-0.088]
 [-0.088]
 [-0.088]] [[86.129]
 [86.129]
 [86.129]
 [89.898]
 [86.129]
 [86.129]
 [86.129]] [[1.14 ]
 [1.14 ]
 [1.14 ]
 [1.211]
 [1.14 ]
 [1.14 ]
 [1.14 ]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5478333333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5478333333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5478333333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5478333333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5478333333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.714]
 [0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]] [[29.095]
 [29.025]
 [29.095]
 [29.095]
 [29.095]
 [29.095]
 [29.095]] [[1.056]
 [1.169]
 [1.056]
 [1.056]
 [1.056]
 [1.056]
 [1.056]]
printing an ep nov before normalisation:  24.856193937020343
printing an ep nov before normalisation:  23.22981899945532
actor:  1 policy actor:  1  step number:  66 total reward:  0.16666666666666574  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.5478333333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.32666666666666655  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5478333333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.813530034100804
Printing some Q and Qe and total Qs values:  [[-0.046]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.046]
 [-0.02 ]
 [-0.046]] [[45.618]
 [45.618]
 [45.618]
 [45.618]
 [45.618]
 [49.631]
 [45.618]] [[0.89 ]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [0.89 ]
 [1.089]
 [0.89 ]]
printing an ep nov before normalisation:  44.75159874147407
maxi score, test score, baseline:  -0.5478333333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5478333333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.846]
 [0.609]
 [0.646]
 [0.609]
 [0.608]
 [0.647]] [[42.033]
 [38.585]
 [43.091]
 [43.741]
 [42.403]
 [45.043]
 [41.815]] [[0.648]
 [0.846]
 [0.609]
 [0.646]
 [0.609]
 [0.608]
 [0.647]]
maxi score, test score, baseline:  -0.5478333333333334 0.6773333333333333 0.6773333333333333
actor:  0 policy actor:  0  step number:  50 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  59 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.3770, 0.0084, 0.1334, 0.1040, 0.1522, 0.1076, 0.1174],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0209, 0.9201, 0.0106, 0.0101, 0.0064, 0.0034, 0.0285],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0796, 0.0683, 0.3831, 0.1164, 0.0695, 0.1803, 0.1028],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0716, 0.1110, 0.0967, 0.4281, 0.0880, 0.1113, 0.0933],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1158, 0.0121, 0.0880, 0.1450, 0.4652, 0.0889, 0.0850],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0760, 0.0046, 0.1677, 0.0873, 0.0533, 0.5233, 0.0877],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1035, 0.2715, 0.1212, 0.1448, 0.1100, 0.1034, 0.1455],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.29034582780085
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  40.060134782154535
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.4845757484436
Printing some Q and Qe and total Qs values:  [[-0.111]
 [ 0.103]
 [ 0.061]
 [ 0.035]
 [-0.048]
 [-0.021]
 [-0.009]] [[36.184]
 [44.104]
 [37.628]
 [37.68 ]
 [40.677]
 [40.137]
 [38.485]] [[0.37 ]
 [0.843]
 [0.589]
 [0.565]
 [0.58 ]
 [0.59 ]
 [0.548]]
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
printing an ep nov before normalisation:  63.849613515259776
printing an ep nov before normalisation:  27.12557315826416
printing an ep nov before normalisation:  33.78004312515259
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.76700340698085
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.701726742435305
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
printing an ep nov before normalisation:  27.580357373391948
printing an ep nov before normalisation:  32.60346968088129
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  73.64898493854776
siam score:  -0.86047393
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.2567, 0.0262, 0.0967, 0.1162, 0.2526, 0.1306, 0.1210],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0195, 0.8736, 0.0197, 0.0229, 0.0156, 0.0128, 0.0359],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1350, 0.0417, 0.2967, 0.1097, 0.1228, 0.1523, 0.1419],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1581, 0.0326, 0.1428, 0.1942, 0.1411, 0.1310, 0.2001],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1652, 0.0362, 0.1189, 0.1229, 0.3063, 0.1107, 0.1397],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1158, 0.0394, 0.1313, 0.1053, 0.1260, 0.3576, 0.1247],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2115, 0.0427, 0.1315, 0.1527, 0.1379, 0.1300, 0.1936],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.279]
 [0.392]
 [0.282]
 [0.314]
 [0.345]
 [0.317]
 [0.308]] [[43.924]
 [41.793]
 [44.208]
 [43.532]
 [42.692]
 [43.655]
 [43.104]] [[1.389]
 [1.393]
 [1.406]
 [1.404]
 [1.392]
 [1.413]
 [1.376]]
printing an ep nov before normalisation:  52.62247975438083
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
actor:  1 policy actor:  1  step number:  46 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8594742
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  63 total reward:  0.25333333333333263  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.87966823577881
printing an ep nov before normalisation:  36.10942689312657
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.60450343597223
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]] [[51.337]
 [51.337]
 [51.337]
 [51.337]
 [51.337]
 [51.337]
 [51.337]] [[2.349]
 [2.349]
 [2.349]
 [2.349]
 [2.349]
 [2.349]
 [2.349]]
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5453133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.386729453549705
actor:  0 policy actor:  0  step number:  66 total reward:  0.03333333333333233  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  46.10213567987934
actor:  1 policy actor:  1  step number:  54 total reward:  0.2599999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5432466666666667 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5432466666666667 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.35711447564318
maxi score, test score, baseline:  -0.5432466666666667 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5432466666666667 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5432466666666667 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.44366991621365
printing an ep nov before normalisation:  29.86323895423865
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5432466666666667 0.6773333333333333 0.6773333333333333
actor:  1 policy actor:  1  step number:  53 total reward:  0.4133333333333329  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5432466666666667 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5432466666666667 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5432466666666667 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  46.38574051268134
siam score:  -0.8529991
printing an ep nov before normalisation:  20.614518547235882
maxi score, test score, baseline:  -0.5432466666666667 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5432466666666667 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5432466666666667 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5432466666666667 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5405533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.50009222194907
maxi score, test score, baseline:  -0.5405533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5405533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5405533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5405533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5405533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5405533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.4627742767334
actor:  1 policy actor:  1  step number:  54 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5405533333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5405533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  0.206666666666666  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5405533333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5405533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.72963891287775
maxi score, test score, baseline:  -0.5405533333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5405533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5405533333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5405533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5405533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0014562472279067151
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.5405533333333334 0.6773333333333333 0.6773333333333333
printing an ep nov before normalisation:  36.03111838877137
maxi score, test score, baseline:  -0.5405533333333334 0.6773333333333333 0.6773333333333333
printing an ep nov before normalisation:  36.445075861585345
maxi score, test score, baseline:  -0.5405533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5405533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5405533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.151929854610458
actor:  1 policy actor:  1  step number:  58 total reward:  0.1799999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5405533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5405533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5405533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actor:  0 policy actor:  0  step number:  57 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5381 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5381 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5381 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.38584541608034
maxi score, test score, baseline:  -0.5381 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5381 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.056283973680735
actor:  0 policy actor:  0  step number:  44 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5354466666666667 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.952704184061126
maxi score, test score, baseline:  -0.5354466666666667 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.34616418040715
maxi score, test score, baseline:  -0.5354466666666667 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.61189185800716
printing an ep nov before normalisation:  37.99680402226606
maxi score, test score, baseline:  -0.5354466666666667 0.6773333333333333 0.6773333333333333
actor:  0 policy actor:  0  step number:  45 total reward:  0.2933333333333331  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.53286 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.53286 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.53286 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.233735685463714
actor:  0 policy actor:  0  step number:  45 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.60255262441576
printing an ep nov before normalisation:  51.316122129898645
maxi score, test score, baseline:  -0.5300600000000001 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.32372578956607
maxi score, test score, baseline:  -0.5300600000000001 0.6773333333333333 0.6773333333333333
printing an ep nov before normalisation:  45.71808687092996
Printing some Q and Qe and total Qs values:  [[-0.033]
 [ 0.204]
 [-0.037]
 [-0.038]
 [-0.037]
 [-0.036]
 [-0.03 ]] [[31.805]
 [40.576]
 [32.732]
 [32.778]
 [32.462]
 [32.443]
 [31.508]] [[0.408]
 [0.889]
 [0.43 ]
 [0.43 ]
 [0.423]
 [0.424]
 [0.403]]
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.397]
 [0.33 ]
 [0.337]
 [0.337]
 [0.338]
 [0.337]] [[47.477]
 [38.295]
 [49.039]
 [47.477]
 [47.477]
 [50.384]
 [47.477]] [[0.933]
 [0.768]
 [0.964]
 [0.933]
 [0.933]
 [1.005]
 [0.933]]
actions average: 
K:  3  action  0 :  tensor([0.2758, 0.1090, 0.1261, 0.0989, 0.1302, 0.1258, 0.1343],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0098, 0.9061, 0.0098, 0.0212, 0.0109, 0.0096, 0.0325],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0973, 0.0184, 0.3427, 0.1535, 0.1224, 0.1447, 0.1208],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0635, 0.1064, 0.0847, 0.4718, 0.1040, 0.0770, 0.0925],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1215, 0.0187, 0.1262, 0.1127, 0.3291, 0.1374, 0.1546],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1210, 0.0207, 0.1919, 0.1183, 0.1377, 0.2508, 0.1596],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1398, 0.1311, 0.1572, 0.1392, 0.1497, 0.1517, 0.1314],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.704346817442854
maxi score, test score, baseline:  -0.5300600000000001 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5300600000000001 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.41333333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5300600000000001 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5300600000000001 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.56417708541693
maxi score, test score, baseline:  -0.5300600000000001 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5300600000000001 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5300600000000001 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5300600000000001 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.86811511194219
maxi score, test score, baseline:  -0.5300600000000001 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.273]
 [ 0.341]
 [ 0.273]
 [ 0.27 ]
 [ 0.187]
 [-0.022]
 [ 0.273]] [[43.57 ]
 [50.812]
 [43.57 ]
 [40.667]
 [43.353]
 [42.1  ]
 [43.57 ]] [[1.459]
 [1.891]
 [1.459]
 [1.31 ]
 [1.362]
 [1.091]
 [1.459]]
Printing some Q and Qe and total Qs values:  [[-0.093]
 [-0.093]
 [-0.093]
 [-0.093]
 [-0.093]
 [-0.098]
 [-0.093]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.093]
 [-0.093]
 [-0.093]
 [-0.093]
 [-0.093]
 [-0.098]
 [-0.093]]
maxi score, test score, baseline:  -0.5300600000000001 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5300600000000001 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.18477213441134
printing an ep nov before normalisation:  36.396241188049316
maxi score, test score, baseline:  -0.5300600000000001 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5300600000000001 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.10825738825481
actions average: 
K:  3  action  0 :  tensor([0.1996, 0.0082, 0.1426, 0.1423, 0.2158, 0.1442, 0.1474],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0134, 0.9317, 0.0081, 0.0121, 0.0034, 0.0038, 0.0276],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1003, 0.0589, 0.3675, 0.1122, 0.1008, 0.1455, 0.1147],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0867, 0.1291, 0.0999, 0.3320, 0.0963, 0.1171, 0.1390],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1918, 0.0190, 0.1268, 0.1407, 0.2826, 0.1250, 0.1141],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0989, 0.0147, 0.2266, 0.1201, 0.1096, 0.3172, 0.1130],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0951, 0.2348, 0.1291, 0.1297, 0.1067, 0.1376, 0.1669],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5300600000000001 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.3258, 0.0503, 0.1240, 0.1183, 0.1402, 0.1153, 0.1261],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0047, 0.9526, 0.0066, 0.0084, 0.0035, 0.0031, 0.0211],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1521, 0.0017, 0.2401, 0.1360, 0.1437, 0.1922, 0.1342],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1275, 0.0160, 0.1328, 0.3144, 0.1168, 0.1268, 0.1656],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1504, 0.0076, 0.1016, 0.1155, 0.4027, 0.1007, 0.1214],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1357, 0.0031, 0.1801, 0.1440, 0.1379, 0.2641, 0.1351],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1580, 0.0168, 0.1534, 0.1521, 0.1413, 0.1457, 0.2327],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5300600000000001 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5300600000000001 0.6773333333333333 0.6773333333333333
UNIT TEST: sample policy line 217 mcts : [0.041 0.306 0.306 0.041 0.061 0.204 0.041]
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.434]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]] [[36.05]
 [42.35]
 [36.05]
 [36.05]
 [36.05]
 [36.05]
 [36.05]] [[1.329]
 [1.749]
 [1.329]
 [1.329]
 [1.329]
 [1.329]
 [1.329]]
maxi score, test score, baseline:  -0.5300600000000001 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  1.0
siam score:  -0.85461485
maxi score, test score, baseline:  -0.5300600000000001 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5300600000000001 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.836]
 [0.927]
 [0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.836]] [[44.803]
 [37.101]
 [44.803]
 [44.803]
 [44.803]
 [44.803]
 [44.803]] [[0.836]
 [0.927]
 [0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.836]]
maxi score, test score, baseline:  -0.5300600000000001 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.55535877494865
printing an ep nov before normalisation:  32.7756172311199
maxi score, test score, baseline:  -0.5300600000000001 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  57 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  86.8665486458752
maxi score, test score, baseline:  -0.52758 0.6773333333333333 0.6773333333333333
printing an ep nov before normalisation:  39.45666292226711
maxi score, test score, baseline:  -0.52758 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.52758 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  1.657693246670533e-05
maxi score, test score, baseline:  -0.52758 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.54299068450928
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.52758 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.52758 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.52758 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.086]
 [-0.077]
 [-0.086]
 [-0.086]
 [-0.086]
 [-0.086]
 [-0.086]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.086]
 [-0.077]
 [-0.086]
 [-0.086]
 [-0.086]
 [-0.086]
 [-0.086]]
maxi score, test score, baseline:  -0.52758 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.08122975090932
maxi score, test score, baseline:  -0.52758 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.52758 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.067]
 [-0.09 ]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]
 [-0.067]] [[15.608]
 [29.258]
 [15.608]
 [15.608]
 [15.608]
 [15.608]
 [15.608]] [[0.109]
 [0.241]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]]
maxi score, test score, baseline:  -0.52758 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  51 total reward:  0.4133333333333329  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5247533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.4563, 0.0118, 0.0820, 0.0978, 0.1381, 0.1190, 0.0951],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0018,     0.9679,     0.0031,     0.0033,     0.0003,     0.0010,
            0.0226], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1242, 0.0150, 0.3409, 0.1066, 0.1250, 0.1807, 0.1076],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1245, 0.0272, 0.1417, 0.2990, 0.1234, 0.1447, 0.1395],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1282, 0.0178, 0.1420, 0.1338, 0.2863, 0.1578, 0.1341],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1405, 0.0142, 0.1669, 0.1254, 0.1053, 0.2917, 0.1560],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1097, 0.1843, 0.1247, 0.1262, 0.1147, 0.1280, 0.2123],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5247533333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5247533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.64524917349431
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5247533333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5247533333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5247533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5247533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5247533333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5247533333333334 0.6773333333333333 0.6773333333333333
actions average: 
K:  2  action  0 :  tensor([0.4380, 0.0204, 0.0893, 0.1024, 0.1249, 0.0923, 0.1328],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0053, 0.9340, 0.0112, 0.0135, 0.0030, 0.0071, 0.0259],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1247, 0.0453, 0.4150, 0.0919, 0.0888, 0.1065, 0.1277],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1519, 0.0169, 0.1491, 0.2406, 0.1376, 0.1630, 0.1409],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1133, 0.0113, 0.0733, 0.0963, 0.5418, 0.0904, 0.0736],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1369, 0.0172, 0.1883, 0.1201, 0.1097, 0.3108, 0.1170],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1401, 0.0685, 0.1413, 0.1388, 0.1236, 0.1854, 0.2024],
       grad_fn=<DivBackward0>)
actions average: 
K:  4  action  0 :  tensor([0.2663, 0.0299, 0.1360, 0.1321, 0.1002, 0.1666, 0.1689],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0234, 0.8995, 0.0166, 0.0158, 0.0063, 0.0100, 0.0284],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1895, 0.0266, 0.2881, 0.1067, 0.1010, 0.1129, 0.1752],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1337, 0.0028, 0.1434, 0.3118, 0.1293, 0.1394, 0.1396],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1123, 0.0200, 0.0590, 0.0859, 0.5955, 0.0620, 0.0653],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1358, 0.0327, 0.1992, 0.1351, 0.1388, 0.2249, 0.1334],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1636, 0.0581, 0.1367, 0.0988, 0.0941, 0.1241, 0.3246],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]
 [0.367]] [[50.041]
 [50.041]
 [50.041]
 [50.041]
 [50.041]
 [50.041]
 [50.041]] [[2.213]
 [2.213]
 [2.213]
 [2.213]
 [2.213]
 [2.213]
 [2.213]]
Printing some Q and Qe and total Qs values:  [[-0.105]
 [-0.08 ]
 [-0.103]
 [-0.104]
 [-0.104]
 [-0.093]
 [-0.101]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.105]
 [-0.08 ]
 [-0.103]
 [-0.104]
 [-0.104]
 [-0.093]
 [-0.101]]
printing an ep nov before normalisation:  50.40463868091099
line 256 mcts: sample exp_bonus 28.26839570102478
printing an ep nov before normalisation:  42.13670803960252
printing an ep nov before normalisation:  46.004463920998795
maxi score, test score, baseline:  -0.5247533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.967282722172136
maxi score, test score, baseline:  -0.5247533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5247533333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5247533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5247533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  46 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.06 ]
 [-0.027]
 [-0.064]
 [-0.06 ]
 [-0.064]
 [-0.061]
 [-0.053]] [[28.436]
 [32.364]
 [28.179]
 [28.175]
 [28.811]
 [28.252]
 [28.825]] [[0.292]
 [0.421]
 [0.281]
 [0.285]
 [0.297]
 [0.287]
 [0.308]]
Printing some Q and Qe and total Qs values:  [[-0.062]
 [-0.036]
 [-0.06 ]
 [-0.06 ]
 [-0.06 ]
 [-0.054]
 [-0.059]] [[28.518]
 [31.199]
 [27.779]
 [28.175]
 [28.169]
 [30.094]
 [28.564]] [[0.291]
 [0.383]
 [0.275]
 [0.285]
 [0.285]
 [0.338]
 [0.296]]
maxi score, test score, baseline:  -0.5220466666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.038]
 [ 0.325]
 [ 0.038]
 [ 0.038]
 [ 0.056]
 [-0.05 ]
 [-0.013]] [[42.498]
 [44.874]
 [42.498]
 [42.498]
 [45.024]
 [42.502]
 [39.547]] [[0.482]
 [0.814]
 [0.482]
 [0.482]
 [0.548]
 [0.394]
 [0.375]]
maxi score, test score, baseline:  -0.5220466666666668 0.6773333333333333 0.6773333333333333
printing an ep nov before normalisation:  45.61021566653749
maxi score, test score, baseline:  -0.5220466666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.61950472339557
printing an ep nov before normalisation:  27.93928861618042
actor:  1 policy actor:  1  step number:  63 total reward:  0.25333333333333263  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  58 total reward:  0.17999999999999938  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  30.268094280720092
Printing some Q and Qe and total Qs values:  [[0.259]
 [0.453]
 [0.206]
 [0.202]
 [0.228]
 [0.398]
 [0.195]] [[42.147]
 [40.328]
 [36.382]
 [35.896]
 [38.409]
 [39.474]
 [35.575]] [[0.259]
 [0.453]
 [0.206]
 [0.202]
 [0.228]
 [0.398]
 [0.195]]
printing an ep nov before normalisation:  55.87226779478793
printing an ep nov before normalisation:  36.313228607177734
printing an ep nov before normalisation:  56.24052461646712
Printing some Q and Qe and total Qs values:  [[0.976]
 [0.986]
 [0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]] [[28.93 ]
 [31.348]
 [28.93 ]
 [28.93 ]
 [28.93 ]
 [28.93 ]
 [28.93 ]] [[0.976]
 [0.986]
 [0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]]
maxi score, test score, baseline:  -0.5220466666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  41 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8565696
maxi score, test score, baseline:  -0.5190333333333335 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.17128849029541
Printing some Q and Qe and total Qs values:  [[-0.021]
 [-0.008]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]] [[50.544]
 [44.153]
 [50.544]
 [50.544]
 [50.544]
 [50.544]
 [50.544]] [[0.646]
 [0.495]
 [0.646]
 [0.646]
 [0.646]
 [0.646]
 [0.646]]
printing an ep nov before normalisation:  49.1449491292042
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.5190333333333335 0.6773333333333333 0.6773333333333333
actor:  1 policy actor:  1  step number:  57 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  13.422368525256172
maxi score, test score, baseline:  -0.5190333333333335 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5190333333333335 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5190333333333335 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.58460533151095
maxi score, test score, baseline:  -0.5190333333333335 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5190333333333335 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5190333333333335 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.816]
 [0.734]
 [0.699]
 [0.658]
 [0.615]
 [0.76 ]] [[41.487]
 [36.1  ]
 [45.144]
 [40.856]
 [41.786]
 [44.013]
 [42.067]] [[0.434]
 [0.816]
 [0.734]
 [0.699]
 [0.658]
 [0.615]
 [0.76 ]]
siam score:  -0.8544156
Printing some Q and Qe and total Qs values:  [[-0.094]
 [-0.094]
 [-0.094]
 [-0.094]
 [-0.094]
 [-0.094]
 [-0.094]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.094]
 [-0.094]
 [-0.094]
 [-0.094]
 [-0.094]
 [-0.094]
 [-0.094]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5190333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.852823
maxi score, test score, baseline:  -0.5190333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5190333333333333 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5190333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5190333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5190333333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  53 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.037]
 [-0.013]
 [-0.04 ]
 [-0.039]
 [-0.039]
 [-0.039]
 [-0.038]] [[27.677]
 [43.589]
 [27.448]
 [27.309]
 [27.365]
 [27.349]
 [27.362]] [[0.338]
 [0.863]
 [0.328]
 [0.324]
 [0.326]
 [0.326]
 [0.327]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.5166066666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  36 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.5135533333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.2300, 0.0418, 0.1441, 0.1442, 0.1367, 0.1364, 0.1668],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0236, 0.8699, 0.0193, 0.0260, 0.0076, 0.0041, 0.0495],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1092, 0.1146, 0.2036, 0.1690, 0.1296, 0.1500, 0.1241],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1423, 0.1437, 0.1367, 0.1481, 0.1446, 0.1215, 0.1631],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1537, 0.1070, 0.1056, 0.1267, 0.2894, 0.0914, 0.1261],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1041, 0.0095, 0.1572, 0.1324, 0.1187, 0.3498, 0.1283],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1490, 0.0727, 0.1076, 0.1520, 0.1467, 0.1254, 0.2466],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5135533333333333 0.6773333333333333 0.6773333333333333
printing an ep nov before normalisation:  35.16536474227905
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.535]
 [0.451]
 [0.437]
 [0.386]
 [0.373]
 [0.464]] [[64.473]
 [50.853]
 [57.799]
 [59.296]
 [61.259]
 [58.673]
 [56.852]] [[1.764]
 [1.411]
 [1.527]
 [1.555]
 [1.561]
 [1.474]
 [1.513]]
printing an ep nov before normalisation:  31.165729613755634
maxi score, test score, baseline:  -0.5135533333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5135533333333333 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.127784163637514
printing an ep nov before normalisation:  50.16371618984488
actor:  1 policy actor:  1  step number:  43 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8581079
printing an ep nov before normalisation:  45.82662469976576
printing an ep nov before normalisation:  0.053866275596590185
siam score:  -0.8583453
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  12.031588554382324
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.52817152519891
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.39999999999999947  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.656]
 [0.391]
 [0.391]
 [0.493]
 [0.391]
 [0.47 ]] [[36.827]
 [37.277]
 [36.827]
 [36.827]
 [36.836]
 [36.827]
 [37.136]] [[1.08 ]
 [1.361]
 [1.08 ]
 [1.08 ]
 [1.182]
 [1.08 ]
 [1.171]]
printing an ep nov before normalisation:  40.12377806547086
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.2674, 0.0169, 0.1552, 0.1435, 0.1423, 0.1552, 0.1197],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0096, 0.9344, 0.0089, 0.0110, 0.0026, 0.0028, 0.0307],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1052, 0.0139, 0.3111, 0.1538, 0.1112, 0.1888, 0.1160],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1281, 0.0045, 0.1467, 0.2931, 0.1394, 0.1632, 0.1251],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1023, 0.0066, 0.0818, 0.0837, 0.5714, 0.0962, 0.0580],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1139, 0.0270, 0.1297, 0.1806, 0.1524, 0.3101, 0.0862],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1659, 0.0270, 0.1801, 0.1699, 0.1407, 0.1767, 0.1397],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  60 total reward:  0.37999999999999934  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  78.26604639179355
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.901583454459534
Printing some Q and Qe and total Qs values:  [[-0.013]
 [-0.013]
 [-0.013]
 [-0.013]
 [-0.022]
 [-0.013]
 [-0.013]] [[50.086]
 [50.086]
 [50.086]
 [50.086]
 [47.721]
 [50.086]
 [50.086]] [[1.625]
 [1.625]
 [1.625]
 [1.625]
 [1.494]
 [1.625]
 [1.625]]
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.301036495971275
printing an ep nov before normalisation:  48.681759657941306
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
printing an ep nov before normalisation:  36.483028337240405
maxi score, test score, baseline:  -0.5135533333333334 0.6773333333333333 0.6773333333333333
actor:  1 policy actor:  1  step number:  54 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  58 total reward:  0.1933333333333327  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5111666666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.085]
 [-0.106]
 [-0.085]
 [-0.085]
 [-0.085]
 [-0.085]
 [-0.085]] [[41.692]
 [61.75 ]
 [41.692]
 [41.692]
 [41.692]
 [41.692]
 [41.692]] [[0.116]
 [0.352]
 [0.116]
 [0.116]
 [0.116]
 [0.116]
 [0.116]]
maxi score, test score, baseline:  -0.5111666666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5111666666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5111666666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5111666666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.07589080706629
maxi score, test score, baseline:  -0.5111666666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5111666666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.932027638388334
maxi score, test score, baseline:  -0.5111666666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.20500993728638
maxi score, test score, baseline:  -0.5111666666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.230338176202636
actor:  1 policy actor:  1  step number:  57 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]] [[52.086]
 [52.086]
 [52.086]
 [52.086]
 [52.086]
 [52.086]
 [52.086]] [[1.852]
 [1.852]
 [1.852]
 [1.852]
 [1.852]
 [1.852]
 [1.852]]
maxi score, test score, baseline:  -0.5111666666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.32132503850131
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.795]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]] [[33.362]
 [44.908]
 [33.362]
 [33.362]
 [33.362]
 [33.362]
 [33.362]] [[0.692]
 [0.795]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]]
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.83 ]
 [0.703]
 [0.701]
 [0.697]
 [0.696]
 [0.69 ]] [[45.926]
 [41.978]
 [46.88 ]
 [46.937]
 [45.374]
 [45.84 ]
 [45.302]] [[0.735]
 [0.83 ]
 [0.703]
 [0.701]
 [0.697]
 [0.696]
 [0.69 ]]
actor:  0 policy actor:  0  step number:  45 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5084733333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5084733333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5084733333333334 0.6773333333333333 0.6773333333333333
printing an ep nov before normalisation:  43.64656570939286
Printing some Q and Qe and total Qs values:  [[-0.017]
 [-0.   ]
 [-0.017]
 [-0.017]
 [-0.016]
 [-0.016]
 [ 0.003]] [[30.35 ]
 [37.453]
 [31.644]
 [31.334]
 [31.258]
 [31.127]
 [36.103]] [[0.697]
 [1.144]
 [0.775]
 [0.757]
 [0.753]
 [0.745]
 [1.065]]
maxi score, test score, baseline:  -0.5084733333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5084733333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5084733333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.3552360534668
maxi score, test score, baseline:  -0.5084733333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.1876, 0.0816, 0.1699, 0.1148, 0.1834, 0.1062, 0.1565],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0032, 0.9601, 0.0034, 0.0042, 0.0011, 0.0011, 0.0269],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1069, 0.0366, 0.3374, 0.1261, 0.1115, 0.1383, 0.1434],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1042, 0.0109, 0.1265, 0.3079, 0.1530, 0.1143, 0.1832],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1180, 0.0342, 0.1126, 0.1169, 0.3937, 0.1044, 0.1202],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1132, 0.0086, 0.1826, 0.1426, 0.1308, 0.2956, 0.1267],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1164, 0.1800, 0.1235, 0.1556, 0.1038, 0.0925, 0.2282],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.624]
 [0.617]
 [0.628]
 [0.632]
 [0.637]
 [0.629]
 [0.629]] [[32.322]
 [28.32 ]
 [35.821]
 [35.675]
 [36.994]
 [39.587]
 [32.585]] [[0.624]
 [0.617]
 [0.628]
 [0.632]
 [0.637]
 [0.629]
 [0.629]]
siam score:  -0.8632722
actor:  0 policy actor:  0  step number:  46 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5059533333333335 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5059533333333335 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.185685630382324
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.651]
 [0.651]
 [0.512]
 [0.651]
 [0.512]
 [0.651]] [[13.341]
 [12.861]
 [13.563]
 [23.372]
 [13.654]
 [23.372]
 [13.004]] [[1.302]
 [1.279]
 [1.313]
 [1.654]
 [1.318]
 [1.654]
 [1.286]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.2533333333333331  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.5059533333333335 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.798]
 [0.845]
 [0.743]
 [0.744]
 [0.754]
 [0.745]
 [0.75 ]] [[27.55 ]
 [29.696]
 [25.155]
 [25.033]
 [24.989]
 [25.413]
 [25.177]] [[0.798]
 [0.845]
 [0.743]
 [0.744]
 [0.754]
 [0.745]
 [0.75 ]]
maxi score, test score, baseline:  -0.5059533333333335 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5059533333333335 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8632016
actor:  0 policy actor:  0  step number:  42 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.5030866666666667 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5030866666666667 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.65022194303724
line 256 mcts: sample exp_bonus 46.636623443126375
maxi score, test score, baseline:  -0.5030866666666667 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5030866666666667 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5030866666666667 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8623567
actor:  1 policy actor:  1  step number:  77 total reward:  0.1866666666666651  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  52.34753933907834
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.93121023718835
maxi score, test score, baseline:  -0.5030866666666667 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.340784087414676
maxi score, test score, baseline:  -0.5030866666666667 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5030866666666667 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.331662492785405
actions average: 
K:  2  action  0 :  tensor([0.1884, 0.0826, 0.1545, 0.1346, 0.1413, 0.1660, 0.1327],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0138, 0.9069, 0.0188, 0.0136, 0.0074, 0.0090, 0.0305],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0837, 0.0195, 0.3358, 0.1361, 0.1193, 0.1871, 0.1185],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1009, 0.0780, 0.0999, 0.3509, 0.1067, 0.1237, 0.1399],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0954, 0.0052, 0.0851, 0.0837, 0.5542, 0.1047, 0.0717],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1019, 0.0178, 0.1928, 0.1254, 0.1504, 0.2862, 0.1255],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1047, 0.2854, 0.1025, 0.1399, 0.0839, 0.0996, 0.1842],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.5030866666666667 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  57 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  2.0
siam score:  -0.85001874
maxi score, test score, baseline:  -0.5006066666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.59500903133281
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5006066666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.934781477427276
maxi score, test score, baseline:  -0.5006066666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5006066666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.84444916
maxi score, test score, baseline:  -0.5006066666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5006066666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5006066666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5006066666666668 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.5006066666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5006066666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  0.29333333333333256  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5006066666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5006066666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.163 0.163 0.082 0.143 0.102 0.286 0.061]
maxi score, test score, baseline:  -0.5006066666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 44.79064060261508
actor:  1 policy actor:  1  step number:  69 total reward:  0.14666666666666617  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.5006066666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.65481592339865
maxi score, test score, baseline:  -0.5006066666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5006066666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5006066666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.5006066666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  61 total reward:  0.23999999999999977  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8635897
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
printing an ep nov before normalisation:  37.00134876335784
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.068]
 [-0.033]
 [-0.068]
 [-0.089]
 [-0.068]
 [-0.068]
 [-0.082]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.068]
 [-0.033]
 [-0.068]
 [-0.089]
 [-0.068]
 [-0.068]
 [-0.082]]
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.019]
 [-0.036]
 [-0.037]
 [-0.036]
 [-0.041]
 [-0.034]] [[44.062]
 [40.5  ]
 [43.9  ]
 [44.481]
 [43.999]
 [44.278]
 [42.439]] [[0.788]
 [0.676]
 [0.782]
 [0.801]
 [0.786]
 [0.791]
 [0.732]]
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.348513969938566
printing an ep nov before normalisation:  39.85454836522093
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
actor:  1 policy actor:  1  step number:  51 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
printing an ep nov before normalisation:  48.55459441749473
printing an ep nov before normalisation:  39.857471585455166
siam score:  -0.8673984
printing an ep nov before normalisation:  39.25799648385632
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.09749989055586
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.3199999999999993  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.175]
 [-0.175]
 [-0.175]
 [-0.154]
 [-0.175]
 [-0.106]
 [-0.175]] [[38.639]
 [38.639]
 [38.639]
 [42.313]
 [38.639]
 [42.481]
 [38.639]] [[0.759]
 [0.759]
 [0.759]
 [0.985]
 [0.759]
 [1.044]
 [0.759]]
siam score:  -0.87091887
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.41099262237549
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.475388700690004
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.870103
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.1327317181772
printing an ep nov before normalisation:  37.578281057363654
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.973905515632595
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.381292998070364
printing an ep nov before normalisation:  58.03001686719974
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.582]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]] [[36.89]
 [42.23]
 [36.89]
 [36.89]
 [36.89]
 [36.89]
 [36.89]] [[0.897]
 [1.208]
 [0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.897]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  68 total reward:  0.13999999999999913  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.089]
 [-0.089]
 [-0.089]
 [-0.089]
 [-0.089]
 [-0.089]
 [-0.089]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.089]
 [-0.089]
 [-0.089]
 [-0.089]
 [-0.089]
 [-0.089]
 [-0.089]]
Printing some Q and Qe and total Qs values:  [[-0.111]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.111]
 [-0.058]
 [-0.111]] [[47.877]
 [47.877]
 [47.877]
 [47.877]
 [47.877]
 [50.241]
 [47.877]] [[0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.532]
 [0.436]]
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.02  0.184 0.041 0.143 0.367 0.143 0.102]
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.18591832026315
using explorer policy with actor:  1
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actor:  1 policy actor:  1  step number:  43 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.04022576748669
printing an ep nov before normalisation:  40.512681457738296
siam score:  -0.86722106
printing an ep nov before normalisation:  16.910123825073242
actor:  1 policy actor:  1  step number:  54 total reward:  0.09999999999999942  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  62 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.458]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.366]
 [0.458]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]]
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.39978322432388
printing an ep nov before normalisation:  45.97445487976074
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.694]
 [0.653]
 [0.637]
 [0.629]
 [0.642]
 [0.611]] [[44.549]
 [42.224]
 [44.878]
 [45.822]
 [44.631]
 [44.524]
 [45.323]] [[0.606]
 [0.694]
 [0.653]
 [0.637]
 [0.629]
 [0.642]
 [0.611]]
printing an ep nov before normalisation:  39.205147673307344
maxi score, test score, baseline:  -0.4981266666666668 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  52 total reward:  0.20666666666666633  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.085]
 [-0.085]
 [-0.085]
 [-0.085]
 [-0.085]
 [-0.085]
 [-0.085]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.085]
 [-0.085]
 [-0.085]
 [-0.085]
 [-0.085]
 [-0.085]
 [-0.085]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.4957133333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.4957133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4957133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4957133333333334 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.4957133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.195]
 [0.225]
 [0.195]
 [0.045]
 [0.195]
 [0.195]
 [0.044]] [[35.763]
 [45.461]
 [35.763]
 [43.295]
 [35.763]
 [35.763]
 [37.683]] [[0.79 ]
 [1.153]
 [0.79 ]
 [0.898]
 [0.79 ]
 [0.79 ]
 [0.705]]
maxi score, test score, baseline:  -0.4957133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.4957133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4957133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.269]
 [0.576]
 [0.273]
 [0.29 ]
 [0.274]
 [0.299]
 [0.316]] [[42.996]
 [33.803]
 [45.741]
 [47.339]
 [50.966]
 [53.214]
 [46.311]] [[0.962]
 [0.979]
 [1.052]
 [1.119]
 [1.217]
 [1.313]
 [1.113]]
printing an ep nov before normalisation:  39.081034594203906
maxi score, test score, baseline:  -0.4957133333333334 0.6773333333333333 0.6773333333333333
actor:  1 policy actor:  1  step number:  69 total reward:  0.11999999999999944  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.105]
 [-0.086]
 [-0.105]
 [-0.105]
 [-0.083]
 [-0.105]
 [-0.084]] [[ 0.   ]
 [51.658]
 [ 0.   ]
 [ 0.   ]
 [50.681]
 [ 0.   ]
 [49.645]] [[-0.313]
 [ 0.429]
 [-0.313]
 [-0.313]
 [ 0.418]
 [-0.313]
 [ 0.403]]
printing an ep nov before normalisation:  63.91568127039566
printing an ep nov before normalisation:  75.29168117977713
printing an ep nov before normalisation:  48.946775349018644
printing an ep nov before normalisation:  48.00749501927046
using explorer policy with actor:  1
printing an ep nov before normalisation:  14.127277089717175
actions average: 
K:  1  action  0 :  tensor([0.3437, 0.0059, 0.1026, 0.1304, 0.1401, 0.1307, 0.1466],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0056, 0.9472, 0.0073, 0.0085, 0.0025, 0.0039, 0.0250],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1307, 0.0061, 0.2143, 0.1383, 0.1329, 0.2340, 0.1436],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1187, 0.0136, 0.1167, 0.3139, 0.1199, 0.1607, 0.1564],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0910, 0.0084, 0.0419, 0.0585, 0.6719, 0.0685, 0.0598],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1197, 0.0808, 0.1718, 0.1173, 0.1130, 0.2720, 0.1255],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1289, 0.1410, 0.1142, 0.1312, 0.1138, 0.1235, 0.2474],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.4957133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4957133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 42.987864342929576
actor:  1 policy actor:  1  step number:  56 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  48 total reward:  0.4199999999999995  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.4957133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4957133333333334 0.6773333333333333 0.6773333333333333
printing an ep nov before normalisation:  40.38844585418701
printing an ep nov before normalisation:  47.19730434410476
maxi score, test score, baseline:  -0.4957133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4957133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4957133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4957133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.740704382454396
printing an ep nov before normalisation:  39.921586653054604
printing an ep nov before normalisation:  47.83467310383685
actor:  1 policy actor:  1  step number:  48 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  34.90653038024902
maxi score, test score, baseline:  -0.4957133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.662]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]] [[36.153]
 [43.815]
 [36.153]
 [36.153]
 [36.153]
 [36.153]
 [36.153]] [[0.59 ]
 [0.662]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.59 ]]
maxi score, test score, baseline:  -0.4957133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.240357801559178
Printing some Q and Qe and total Qs values:  [[0.69]
 [0.69]
 [0.69]
 [0.69]
 [0.69]
 [0.69]
 [0.69]] [[39.041]
 [39.041]
 [39.041]
 [39.041]
 [39.041]
 [39.041]
 [39.041]] [[0.69]
 [0.69]
 [0.69]
 [0.69]
 [0.69]
 [0.69]
 [0.69]]
printing an ep nov before normalisation:  36.645119212239244
printing an ep nov before normalisation:  33.86139474005333
printing an ep nov before normalisation:  43.05630969873474
printing an ep nov before normalisation:  32.610916760941805
actor:  1 policy actor:  1  step number:  59 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  30.51868036337586
printing an ep nov before normalisation:  30.468842174265966
maxi score, test score, baseline:  -0.4957133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4957133333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  62 total reward:  0.04666666666666608  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.4936200000000001 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4936200000000001 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8624761
maxi score, test score, baseline:  -0.4936200000000001 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4936200000000001 0.6773333333333333 0.6773333333333333
actor:  1 policy actor:  1  step number:  67 total reward:  0.0933333333333326  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  35.38020133972168
maxi score, test score, baseline:  -0.4936200000000001 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.88514507607719
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
Printing some Q and Qe and total Qs values:  [[0.609]
 [0.641]
 [0.557]
 [0.533]
 [0.539]
 [0.555]
 [0.579]] [[13.148]
 [13.895]
 [13.564]
 [13.715]
 [13.619]
 [13.456]
 [13.735]] [[2.268]
 [2.395]
 [2.269]
 [2.264]
 [2.258]
 [2.253]
 [2.313]]
printing an ep nov before normalisation:  16.70111171303816
printing an ep nov before normalisation:  38.52425814513639
Printing some Q and Qe and total Qs values:  [[-0.089]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.09 ]
 [-0.087]
 [-0.087]] [[22.665]
 [50.497]
 [17.476]
 [17.709]
 [35.24 ]
 [17.851]
 [18.239]] [[-0.017]
 [ 0.128]
 [-0.042]
 [-0.04 ]
 [ 0.047]
 [-0.039]
 [-0.037]]
printing an ep nov before normalisation:  23.03354731010422
actor:  0 policy actor:  0  step number:  56 total reward:  0.2066666666666659  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.49120666666666674 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.49120666666666674 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.458626219626524
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.4  ]
 [0.362]
 [0.328]
 [0.353]
 [0.313]
 [0.457]] [[41.389]
 [45.273]
 [42.896]
 [42.481]
 [42.466]
 [44.482]
 [41.022]] [[0.554]
 [0.655]
 [0.594]
 [0.556]
 [0.581]
 [0.56 ]
 [0.672]]
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]] [[38.735]
 [38.735]
 [38.735]
 [38.735]
 [38.735]
 [38.735]
 [38.735]] [[13.244]
 [13.244]
 [13.244]
 [13.244]
 [13.244]
 [13.244]
 [13.244]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.132223701916104
printing an ep nov before normalisation:  54.187278747558594
actor:  1 policy actor:  1  step number:  67 total reward:  0.22666666666666602  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.49120666666666674 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.57552321290122
maxi score, test score, baseline:  -0.49120666666666674 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8630421
printing an ep nov before normalisation:  32.77038335800171
actor:  1 policy actor:  1  step number:  64 total reward:  0.2866666666666665  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.49120666666666674 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.64182121192148
printing an ep nov before normalisation:  37.05965149534563
maxi score, test score, baseline:  -0.49120666666666674 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.49120666666666674 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.49120666666666674 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.49120666666666674 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.75]
 [0.75]
 [0.75]
 [0.75]
 [0.75]
 [0.75]
 [0.75]] [[38.746]
 [38.746]
 [38.746]
 [38.746]
 [38.746]
 [38.746]
 [38.746]] [[1.819]
 [1.819]
 [1.819]
 [1.819]
 [1.819]
 [1.819]
 [1.819]]
maxi score, test score, baseline:  -0.49120666666666674 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.49120666666666674 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.49120666666666674 0.6773333333333333 0.6773333333333333
printing an ep nov before normalisation:  6.663257877031962
Printing some Q and Qe and total Qs values:  [[-0.029]
 [ 0.028]
 [-0.021]
 [-0.022]
 [ 0.007]
 [-0.024]
 [ 0.001]] [[22.101]
 [37.454]
 [23.918]
 [23.905]
 [33.794]
 [22.542]
 [33.592]] [[0.071]
 [0.266]
 [0.096]
 [0.095]
 [0.212]
 [0.08 ]
 [0.205]]
printing an ep nov before normalisation:  26.918354034423828
maxi score, test score, baseline:  -0.49120666666666674 0.6773333333333333 0.6773333333333333
actor:  0 policy actor:  0  step number:  46 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.48834000000000005 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.48834000000000005 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.096]
 [-0.039]
 [-0.096]
 [-0.096]
 [-0.096]
 [-0.083]
 [-0.056]] [[ 0.   ]
 [49.235]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [50.553]
 [47.766]] [[-0.459]
 [ 0.378]
 [-0.459]
 [-0.459]
 [-0.459]
 [ 0.355]
 [ 0.338]]
maxi score, test score, baseline:  -0.48834000000000005 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.18628474317084
maxi score, test score, baseline:  -0.48834000000000005 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.48834000000000005 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85992223
printing an ep nov before normalisation:  47.354960970791794
printing an ep nov before normalisation:  40.45825835282482
maxi score, test score, baseline:  -0.48834000000000005 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.48834000000000005 0.6773333333333333 0.6773333333333333
maxi score, test score, baseline:  -0.48834000000000005 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.63055683988
maxi score, test score, baseline:  -0.48834000000000005 0.6773333333333333 0.6773333333333333
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]
 [0.289]] [[37.797]
 [37.797]
 [37.797]
 [37.797]
 [37.797]
 [37.797]
 [37.797]] [[1.603]
 [1.603]
 [1.603]
 [1.603]
 [1.603]
 [1.603]
 [1.603]]
printing an ep nov before normalisation:  43.79690254475691
actor:  1 policy actor:  1  step number:  64 total reward:  0.09999999999999942  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.48834000000000005 0.6773333333333333 0.6773333333333333
printing an ep nov before normalisation:  34.59291458129883
actor:  1 policy actor:  1  step number:  62 total reward:  0.16666666666666607  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  4  action  0 :  tensor([0.4024, 0.0304, 0.1141, 0.0531, 0.2344, 0.0629, 0.1027],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0140, 0.8964, 0.0154, 0.0175, 0.0108, 0.0116, 0.0343],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1126, 0.1407, 0.2008, 0.1158, 0.1340, 0.1594, 0.1366],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1182, 0.0217, 0.1270, 0.2695, 0.1722, 0.1297, 0.1618],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0905, 0.0352, 0.0722, 0.0961, 0.5504, 0.0732, 0.0824],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1219, 0.0844, 0.1658, 0.1245, 0.1552, 0.2154, 0.1327],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1333, 0.0926, 0.1429, 0.1586, 0.1321, 0.1471, 0.1934],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.48834000000000005 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.48834000000000005 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.48834000000000005 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 43.10443775178672
printing an ep nov before normalisation:  39.817872196229324
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.476]
 [0.352]
 [0.352]
 [0.352]
 [0.352]
 [0.352]] [[30.951]
 [39.136]
 [30.951]
 [30.951]
 [30.951]
 [30.951]
 [30.951]] [[0.507]
 [0.724]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.19333333333333302  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.48834000000000005 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.02  0.694 0.041 0.041 0.143 0.02  0.041]
printing an ep nov before normalisation:  34.63508578936065
maxi score, test score, baseline:  -0.48834000000000005 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  36 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.4853933333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.1599999999999996  reward:  1.0 rdn_beta:  1.333
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.564]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.456]
 [0.564]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]]
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.523]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.38 ]
 [0.523]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]
 [0.38 ]]
siam score:  -0.8526762
maxi score, test score, baseline:  -0.4853933333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.047774901463825
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.4853933333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4853933333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.939]
 [0.994]
 [0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]] [[42.523]
 [42.361]
 [42.523]
 [42.523]
 [42.523]
 [42.523]
 [42.523]] [[0.939]
 [0.994]
 [0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]]
maxi score, test score, baseline:  -0.4853933333333334 0.6773333333333333 0.6773333333333333
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.792]
 [0.918]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]] [[37.094]
 [43.683]
 [37.094]
 [37.094]
 [37.094]
 [37.094]
 [37.094]] [[0.792]
 [0.918]
 [0.792]
 [0.792]
 [0.792]
 [0.792]
 [0.792]]
printing an ep nov before normalisation:  37.438209772405706
printing an ep nov before normalisation:  48.18309447494795
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  36.127915923405176
printing an ep nov before normalisation:  40.39168097982225
printing an ep nov before normalisation:  39.33795408317569
printing an ep nov before normalisation:  20.123634285250063
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  28 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  1 policy actor:  1  step number:  57 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.4283133333333334 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.4283133333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.188386174113454
actor:  1 policy actor:  1  step number:  56 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.4283133333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4283133333333334 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  32.752986515359
maxi score, test score, baseline:  -0.4283133333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.448137305138392
maxi score, test score, baseline:  -0.4283133333333334 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.4283133333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4283133333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4283133333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.16666666666666607  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  59.82092566457191
maxi score, test score, baseline:  -0.4283133333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.39858652634871
printing an ep nov before normalisation:  55.16552012509763
maxi score, test score, baseline:  -0.4283133333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  53 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.42586000000000007 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.081]
 [-0.068]
 [-0.074]
 [-0.073]
 [-0.077]
 [-0.073]
 [-0.079]] [[53.46 ]
 [50.991]
 [50.817]
 [50.701]
 [51.244]
 [50.278]
 [50.876]] [[0.801]
 [0.731]
 [0.719]
 [0.716]
 [0.73 ]
 [0.702]
 [0.716]]
siam score:  -0.8592808
maxi score, test score, baseline:  -0.42586000000000007 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.52835054060469
maxi score, test score, baseline:  -0.42586000000000007 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.71 ]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.709]] [[32.224]
 [38.912]
 [32.224]
 [32.224]
 [32.224]
 [32.224]
 [41.041]] [[0.678]
 [0.71 ]
 [0.678]
 [0.678]
 [0.678]
 [0.678]
 [0.709]]
maxi score, test score, baseline:  -0.42586000000000007 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  71 total reward:  0.06666666666666587  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.42586000000000007 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.42586000000000007 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.42586000000000007 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.42586000000000007 0.6913333333333335 0.6913333333333335
actor:  0 policy actor:  0  step number:  48 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  29.269164112864363
maxi score, test score, baseline:  -0.4232333333333334 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.4232333333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.55697515164334
actor:  0 policy actor:  0  step number:  58 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  41.41423092265809
actor:  1 policy actor:  1  step number:  45 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.4208466666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.546]
 [0.544]
 [0.546]
 [0.545]
 [0.54 ]
 [0.544]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.549]
 [0.546]
 [0.544]
 [0.546]
 [0.545]
 [0.54 ]
 [0.544]]
maxi score, test score, baseline:  -0.4208466666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.969729679573206
maxi score, test score, baseline:  -0.4208466666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.673724298253504
Printing some Q and Qe and total Qs values:  [[0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]] [[42.518]
 [42.518]
 [42.518]
 [42.518]
 [42.518]
 [42.518]
 [42.518]] [[0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]
 [0.791]]
maxi score, test score, baseline:  -0.4208466666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  43 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.4181000000000001 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  65.23163453398743
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4181000000000001 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.4181000000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  63.05418584059665
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4181000000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4181000000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4181000000000001 0.6913333333333335 0.6913333333333335
using explorer policy with actor:  1
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.4181000000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  74 total reward:  0.19333333333333202  reward:  1.0 rdn_beta:  1.667
siam score:  -0.86367464
printing an ep nov before normalisation:  35.46372329763077
actions average: 
K:  0  action  0 :  tensor([0.2804, 0.0185, 0.1371, 0.1472, 0.1463, 0.1262, 0.1442],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0026,     0.9641,     0.0024,     0.0019,     0.0007,     0.0005,
            0.0278], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1153, 0.0372, 0.3885, 0.1156, 0.1052, 0.1157, 0.1223],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1353, 0.0051, 0.1339, 0.3476, 0.1309, 0.1112, 0.1360],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1187, 0.0084, 0.0794, 0.0892, 0.5494, 0.0725, 0.0823],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1461, 0.0276, 0.1396, 0.1533, 0.1573, 0.2269, 0.1492],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1120, 0.2046, 0.1046, 0.1264, 0.1033, 0.0928, 0.2562],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.4181000000000001 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.4181000000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.3180814377184
maxi score, test score, baseline:  -0.4181000000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  46.314717445862705
maxi score, test score, baseline:  -0.4181000000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  63 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.4159400000000001 0.6913333333333335 0.6913333333333335
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4159400000000001 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.4159400000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.25999999999999934  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.4159400000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.71182423384166
maxi score, test score, baseline:  -0.4159400000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4159400000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3933333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.4159400000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.221009254455566
maxi score, test score, baseline:  -0.4159400000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4159400000000001 0.6913333333333335 0.6913333333333335
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4159400000000001 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.4159400000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.020650341706215
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.1588310823316874
actor:  1 policy actor:  1  step number:  64 total reward:  0.11333333333333295  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.164]
 [0.244]
 [0.244]
 [0.244]
 [0.162]
 [0.244]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.244]
 [0.164]
 [0.244]
 [0.244]
 [0.244]
 [0.162]
 [0.244]]
maxi score, test score, baseline:  -0.4159400000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4159400000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4159400000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.719498775732156
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.41594000000000003 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.41594000000000003 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.8066216422947
maxi score, test score, baseline:  -0.41594000000000003 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  46.24535269347309
maxi score, test score, baseline:  -0.41594000000000003 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.41594000000000003 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  60 total reward:  0.12666666666666593  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  44.01185069067463
Printing some Q and Qe and total Qs values:  [[-0.009]
 [ 0.046]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]
 [-0.009]] [[35.85 ]
 [47.026]
 [35.85 ]
 [35.85 ]
 [35.85 ]
 [35.85 ]
 [35.85 ]] [[0.616]
 [1.043]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]]
printing an ep nov before normalisation:  52.304544106926
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.36095476150513
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  64 total reward:  0.2866666666666655  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  66.16275857115915
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
line 256 mcts: sample exp_bonus 60.25817901708075
siam score:  -0.866548
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  39.134752201141055
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.47710686478092
Printing some Q and Qe and total Qs values:  [[-0.024]
 [-0.063]
 [-0.086]
 [-0.081]
 [-0.04 ]
 [-0.081]
 [-0.087]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.024]
 [-0.063]
 [-0.086]
 [-0.081]
 [-0.04 ]
 [-0.081]
 [-0.087]]
actions average: 
K:  2  action  0 :  tensor([0.3787, 0.0345, 0.0857, 0.1219, 0.1395, 0.1197, 0.1200],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0065, 0.9478, 0.0053, 0.0130, 0.0023, 0.0037, 0.0214],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0914, 0.0727, 0.2170, 0.1362, 0.0901, 0.2692, 0.1235],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1240, 0.0314, 0.1271, 0.2769, 0.1213, 0.1678, 0.1516],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1224, 0.0090, 0.0839, 0.1407, 0.4107, 0.1226, 0.1107],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1335, 0.0056, 0.1287, 0.1637, 0.1207, 0.3150, 0.1328],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1034, 0.1129, 0.1344, 0.1435, 0.1041, 0.1882, 0.2135],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.22617891543393
printing an ep nov before normalisation:  45.18110125014471
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.104]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]] [[49.227]
 [52.278]
 [49.227]
 [49.227]
 [49.227]
 [49.227]
 [49.227]] [[1.248]
 [1.452]
 [1.248]
 [1.248]
 [1.248]
 [1.248]
 [1.248]]
actor:  1 policy actor:  1  step number:  57 total reward:  0.35999999999999943  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  41.98147017549628
printing an ep nov before normalisation:  18.9086616451792
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.687355076430094
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.578458517015957
printing an ep nov before normalisation:  24.872024059295654
actor:  1 policy actor:  1  step number:  50 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.4136866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  48 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  2  action  0 :  tensor([0.2531, 0.0059, 0.1215, 0.1519, 0.1674, 0.1672, 0.1331],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0064, 0.9588, 0.0063, 0.0047, 0.0021, 0.0024, 0.0192],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1286, 0.0080, 0.2882, 0.1312, 0.0994, 0.1822, 0.1624],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1285, 0.1143, 0.1247, 0.1630, 0.1729, 0.1273, 0.1693],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1345, 0.0083, 0.0955, 0.1294, 0.3937, 0.1167, 0.1219],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0732, 0.0148, 0.2688, 0.0963, 0.0982, 0.3599, 0.0888],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1794, 0.0550, 0.1509, 0.1324, 0.1076, 0.1055, 0.2692],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  53.469622179774184
Printing some Q and Qe and total Qs values:  [[-0.069]
 [-0.043]
 [-0.067]
 [-0.057]
 [-0.063]
 [-0.066]
 [-0.063]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.069]
 [-0.043]
 [-0.067]
 [-0.057]
 [-0.063]
 [-0.066]
 [-0.063]]
maxi score, test score, baseline:  -0.41100666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.65424423734113
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.431]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]] [[56.838]
 [54.651]
 [56.838]
 [56.838]
 [56.838]
 [56.838]
 [56.838]] [[1.297]
 [1.359]
 [1.297]
 [1.297]
 [1.297]
 [1.297]
 [1.297]]
maxi score, test score, baseline:  -0.41100666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.66849123833844
maxi score, test score, baseline:  -0.41100666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.41100666666666674 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.41100666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.2826, 0.0841, 0.1131, 0.1205, 0.1379, 0.1114, 0.1503],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0130, 0.9464, 0.0066, 0.0063, 0.0059, 0.0057, 0.0161],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0778, 0.0171, 0.4530, 0.1149, 0.0813, 0.1740, 0.0819],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1385, 0.0077, 0.1316, 0.2478, 0.1402, 0.1641, 0.1700],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1706, 0.0024, 0.1233, 0.1267, 0.3277, 0.1284, 0.1210],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1245, 0.0047, 0.1739, 0.1604, 0.1191, 0.3055, 0.1119],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.2017, 0.1009, 0.0963, 0.1055, 0.0980, 0.1028, 0.2948],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.41100666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.59915838970272
actor:  1 policy actor:  1  step number:  65 total reward:  0.25333333333333263  reward:  1.0 rdn_beta:  1.0
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[-0.0000],
        [-0.1834],
        [ 0.0239],
        [-0.0000],
        [-0.0000],
        [ 0.4633],
        [-0.1746],
        [-0.0860],
        [-0.0000],
        [ 0.2425]], dtype=torch.float64)
0.9734999999999999 0.9734999999999999
-0.032346567066 -0.21579136680794866
-0.032346567066 -0.008462856199502274
-0.9516369486359999 -0.9516369486359999
0.99 0.99
-0.084359833866 0.37894293448029964
-0.057834381198 -0.2324054060392563
-0.032346567066 -0.1183210493802915
-0.8756220000000001 -0.8756220000000001
-0.070771701198 0.1717083629862167
printing an ep nov before normalisation:  49.084362660899
maxi score, test score, baseline:  -0.41100666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.128]
 [-0.22 ]
 [-0.135]
 [-0.127]
 [-0.13 ]
 [-0.131]
 [-0.132]] [[47.248]
 [48.138]
 [46.127]
 [45.425]
 [45.738]
 [45.568]
 [45.889]] [[0.152]
 [0.069]
 [0.132]
 [0.132]
 [0.132]
 [0.13 ]
 [0.133]]
maxi score, test score, baseline:  -0.41100666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.41100666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.41100666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.41100666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.43731582160436
maxi score, test score, baseline:  -0.41100666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.41100666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.28183890125941
printing an ep nov before normalisation:  37.47885365553465
maxi score, test score, baseline:  -0.41100666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.57623456311514
printing an ep nov before normalisation:  41.18787679509873
maxi score, test score, baseline:  -0.4110066666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.010319153582827312
printing an ep nov before normalisation:  44.091470976957915
line 256 mcts: sample exp_bonus 38.147508257766226
printing an ep nov before normalisation:  16.79964224750464
maxi score, test score, baseline:  -0.4110066666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4110066666666667 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.4110066666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.136]
 [-0.049]
 [ 0.05 ]
 [ 0.042]
 [ 0.055]
 [ 0.069]
 [ 0.087]] [[15.365]
 [22.66 ]
 [15.401]
 [15.414]
 [15.336]
 [15.284]
 [15.167]] [[1.007]
 [1.236]
 [0.923]
 [0.916]
 [0.924]
 [0.936]
 [0.946]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.91926428779913
maxi score, test score, baseline:  -0.4110066666666667 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.4110066666666667 0.6913333333333335 0.6913333333333335
actions average: 
K:  2  action  0 :  tensor([0.3742, 0.0384, 0.1016, 0.1139, 0.1295, 0.1277, 0.1149],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0059, 0.9376, 0.0075, 0.0136, 0.0057, 0.0078, 0.0219],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0789, 0.0104, 0.2899, 0.1368, 0.1066, 0.2952, 0.0821],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1371, 0.0069, 0.1334, 0.2623, 0.1588, 0.1529, 0.1486],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1356, 0.0057, 0.1058, 0.1372, 0.3750, 0.1309, 0.1098],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0987, 0.0314, 0.1252, 0.1259, 0.1108, 0.4067, 0.1013],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0964, 0.1238, 0.0966, 0.2209, 0.0933, 0.0947, 0.2743],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.446]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]] [[26.909]
 [29.59 ]
 [26.909]
 [26.909]
 [26.909]
 [26.909]
 [26.909]] [[0.333]
 [0.446]
 [0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]]
printing an ep nov before normalisation:  46.85286045074463
maxi score, test score, baseline:  -0.4110066666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.13828673312284
siam score:  -0.85669357
maxi score, test score, baseline:  -0.4110066666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.1933333333333327  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  58 total reward:  0.05999999999999939  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.40888666666666673 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  28.05318063957865
printing an ep nov before normalisation:  45.21915060367591
line 256 mcts: sample exp_bonus 26.420645713806152
actor:  0 policy actor:  0  step number:  52 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  62 total reward:  0.15333333333333266  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  41.36327714250116
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4064200000000001 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.4064200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.703552712945864
maxi score, test score, baseline:  -0.4064200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4064200000000001 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.4064200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.1549054237981
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.438]
 [0.326]
 [0.326]
 [0.326]
 [0.326]
 [0.326]] [[44.328]
 [42.764]
 [44.328]
 [44.328]
 [44.328]
 [44.328]
 [44.328]] [[0.589]
 [0.682]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]]
maxi score, test score, baseline:  -0.4064200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4064200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.54880607788929
maxi score, test score, baseline:  -0.4064200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  48.36085854587081
maxi score, test score, baseline:  -0.4064200000000001 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.4064200000000001 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  52.09792398076753
printing an ep nov before normalisation:  42.817587790473205
printing an ep nov before normalisation:  56.8708018478684
maxi score, test score, baseline:  -0.4064200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.681271076202393
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.668]
 [0.511]
 [0.471]
 [0.511]
 [0.42 ]
 [0.622]] [[38.655]
 [37.83 ]
 [44.774]
 [48.189]
 [44.774]
 [47.277]
 [37.409]] [[0.635]
 [0.865]
 [0.785]
 [0.783]
 [0.785]
 [0.721]
 [0.814]]
printing an ep nov before normalisation:  41.69098107057607
maxi score, test score, baseline:  -0.4064200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4064200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.3680, 0.0031, 0.0670, 0.1203, 0.2514, 0.0907, 0.0995],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0050,     0.9634,     0.0028,     0.0046,     0.0005,     0.0006,
            0.0230], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1141, 0.0954, 0.3005, 0.1066, 0.0899, 0.1578, 0.1355],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0891, 0.0933, 0.0708, 0.4147, 0.0985, 0.1022, 0.1312],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1539, 0.0062, 0.0545, 0.0814, 0.5466, 0.0710, 0.0864],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0706, 0.0067, 0.0843, 0.0851, 0.0823, 0.5821, 0.0889],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1161, 0.1523, 0.0939, 0.1266, 0.1055, 0.1071, 0.2985],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  56 total reward:  0.23333333333333317  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8505669
Printing some Q and Qe and total Qs values:  [[0.63 ]
 [0.699]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]] [[13.249]
 [30.598]
 [13.249]
 [13.249]
 [13.249]
 [13.249]
 [13.249]] [[0.63 ]
 [0.699]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]
 [0.63 ]]
maxi score, test score, baseline:  -0.4064200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.4064200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4064200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4064200000000001 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.4064200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  61 total reward:  0.1599999999999997  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  54 total reward:  0.24666666666666648  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.40410000000000007 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.40410000000000007 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.40410000000000007 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.621388974603576
actor:  0 policy actor:  0  step number:  38 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.4011533333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.69460616437833
printing an ep nov before normalisation:  46.074980323200585
maxi score, test score, baseline:  -0.4011533333333334 0.6913333333333335 0.6913333333333335
actor:  1 policy actor:  1  step number:  42 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.4011533333333334 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  32.81325817108154
maxi score, test score, baseline:  -0.4011533333333334 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.4011533333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4011533333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.78050279362872
maxi score, test score, baseline:  -0.4011533333333334 0.6913333333333335 0.6913333333333335
actor:  1 policy actor:  1  step number:  42 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  43.43809740883964
maxi score, test score, baseline:  -0.4011533333333334 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  39.24412655369886
maxi score, test score, baseline:  -0.4011533333333334 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  30.21815382608226
maxi score, test score, baseline:  -0.4011533333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.4011533333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4011533333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4011533333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4011533333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.03522162810903
actions average: 
K:  4  action  0 :  tensor([0.1843, 0.1467, 0.1430, 0.1312, 0.1309, 0.1216, 0.1422],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0219, 0.8569, 0.0252, 0.0197, 0.0101, 0.0140, 0.0522],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0735, 0.1214, 0.3776, 0.0933, 0.0656, 0.1602, 0.1083],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0985, 0.1188, 0.1080, 0.3193, 0.1108, 0.1218, 0.1228],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1179, 0.1974, 0.1043, 0.0889, 0.2915, 0.0868, 0.1132],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1761, 0.0371, 0.1809, 0.1561, 0.1685, 0.1365, 0.1447],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1350, 0.1594, 0.1261, 0.1199, 0.1286, 0.1285, 0.2025],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.4011533333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4011533333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.08271226818692412
actor:  1 policy actor:  1  step number:  79 total reward:  0.06666666666666665  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.4011533333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4011533333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.4011533333333334 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.4011533333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.72113206890841
printing an ep nov before normalisation:  34.73880345653275
using explorer policy with actor:  1
printing an ep nov before normalisation:  6.260001441660279e-06
actor:  0 policy actor:  0  step number:  52 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  36.73317880022959
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3986600000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3986600000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.11999999999999944  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3986600000000001 0.6913333333333335 0.6913333333333335
siam score:  -0.8497593
maxi score, test score, baseline:  -0.3986600000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.31652422001424
maxi score, test score, baseline:  -0.3986600000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.86422920227051
maxi score, test score, baseline:  -0.3986600000000001 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  39.91311439987267
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.139]
 [0.035]
 [0.022]
 [0.023]
 [0.029]
 [0.033]] [[27.595]
 [33.001]
 [29.472]
 [29.199]
 [29.134]
 [29.269]
 [28.642]] [[0.565]
 [0.908]
 [0.654]
 [0.629]
 [0.627]
 [0.64 ]
 [0.617]]
maxi score, test score, baseline:  -0.3986600000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.85873244669538
maxi score, test score, baseline:  -0.3986600000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.107128064980202
actor:  1 policy actor:  1  step number:  68 total reward:  0.2333333333333325  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3986600000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.569440710318986
actor:  1 policy actor:  1  step number:  70 total reward:  0.12666666666666593  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.471]
 [0.261]
 [0.297]
 [0.336]
 [0.364]
 [0.261]] [[41.265]
 [38.612]
 [41.265]
 [43.287]
 [41.545]
 [41.295]
 [41.265]] [[1.966]
 [1.999]
 [1.966]
 [2.137]
 [2.059]
 [2.07 ]
 [1.966]]
printing an ep nov before normalisation:  0.2842615626815359
maxi score, test score, baseline:  -0.3986600000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.420348635411315
maxi score, test score, baseline:  -0.3986600000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3986600000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3986600000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3986600000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  39 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.15 ]
 [0.062]
 [0.15 ]
 [0.415]
 [0.083]
 [0.104]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.434]
 [0.15 ]
 [0.062]
 [0.15 ]
 [0.415]
 [0.083]
 [0.104]]
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]] [[53.557]
 [53.557]
 [53.557]
 [53.557]
 [53.557]
 [53.557]
 [53.557]] [[0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]
 [0.423]]
maxi score, test score, baseline:  -0.3958333333333335 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.676943262068896
actor:  1 policy actor:  1  step number:  59 total reward:  0.30666666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.3958333333333335 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3958333333333335 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.39303333333333346 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.39303333333333346 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.39303333333333346 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.39303333333333346 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.1603, 0.0084, 0.1488, 0.1595, 0.2007, 0.1620, 0.1603],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0080, 0.9217, 0.0180, 0.0165, 0.0017, 0.0034, 0.0307],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1367, 0.0097, 0.2825, 0.1355, 0.1087, 0.1403, 0.1866],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0960, 0.0288, 0.1167, 0.3300, 0.1223, 0.1292, 0.1769],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1246, 0.0550, 0.0977, 0.0926, 0.4353, 0.0971, 0.0977],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0597, 0.0036, 0.1387, 0.0988, 0.0691, 0.5349, 0.0953],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1182, 0.1528, 0.1246, 0.1089, 0.0844, 0.1002, 0.3109],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.25380083463516
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.39303333333333346 0.6913333333333335 0.6913333333333335
actor:  1 policy actor:  1  step number:  60 total reward:  0.24666666666666637  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.39303333333333346 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  0.16666666666666607  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.39303333333333346 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.806]
 [0.868]
 [0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.725]] [[54.313]
 [41.206]
 [54.313]
 [54.313]
 [54.313]
 [54.313]
 [56.868]] [[0.806]
 [0.868]
 [0.806]
 [0.806]
 [0.806]
 [0.806]
 [0.725]]
printing an ep nov before normalisation:  34.657979867295644
maxi score, test score, baseline:  -0.39303333333333346 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.39303333333333346 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.39303333333333346 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  48 total reward:  0.2733333333333331  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.056]]
maxi score, test score, baseline:  -0.39048666666666676 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.533772891820547
maxi score, test score, baseline:  -0.3904866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]
 [0.365]] [[39.162]
 [39.162]
 [39.162]
 [39.162]
 [39.162]
 [39.162]
 [39.162]] [[1.581]
 [1.581]
 [1.581]
 [1.581]
 [1.581]
 [1.581]
 [1.581]]
maxi score, test score, baseline:  -0.3904866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3904866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3904866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.528643905516574
maxi score, test score, baseline:  -0.3904866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3904866666666667 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  31.30019506857167
maxi score, test score, baseline:  -0.3904866666666667 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  33.225072469716665
maxi score, test score, baseline:  -0.3904866666666667 0.6913333333333335 0.6913333333333335
siam score:  -0.8507772
maxi score, test score, baseline:  -0.3904866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  42 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.17942921220925
maxi score, test score, baseline:  -0.38751333333333343 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.571]
 [0.44 ]
 [0.44 ]
 [0.436]
 [0.44 ]
 [0.454]] [[29.538]
 [36.283]
 [29.538]
 [29.538]
 [26.963]
 [29.538]
 [32.49 ]] [[0.44 ]
 [0.571]
 [0.44 ]
 [0.44 ]
 [0.436]
 [0.44 ]
 [0.454]]
maxi score, test score, baseline:  -0.38751333333333343 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.61486828585585
actor:  0 policy actor:  0  step number:  60 total reward:  0.13999999999999957  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.38523333333333337 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.38523333333333337 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  54 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.934934683926215
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.28008094603677
printing an ep nov before normalisation:  31.520668277596847
siam score:  -0.8536863
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]] [[45.875]
 [45.875]
 [45.875]
 [45.875]
 [45.875]
 [45.875]
 [45.875]] [[1.388]
 [1.388]
 [1.388]
 [1.388]
 [1.388]
 [1.388]
 [1.388]]
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  39.44656848907471
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.17410665412745
actor:  1 policy actor:  1  step number:  55 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
siam score:  -0.85649353
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  36.74656197366662
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.93924953756705
printing an ep nov before normalisation:  47.3296594619751
printing an ep nov before normalisation:  37.99480332655865
printing an ep nov before normalisation:  0.0
Printing some Q and Qe and total Qs values:  [[-0.083]
 [-0.081]
 [-0.085]
 [-0.083]
 [-0.083]
 [-0.107]
 [-0.11 ]] [[52.974]
 [53.209]
 [53.994]
 [54.565]
 [54.52 ]
 [54.62 ]
 [54.185]] [[0.811]
 [0.82 ]
 [0.84 ]
 [0.858]
 [0.856]
 [0.836]
 [0.82 ]]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8578491
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[ 0.01 ]
 [ 0.037]
 [ 0.006]
 [-0.013]
 [-0.003]
 [ 0.008]
 [-0.021]] [[36.836]
 [36.649]
 [36.574]
 [37.577]
 [38.534]
 [37.748]
 [37.63 ]] [[0.681]
 [0.701]
 [0.668]
 [0.687]
 [0.734]
 [0.714]
 [0.681]]
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.074083733144224
printing an ep nov before normalisation:  33.53536459092768
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3828200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  36 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3798466666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3798466666666668 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.3798466666666668 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3798466666666668 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.44707791996182
actions average: 
K:  3  action  0 :  tensor([0.4785, 0.0189, 0.0793, 0.0919, 0.1447, 0.0969, 0.0898],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0050,     0.9541,     0.0034,     0.0031,     0.0008,     0.0006,
            0.0330], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1298, 0.0294, 0.4171, 0.0808, 0.1231, 0.1046, 0.1152],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1066, 0.0046, 0.1102, 0.3431, 0.1181, 0.1261, 0.1914],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1245, 0.0061, 0.1152, 0.1406, 0.3432, 0.1374, 0.1330],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1230, 0.0416, 0.1788, 0.1448, 0.1512, 0.2293, 0.1313],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1013, 0.1759, 0.1229, 0.1312, 0.1222, 0.1149, 0.2316],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.3798466666666668 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3798466666666668 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3798466666666668 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3798466666666668 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.872754368830115
line 256 mcts: sample exp_bonus 32.78482368931903
maxi score, test score, baseline:  -0.3798466666666668 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.3798466666666668 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3798466666666668 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3798466666666668 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.097]
 [0.234]
 [0.13 ]
 [0.108]
 [0.106]
 [0.143]
 [0.128]] [[36.312]
 [41.255]
 [34.925]
 [36.584]
 [37.154]
 [35.741]
 [33.413]] [[1.182]
 [1.607]
 [1.136]
 [1.21 ]
 [1.241]
 [1.195]
 [1.046]]
maxi score, test score, baseline:  -0.3798466666666668 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3798466666666668 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.043]
 [-0.031]
 [-0.044]
 [-0.042]
 [-0.045]
 [-0.044]
 [-0.044]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.043]
 [-0.031]
 [-0.044]
 [-0.042]
 [-0.045]
 [-0.044]
 [-0.044]]
printing an ep nov before normalisation:  41.223795757471414
maxi score, test score, baseline:  -0.3798466666666668 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.12112808227539
actor:  1 policy actor:  1  step number:  54 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.3798466666666668 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3798466666666668 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3798466666666668 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.3798466666666668 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.3798466666666668 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3798466666666668 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.29515665399035
Printing some Q and Qe and total Qs values:  [[-0.096]
 [-0.082]
 [-0.096]
 [-0.096]
 [-0.096]
 [-0.096]
 [-0.096]] [[32.439]
 [35.493]
 [32.439]
 [32.439]
 [32.439]
 [32.439]
 [32.439]] [[0.041]
 [0.082]
 [0.041]
 [0.041]
 [0.041]
 [0.041]
 [0.041]]
maxi score, test score, baseline:  -0.3798466666666668 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.24006056423915
printing an ep nov before normalisation:  44.13793802491175
maxi score, test score, baseline:  -0.3798466666666668 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  48 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  40.22951877709585
actor:  0 policy actor:  0  step number:  31 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  57.958569379689
maxi score, test score, baseline:  -0.3740466666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.082]
 [0.056]
 [0.034]
 [0.071]
 [0.048]
 [0.04 ]
 [0.052]] [[44.925]
 [42.703]
 [46.964]
 [47.225]
 [47.685]
 [47.546]
 [48.993]] [[1.753]
 [1.599]
 [1.822]
 [1.874]
 [1.878]
 [1.862]
 [1.958]]
siam score:  -0.8522245
maxi score, test score, baseline:  -0.3740466666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.45585997748001
maxi score, test score, baseline:  -0.3740466666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.75156553412618
maxi score, test score, baseline:  -0.3740466666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3740466666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3740466666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.1999999999999994  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.3740466666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.229562520980835
actor:  1 policy actor:  1  step number:  75 total reward:  0.13333333333333208  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3740466666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]
 [0.338]]
Printing some Q and Qe and total Qs values:  [[0.802]
 [0.875]
 [0.804]
 [0.812]
 [0.812]
 [0.809]
 [0.8  ]] [[45.166]
 [39.834]
 [45.105]
 [45.377]
 [45.433]
 [45.066]
 [45.786]] [[0.802]
 [0.875]
 [0.804]
 [0.812]
 [0.812]
 [0.809]
 [0.8  ]]
maxi score, test score, baseline:  -0.3740466666666667 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  44.90873528912737
actor:  0 policy actor:  0  step number:  42 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.077]
 [-0.077]
 [-0.053]
 [-0.077]
 [-0.077]
 [-0.049]
 [-0.077]] [[53.251]
 [53.251]
 [67.84 ]
 [53.251]
 [53.251]
 [59.968]
 [53.251]] [[1.017]
 [1.017]
 [1.627]
 [1.017]
 [1.017]
 [1.315]
 [1.017]]
printing an ep nov before normalisation:  49.485993622890874
maxi score, test score, baseline:  -0.37120666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.37120666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.473067352212595
maxi score, test score, baseline:  -0.37120666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.99669703446995
maxi score, test score, baseline:  -0.37120666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.37120666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.915963782054874
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.068804116813915
actor:  1 policy actor:  1  step number:  61 total reward:  0.2133333333333326  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.37120666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  67 total reward:  0.03999999999999959  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.37120666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.37120666666666674 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.37120666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.37120666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.37120666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.37120666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  45 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.36846000000000007 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.36846000000000007 0.6913333333333335 0.6913333333333335
actor:  1 policy actor:  1  step number:  56 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.36846000000000007 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.63296682971181
maxi score, test score, baseline:  -0.36846000000000007 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.30600470409486
maxi score, test score, baseline:  -0.36846000000000007 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.36846000000000007 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.61691443110131
actor:  0 policy actor:  0  step number:  49 total reward:  0.31999999999999984  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  32.815676532503154
printing an ep nov before normalisation:  40.7131577605063
siam score:  -0.85190094
actor:  1 policy actor:  1  step number:  58 total reward:  0.1933333333333327  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3658200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.49984297696963
maxi score, test score, baseline:  -0.3658200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.21721610153733
printing an ep nov before normalisation:  42.600722312927246
siam score:  -0.85056704
maxi score, test score, baseline:  -0.3658200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.3658200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.885199546813965
Printing some Q and Qe and total Qs values:  [[-0.09 ]
 [-0.072]
 [-0.09 ]
 [-0.09 ]
 [-0.09 ]
 [-0.09 ]
 [-0.09 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.09 ]
 [-0.072]
 [-0.09 ]
 [-0.09 ]
 [-0.09 ]
 [-0.09 ]
 [-0.09 ]]
Printing some Q and Qe and total Qs values:  [[-0.151]
 [-0.151]
 [-0.151]
 [-0.151]
 [-0.151]
 [-0.151]
 [-0.151]] [[63.592]
 [63.592]
 [63.592]
 [63.592]
 [63.592]
 [63.592]
 [63.592]] [[1.516]
 [1.516]
 [1.516]
 [1.516]
 [1.516]
 [1.516]
 [1.516]]
maxi score, test score, baseline:  -0.3658200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3658200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3658200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.87156078135039
maxi score, test score, baseline:  -0.3658200000000001 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  56.90097477798374
actor:  0 policy actor:  0  step number:  53 total reward:  0.1866666666666661  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.36344666666666675 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
printing an ep nov before normalisation:  39.80943737696444
maxi score, test score, baseline:  -0.36344666666666675 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.066]
 [-0.052]
 [-0.066]
 [-0.065]
 [-0.096]
 [-0.066]
 [-0.064]] [[29.576]
 [50.186]
 [29.576]
 [49.135]
 [37.713]
 [29.576]
 [55.41 ]] [[-0.059]
 [ 0.159]
 [-0.059]
 [ 0.136]
 [-0.008]
 [-0.059]
 [ 0.198]]
printing an ep nov before normalisation:  48.03455474398604
printing an ep nov before normalisation:  0.03361247426909131
actor:  0 policy actor:  0  step number:  60 total reward:  0.08666666666666623  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.36127333333333345 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 40.03339684073257
maxi score, test score, baseline:  -0.36127333333333345 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.36127333333333345 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.79194497047992
Printing some Q and Qe and total Qs values:  [[0.786]
 [0.822]
 [0.794]
 [0.788]
 [0.777]
 [0.79 ]
 [0.798]] [[29.163]
 [20.866]
 [23.369]
 [29.07 ]
 [30.434]
 [29.639]
 [28.6  ]] [[0.786]
 [0.822]
 [0.794]
 [0.788]
 [0.777]
 [0.79 ]
 [0.798]]
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.832]
 [0.564]
 [0.642]
 [0.649]
 [0.683]
 [0.651]] [[41.261]
 [34.883]
 [37.538]
 [42.166]
 [39.582]
 [41.261]
 [40.575]] [[0.683]
 [0.832]
 [0.564]
 [0.642]
 [0.649]
 [0.683]
 [0.651]]
actor:  0 policy actor:  0  step number:  52 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.443]
 [0.387]
 [0.387]
 [0.387]
 [0.378]
 [0.387]] [[29.652]
 [33.643]
 [26.504]
 [26.504]
 [29.845]
 [28.538]
 [26.504]] [[0.985]
 [1.141]
 [0.803]
 [0.803]
 [0.935]
 [0.875]
 [0.803]]
printing an ep nov before normalisation:  43.459014613397784
maxi score, test score, baseline:  -0.3586466666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.78769095259047
maxi score, test score, baseline:  -0.3586466666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  62 total reward:  0.0733333333333327  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  40.452392986755925
printing an ep nov before normalisation:  45.83581830060003
maxi score, test score, baseline:  -0.35650000000000004 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  32.21737213976983
maxi score, test score, baseline:  -0.3565000000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]
 [0.014]] [[41.271]
 [41.271]
 [41.271]
 [41.271]
 [41.271]
 [41.271]
 [41.271]] [[0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]]
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([0.3370, 0.0073, 0.1093, 0.1201, 0.1913, 0.1148, 0.1202],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0088, 0.9543, 0.0082, 0.0046, 0.0031, 0.0030, 0.0179],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1348, 0.0036, 0.2987, 0.1297, 0.1366, 0.1568, 0.1398],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1360, 0.0350, 0.1244, 0.2646, 0.1526, 0.1428, 0.1446],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0946, 0.0275, 0.0481, 0.0819, 0.6323, 0.0615, 0.0541],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0916, 0.0674, 0.1488, 0.1044, 0.0877, 0.3969, 0.1032],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0936, 0.1863, 0.0826, 0.0901, 0.0770, 0.0804, 0.3900],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.3565000000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.58195439465876
maxi score, test score, baseline:  -0.3565000000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.55667144612252
maxi score, test score, baseline:  -0.3565000000000001 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.3565000000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3565000000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3565000000000001 0.6913333333333335 0.6913333333333335
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  66 total reward:  0.11333333333333295  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.3565000000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.75774002075195
maxi score, test score, baseline:  -0.3565000000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]] [[35.84]
 [35.84]
 [35.84]
 [35.84]
 [35.84]
 [35.84]
 [35.84]] [[1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]
 [1.634]]
actor:  1 policy actor:  1  step number:  71 total reward:  0.013333333333332531  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3565000000000001 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  42.5576929469657
actor:  1 policy actor:  1  step number:  60 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.3565000000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85145676
maxi score, test score, baseline:  -0.3565000000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.07 ]
 [-0.07 ]
 [-0.055]
 [-0.1  ]
 [-0.156]
 [-0.055]
 [-0.068]] [[42.643]
 [42.643]
 [37.365]
 [42.847]
 [39.856]
 [40.562]
 [38.088]] [[0.587]
 [0.587]
 [0.443]
 [0.563]
 [0.418]
 [0.539]
 [0.452]]
actor:  0 policy actor:  0  step number:  41 total reward:  0.45333333333333303  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  40.98919849962502
printing an ep nov before normalisation:  42.75607067884434
maxi score, test score, baseline:  -0.3535933333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.451]
 [0.369]
 [0.403]
 [0.399]
 [0.395]
 [0.392]] [[28.407]
 [34.375]
 [31.186]
 [28.127]
 [28.149]
 [28.169]
 [28.16 ]] [[0.92 ]
 [1.334]
 [1.084]
 [0.956]
 [0.953]
 [0.95 ]
 [0.946]]
printing an ep nov before normalisation:  39.28509912253396
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.521]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]] [[45.299]
 [42.2  ]
 [45.299]
 [45.299]
 [45.299]
 [45.299]
 [45.299]] [[0.87 ]
 [0.992]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]
 [0.87 ]]
actions average: 
K:  0  action  0 :  tensor([0.2301, 0.0052, 0.1694, 0.1207, 0.1751, 0.1686, 0.1310],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0011,     0.9917,     0.0017,     0.0005,     0.0001,     0.0001,
            0.0047], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1050, 0.0105, 0.4081, 0.1212, 0.1122, 0.1153, 0.1277],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1552, 0.0262, 0.1374, 0.2459, 0.1606, 0.1284, 0.1463],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0972, 0.0092, 0.0944, 0.1213, 0.5106, 0.0828, 0.0846],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0901, 0.0070, 0.1415, 0.1217, 0.1025, 0.4387, 0.0984],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1492, 0.0150, 0.1583, 0.1833, 0.1490, 0.1427, 0.2024],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  24.351639287656806
actions average: 
K:  2  action  0 :  tensor([0.4077, 0.0075, 0.0863, 0.0746, 0.2615, 0.0798, 0.0827],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0184, 0.8957, 0.0162, 0.0141, 0.0118, 0.0111, 0.0326],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1082, 0.0112, 0.3067, 0.1403, 0.1271, 0.1823, 0.1242],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0896, 0.0271, 0.0935, 0.4113, 0.1262, 0.0959, 0.1564],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1399, 0.0066, 0.1367, 0.1615, 0.2983, 0.1426, 0.1144],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0837, 0.0668, 0.1311, 0.0878, 0.0948, 0.4436, 0.0922],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1097, 0.1614, 0.1208, 0.1144, 0.1155, 0.1086, 0.2695],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  56.98375282585352
maxi score, test score, baseline:  -0.3535933333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  68 total reward:  0.019999999999998908  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8540495
maxi score, test score, baseline:  -0.3535933333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8537819
siam score:  -0.8535736
actor:  0 policy actor:  0  step number:  34 total reward:  0.5933333333333335  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  55.605481600316665
maxi score, test score, baseline:  -0.35040666666666676 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.35040666666666676 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.35040666666666676 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.12 ]
 [0.353]
 [0.353]
 [0.357]
 [0.353]
 [0.264]] [[40.714]
 [39.854]
 [44.995]
 [44.995]
 [47.809]
 [44.995]
 [42.415]] [[1.41 ]
 [1.188]
 [1.768]
 [1.768]
 [1.962]
 [1.768]
 [1.505]]
maxi score, test score, baseline:  -0.35040666666666676 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 40.20741580190567
printing an ep nov before normalisation:  49.51723601757074
printing an ep nov before normalisation:  53.548884505559904
printing an ep nov before normalisation:  41.44707087723586
maxi score, test score, baseline:  -0.35040666666666676 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  48 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3477800000000001 0.6913333333333335 0.6913333333333335
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.023]
 [-0.036]
 [-0.043]
 [-0.043]
 [-0.036]
 [-0.036]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.036]
 [-0.023]
 [-0.036]
 [-0.043]
 [-0.043]
 [-0.036]
 [-0.036]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.146]
 [-0.216]
 [-0.162]
 [-0.149]
 [-0.181]
 [-0.162]
 [-0.158]] [[33.917]
 [30.599]
 [36.531]
 [34.982]
 [27.271]
 [36.28 ]
 [36.635]] [[0.348]
 [0.229]
 [0.37 ]
 [0.36 ]
 [0.216]
 [0.366]
 [0.375]]
maxi score, test score, baseline:  -0.3477800000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.23296904633757
maxi score, test score, baseline:  -0.3477800000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.323]
 [0.374]
 [0.326]
 [0.323]
 [0.323]
 [0.323]
 [0.323]] [[43.762]
 [39.003]
 [44.434]
 [43.762]
 [43.762]
 [43.762]
 [43.762]] [[0.955]
 [0.871]
 [0.976]
 [0.955]
 [0.955]
 [0.955]
 [0.955]]
printing an ep nov before normalisation:  45.702436271677364
maxi score, test score, baseline:  -0.3477800000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3477800000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.735]
 [0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]] [[36.219]
 [32.989]
 [36.219]
 [36.219]
 [36.219]
 [36.219]
 [36.219]] [[0.988]
 [1.038]
 [0.988]
 [0.988]
 [0.988]
 [0.988]
 [0.988]]
printing an ep nov before normalisation:  42.85919848810435
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3477800000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3477800000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.878]
 [0.878]
 [0.878]
 [0.878]
 [0.882]
 [0.878]
 [0.878]] [[45.397]
 [45.397]
 [45.397]
 [45.397]
 [44.97 ]
 [45.397]
 [45.397]] [[0.878]
 [0.878]
 [0.878]
 [0.878]
 [0.882]
 [0.878]
 [0.878]]
printing an ep nov before normalisation:  44.20043667062468
printing an ep nov before normalisation:  32.456254959106445
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.654]
 [0.522]
 [0.613]
 [0.513]
 [0.464]
 [0.55 ]] [[44.307]
 [46.245]
 [50.526]
 [44.307]
 [49.81 ]
 [47.121]
 [46.74 ]] [[1.504]
 [1.61 ]
 [1.623]
 [1.504]
 [1.59 ]
 [1.45 ]
 [1.524]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  48 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  40.845012557363695
maxi score, test score, baseline:  -0.3477800000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.34778000000000014 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.34778000000000014 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.34778000000000014 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  54.381537437438965
maxi score, test score, baseline:  -0.34778000000000014 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  53.10191325666438
printing an ep nov before normalisation:  51.2155916937072
actor:  0 policy actor:  0  step number:  44 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.3448600000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.668904499232575
Printing some Q and Qe and total Qs values:  [[0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]
 [0.389]] [[39.579]
 [39.579]
 [39.579]
 [39.579]
 [39.579]
 [39.579]
 [39.579]] [[2.044]
 [2.044]
 [2.044]
 [2.044]
 [2.044]
 [2.044]
 [2.044]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  39.5700036929174
maxi score, test score, baseline:  -0.3448600000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.265]
 [ 0.234]
 [ 0.265]
 [ 0.265]
 [ 0.265]
 [-0.081]
 [ 0.265]] [[42.5  ]
 [43.149]
 [42.5  ]
 [42.5  ]
 [42.5  ]
 [36.956]
 [42.5  ]] [[1.691]
 [1.701]
 [1.691]
 [1.691]
 [1.691]
 [0.992]
 [1.691]]
maxi score, test score, baseline:  -0.3448600000000001 0.6913333333333335 0.6913333333333335
siam score:  -0.8583033
actor:  1 policy actor:  1  step number:  59 total reward:  0.38666666666666616  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  55 total reward:  0.2133333333333326  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 66.54792754547246
printing an ep nov before normalisation:  47.81808376312256
printing an ep nov before normalisation:  38.32692867262881
actor:  1 policy actor:  1  step number:  43 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.92504079226913
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.3448600000000001 0.6913333333333335 0.6913333333333335
actor:  1 policy actor:  1  step number:  57 total reward:  0.14666666666666606  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.083]
 [0.148]
 [0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.083]
 [0.148]
 [0.083]
 [0.083]
 [0.083]
 [0.083]
 [0.083]]
printing an ep nov before normalisation:  45.60205705887352
actor:  1 policy actor:  1  step number:  74 total reward:  0.01999999999999924  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.3448600000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.3512, 0.0046, 0.1328, 0.1134, 0.1301, 0.1462, 0.1216],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0123, 0.9037, 0.0124, 0.0181, 0.0070, 0.0084, 0.0382],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1235, 0.0149, 0.3216, 0.1341, 0.1302, 0.1423, 0.1334],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1033, 0.1418, 0.1393, 0.1519, 0.1344, 0.1798, 0.1494],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1189, 0.0438, 0.0896, 0.0957, 0.4530, 0.0997, 0.0993],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0981, 0.0509, 0.1574, 0.1389, 0.1054, 0.3248, 0.1245],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0553, 0.3096, 0.1559, 0.1428, 0.0546, 0.0562, 0.2255],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.3448600000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  56 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  57.05159845077097
siam score:  -0.8495233
maxi score, test score, baseline:  -0.34212666666666675 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.68519705950083
maxi score, test score, baseline:  -0.34212666666666675 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.34212666666666675 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  38.12106052373632
maxi score, test score, baseline:  -0.34212666666666675 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.568]
 [0.418]
 [0.505]
 [0.423]
 [0.475]
 [0.571]] [[45.943]
 [43.58 ]
 [45.934]
 [42.715]
 [46.009]
 [44.936]
 [43.638]] [[1.043]
 [1.053]
 [0.958]
 [0.97 ]
 [0.964]
 [0.991]
 [1.057]]
maxi score, test score, baseline:  -0.34212666666666675 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.34212666666666675 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.34212666666666675 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.34212666666666675 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.34212666666666675 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.863419669656025
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  34.32883977890015
printing an ep nov before normalisation:  44.20269791368329
printing an ep nov before normalisation:  31.661923790557154
maxi score, test score, baseline:  -0.34212666666666675 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.1334, 0.1133, 0.1318, 0.1589, 0.1600, 0.1530, 0.1497],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0049, 0.9489, 0.0060, 0.0056, 0.0011, 0.0016, 0.0317],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1061, 0.0565, 0.3065, 0.1220, 0.1136, 0.1632, 0.1321],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1280, 0.0060, 0.1538, 0.2228, 0.1559, 0.1703, 0.1632],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1444, 0.0056, 0.0640, 0.0679, 0.5923, 0.0530, 0.0727],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0988, 0.0226, 0.1265, 0.1190, 0.1189, 0.3849, 0.1292],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1126, 0.0970, 0.1215, 0.1782, 0.1497, 0.1060, 0.2349],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  51 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.693]
 [0.817]
 [0.73 ]
 [0.667]
 [0.772]
 [0.449]
 [0.767]] [[34.492]
 [44.494]
 [39.626]
 [34.599]
 [36.45 ]
 [37.769]
 [39.126]] [[0.693]
 [0.817]
 [0.73 ]
 [0.667]
 [0.772]
 [0.449]
 [0.767]]
maxi score, test score, baseline:  -0.34212666666666675 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.34212666666666675 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.062]
 [-0.062]
 [-0.062]
 [-0.062]
 [-0.038]
 [-0.037]
 [-0.062]] [[69.843]
 [69.843]
 [69.843]
 [69.843]
 [64.906]
 [64.235]
 [69.843]] [[1.605]
 [1.605]
 [1.605]
 [1.605]
 [1.43 ]
 [1.405]
 [1.605]]
printing an ep nov before normalisation:  28.792932434671307
printing an ep nov before normalisation:  47.18753730239124
printing an ep nov before normalisation:  47.37642230121385
actor:  0 policy actor:  0  step number:  57 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  0.00018935686625809467
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  -0.3394866666666667 0.6913333333333335 0.6913333333333335
actor:  1 policy actor:  1  step number:  46 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.787]
 [0.787]
 [0.787]
 [0.787]
 [0.787]
 [0.787]
 [0.787]] [[33.04]
 [33.04]
 [33.04]
 [33.04]
 [33.04]
 [33.04]
 [33.04]] [[0.787]
 [0.787]
 [0.787]
 [0.787]
 [0.787]
 [0.787]
 [0.787]]
maxi score, test score, baseline:  -0.3394866666666667 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.3394866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.768]
 [0.773]
 [0.773]
 [0.773]
 [0.896]
 [0.773]
 [0.773]] [[47.174]
 [43.342]
 [43.342]
 [43.342]
 [52.069]
 [43.342]
 [43.342]] [[0.768]
 [0.773]
 [0.773]
 [0.773]
 [0.896]
 [0.773]
 [0.773]]
maxi score, test score, baseline:  -0.3394866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3394866666666667 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  50 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  42.39037188084772
using explorer policy with actor:  1
siam score:  -0.864757
maxi score, test score, baseline:  -0.33712666666666674 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.33712666666666674 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  44 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 0.0
actor:  1 policy actor:  1  step number:  51 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.3344466666666668 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.734]
 [0.842]
 [0.734]
 [0.735]
 [0.736]
 [0.737]
 [0.734]] [[51.64 ]
 [42.157]
 [49.885]
 [50.31 ]
 [50.291]
 [50.122]
 [49.711]] [[0.734]
 [0.842]
 [0.734]
 [0.735]
 [0.736]
 [0.737]
 [0.734]]
maxi score, test score, baseline:  -0.3344466666666668 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.73540749418758
maxi score, test score, baseline:  -0.3344466666666668 0.6913333333333335 0.6913333333333335
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.387]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]] [[42.176]
 [41.717]
 [42.176]
 [42.176]
 [42.176]
 [42.176]
 [42.176]] [[0.875]
 [0.909]
 [0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]]
printing an ep nov before normalisation:  35.207599208421065
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.35 ]
 [0.28 ]
 [0.28 ]
 [0.282]
 [0.282]
 [0.28 ]] [[43.951]
 [41.879]
 [43.951]
 [43.951]
 [43.496]
 [43.629]
 [43.951]] [[1.883]
 [1.803]
 [1.883]
 [1.883]
 [1.853]
 [1.862]
 [1.883]]
printing an ep nov before normalisation:  46.170356880567994
actor:  0 policy actor:  0  step number:  44 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.3316333333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3316333333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.22666666666666602  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  50 total reward:  0.38  reward:  1.0 rdn_beta:  1.667
UNIT TEST: sample policy line 217 mcts : [0.102 0.449 0.102 0.102 0.102 0.102 0.041]
maxi score, test score, baseline:  -0.3316333333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3316333333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.14097389044195
maxi score, test score, baseline:  -0.3316333333333334 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.3316333333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.25237846374512
maxi score, test score, baseline:  -0.3316333333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3316333333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.0666666666666661  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.3316333333333334 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.8593136
printing an ep nov before normalisation:  47.43354256759975
printing an ep nov before normalisation:  44.13365882150579
printing an ep nov before normalisation:  35.451373747678296
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.833]
 [0.831]
 [0.741]
 [0.831]
 [0.742]
 [0.758]] [[31.489]
 [43.615]
 [39.075]
 [31.309]
 [38.869]
 [31.181]
 [36.334]] [[0.738]
 [0.833]
 [0.831]
 [0.741]
 [0.831]
 [0.742]
 [0.758]]
Printing some Q and Qe and total Qs values:  [[0.22 ]
 [0.34 ]
 [0.243]
 [0.238]
 [0.237]
 [0.237]
 [0.346]] [[47.47 ]
 [41.512]
 [45.032]
 [45.218]
 [45.226]
 [45.266]
 [41.42 ]] [[1.171]
 [1.075]
 [1.106]
 [1.107]
 [1.107]
 [1.108]
 [1.078]]
actor:  0 policy actor:  0  step number:  59 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.3292866666666668 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.77143572872546
actor:  1 policy actor:  1  step number:  60 total reward:  0.13999999999999957  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.3292866666666668 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.917503356933594
actor:  0 policy actor:  0  step number:  70 total reward:  0.03333333333333277  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.036]
 [0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[38.685]
 [58.009]
 [38.685]
 [38.685]
 [38.685]
 [38.685]
 [38.685]] [[0.404]
 [0.817]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]]
maxi score, test score, baseline:  -0.3272200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3272200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.33230706164204
line 256 mcts: sample exp_bonus 43.59579409378414
maxi score, test score, baseline:  -0.3272200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.3272200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.3272200000000001 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.3272200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.478678703308105
maxi score, test score, baseline:  -0.3272200000000001 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.21333333333333304  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.32722000000000007 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.20743953955105
printing an ep nov before normalisation:  42.822280417379034
actor:  1 policy actor:  1  step number:  42 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.32722000000000007 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.276811444985995
printing an ep nov before normalisation:  46.969451904296875
printing an ep nov before normalisation:  46.3271993124615
printing an ep nov before normalisation:  43.21462631225586
maxi score, test score, baseline:  -0.32722000000000007 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.28238208053307
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.99482384387646
maxi score, test score, baseline:  -0.32722000000000007 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  32.99726196115976
printing an ep nov before normalisation:  41.75339466490066
maxi score, test score, baseline:  -0.32722000000000007 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.797006243893975
actor:  1 policy actor:  1  step number:  62 total reward:  0.27333333333333265  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  29.676408767700195
actor:  1 policy actor:  1  step number:  56 total reward:  0.16666666666666596  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.071]
 [-0.071]
 [-0.071]
 [-0.071]
 [-0.083]
 [-0.071]
 [-0.071]] [[40.916]
 [40.916]
 [40.916]
 [40.916]
 [55.618]
 [40.916]
 [40.916]] [[0.633]
 [0.633]
 [0.633]
 [0.633]
 [1.114]
 [0.633]
 [0.633]]
siam score:  -0.85635924
maxi score, test score, baseline:  -0.32722000000000007 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  -0.32722000000000007 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  0.5666666666666668  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.32722000000000007 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  38.43950884682792
maxi score, test score, baseline:  -0.32722000000000007 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.32722000000000007 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.936]
 [0.936]
 [0.936]
 [0.936]
 [0.936]
 [0.936]
 [0.936]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.936]
 [0.936]
 [0.936]
 [0.936]
 [0.936]
 [0.936]
 [0.936]]
siam score:  -0.8619886
printing an ep nov before normalisation:  24.951195806370308
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]] [[34.401]
 [34.401]
 [34.401]
 [34.401]
 [34.401]
 [34.401]
 [34.401]] [[2.12]
 [2.12]
 [2.12]
 [2.12]
 [2.12]
 [2.12]
 [2.12]]
printing an ep nov before normalisation:  45.19317751746011
printing an ep nov before normalisation:  58.71821431226678
printing an ep nov before normalisation:  46.89812183380127
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.32722000000000007 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.4224353509033
Printing some Q and Qe and total Qs values:  [[1.017]
 [1.031]
 [1.017]
 [1.017]
 [1.017]
 [1.017]
 [1.017]] [[34.537]
 [38.091]
 [34.537]
 [34.537]
 [34.537]
 [34.537]
 [34.537]] [[1.017]
 [1.031]
 [1.017]
 [1.017]
 [1.017]
 [1.017]
 [1.017]]
Printing some Q and Qe and total Qs values:  [[0.805]
 [0.979]
 [0.805]
 [0.805]
 [0.805]
 [0.805]
 [0.805]] [[40.954]
 [32.827]
 [40.954]
 [40.954]
 [40.954]
 [40.954]
 [40.954]] [[0.805]
 [0.979]
 [0.805]
 [0.805]
 [0.805]
 [0.805]
 [0.805]]
printing an ep nov before normalisation:  42.26808547973633
Printing some Q and Qe and total Qs values:  [[0.645]
 [0.916]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]] [[39.919]
 [36.439]
 [39.919]
 [39.919]
 [39.919]
 [39.919]
 [39.919]] [[0.645]
 [0.916]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  31.372456014650243
printing an ep nov before normalisation:  31.162129719858743
printing an ep nov before normalisation:  34.92293834686279
printing an ep nov before normalisation:  31.925792694091797
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  44.2663062004477
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  1 policy actor:  1  step number:  54 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.25951333333333343 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.25951333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.89466226659602
maxi score, test score, baseline:  -0.25951333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.465147035889956
maxi score, test score, baseline:  -0.25951333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.095]
 [-0.064]
 [-0.07 ]
 [-0.075]
 [-0.083]
 [-0.07 ]
 [-0.07 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.095]
 [-0.064]
 [-0.07 ]
 [-0.075]
 [-0.083]
 [-0.07 ]
 [-0.07 ]]
Printing some Q and Qe and total Qs values:  [[-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]
 [-0.087]]
maxi score, test score, baseline:  -0.25951333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85929155
maxi score, test score, baseline:  -0.25951333333333343 0.6926666666666669 0.6926666666666669
actor:  1 policy actor:  1  step number:  76 total reward:  0.006666666666665821  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.25951333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.888599568813625
actor:  1 policy actor:  1  step number:  51 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.25951333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.333319002306204
printing an ep nov before normalisation:  42.48936653137207
maxi score, test score, baseline:  -0.25951333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.467378079952155
maxi score, test score, baseline:  -0.25951333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.25951333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.25951333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.25951333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  69 total reward:  0.13333333333333264  reward:  1.0 rdn_beta:  1.333
siam score:  -0.84774184
Printing some Q and Qe and total Qs values:  [[-0.098]
 [-0.095]
 [-0.098]
 [-0.1  ]
 [-0.1  ]
 [-0.099]
 [-0.1  ]] [[52.706]
 [39.327]
 [52.728]
 [52.556]
 [52.556]
 [52.171]
 [51.019]] [[1.525]
 [0.731]
 [1.525]
 [1.514]
 [1.514]
 [1.491]
 [1.422]]
maxi score, test score, baseline:  -0.25951333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.19999999999999962  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  39.59517434214823
printing an ep nov before normalisation:  47.01616576902495
Printing some Q and Qe and total Qs values:  [[-0.086]
 [-0.054]
 [-0.079]
 [-0.07 ]
 [-0.07 ]
 [-0.072]
 [-0.086]] [[42.745]
 [40.561]
 [44.714]
 [50.666]
 [51.199]
 [47.462]
 [42.745]] [[0.904]
 [0.848]
 [0.989]
 [1.234]
 [1.256]
 [1.105]
 [0.904]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.42268232494626
maxi score, test score, baseline:  -0.25951333333333343 0.6926666666666669 0.6926666666666669
printing an ep nov before normalisation:  49.50389927776494
printing an ep nov before normalisation:  39.60983791214874
maxi score, test score, baseline:  -0.25951333333333343 0.6926666666666669 0.6926666666666669
Printing some Q and Qe and total Qs values:  [[0.424]
 [0.462]
 [0.424]
 [0.424]
 [0.424]
 [0.424]
 [0.424]] [[43.738]
 [44.128]
 [43.738]
 [43.738]
 [43.738]
 [43.738]
 [43.738]] [[1.814]
 [1.877]
 [1.814]
 [1.814]
 [1.814]
 [1.814]
 [1.814]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.25951333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.25951333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.52104457289251
using explorer policy with actor:  1
printing an ep nov before normalisation:  16.32184269708354
maxi score, test score, baseline:  -0.25951333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  74 total reward:  0.08666666666666523  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.25951333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.016]
 [ 0.051]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]
 [-0.016]] [[36.372]
 [34.031]
 [36.372]
 [36.372]
 [36.372]
 [36.372]
 [36.372]] [[1.014]
 [0.952]
 [1.014]
 [1.014]
 [1.014]
 [1.014]
 [1.014]]
maxi score, test score, baseline:  -0.25951333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.37644603423348
maxi score, test score, baseline:  -0.25951333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.23561412115851
printing an ep nov before normalisation:  43.350953208930356
printing an ep nov before normalisation:  14.365352392196655
actor:  0 policy actor:  0  step number:  37 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  62.912310618006444
printing an ep nov before normalisation:  44.55560051711653
actor:  1 policy actor:  1  step number:  48 total reward:  0.4199999999999995  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2565800000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.70649719846972
maxi score, test score, baseline:  -0.2565800000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.54089600376308
printing an ep nov before normalisation:  65.72855275057626
Printing some Q and Qe and total Qs values:  [[0.39 ]
 [0.469]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]] [[38.444]
 [39.584]
 [33.211]
 [38.95 ]
 [33.211]
 [33.211]
 [33.211]] [[1.568]
 [1.719]
 [1.296]
 [1.654]
 [1.296]
 [1.296]
 [1.296]]
printing an ep nov before normalisation:  34.77433735680932
printing an ep nov before normalisation:  42.73999316501102
maxi score, test score, baseline:  -0.2565800000000001 0.6926666666666669 0.6926666666666669
line 256 mcts: sample exp_bonus 40.700642326620766
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.66361951828003
maxi score, test score, baseline:  -0.2565800000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  0.17999999999999972  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8496124
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.2565800000000001 0.6926666666666669 0.6926666666666669
printing an ep nov before normalisation:  46.931446923149956
actor:  1 policy actor:  1  step number:  58 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  2.0
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.188]
 [0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]] [[40.01]
 [40.74]
 [40.01]
 [40.01]
 [40.01]
 [40.01]
 [40.01]] [[0.302]
 [0.463]
 [0.302]
 [0.302]
 [0.302]
 [0.302]
 [0.302]]
printing an ep nov before normalisation:  36.33660078048706
maxi score, test score, baseline:  -0.2565800000000001 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.2565800000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 45.2113901999199
printing an ep nov before normalisation:  30.480964183807373
printing an ep nov before normalisation:  46.61750204625415
maxi score, test score, baseline:  -0.2565800000000001 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.2565800000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.10706136761967
printing an ep nov before normalisation:  43.01626509303164
actor:  0 policy actor:  0  step number:  54 total reward:  0.28666666666666596  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  48 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.411]
 [0.316]
 [0.314]
 [0.297]
 [0.315]
 [0.311]] [[47.38 ]
 [47.131]
 [51.621]
 [51.566]
 [47.38 ]
 [51.33 ]
 [52.134]] [[1.059]
 [1.164]
 [1.21 ]
 [1.207]
 [1.059]
 [1.2  ]
 [1.221]]
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.456]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]] [[41.582]
 [43.737]
 [41.582]
 [41.582]
 [41.582]
 [41.582]
 [41.582]] [[1.507]
 [1.78 ]
 [1.507]
 [1.507]
 [1.507]
 [1.507]
 [1.507]]
maxi score, test score, baseline:  -0.25124666666666673 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.8483568
maxi score, test score, baseline:  -0.25124666666666673 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.1249069283395
actor:  1 policy actor:  1  step number:  41 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.25124666666666673 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  66 total reward:  0.13999999999999968  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  60 total reward:  0.24666666666666637  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 43.41875167609405
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.25124666666666673 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85021895
maxi score, test score, baseline:  -0.25124666666666673 0.6926666666666669 0.6926666666666669
actor:  1 policy actor:  1  step number:  59 total reward:  0.2266666666666659  reward:  1.0 rdn_beta:  0.667
UNIT TEST: sample policy line 217 mcts : [0.327 0.143 0.061 0.082 0.082 0.245 0.061]
printing an ep nov before normalisation:  74.73573984266919
maxi score, test score, baseline:  -0.25124666666666673 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]
 [0.427]] [[44.41]
 [44.41]
 [44.41]
 [44.41]
 [44.41]
 [44.41]
 [44.41]] [[1.773]
 [1.773]
 [1.773]
 [1.773]
 [1.773]
 [1.773]
 [1.773]]
line 256 mcts: sample exp_bonus 33.03269015162696
actions average: 
K:  0  action  0 :  tensor([0.1934, 0.0026, 0.1160, 0.1311, 0.2949, 0.1230, 0.1390],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0079, 0.9620, 0.0043, 0.0037, 0.0018, 0.0022, 0.0180],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1115, 0.0126, 0.4607, 0.1026, 0.0836, 0.1136, 0.1155],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1301, 0.0171, 0.1225, 0.3379, 0.1166, 0.1507, 0.1251],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1663, 0.0015, 0.0968, 0.1332, 0.3683, 0.1086, 0.1252],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0835, 0.0011, 0.1161, 0.0803, 0.0817, 0.5463, 0.0909],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1231, 0.1804, 0.0982, 0.1465, 0.1087, 0.1053, 0.2378],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  36.959760533683536
maxi score, test score, baseline:  -0.25124666666666673 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.25124666666666673 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  0.31999999999999984  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  66 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.24860666666666673 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.24860666666666673 0.6926666666666669 0.6926666666666669
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.059]
 [-0.055]
 [-0.064]
 [-0.063]
 [-0.05 ]
 [-0.059]
 [-0.059]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.059]
 [-0.055]
 [-0.064]
 [-0.063]
 [-0.05 ]
 [-0.059]
 [-0.059]]
printing an ep nov before normalisation:  44.494599460391974
actions average: 
K:  0  action  0 :  tensor([0.2856, 0.0174, 0.1223, 0.1291, 0.1480, 0.1424, 0.1553],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0051, 0.9574, 0.0047, 0.0040, 0.0028, 0.0034, 0.0227],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0469, 0.0319, 0.6376, 0.0399, 0.0355, 0.1065, 0.1017],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1090, 0.0079, 0.1000, 0.3770, 0.1442, 0.1154, 0.1466],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1099, 0.0018, 0.1087, 0.1106, 0.4089, 0.1294, 0.1307],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0655, 0.0050, 0.1079, 0.0859, 0.1007, 0.5434, 0.0916],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1598, 0.0563, 0.1137, 0.1208, 0.1688, 0.1364, 0.2444],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  49.73141494435049
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]
 [-0.007]] [[27.143]
 [27.143]
 [27.143]
 [27.143]
 [27.143]
 [27.143]
 [27.143]] [[0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]
 [0.159]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.21208250161631
printing an ep nov before normalisation:  31.99543985343201
printing an ep nov before normalisation:  32.697474357893896
maxi score, test score, baseline:  -0.24860666666666673 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 40.212239994764985
maxi score, test score, baseline:  -0.24860666666666673 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.24860666666666673 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  64 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  54 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  12.584808823265694
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.2120, 0.0071, 0.1370, 0.1537, 0.1759, 0.1579, 0.1564],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0042,     0.9680,     0.0013,     0.0065,     0.0013,     0.0009,
            0.0178], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0830, 0.0059, 0.5501, 0.0751, 0.0836, 0.1120, 0.0903],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0915, 0.0530, 0.0815, 0.4140, 0.1186, 0.1111, 0.1302],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0986, 0.0030, 0.0622, 0.0845, 0.5913, 0.0680, 0.0923],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1316, 0.0200, 0.1288, 0.1452, 0.1789, 0.2573, 0.1381],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1885, 0.0261, 0.2133, 0.1149, 0.1372, 0.1105, 0.2095],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.053]
 [-0.054]
 [-0.054]
 [-0.054]
 [-0.059]
 [-0.055]
 [-0.055]] [[50.625]
 [49.147]
 [51.774]
 [51.769]
 [51.461]
 [51.847]
 [52.566]] [[1.174]
 [1.102]
 [1.228]
 [1.228]
 [1.208]
 [1.23 ]
 [1.264]]
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
printing an ep nov before normalisation:  42.103429684399075
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.72976216380355
actor:  1 policy actor:  1  step number:  52 total reward:  0.32666666666666655  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.27871360894089
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.407]
 [0.409]] [[40.08 ]
 [40.08 ]
 [40.08 ]
 [40.08 ]
 [40.08 ]
 [44.158]
 [40.08 ]] [[1.225]
 [1.225]
 [1.225]
 [1.225]
 [1.225]
 [1.372]
 [1.225]]
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.82100296020508
printing an ep nov before normalisation:  49.45787663378435
actor:  1 policy actor:  1  step number:  52 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  62 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
printing an ep nov before normalisation:  43.7145571416816
printing an ep nov before normalisation:  21.24354124069214
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.296695386995346
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.3379, 0.0019, 0.1482, 0.1164, 0.1340, 0.1209, 0.1407],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0076, 0.9421, 0.0055, 0.0039, 0.0014, 0.0017, 0.0378],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1150, 0.0040, 0.4108, 0.0968, 0.0924, 0.1660, 0.1149],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1139, 0.0228, 0.1323, 0.3480, 0.1185, 0.1315, 0.1329],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1280, 0.0135, 0.1411, 0.1080, 0.3454, 0.1218, 0.1423],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1348, 0.0099, 0.1634, 0.1334, 0.1418, 0.2639, 0.1528],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1064, 0.1207, 0.1514, 0.1341, 0.0884, 0.0849, 0.3140],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  43.9047038526414
actor:  1 policy actor:  1  step number:  59 total reward:  0.03999999999999926  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.028]
 [-0.031]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]] [[51.793]
 [48.934]
 [51.793]
 [51.793]
 [51.793]
 [51.793]
 [51.793]] [[0.23 ]
 [0.196]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]]
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.11333333333333284  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.707]
 [0.557]
 [0.557]
 [0.557]
 [0.557]
 [0.557]] [[41.601]
 [42.768]
 [41.601]
 [41.601]
 [41.601]
 [41.601]
 [41.601]] [[0.775]
 [0.935]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]]
printing an ep nov before normalisation:  38.84301030006135
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]] [[40.021]
 [40.021]
 [40.021]
 [40.021]
 [40.021]
 [40.021]
 [40.021]] [[0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]
 [0.922]]
actor:  1 policy actor:  1  step number:  41 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.556]
 [0.413]
 [0.395]
 [0.394]
 [0.354]
 [0.524]] [[50.552]
 [41.469]
 [49.556]
 [50.554]
 [50.934]
 [49.534]
 [50.552]] [[2.462]
 [1.98 ]
 [2.295]
 [2.333]
 [2.354]
 [2.235]
 [2.462]]
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.98634719848633
printing an ep nov before normalisation:  40.81702709197998
actor:  1 policy actor:  1  step number:  44 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  68 total reward:  0.09999999999999931  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.285733598969216
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.4534, 0.0424, 0.0781, 0.0739, 0.1804, 0.0863, 0.0856],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0099, 0.9636, 0.0046, 0.0047, 0.0018, 0.0020, 0.0134],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0940, 0.0045, 0.3844, 0.1017, 0.1001, 0.2178, 0.0974],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1241, 0.0963, 0.1134, 0.2538, 0.1416, 0.1529, 0.1180],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1393, 0.0040, 0.0865, 0.1031, 0.4752, 0.0975, 0.0944],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1060, 0.0169, 0.1803, 0.1168, 0.1071, 0.3471, 0.1257],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0932, 0.2586, 0.0821, 0.1025, 0.1424, 0.1006, 0.2207],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  62.086299372762596
printing an ep nov before normalisation:  45.99072668287489
Printing some Q and Qe and total Qs values:  [[-0.031]
 [-0.051]
 [-0.033]
 [-0.033]
 [-0.033]
 [-0.037]
 [-0.033]] [[43.218]
 [49.164]
 [45.991]
 [45.991]
 [45.991]
 [45.036]
 [45.991]] [[0.176]
 [0.216]
 [0.201]
 [0.201]
 [0.201]
 [0.188]
 [0.201]]
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.78 ]
 [0.614]
 [0.636]
 [0.626]
 [0.597]
 [0.672]] [[41.44 ]
 [46.321]
 [43.129]
 [41.675]
 [41.509]
 [42.497]
 [43.065]] [[0.841]
 [1.026]
 [0.832]
 [0.84 ]
 [0.828]
 [0.808]
 [0.889]]
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.38666666666666616  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.153]
 [0.057]
 [0.14 ]
 [0.054]
 [0.05 ]
 [0.096]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.032]
 [0.153]
 [0.057]
 [0.14 ]
 [0.054]
 [0.05 ]
 [0.096]]
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2486066666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.509239196777344
Printing some Q and Qe and total Qs values:  [[0.775]
 [0.908]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]] [[38.327]
 [37.317]
 [38.327]
 [38.327]
 [38.327]
 [38.327]
 [38.327]] [[0.775]
 [0.908]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]]
maxi score, test score, baseline:  -0.24860666666666675 0.6926666666666669 0.6926666666666669
printing an ep nov before normalisation:  51.80343152287796
printing an ep nov before normalisation:  26.5015968566274
printing an ep nov before normalisation:  29.810194224809663
maxi score, test score, baseline:  -0.24860666666666675 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.24860666666666675 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  57 total reward:  0.18666666666666587  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  59 total reward:  0.173333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.24623333333333344 0.6926666666666669 0.6926666666666669
actions average: 
K:  1  action  0 :  tensor([0.4363, 0.0417, 0.0890, 0.0847, 0.1583, 0.0815, 0.1085],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0061, 0.9639, 0.0062, 0.0075, 0.0018, 0.0016, 0.0130],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0913, 0.0215, 0.3562, 0.1034, 0.1090, 0.2148, 0.1038],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1589, 0.0029, 0.1342, 0.1779, 0.2026, 0.1710, 0.1525],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1202, 0.0012, 0.0874, 0.0939, 0.4979, 0.1062, 0.0932],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1351, 0.0026, 0.1418, 0.1464, 0.1659, 0.2818, 0.1264],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1262, 0.1020, 0.1145, 0.1374, 0.1281, 0.1197, 0.2721],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.24623333333333344 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.24623333333333344 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.789514474721265
actor:  0 policy actor:  0  step number:  34 total reward:  0.4999999999999999  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.00339724051769
printing an ep nov before normalisation:  29.198365211486816
Printing some Q and Qe and total Qs values:  [[ 0.027]
 [ 0.274]
 [-0.025]
 [ 0.025]
 [ 0.03 ]
 [-0.035]
 [ 0.127]] [[41.781]
 [37.472]
 [42.748]
 [45.846]
 [51.785]
 [47.647]
 [40.863]] [[0.217]
 [0.424]
 [0.175]
 [0.253]
 [0.312]
 [0.209]
 [0.309]]
printing an ep nov before normalisation:  41.0924432893227
printing an ep nov before normalisation:  46.95990588511227
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  35.73735286911578
printing an ep nov before normalisation:  44.94861602783203
actor:  1 policy actor:  1  step number:  59 total reward:  0.18666666666666598  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.24323333333333338 0.6926666666666669 0.6926666666666669
actor:  1 policy actor:  1  step number:  59 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.24323333333333338 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.96308070419588
maxi score, test score, baseline:  -0.24323333333333338 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.2988, 0.0021, 0.1417, 0.1212, 0.1781, 0.1276, 0.1305],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0231, 0.8618, 0.0220, 0.0227, 0.0171, 0.0144, 0.0389],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0728, 0.0652, 0.4524, 0.0644, 0.0723, 0.1189, 0.1540],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1457, 0.0258, 0.0868, 0.3530, 0.1700, 0.1060, 0.1127],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1583, 0.0041, 0.1409, 0.1254, 0.3008, 0.1317, 0.1388],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1104, 0.0272, 0.1429, 0.1090, 0.1178, 0.3949, 0.0979],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0711, 0.2274, 0.0755, 0.1528, 0.0825, 0.0598, 0.3309],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.24323333333333338 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.008974293542174
printing an ep nov before normalisation:  36.04242288796867
actor:  1 policy actor:  1  step number:  45 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  41.20656813712564
actor:  0 policy actor:  0  step number:  33 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  50 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.24008666666666678 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.653133840054462
maxi score, test score, baseline:  -0.24008666666666673 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.24008666666666673 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  69 total reward:  0.15999999999999903  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.223054248916846
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.24008666666666673 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.55592700312059
using explorer policy with actor:  1
printing an ep nov before normalisation:  3.5531757143770903
actor:  0 policy actor:  0  step number:  58 total reward:  0.09999999999999942  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.2378866666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2378866666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2378866666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2378866666666668 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.3695182800293
printing an ep nov before normalisation:  20.567941665649414
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.471]
 [0.573]
 [0.471]
 [0.471]
 [0.471]
 [0.471]
 [0.471]] [[37.847]
 [36.997]
 [37.847]
 [37.847]
 [37.847]
 [37.847]
 [37.847]] [[1.859]
 [1.904]
 [1.859]
 [1.859]
 [1.859]
 [1.859]
 [1.859]]
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.521]
 [0.449]
 [0.452]
 [0.454]
 [0.477]
 [0.452]] [[33.942]
 [40.661]
 [33.814]
 [33.548]
 [33.607]
 [39.91 ]
 [34.004]] [[0.78 ]
 [1.006]
 [0.782]
 [0.779]
 [0.781]
 [0.946]
 [0.789]]
Printing some Q and Qe and total Qs values:  [[-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.025]
 [-0.041]] [[53.24 ]
 [53.24 ]
 [53.24 ]
 [53.24 ]
 [53.24 ]
 [61.911]
 [53.24 ]] [[0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.917]
 [0.669]]
actor:  1 policy actor:  1  step number:  57 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.23788666666666672 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.367]
 [0.51 ]
 [0.367]
 [0.367]
 [0.367]
 [0.34 ]
 [0.367]] [[47.743]
 [45.642]
 [47.743]
 [47.743]
 [47.743]
 [49.355]
 [47.743]] [[1.297]
 [1.367]
 [1.297]
 [1.297]
 [1.297]
 [1.326]
 [1.297]]
maxi score, test score, baseline:  -0.23788666666666672 0.6926666666666669 0.6926666666666669
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.759]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]] [[30.307]
 [39.916]
 [30.307]
 [30.307]
 [30.307]
 [30.307]
 [30.307]] [[0.675]
 [0.759]
 [0.675]
 [0.675]
 [0.675]
 [0.675]
 [0.675]]
maxi score, test score, baseline:  -0.23788666666666672 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.662]
 [0.796]
 [0.703]
 [0.664]
 [0.66 ]
 [0.703]
 [0.675]] [[42.327]
 [40.703]
 [42.423]
 [44.482]
 [43.915]
 [42.423]
 [42.204]] [[0.662]
 [0.796]
 [0.703]
 [0.664]
 [0.66 ]
 [0.703]
 [0.675]]
maxi score, test score, baseline:  -0.23788666666666672 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.25333333333333274  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  55.67660504220276
maxi score, test score, baseline:  -0.23788666666666677 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.438565165469285
maxi score, test score, baseline:  -0.23788666666666677 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.23788666666666677 0.6926666666666669 0.6926666666666669
printing an ep nov before normalisation:  42.197002253069265
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.23788666666666677 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.044]
 [-0.027]
 [-0.044]
 [-0.044]
 [-0.044]
 [-0.044]
 [-0.044]] [[19.746]
 [42.384]
 [19.746]
 [19.746]
 [19.746]
 [19.746]
 [19.746]] [[0.355]
 [1.262]
 [0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.355]]
maxi score, test score, baseline:  -0.23788666666666677 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.23788666666666677 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.23788666666666677 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.28595187166051
actor:  1 policy actor:  1  step number:  50 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.23788666666666677 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.23788666666666677 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.17999999999999938  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.637]
 [0.572]
 [0.522]
 [0.522]
 [0.522]
 [0.522]] [[30.303]
 [38.772]
 [28.988]
 [30.303]
 [30.303]
 [30.303]
 [30.303]] [[1.324]
 [1.858]
 [1.309]
 [1.324]
 [1.324]
 [1.324]
 [1.324]]
Printing some Q and Qe and total Qs values:  [[0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]] [[38.223]
 [38.223]
 [38.223]
 [38.223]
 [38.223]
 [38.223]
 [38.223]] [[1.512]
 [1.512]
 [1.512]
 [1.512]
 [1.512]
 [1.512]
 [1.512]]
UNIT TEST: sample policy line 217 mcts : [0.163 0.184 0.122 0.102 0.102 0.102 0.224]
printing an ep nov before normalisation:  42.516910516395406
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.23788666666666677 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.23788666666666677 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.68 ]
 [0.756]
 [0.679]
 [0.653]
 [0.552]
 [0.582]
 [0.695]] [[47.131]
 [43.155]
 [47.756]
 [47.615]
 [49.075]
 [49.547]
 [46.837]] [[1.216]
 [1.206]
 [1.228]
 [1.2  ]
 [1.13 ]
 [1.17 ]
 [1.225]]
maxi score, test score, baseline:  -0.23788666666666677 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  52 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8533341
maxi score, test score, baseline:  -0.23788666666666677 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.1421, 0.0340, 0.1369, 0.1345, 0.2588, 0.1466, 0.1470],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0130, 0.9216, 0.0116, 0.0150, 0.0056, 0.0048, 0.0283],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0920, 0.2290, 0.2105, 0.1291, 0.0859, 0.0766, 0.1769],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0958, 0.1033, 0.1256, 0.2775, 0.1201, 0.1138, 0.1640],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0870, 0.0057, 0.0812, 0.1267, 0.4607, 0.1227, 0.1161],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1123, 0.0297, 0.1473, 0.1281, 0.1353, 0.3106, 0.1366],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1344, 0.1715, 0.0888, 0.1055, 0.0784, 0.0762, 0.3452],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.016]
 [-0.016]
 [ 0.038]
 [-0.013]
 [-0.036]
 [-0.013]
 [ 0.157]] [[71.124]
 [71.124]
 [67.282]
 [67.788]
 [72.992]
 [79.873]
 [63.077]] [[0.951]
 [0.951]
 [0.872]
 [0.839]
 [0.997]
 [1.258]
 [0.846]]
Printing some Q and Qe and total Qs values:  [[ 0.171]
 [ 0.171]
 [ 0.171]
 [ 0.171]
 [ 0.147]
 [-0.   ]
 [ 0.171]] [[42.933]
 [42.933]
 [42.933]
 [42.933]
 [45.211]
 [44.181]
 [42.933]] [[1.222]
 [1.222]
 [1.222]
 [1.222]
 [1.307]
 [1.111]
 [1.222]]
printing an ep nov before normalisation:  43.49442158800805
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.256]
 [0.294]
 [0.256]
 [0.256]
 [0.262]
 [0.256]
 [0.256]] [[39.554]
 [41.222]
 [39.554]
 [39.554]
 [37.777]
 [39.554]
 [39.554]] [[1.415]
 [1.539]
 [1.415]
 [1.415]
 [1.329]
 [1.415]
 [1.415]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.2183, 0.0072, 0.1409, 0.1556, 0.1478, 0.1722, 0.1580],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0051, 0.9555, 0.0062, 0.0114, 0.0014, 0.0013, 0.0191],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1277, 0.0210, 0.2767, 0.1537, 0.1498, 0.1264, 0.1447],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1031, 0.0179, 0.1157, 0.2872, 0.1504, 0.2209, 0.1048],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1327, 0.0088, 0.1075, 0.1233, 0.3817, 0.1220, 0.1240],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1152, 0.0222, 0.1468, 0.1086, 0.1043, 0.3795, 0.1234],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1122, 0.0047, 0.1277, 0.2007, 0.1020, 0.1255, 0.3271],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.23788666666666677 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.58186110211342
printing an ep nov before normalisation:  43.3378550242149
actor:  1 policy actor:  1  step number:  51 total reward:  0.4  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]] [[35.624]
 [35.624]
 [35.624]
 [35.624]
 [35.624]
 [35.624]
 [35.624]] [[1.282]
 [1.282]
 [1.282]
 [1.282]
 [1.282]
 [1.282]
 [1.282]]
printing an ep nov before normalisation:  29.227967262268066
Printing some Q and Qe and total Qs values:  [[0.748]
 [0.929]
 [0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]] [[44.661]
 [37.153]
 [44.661]
 [44.661]
 [44.661]
 [44.661]
 [44.661]] [[0.748]
 [0.929]
 [0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]]
printing an ep nov before normalisation:  44.37258395922855
maxi score, test score, baseline:  -0.23788666666666677 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.23788666666666677 0.6926666666666669 0.6926666666666669
actions average: 
K:  4  action  0 :  tensor([0.2823, 0.0475, 0.1160, 0.1323, 0.1470, 0.1300, 0.1449],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0127, 0.9112, 0.0099, 0.0122, 0.0090, 0.0092, 0.0358],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1346, 0.0502, 0.2265, 0.1214, 0.1495, 0.1919, 0.1260],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0861, 0.1087, 0.0740, 0.3883, 0.0925, 0.0876, 0.1628],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1570, 0.0895, 0.0697, 0.0875, 0.4461, 0.0721, 0.0781],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0907, 0.1686, 0.1698, 0.0959, 0.0885, 0.3058, 0.0808],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1524, 0.1602, 0.1099, 0.1281, 0.1322, 0.0959, 0.2212],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  48.24095621156426
actor:  1 policy actor:  1  step number:  52 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  35.30625696921737
maxi score, test score, baseline:  -0.23788666666666677 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  50 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.23552666666666675 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.53771162033081
printing an ep nov before normalisation:  30.924392938973607
maxi score, test score, baseline:  -0.23552666666666675 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.587]
 [0.442]
 [0.449]
 [0.453]
 [0.421]
 [0.386]] [[43.449]
 [35.728]
 [43.316]
 [43.349]
 [43.459]
 [47.635]
 [42.519]] [[1.32 ]
 [1.218]
 [1.309]
 [1.316]
 [1.323]
 [1.421]
 [1.228]]
siam score:  -0.86054474
maxi score, test score, baseline:  -0.23552666666666675 0.6926666666666669 0.6926666666666669
Printing some Q and Qe and total Qs values:  [[-0.013]
 [-0.013]
 [-0.013]
 [-0.013]
 [ 0.055]
 [-0.013]
 [-0.013]] [[43.218]
 [43.218]
 [43.218]
 [43.218]
 [44.096]
 [43.218]
 [43.218]] [[0.633]
 [0.633]
 [0.633]
 [0.633]
 [0.718]
 [0.633]
 [0.633]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8652294
printing an ep nov before normalisation:  52.711021900177
maxi score, test score, baseline:  -0.23552666666666675 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.23552666666666675 0.6926666666666669 0.6926666666666669
printing an ep nov before normalisation:  47.47399522224929
maxi score, test score, baseline:  -0.23552666666666675 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.23552666666666675 0.6926666666666669 0.6926666666666669
actor:  1 policy actor:  1  step number:  49 total reward:  0.48  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.23552666666666675 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.23552666666666675 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.260934543846425
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.589]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]] [[39.036]
 [36.077]
 [39.036]
 [39.036]
 [39.036]
 [39.036]
 [39.036]] [[1.248]
 [1.321]
 [1.248]
 [1.248]
 [1.248]
 [1.248]
 [1.248]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.23552666666666675 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.23552666666666675 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.14081459858321
printing an ep nov before normalisation:  34.45369649222721
printing an ep nov before normalisation:  26.594700813293457
actor:  1 policy actor:  1  step number:  52 total reward:  0.28666666666666596  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.42226074264687
maxi score, test score, baseline:  -0.23552666666666675 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.23552666666666675 0.6926666666666669 0.6926666666666669
printing an ep nov before normalisation:  35.19423433226352
maxi score, test score, baseline:  -0.23552666666666675 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  74 total reward:  0.09999999999999931  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]]
printing an ep nov before normalisation:  37.876469238057
actions average: 
K:  4  action  0 :  tensor([0.1265, 0.0308, 0.1404, 0.1815, 0.1488, 0.1866, 0.1855],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0081, 0.9310, 0.0076, 0.0151, 0.0048, 0.0055, 0.0279],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1332, 0.0200, 0.2777, 0.1370, 0.1110, 0.1572, 0.1639],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1053, 0.3243, 0.0930, 0.1329, 0.1244, 0.1244, 0.0957],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0888, 0.0111, 0.0643, 0.1836, 0.4736, 0.0935, 0.0851],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1211, 0.0055, 0.1517, 0.1630, 0.1659, 0.2697, 0.1231],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1298, 0.0582, 0.1255, 0.1879, 0.1295, 0.1596, 0.2095],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.23552666666666675 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.23552666666666675 0.6926666666666669 0.6926666666666669
printing an ep nov before normalisation:  42.81918219598197
printing an ep nov before normalisation:  56.93647143085013
maxi score, test score, baseline:  -0.23552666666666675 0.6926666666666669 0.6926666666666669
actor:  1 policy actor:  1  step number:  62 total reward:  0.3399999999999992  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.23552666666666675 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.3677, 0.0026, 0.1003, 0.1228, 0.1507, 0.1424, 0.1135],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0021,     0.9748,     0.0014,     0.0020,     0.0004,     0.0003,
            0.0190], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1246, 0.0026, 0.2605, 0.1574, 0.1524, 0.1642, 0.1382],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1050, 0.0069, 0.0988, 0.4102, 0.1258, 0.1355, 0.1178],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0997, 0.0025, 0.0923, 0.1222, 0.4410, 0.1252, 0.1171],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1072, 0.0021, 0.1576, 0.1157, 0.1032, 0.4042, 0.1099],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1274, 0.0795, 0.1339, 0.1629, 0.1445, 0.1832, 0.1685],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[-0.004]
 [-0.004]
 [-0.027]
 [-0.004]
 [-0.021]
 [-0.004]
 [-0.004]] [[37.444]
 [37.444]
 [38.081]
 [37.444]
 [36.237]
 [37.444]
 [37.444]] [[1.49 ]
 [1.49 ]
 [1.507]
 [1.49 ]
 [1.397]
 [1.49 ]
 [1.49 ]]
printing an ep nov before normalisation:  21.046056747436523
printing an ep nov before normalisation:  51.88232758111551
Printing some Q and Qe and total Qs values:  [[-0.026]
 [-0.026]
 [-0.031]
 [-0.026]
 [-0.026]
 [-0.027]
 [-0.026]] [[43.189]
 [43.189]
 [49.944]
 [43.189]
 [43.189]
 [49.836]
 [43.189]] [[1.282]
 [1.282]
 [1.757]
 [1.282]
 [1.282]
 [1.753]
 [1.282]]
actor:  0 policy actor:  0  step number:  50 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.23292666666666675 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.23292666666666675 0.6926666666666669 0.6926666666666669
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]] [[58.169]
 [58.169]
 [58.169]
 [58.169]
 [58.169]
 [58.169]
 [58.169]] [[0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]]
printing an ep nov before normalisation:  26.4428025472353
printing an ep nov before normalisation:  0.03874894459386269
maxi score, test score, baseline:  -0.23292666666666675 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.23292666666666675 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.23292666666666675 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.13532535262523
printing an ep nov before normalisation:  41.33061701362938
printing an ep nov before normalisation:  26.401858871761846
printing an ep nov before normalisation:  28.450478562409764
Printing some Q and Qe and total Qs values:  [[ 0.   ]
 [ 0.073]
 [-0.001]
 [-0.001]
 [ 0.01 ]
 [-0.   ]
 [-0.   ]] [[25.805]
 [37.095]
 [26.584]
 [26.454]
 [26.081]
 [26.399]
 [26.669]] [[0.471]
 [0.951]
 [0.498]
 [0.494]
 [0.491]
 [0.492]
 [0.502]]
actor:  0 policy actor:  0  step number:  57 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.23034000000000013 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.23034000000000013 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.23034000000000013 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  41 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  35.238734724709595
Printing some Q and Qe and total Qs values:  [[0.15 ]
 [0.217]
 [0.15 ]
 [0.079]
 [0.081]
 [0.081]
 [0.095]] [[44.432]
 [40.421]
 [44.432]
 [47.158]
 [47.081]
 [46.603]
 [47.048]] [[1.308]
 [1.172]
 [1.308]
 [1.376]
 [1.373]
 [1.349]
 [1.386]]
maxi score, test score, baseline:  -0.22748666666666678 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.5303568316035
printing an ep nov before normalisation:  36.03517387891917
printing an ep nov before normalisation:  42.1688274327364
maxi score, test score, baseline:  -0.22748666666666678 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.86176381445781
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.722]
 [0.712]
 [0.726]
 [0.606]
 [0.603]
 [0.736]] [[34.317]
 [33.668]
 [32.17 ]
 [32.596]
 [34.253]
 [35.162]
 [31.701]] [[0.697]
 [0.722]
 [0.712]
 [0.726]
 [0.606]
 [0.603]
 [0.736]]
actor:  0 policy actor:  0  step number:  52 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actor:  1 policy actor:  1  step number:  79 total reward:  0.1866666666666661  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.22512666666666678 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.69 ]
 [0.664]
 [0.642]
 [0.642]
 [0.66 ]
 [0.715]] [[33.37 ]
 [43.678]
 [35.245]
 [33.37 ]
 [33.37 ]
 [39.992]
 [33.642]] [[1.691]
 [2.064]
 [1.773]
 [1.691]
 [1.691]
 [1.918]
 [1.773]]
actor:  1 policy actor:  1  step number:  41 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.22512666666666678 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.6546197518022
actor:  1 policy actor:  1  step number:  43 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.22512666666666678 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 50.40785259882604
maxi score, test score, baseline:  -0.22512666666666678 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.22512666666666678 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.22512666666666678 0.6926666666666669 0.6926666666666669
printing an ep nov before normalisation:  35.6365091958611
Printing some Q and Qe and total Qs values:  [[ 0.238]
 [ 0.261]
 [-0.001]
 [ 0.291]
 [ 0.302]
 [ 0.041]
 [ 0.252]] [[41.955]
 [40.535]
 [46.351]
 [43.581]
 [43.053]
 [43.776]
 [44.494]] [[1.568]
 [1.511]
 [1.576]
 [1.712]
 [1.694]
 [1.473]
 [1.725]]
maxi score, test score, baseline:  -0.22512666666666678 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.22512666666666678 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.22512666666666678 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8592919
maxi score, test score, baseline:  -0.22512666666666678 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.22512666666666678 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.115]
 [0.119]
 [0.119]
 [0.141]
 [0.119]
 [0.119]] [[39.132]
 [41.071]
 [39.132]
 [39.132]
 [39.502]
 [39.132]
 [39.132]] [[1.249]
 [1.348]
 [1.249]
 [1.249]
 [1.29 ]
 [1.249]
 [1.249]]
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.659]
 [0.523]
 [0.526]
 [0.504]
 [0.52 ]
 [0.499]] [[46.   ]
 [42.574]
 [47.756]
 [48.208]
 [48.405]
 [50.58 ]
 [47.48 ]] [[2.015]
 [1.97 ]
 [2.114]
 [2.142]
 [2.13 ]
 [2.264]
 [2.075]]
printing an ep nov before normalisation:  38.6959327073323
maxi score, test score, baseline:  -0.22512666666666678 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.22512666666666678 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.812346861651505
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  55.74366401288946
printing an ep nov before normalisation:  34.567699246052975
maxi score, test score, baseline:  -0.22512666666666678 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.82 ]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]] [[39.822]
 [38.369]
 [39.822]
 [39.822]
 [39.822]
 [39.822]
 [39.822]] [[1.807]
 [1.847]
 [1.807]
 [1.807]
 [1.807]
 [1.807]
 [1.807]]
actor:  0 policy actor:  0  step number:  51 total reward:  0.4  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.04231842252068
actor:  1 policy actor:  1  step number:  55 total reward:  0.2933333333333331  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.22232666666666678 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.22232666666666678 0.6926666666666669 0.6926666666666669
actor:  1 policy actor:  1  step number:  62 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.22232666666666678 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.26565660677789
printing an ep nov before normalisation:  40.41780798176554
maxi score, test score, baseline:  -0.22232666666666678 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.22232666666666678 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  37 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.21939333333333344 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.85490788715672
printing an ep nov before normalisation:  32.60407209396362
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.314]
 [0.9  ]
 [0.314]
 [0.9  ]
 [0.314]
 [0.314]] [[ 0.003]
 [ 0.003]
 [40.329]
 [ 0.003]
 [40.329]
 [ 0.003]
 [ 0.003]] [[0.314]
 [0.314]
 [0.9  ]
 [0.314]
 [0.9  ]
 [0.314]
 [0.314]]
actor:  0 policy actor:  0  step number:  41 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.21654000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.199]
 [0.358]
 [0.199]
 [0.199]
 [0.297]
 [0.199]
 [0.199]] [[41.268]
 [41.014]
 [41.268]
 [41.268]
 [39.875]
 [41.268]
 [41.268]] [[1.533]
 [1.677]
 [1.533]
 [1.533]
 [1.549]
 [1.533]
 [1.533]]
maxi score, test score, baseline:  -0.21654000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8639936
printing an ep nov before normalisation:  49.13308029809424
maxi score, test score, baseline:  -0.21654000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.97144902238298
actor:  1 policy actor:  1  step number:  57 total reward:  0.39999999999999947  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  41 total reward:  0.52  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  52.623200199038564
maxi score, test score, baseline:  -0.21350000000000008 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.418]
 [ 0.51 ]
 [-0.074]
 [ 0.418]
 [ 0.14 ]
 [ 0.418]
 [ 0.418]] [[43.198]
 [45.565]
 [33.309]
 [43.198]
 [37.613]
 [43.198]
 [43.198]] [[1.269]
 [1.443]
 [0.432]
 [1.269]
 [0.796]
 [1.269]
 [1.269]]
maxi score, test score, baseline:  -0.21350000000000008 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.398805368262664
maxi score, test score, baseline:  -0.21350000000000008 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.90783620468616
printing an ep nov before normalisation:  46.08483876397774
maxi score, test score, baseline:  -0.21350000000000008 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.21350000000000008 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  32.89173053370781
printing an ep nov before normalisation:  27.13326930999756
maxi score, test score, baseline:  -0.21350000000000008 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.72650744278603
printing an ep nov before normalisation:  42.93236960854582
printing an ep nov before normalisation:  53.11896844967438
maxi score, test score, baseline:  -0.21350000000000008 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.21350000000000008 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.342]
 [ 0.342]
 [ 0.342]
 [ 0.342]
 [ 0.126]
 [-0.02 ]
 [ 0.342]] [[47.486]
 [47.486]
 [47.486]
 [47.486]
 [58.466]
 [52.786]
 [47.486]] [[1.478]
 [1.478]
 [1.478]
 [1.478]
 [1.737]
 [1.345]
 [1.478]]
actions average: 
K:  0  action  0 :  tensor([0.4675, 0.0039, 0.0909, 0.0760, 0.2044, 0.0791, 0.0781],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0030, 0.9700, 0.0029, 0.0035, 0.0015, 0.0014, 0.0177],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1218, 0.0136, 0.3330, 0.1138, 0.1164, 0.1656, 0.1358],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1334, 0.0023, 0.1318, 0.3261, 0.1321, 0.1257, 0.1486],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1376, 0.0025, 0.1118, 0.1065, 0.4193, 0.1254, 0.0970],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1465, 0.0044, 0.1864, 0.1224, 0.1371, 0.2693, 0.1339],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1060, 0.1018, 0.0913, 0.1169, 0.1474, 0.0819, 0.3546],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.21350000000000008 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.17619828373201
maxi score, test score, baseline:  -0.21350000000000008 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.739]
 [0.572]
 [0.543]
 [0.543]
 [0.656]
 [0.543]] [[36.084]
 [34.904]
 [37.725]
 [36.084]
 [36.084]
 [35.317]
 [36.084]] [[2.269]
 [2.355]
 [2.452]
 [2.269]
 [2.269]
 [2.311]
 [2.269]]
printing an ep nov before normalisation:  42.586469877126376
actor:  1 policy actor:  1  step number:  57 total reward:  0.3333333333333327  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  40 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  42 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  42.19265364686113
maxi score, test score, baseline:  -0.21350000000000008 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.21350000000000008 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]] [[42.739]
 [42.739]
 [42.739]
 [42.739]
 [42.739]
 [42.739]
 [42.739]] [[0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]
 [0.629]]
printing an ep nov before normalisation:  56.35906866728882
maxi score, test score, baseline:  -0.2135000000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.08813503539537
printing an ep nov before normalisation:  79.40742726083288
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.639]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]] [[31.913]
 [45.877]
 [31.913]
 [31.913]
 [31.913]
 [31.913]
 [31.913]] [[0.612]
 [0.88 ]
 [0.612]
 [0.612]
 [0.612]
 [0.612]
 [0.612]]
printing an ep nov before normalisation:  47.65679481200636
printing an ep nov before normalisation:  48.2579353257682
Printing some Q and Qe and total Qs values:  [[0.624]
 [0.834]
 [0.624]
 [0.718]
 [0.715]
 [0.624]
 [0.624]] [[44.002]
 [37.739]
 [44.002]
 [42.313]
 [45.797]
 [44.002]
 [44.002]] [[0.931]
 [1.053]
 [0.931]
 [1.002]
 [1.048]
 [0.931]
 [0.931]]
printing an ep nov before normalisation:  34.1248083114624
siam score:  -0.8462086
maxi score, test score, baseline:  -0.2135000000000001 0.6926666666666669 0.6926666666666669
actor:  1 policy actor:  1  step number:  49 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]]
using explorer policy with actor:  1
siam score:  -0.84953123
line 256 mcts: sample exp_bonus 47.01668180903083
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.554]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]] [[43.081]
 [42.967]
 [43.081]
 [43.081]
 [43.081]
 [43.081]
 [43.081]] [[1.169]
 [1.312]
 [1.169]
 [1.169]
 [1.169]
 [1.169]
 [1.169]]
maxi score, test score, baseline:  -0.2135000000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2135000000000001 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.2135000000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.99947214126587
printing an ep nov before normalisation:  38.0425884237041
actor:  1 policy actor:  1  step number:  42 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  38.38635594066246
actor:  1 policy actor:  1  step number:  57 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.2135000000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.75769516893136
printing an ep nov before normalisation:  58.03731309887833
Printing some Q and Qe and total Qs values:  [[-0.009]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]
 [-0.027]] [[52.041]
 [42.225]
 [42.225]
 [42.225]
 [42.225]
 [42.225]
 [42.225]] [[0.324]
 [0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]]
maxi score, test score, baseline:  -0.2135000000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2135000000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.369]
 [0.363]
 [0.369]
 [0.369]
 [0.369]
 [0.31 ]
 [0.369]] [[44.977]
 [51.813]
 [44.977]
 [44.977]
 [44.977]
 [51.614]
 [44.977]] [[1.624]
 [1.947]
 [1.624]
 [1.624]
 [1.624]
 [1.884]
 [1.624]]
printing an ep nov before normalisation:  40.69194326829674
maxi score, test score, baseline:  -0.2135000000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.05285794546076
maxi score, test score, baseline:  -0.2135000000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.577]
 [0.577]
 [0.577]
 [0.577]
 [0.633]
 [0.577]
 [0.577]] [[38.873]
 [38.873]
 [38.873]
 [38.873]
 [42.506]
 [38.873]
 [38.873]] [[2.178]
 [2.178]
 [2.178]
 [2.178]
 [2.525]
 [2.178]
 [2.178]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.43333333333333335  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.044]
 [-0.044]
 [-0.049]
 [-0.041]
 [-0.037]
 [-0.044]
 [-0.044]] [[46.587]
 [46.587]
 [48.382]
 [48.006]
 [47.823]
 [46.587]
 [46.587]] [[1.505]
 [1.505]
 [1.618]
 [1.602]
 [1.593]
 [1.505]
 [1.505]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  52.243763916523875
maxi score, test score, baseline:  -0.2135000000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2135000000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.7471842640703
maxi score, test score, baseline:  -0.2135000000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2135000000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.371007442474365
printing an ep nov before normalisation:  24.89664869992979
siam score:  -0.8558225
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.32431694821534
actions average: 
K:  4  action  0 :  tensor([0.1951, 0.1712, 0.0858, 0.0808, 0.2767, 0.1108, 0.0795],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0080, 0.9046, 0.0165, 0.0177, 0.0197, 0.0104, 0.0231],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1229, 0.0200, 0.2705, 0.1019, 0.1781, 0.2158, 0.0908],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1071, 0.0579, 0.1310, 0.2259, 0.1354, 0.2182, 0.1245],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0978, 0.0261, 0.0873, 0.1316, 0.4162, 0.1248, 0.1162],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0947, 0.0147, 0.1413, 0.1360, 0.1279, 0.3695, 0.1158],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1738, 0.0648, 0.0752, 0.0835, 0.0784, 0.0773, 0.4470],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[ 0.052]
 [ 0.241]
 [-0.041]
 [ 0.099]
 [ 0.058]
 [-0.004]
 [ 0.071]] [[56.975]
 [42.076]
 [49.129]
 [55.927]
 [54.588]
 [55.831]
 [56.67 ]] [[1.317]
 [0.941]
 [0.927]
 [1.325]
 [1.233]
 [1.218]
 [1.325]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.87969032776378
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.535]
 [0.454]
 [0.441]
 [0.435]
 [0.438]
 [0.444]] [[31.632]
 [38.956]
 [31.054]
 [31.12 ]
 [31.249]
 [31.34 ]
 [31.529]] [[1.18 ]
 [1.706]
 [1.142]
 [1.133]
 [1.136]
 [1.144]
 [1.161]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2135000000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.2135000000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.526]
 [0.637]
 [0.444]
 [0.468]
 [0.503]
 [0.435]
 [0.577]] [[33.205]
 [36.502]
 [35.147]
 [34.706]
 [32.96 ]
 [33.716]
 [33.616]] [[0.526]
 [0.637]
 [0.444]
 [0.468]
 [0.503]
 [0.435]
 [0.577]]
maxi score, test score, baseline:  -0.2135000000000001 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.2135000000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  67 total reward:  0.11999999999999933  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[-0.038]
 [-0.069]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.038]
 [-0.038]] [[48.815]
 [59.629]
 [48.815]
 [48.815]
 [48.815]
 [48.815]
 [48.815]] [[0.839]
 [1.188]
 [0.839]
 [0.839]
 [0.839]
 [0.839]
 [0.839]]
maxi score, test score, baseline:  -0.2135000000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  36 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  36.55342888090929
maxi score, test score, baseline:  -0.2104733333333334 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.2392, 0.0773, 0.1247, 0.1592, 0.1426, 0.1295, 0.1274],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0039,     0.9758,     0.0027,     0.0023,     0.0007,     0.0006,
            0.0140], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0975, 0.0225, 0.3897, 0.0861, 0.0732, 0.2251, 0.1059],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1475, 0.0268, 0.1473, 0.2300, 0.1516, 0.1424, 0.1546],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1418, 0.0069, 0.0946, 0.1263, 0.4251, 0.1017, 0.1037],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0870, 0.0204, 0.1206, 0.1618, 0.1078, 0.4044, 0.0980],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1188, 0.1089, 0.1138, 0.1209, 0.0919, 0.0897, 0.3561],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.2104733333333334 0.6926666666666669 0.6926666666666669
printing an ep nov before normalisation:  41.50702621744355
maxi score, test score, baseline:  -0.2104733333333334 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.262829999483124
maxi score, test score, baseline:  -0.2104733333333334 0.6926666666666669 0.6926666666666669
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  60 total reward:  0.0066666666666660435  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.20846000000000012 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.20846000000000012 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.20846000000000012 0.6926666666666669 0.6926666666666669
actor:  1 policy actor:  1  step number:  43 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  38.39578102809979
printing an ep nov before normalisation:  35.000589947053754
maxi score, test score, baseline:  -0.20846000000000012 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.791326033197876
printing an ep nov before normalisation:  34.800989627838135
maxi score, test score, baseline:  -0.20846000000000012 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.354]
 [0.431]
 [0.354]
 [0.354]
 [0.356]
 [0.36 ]
 [0.347]] [[53.389]
 [39.475]
 [55.298]
 [54.514]
 [54.163]
 [54.378]
 [53.76 ]] [[1.244]
 [0.97 ]
 [1.292]
 [1.272]
 [1.265]
 [1.274]
 [1.246]]
printing an ep nov before normalisation:  43.71198997669738
maxi score, test score, baseline:  -0.20846000000000012 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.20846000000000012 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  40.98912843348085
maxi score, test score, baseline:  -0.20846000000000012 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.20846000000000012 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.20846000000000012 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  68 total reward:  0.1399999999999988  reward:  1.0 rdn_beta:  1.667
siam score:  -0.86225927
printing an ep nov before normalisation:  37.019239937916645
printing an ep nov before normalisation:  40.827178955078125
Printing some Q and Qe and total Qs values:  [[0.645]
 [0.67 ]
 [0.645]
 [0.645]
 [0.645]
 [0.645]
 [0.645]] [[36.668]
 [35.763]
 [36.668]
 [36.668]
 [36.668]
 [36.668]
 [36.668]] [[0.866]
 [0.88 ]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]]
printing an ep nov before normalisation:  58.44732875593708
maxi score, test score, baseline:  -0.20846000000000012 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.20846000000000012 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  45 total reward:  0.4133333333333328  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.024]
 [-0.004]
 [-0.014]
 [-0.023]
 [-0.024]
 [-0.019]
 [-0.015]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.024]
 [-0.004]
 [-0.014]
 [-0.023]
 [-0.024]
 [-0.019]
 [-0.015]]
maxi score, test score, baseline:  -0.20563333333333342 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.59008850938535
printing an ep nov before normalisation:  36.34358583126084
Printing some Q and Qe and total Qs values:  [[-0.025]
 [-0.001]
 [-0.027]
 [-0.027]
 [-0.024]
 [-0.027]
 [-0.028]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.025]
 [-0.001]
 [-0.027]
 [-0.027]
 [-0.024]
 [-0.027]
 [-0.028]]
maxi score, test score, baseline:  -0.20563333333333342 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  56 total reward:  0.16666666666666596  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.2033000000000001 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.2033000000000001 0.6926666666666669 0.6926666666666669
printing an ep nov before normalisation:  39.731300373689564
printing an ep nov before normalisation:  35.073105627935064
printing an ep nov before normalisation:  43.287522746426255
printing an ep nov before normalisation:  45.29188050892306
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.70121157402172
maxi score, test score, baseline:  -0.2033000000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2033000000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[-0.022]
 [-0.004]
 [-0.018]
 [-0.022]
 [-0.018]
 [-0.018]
 [-0.022]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.022]
 [-0.004]
 [-0.018]
 [-0.022]
 [-0.018]
 [-0.018]
 [-0.022]]
maxi score, test score, baseline:  -0.2033000000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.2033000000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.45437254930124
printing an ep nov before normalisation:  56.504627718081665
printing an ep nov before normalisation:  43.319067340001
actor:  1 policy actor:  1  step number:  65 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.2033000000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 36.51954613594477
printing an ep nov before normalisation:  35.11086417243993
printing an ep nov before normalisation:  35.515276756239004
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.741]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]] [[31.382]
 [33.251]
 [31.382]
 [31.382]
 [31.382]
 [31.382]
 [31.382]] [[0.721]
 [0.741]
 [0.721]
 [0.721]
 [0.721]
 [0.721]
 [0.721]]
printing an ep nov before normalisation:  61.536814109353585
siam score:  -0.8446657
printing an ep nov before normalisation:  27.617898561619363
maxi score, test score, baseline:  -0.2033000000000001 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.3779, 0.0106, 0.1007, 0.1057, 0.1570, 0.1090, 0.1391],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0032,     0.9704,     0.0028,     0.0057,     0.0012,     0.0007,
            0.0160], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0713, 0.0078, 0.4366, 0.0892, 0.0834, 0.2182, 0.0935],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1100, 0.0063, 0.1476, 0.2693, 0.1418, 0.1472, 0.1777],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1303, 0.0133, 0.1243, 0.1576, 0.2632, 0.1297, 0.1816],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1196, 0.0402, 0.1197, 0.1444, 0.1075, 0.2854, 0.1831],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1046, 0.0503, 0.1307, 0.1409, 0.1074, 0.1545, 0.3116],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  58 total reward:  0.206666666666666  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.20088666666666674 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.20088666666666674 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.20088666666666674 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.557]
 [0.523]
 [0.523]
 [0.523]
 [0.546]
 [0.523]] [[28.344]
 [35.997]
 [28.344]
 [28.344]
 [28.344]
 [29.547]
 [28.344]] [[0.872]
 [1.084]
 [0.872]
 [0.872]
 [0.872]
 [0.923]
 [0.872]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  38.72264511877905
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.20088666666666674 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.2933333333333332  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.20088666666666674 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.94095533340703
maxi score, test score, baseline:  -0.20088666666666674 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.1555, 0.0102, 0.1784, 0.1626, 0.1666, 0.1674, 0.1593],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0085, 0.9429, 0.0068, 0.0078, 0.0062, 0.0067, 0.0211],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0740, 0.0437, 0.5426, 0.0736, 0.0687, 0.1098, 0.0877],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1033, 0.0613, 0.0882, 0.3423, 0.1700, 0.1131, 0.1219],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1077, 0.0029, 0.0820, 0.0885, 0.5515, 0.0944, 0.0730],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1116, 0.0078, 0.1422, 0.1294, 0.1385, 0.3549, 0.1156],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0955, 0.1516, 0.1069, 0.1397, 0.1139, 0.1188, 0.2736],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.724]
 [0.757]
 [0.658]
 [0.722]
 [0.708]
 [0.724]
 [0.75 ]] [[28.454]
 [34.39 ]
 [22.   ]
 [31.123]
 [23.399]
 [26.818]
 [29.968]] [[0.724]
 [0.757]
 [0.658]
 [0.722]
 [0.708]
 [0.724]
 [0.75 ]]
maxi score, test score, baseline:  -0.20088666666666674 0.6926666666666669 0.6926666666666669
line 256 mcts: sample exp_bonus 40.56544071807152
actor:  1 policy actor:  1  step number:  53 total reward:  0.35999999999999943  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.20088666666666674 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  32.23521011619968
actions average: 
K:  2  action  0 :  tensor([0.4206, 0.0208, 0.0964, 0.1011, 0.1504, 0.0893, 0.1214],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0083, 0.9408, 0.0078, 0.0091, 0.0043, 0.0035, 0.0263],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1096, 0.0238, 0.4349, 0.0962, 0.1151, 0.1094, 0.1111],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1012, 0.0618, 0.1210, 0.3038, 0.1159, 0.1265, 0.1700],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1369, 0.0461, 0.1016, 0.0976, 0.4404, 0.0882, 0.0893],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0731, 0.0508, 0.1709, 0.0752, 0.0953, 0.4540, 0.0807],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1270, 0.1361, 0.1318, 0.1494, 0.1462, 0.1184, 0.1912],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  47 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  -0.19843333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.60945129394531
maxi score, test score, baseline:  -0.19843333333333343 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.19843333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19843333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19843333333333343 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.19843333333333343 0.6926666666666669 0.6926666666666669
printing an ep nov before normalisation:  44.209194711625656
maxi score, test score, baseline:  -0.19843333333333343 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.19843333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.87327486433303
maxi score, test score, baseline:  -0.19843333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  67 total reward:  0.173333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.19843333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3466666666666667  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  45.89770773738821
maxi score, test score, baseline:  -0.19843333333333343 0.6926666666666669 0.6926666666666669
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.01968860626221
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.72770798037551
maxi score, test score, baseline:  -0.19843333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.76113773434003
maxi score, test score, baseline:  -0.19843333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19843333333333343 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.82371624315004
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([0.4220, 0.0712, 0.0969, 0.0985, 0.1161, 0.1024, 0.0929],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0008,     0.9848,     0.0011,     0.0009,     0.0004,     0.0005,
            0.0115], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0715, 0.0014, 0.4914, 0.0895, 0.0829, 0.1860, 0.0774],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1203, 0.0208, 0.1420, 0.3095, 0.1243, 0.1408, 0.1423],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1090, 0.0036, 0.0826, 0.0766, 0.5657, 0.0800, 0.0825],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1086, 0.0039, 0.1414, 0.0993, 0.1116, 0.4263, 0.1089],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1235, 0.0272, 0.1362, 0.1300, 0.1289, 0.1369, 0.3172],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.32250550618585
actor:  0 policy actor:  0  step number:  38 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  13.65148663520813
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.02057286269234737
printing an ep nov before normalisation:  74.63361958234441
printing an ep nov before normalisation:  30.6408716415023
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.84206771850586
printing an ep nov before normalisation:  44.77701663970947
printing an ep nov before normalisation:  49.04397717643953
printing an ep nov before normalisation:  49.010609711020656
actor:  1 policy actor:  1  step number:  57 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  48.61186027623501
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
printing an ep nov before normalisation:  48.152259771454176
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.85056800804655
printing an ep nov before normalisation:  41.40952747735388
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
siam score:  -0.85504425
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 39.79390444256073
printing an ep nov before normalisation:  42.24735362930088
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.039437152446666
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  57.84092075930284
printing an ep nov before normalisation:  0.016473055417520754
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  70 total reward:  0.086666666666666  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  42.72458827389649
actor:  1 policy actor:  1  step number:  44 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.45871850510988
printing an ep nov before normalisation:  35.07269722594877
printing an ep nov before normalisation:  47.46667588494325
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.55803679950576
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.14117430737385916
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.23999999999999988  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]] [[41.459]
 [41.459]
 [41.459]
 [41.459]
 [41.459]
 [41.459]
 [41.459]] [[1.764]
 [1.764]
 [1.764]
 [1.764]
 [1.764]
 [1.764]
 [1.764]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.039]
 [0.193]
 [0.193]] [[46.409]
 [46.409]
 [46.409]
 [46.409]
 [54.974]
 [46.409]
 [46.409]] [[1.479]
 [1.479]
 [1.479]
 [1.479]
 [1.706]
 [1.479]
 [1.479]]
printing an ep nov before normalisation:  49.4973172914525
printing an ep nov before normalisation:  36.952631009005614
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.23999999999999932  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8447075
actor:  1 policy actor:  1  step number:  54 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  67 total reward:  0.11999999999999911  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 57.03812559830603
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.4331927341325
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.632135982809835
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.50133850946874
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.86316108703613
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.804]
 [0.938]
 [0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.822]] [[34.716]
 [32.258]
 [34.716]
 [34.716]
 [34.716]
 [34.716]
 [41.346]] [[0.804]
 [0.938]
 [0.804]
 [0.804]
 [0.804]
 [0.804]
 [0.822]]
printing an ep nov before normalisation:  33.065199851989746
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]
 [0.628]] [[29.163]
 [29.163]
 [29.163]
 [29.163]
 [29.163]
 [29.163]
 [29.163]] [[2.184]
 [2.184]
 [2.184]
 [2.184]
 [2.184]
 [2.184]
 [2.184]]
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.156497716332552
maxi score, test score, baseline:  -0.19554000000000007 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  42 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  61 total reward:  0.2399999999999991  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.1928333333333334 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.1928333333333334 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84264463
maxi score, test score, baseline:  -0.1928333333333334 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1928333333333334 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.187]
 [0.239]
 [0.239]
 [0.027]
 [0.239]
 [0.239]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.239]
 [0.187]
 [0.239]
 [0.239]
 [0.027]
 [0.239]
 [0.239]]
maxi score, test score, baseline:  -0.1928333333333334 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1928333333333334 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.26635733364304
maxi score, test score, baseline:  -0.1928333333333334 0.6926666666666669 0.6926666666666669
printing an ep nov before normalisation:  20.257511406219052
actor:  1 policy actor:  1  step number:  51 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.1928333333333334 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  77 total reward:  0.09333333333333182  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.1928333333333334 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  52 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.67484649412647
maxi score, test score, baseline:  -0.19044666666666674 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.19044666666666674 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.09090897387032
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.755]
 [0.631]
 [0.62 ]
 [0.658]
 [0.688]
 [0.784]] [[46.907]
 [41.889]
 [57.069]
 [61.6  ]
 [61.86 ]
 [56.83 ]
 [43.843]] [[0.64 ]
 [0.755]
 [0.631]
 [0.62 ]
 [0.658]
 [0.688]
 [0.784]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.19044666666666674 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 57.53332368121338
maxi score, test score, baseline:  -0.19044666666666674 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.605555925893405
maxi score, test score, baseline:  -0.19044666666666674 0.6926666666666669 0.6926666666666669
maxi score, test score, baseline:  -0.19044666666666674 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
actor:  0 policy actor:  0  step number:  43 total reward:  0.44000000000000006  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  32.12218483289083
printing an ep nov before normalisation:  38.35254056113107
printing an ep nov before normalisation:  50.20047003577691
printing an ep nov before normalisation:  31.95291519165039
printing an ep nov before normalisation:  38.80416316850513
printing an ep nov before normalisation:  49.37365169557064
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]] [[39.411]
 [39.411]
 [39.411]
 [39.411]
 [39.411]
 [39.411]
 [39.411]] [[0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]]
siam score:  -0.8378222
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  53 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  36.8416536673999
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.18381002181868
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 33.52388126361317
line 256 mcts: sample exp_bonus 40.2842984430988
Printing some Q and Qe and total Qs values:  [[0.848]
 [0.885]
 [0.77 ]
 [0.762]
 [0.783]
 [0.762]
 [0.78 ]] [[33.499]
 [33.766]
 [37.381]
 [38.303]
 [38.172]
 [39.951]
 [39.701]] [[0.848]
 [0.885]
 [0.77 ]
 [0.762]
 [0.783]
 [0.762]
 [0.78 ]]
printing an ep nov before normalisation:  36.438639115979605
maxi score, test score, baseline:  -0.18756666666666674 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.02  0.714 0.02  0.122 0.041 0.041 0.041]
printing an ep nov before normalisation:  42.50077629423618
maxi score, test score, baseline:  -0.18756666666666674 0.6926666666666669 0.6926666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  38.59679698944092
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  0.011465766346532291
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  -0.14543333333333341 0.6916666666666669 0.6916666666666669
actor:  1 policy actor:  1  step number:  55 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.01236786239956
maxi score, test score, baseline:  -0.14543333333333341 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.14543333333333341 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.14543333333333341 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.14543333333333341 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.204]
 [0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]] [[42.736]
 [41.493]
 [42.736]
 [42.736]
 [42.736]
 [42.736]
 [42.736]] [[1.083]
 [1.102]
 [1.083]
 [1.083]
 [1.083]
 [1.083]
 [1.083]]
Printing some Q and Qe and total Qs values:  [[0.209]
 [0.301]
 [0.209]
 [0.209]
 [0.209]
 [0.209]
 [0.209]] [[35.65 ]
 [42.141]
 [35.65 ]
 [35.65 ]
 [35.65 ]
 [35.65 ]
 [35.65 ]] [[0.692]
 [0.956]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]]
maxi score, test score, baseline:  -0.14543333333333341 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.14543333333333341 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.14543333333333341 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.74873962456899
actions average: 
K:  4  action  0 :  tensor([0.2983, 0.0253, 0.1424, 0.1284, 0.1230, 0.1342, 0.1484],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0127, 0.9211, 0.0105, 0.0125, 0.0070, 0.0084, 0.0278],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1308, 0.0473, 0.1765, 0.1894, 0.1506, 0.1361, 0.1693],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1197, 0.0375, 0.1349, 0.3120, 0.1253, 0.1446, 0.1260],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1006, 0.0045, 0.0750, 0.1131, 0.5461, 0.0899, 0.0707],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1033, 0.0070, 0.1832, 0.0933, 0.1090, 0.4103, 0.0938],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0928, 0.3388, 0.0666, 0.0814, 0.0722, 0.0584, 0.2897],
       grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 48.37972742235624
printing an ep nov before normalisation:  35.34788050092852
maxi score, test score, baseline:  -0.14543333333333341 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.27486490905545
printing an ep nov before normalisation:  13.111546039581299
maxi score, test score, baseline:  -0.14543333333333341 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.14543333333333341 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.14543333333333341 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.2700, 0.0514, 0.0907, 0.1293, 0.1913, 0.1347, 0.1327],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0109, 0.9090, 0.0118, 0.0151, 0.0095, 0.0126, 0.0310],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0804, 0.0014, 0.6052, 0.0646, 0.0700, 0.0824, 0.0961],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1247, 0.0934, 0.1020, 0.2379, 0.1258, 0.1446, 0.1716],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1213, 0.0071, 0.1023, 0.1496, 0.3527, 0.1437, 0.1233],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0943, 0.0610, 0.1638, 0.1085, 0.0806, 0.3778, 0.1140],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1022, 0.2337, 0.0939, 0.1113, 0.0900, 0.1137, 0.2552],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.14543333333333341 0.6916666666666669 0.6916666666666669
actor:  0 policy actor:  0  step number:  64 total reward:  0.31333333333333335  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.14604666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.55442411048356
printing an ep nov before normalisation:  55.205135345458984
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.14931333333333346 0.6916666666666669 0.6916666666666669
actor:  1 policy actor:  1  step number:  44 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 39.74516238400549
printing an ep nov before normalisation:  35.09279536743446
Printing some Q and Qe and total Qs values:  [[-0.007]
 [ 0.003]
 [-0.012]
 [-0.012]
 [-0.015]
 [-0.02 ]
 [-0.036]] [[59.402]
 [55.219]
 [63.63 ]
 [62.876]
 [57.787]
 [68.292]
 [55.803]] [[0.945]
 [0.848]
 [1.047]
 [1.027]
 [0.896]
 [1.157]
 [0.825]]
Printing some Q and Qe and total Qs values:  [[0.908]
 [1.01 ]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]] [[47.864]
 [47.676]
 [47.864]
 [47.864]
 [47.864]
 [47.864]
 [47.864]] [[0.908]
 [1.01 ]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]]
maxi score, test score, baseline:  -0.14931333333333346 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  48 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1498200000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.52 ]
 [0.415]
 [0.411]
 [0.414]
 [0.415]
 [0.411]] [[55.672]
 [48.036]
 [59.348]
 [61.635]
 [58.301]
 [58.18 ]
 [61.635]] [[1.501]
 [1.387]
 [1.67 ]
 [1.744]
 [1.633]
 [1.63 ]
 [1.744]]
maxi score, test score, baseline:  -0.1498200000000001 0.6916666666666669 0.6916666666666669
printing an ep nov before normalisation:  52.44859896920925
maxi score, test score, baseline:  -0.1498200000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  72 total reward:  0.0599999999999995  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
Printing some Q and Qe and total Qs values:  [[0.655]
 [0.767]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]] [[33.827]
 [36.117]
 [33.827]
 [33.827]
 [33.827]
 [33.827]
 [33.827]] [[0.655]
 [0.767]
 [0.655]
 [0.655]
 [0.655]
 [0.655]
 [0.655]]
line 256 mcts: sample exp_bonus 31.432502269744873
maxi score, test score, baseline:  -0.1498200000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.83692986
actor:  1 policy actor:  1  step number:  43 total reward:  0.52  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.037]
 [-0.046]
 [-0.049]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.037]
 [-0.046]
 [-0.049]
 [-0.037]
 [-0.037]
 [-0.037]
 [-0.037]]
printing an ep nov before normalisation:  39.44364200707181
maxi score, test score, baseline:  -0.1498200000000001 0.6916666666666669 0.6916666666666669
rdn beta is 0 so we're just using the maxi policy
actions average: 
K:  1  action  0 :  tensor([0.3450, 0.0083, 0.0883, 0.1020, 0.2469, 0.0956, 0.1139],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0025,     0.9743,     0.0016,     0.0022,     0.0007,     0.0005,
            0.0182], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1307, 0.0081, 0.3427, 0.1092, 0.1386, 0.1352, 0.1355],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1428, 0.0276, 0.1129, 0.2898, 0.1569, 0.1173, 0.1528],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1151, 0.0039, 0.0947, 0.1134, 0.4444, 0.1165, 0.1120],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1014, 0.0061, 0.1156, 0.1313, 0.1232, 0.4249, 0.0975],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1365, 0.1262, 0.1311, 0.1349, 0.1614, 0.1286, 0.1812],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  40 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  48 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.98470435220642
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]] [[55.738]
 [55.738]
 [55.738]
 [55.738]
 [55.738]
 [55.738]
 [55.738]] [[1.128]
 [1.128]
 [1.128]
 [1.128]
 [1.128]
 [1.128]
 [1.128]]
Printing some Q and Qe and total Qs values:  [[0.265]
 [0.265]
 [0.265]
 [0.265]
 [0.487]
 [0.265]
 [0.265]] [[44.066]
 [44.066]
 [44.066]
 [44.066]
 [51.609]
 [44.066]
 [44.066]] [[1.14]
 [1.14]
 [1.14]
 [1.14]
 [1.65]
 [1.14]
 [1.14]]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]] [[37.498]
 [40.256]
 [37.498]
 [37.498]
 [37.498]
 [37.498]
 [37.498]] [[1.448]
 [1.583]
 [1.448]
 [1.448]
 [1.448]
 [1.448]
 [1.448]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.4  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.14430000000000007 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.14430000000000007 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  27.927220873578584
line 256 mcts: sample exp_bonus 35.01928157163549
maxi score, test score, baseline:  -0.14430000000000007 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3733333333333334  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 38.41612924441049
printing an ep nov before normalisation:  50.00520188351106
maxi score, test score, baseline:  -0.14430000000000007 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.14430000000000007 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  32 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.1411666666666668 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.14116666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.715]
 [0.652]
 [0.652]] [[44.877]
 [44.877]
 [44.877]
 [44.877]
 [57.245]
 [44.877]
 [44.877]] [[0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.715]
 [0.652]
 [0.652]]
printing an ep nov before normalisation:  61.38494599378797
maxi score, test score, baseline:  -0.14116666666666677 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.14116666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.41636081318237
printing an ep nov before normalisation:  41.156389123992625
maxi score, test score, baseline:  -0.14116666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.14116666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.14116666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  51 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.3266666666666661  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.13871333333333344 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.12426468895936
maxi score, test score, baseline:  -0.13871333333333344 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.9804417840887
maxi score, test score, baseline:  -0.13871333333333344 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.13871333333333344 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.73310206404342
maxi score, test score, baseline:  -0.13871333333333344 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.55796236640948
Printing some Q and Qe and total Qs values:  [[-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]
 [-0.048]] [[71.531]
 [71.531]
 [71.531]
 [71.531]
 [71.531]
 [71.531]
 [71.531]] [[1.283]
 [1.283]
 [1.283]
 [1.283]
 [1.283]
 [1.283]
 [1.283]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  40.7793463396486
maxi score, test score, baseline:  -0.13871333333333344 0.6916666666666669 0.6916666666666669
actions average: 
K:  0  action  0 :  tensor([0.6114, 0.0154, 0.0626, 0.0653, 0.0981, 0.0722, 0.0750],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0045, 0.9788, 0.0030, 0.0017, 0.0016, 0.0014, 0.0091],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1832, 0.0045, 0.2694, 0.1150, 0.1275, 0.1281, 0.1723],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1076, 0.0052, 0.0969, 0.4206, 0.0908, 0.1345, 0.1444],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1362, 0.0044, 0.0951, 0.0837, 0.4643, 0.0958, 0.1206],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1226, 0.0071, 0.1413, 0.1181, 0.1358, 0.3413, 0.1339],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1780, 0.0133, 0.1541, 0.1227, 0.1581, 0.1549, 0.2188],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  44.68795312647575
printing an ep nov before normalisation:  27.785193920135498
maxi score, test score, baseline:  -0.13871333333333344 0.6916666666666669 0.6916666666666669
printing an ep nov before normalisation:  33.691736081444006
printing an ep nov before normalisation:  34.956549020316146
actor:  1 policy actor:  1  step number:  43 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  2.0
siam score:  -0.836241
maxi score, test score, baseline:  -0.13871333333333344 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.13871333333333344 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  34 total reward:  0.54  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1356333333333334 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.534218531979626
printing an ep nov before normalisation:  44.39561473022557
maxi score, test score, baseline:  -0.1356333333333334 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.941071560090386
maxi score, test score, baseline:  -0.1356333333333334 0.6916666666666669 0.6916666666666669
printing an ep nov before normalisation:  42.78577835266347
maxi score, test score, baseline:  -0.1356333333333334 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.1356333333333334 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.7022095661267
Printing some Q and Qe and total Qs values:  [[-0.056]
 [-0.056]
 [-0.056]
 [-0.056]
 [-0.053]
 [-0.033]
 [-0.056]] [[67.603]
 [67.603]
 [67.603]
 [67.603]
 [75.311]
 [75.784]
 [67.603]] [[1.447]
 [1.447]
 [1.447]
 [1.447]
 [1.757]
 [1.795]
 [1.447]]
maxi score, test score, baseline:  -0.1356333333333334 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.02258051202259
actor:  1 policy actor:  1  step number:  54 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8348474
maxi score, test score, baseline:  -0.1356333333333334 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  51.72950469909762
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1356333333333334 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1356333333333334 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.95761806970401
printing an ep nov before normalisation:  35.686266370364365
printing an ep nov before normalisation:  40.36713926536564
Printing some Q and Qe and total Qs values:  [[0.791]
 [0.498]
 [0.498]
 [0.778]
 [0.498]
 [0.902]
 [0.498]] [[25.642]
 [33.046]
 [33.046]
 [25.051]
 [33.046]
 [28.2  ]
 [33.046]] [[1.299]
 [1.356]
 [1.356]
 [1.258]
 [1.356]
 [1.532]
 [1.356]]
Printing some Q and Qe and total Qs values:  [[0.723]
 [0.735]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]] [[12.248]
 [41.049]
 [12.248]
 [12.248]
 [12.248]
 [12.248]
 [12.248]] [[0.723]
 [0.735]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]]
actor:  1 policy actor:  1  step number:  39 total reward:  0.45333333333333303  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  67 total reward:  0.03999999999999937  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.1356333333333334 0.6916666666666669 0.6916666666666669
printing an ep nov before normalisation:  37.922492027282715
printing an ep nov before normalisation:  39.7639589342239
Printing some Q and Qe and total Qs values:  [[-0.008]
 [ 0.018]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]] [[42.972]
 [39.979]
 [42.972]
 [42.972]
 [42.972]
 [42.972]
 [42.972]] [[1.659]
 [1.458]
 [1.659]
 [1.659]
 [1.659]
 [1.659]
 [1.659]]
actor:  0 policy actor:  0  step number:  60 total reward:  0.23333333333333317  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.13316666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.105660952624845
printing an ep nov before normalisation:  40.74597139262905
printing an ep nov before normalisation:  60.51276119933281
printing an ep nov before normalisation:  43.396086404630836
printing an ep nov before normalisation:  41.21013575753434
maxi score, test score, baseline:  -0.13316666666666677 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.13316666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.17718279732484
actions average: 
K:  4  action  0 :  tensor([0.3036, 0.0095, 0.1172, 0.1358, 0.1446, 0.1499, 0.1394],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0142, 0.9041, 0.0119, 0.0244, 0.0090, 0.0089, 0.0275],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1250, 0.1119, 0.2865, 0.1014, 0.1029, 0.1297, 0.1425],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1161, 0.0055, 0.1032, 0.2627, 0.1893, 0.1785, 0.1447],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1043, 0.1829, 0.0744, 0.0827, 0.3729, 0.1017, 0.0811],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1578, 0.0065, 0.1366, 0.1636, 0.1624, 0.2208, 0.1522],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([    0.0004,     0.9303,     0.0028,     0.0056,     0.0002,     0.0013,
            0.0593], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.13316666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.13316666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  47 total reward:  0.3066666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.13055333333333344 0.6916666666666669 0.6916666666666669
actor:  1 policy actor:  1  step number:  57 total reward:  0.2533333333333331  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.13055333333333344 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.13055333333333344 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.13055333333333344 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.13055333333333344 0.6916666666666669 0.6916666666666669
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.372]
 [0.296]
 [0.296]
 [0.296]
 [0.296]
 [0.296]] [[30.529]
 [38.502]
 [30.529]
 [30.529]
 [30.529]
 [30.529]
 [30.529]] [[1.139]
 [1.645]
 [1.139]
 [1.139]
 [1.139]
 [1.139]
 [1.139]]
printing an ep nov before normalisation:  37.332336373771085
siam score:  -0.8383528
maxi score, test score, baseline:  -0.13055333333333344 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.83638114
maxi score, test score, baseline:  -0.13055333333333344 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.13055333333333344 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.8916],
        [ 0.1627],
        [ 0.0704],
        [ 0.2449],
        [-0.0000],
        [-0.0000],
        [ 0.3939],
        [ 0.1627],
        [ 0.4385],
        [-0.0939]], dtype=torch.float64)
-0.084359833866 0.8072563326635349
-0.084359833866 0.07831949342580506
-0.058614567066 0.011738705276566286
-0.071422513866 0.17345164252364154
-0.2875619999999995 -0.2875619999999995
-0.48068874480000007 -0.48068874480000007
-0.032346567066 0.3615240915100919
-0.084359833866 0.07831949342580506
-0.09703970119800001 0.34142519264725835
-0.032346567066 -0.12621265942230236
Printing some Q and Qe and total Qs values:  [[0.12 ]
 [0.171]
 [0.12 ]
 [0.075]
 [0.12 ]
 [0.12 ]
 [0.12 ]] [[53.607]
 [51.14 ]
 [53.607]
 [55.979]
 [53.607]
 [53.607]
 [53.607]] [[1.673]
 [1.608]
 [1.673]
 [1.739]
 [1.673]
 [1.673]
 [1.673]]
printing an ep nov before normalisation:  41.49202182779669
maxi score, test score, baseline:  -0.13055333333333344 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.13055333333333344 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  90 total reward:  0.019999999999998685  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.75404542119138
printing an ep nov before normalisation:  41.191169133521875
actor:  1 policy actor:  1  step number:  64 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.333
UNIT TEST: sample policy line 217 mcts : [0.02  0.898 0.02  0.02  0.    0.02  0.02 ]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  51.363522882250194
maxi score, test score, baseline:  -0.13055333333333344 0.6916666666666669 0.6916666666666669
printing an ep nov before normalisation:  26.26893654364663
actor:  0 policy actor:  0  step number:  58 total reward:  0.0733333333333327  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  35.92714630736628
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.836]
 [0.727]
 [0.66 ]
 [0.727]
 [0.727]
 [0.727]] [[29.564]
 [35.456]
 [29.564]
 [27.808]
 [29.564]
 [29.564]
 [29.564]] [[0.727]
 [0.836]
 [0.727]
 [0.66 ]
 [0.727]
 [0.727]
 [0.727]]
maxi score, test score, baseline:  -0.12840666666666678 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.12840666666666678 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.788]
 [0.967]
 [0.806]
 [0.806]
 [0.808]
 [0.803]
 [0.802]] [[36.148]
 [34.712]
 [36.134]
 [36.913]
 [36.433]
 [36.741]
 [36.477]] [[0.788]
 [0.967]
 [0.806]
 [0.806]
 [0.808]
 [0.803]
 [0.802]]
printing an ep nov before normalisation:  39.533924813139805
maxi score, test score, baseline:  -0.12840666666666678 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.94684746884049
Printing some Q and Qe and total Qs values:  [[-0.036]
 [-0.015]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]
 [-0.036]] [[13.388]
 [33.783]
 [13.388]
 [13.388]
 [13.388]
 [13.388]
 [13.388]] [[-0.021]
 [ 0.144]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]
 [-0.021]]
maxi score, test score, baseline:  -0.12840666666666678 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.31442650028536
printing an ep nov before normalisation:  34.405665047104605
actor:  0 policy actor:  0  step number:  52 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.12594000000000008 0.6916666666666669 0.6916666666666669
Printing some Q and Qe and total Qs values:  [[0.789]
 [0.988]
 [0.782]
 [0.775]
 [0.789]
 [0.734]
 [0.812]] [[41.582]
 [32.967]
 [41.875]
 [41.285]
 [41.582]
 [47.029]
 [41.008]] [[1.03 ]
 [1.127]
 [1.026]
 [1.011]
 [1.03 ]
 [1.038]
 [1.045]]
maxi score, test score, baseline:  -0.12594000000000008 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  0.333
UNIT TEST: sample policy line 217 mcts : [0.061 0.102 0.061 0.163 0.02  0.082 0.51 ]
printing an ep nov before normalisation:  66.62419220809078
printing an ep nov before normalisation:  29.294798374176025
maxi score, test score, baseline:  -0.12594000000000008 0.6916666666666669 0.6916666666666669
printing an ep nov before normalisation:  41.178437341719835
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.372]
 [0.394]
 [0.374]
 [0.322]
 [0.415]
 [0.391]] [[49.11 ]
 [47.053]
 [45.005]
 [48.314]
 [48.548]
 [47.078]
 [47.087]] [[1.948]
 [1.836]
 [1.761]
 [1.898]
 [1.858]
 [1.88 ]
 [1.857]]
siam score:  -0.84148943
siam score:  -0.84200865
actor:  1 policy actor:  1  step number:  46 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.12594000000000008 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8413178
siam score:  -0.8412297
maxi score, test score, baseline:  -0.12594000000000008 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.39481590054324
printing an ep nov before normalisation:  0.03704231356095988
actor:  1 policy actor:  1  step number:  65 total reward:  0.06666666666666587  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  3  action  0 :  tensor([0.2678, 0.0850, 0.1328, 0.1303, 0.1199, 0.1306, 0.1335],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0151, 0.8899, 0.0265, 0.0113, 0.0103, 0.0130, 0.0339],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0884, 0.0425, 0.4274, 0.0986, 0.1003, 0.1276, 0.1153],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1172, 0.1385, 0.1093, 0.2294, 0.1220, 0.1365, 0.1471],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1234, 0.0566, 0.0862, 0.0828, 0.4486, 0.0942, 0.1083],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0847, 0.0054, 0.1929, 0.1280, 0.1184, 0.3396, 0.1310],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1305, 0.1387, 0.1259, 0.1350, 0.1114, 0.1311, 0.2274],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.409860610961914
actor:  1 policy actor:  1  step number:  48 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  58.57216111464136
printing an ep nov before normalisation:  65.72070202059017
maxi score, test score, baseline:  -0.12594000000000008 0.6916666666666669 0.6916666666666669
actor:  1 policy actor:  1  step number:  56 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  2  action  0 :  tensor([0.1572, 0.0031, 0.1363, 0.1886, 0.1558, 0.2145, 0.1444],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0170, 0.8879, 0.0120, 0.0197, 0.0097, 0.0165, 0.0372],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0751, 0.0371, 0.4831, 0.0724, 0.0709, 0.1625, 0.0989],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1048, 0.0103, 0.1066, 0.3936, 0.0934, 0.1663, 0.1249],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1230, 0.0006, 0.0752, 0.1017, 0.5087, 0.0992, 0.0915],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1275, 0.0049, 0.1116, 0.1708, 0.1278, 0.3241, 0.1335],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1263, 0.1582, 0.1019, 0.1237, 0.1219, 0.1482, 0.2198],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.461833797243415
maxi score, test score, baseline:  -0.12594000000000008 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.12594000000000008 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.12594000000000008 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.80273369380411
maxi score, test score, baseline:  -0.12594000000000008 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.977882552589552
maxi score, test score, baseline:  -0.12594000000000008 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.23101509936824
UNIT TEST: sample policy line 217 mcts : [0.02  0.796 0.    0.061 0.061 0.041 0.02 ]
printing an ep nov before normalisation:  33.723571929900345
printing an ep nov before normalisation:  34.535114886852725
actor:  0 policy actor:  0  step number:  50 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  57 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.12564666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.03704645178418
actor:  1 policy actor:  1  step number:  59 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  32.72660406877456
maxi score, test score, baseline:  -0.12564666666666677 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.12564666666666677 0.6916666666666669 0.6916666666666669
printing an ep nov before normalisation:  0.03657083131628269
printing an ep nov before normalisation:  40.23237487604933
maxi score, test score, baseline:  -0.12564666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.12564666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.12564666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.12564666666666677 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.12564666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.99672819501704
printing an ep nov before normalisation:  37.7305547000194
maxi score, test score, baseline:  -0.12564666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  42 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  24.673645261229254
printing an ep nov before normalisation:  34.51048023540884
actor:  1 policy actor:  1  step number:  55 total reward:  0.34666666666666657  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.1228600000000001 0.6916666666666669 0.6916666666666669
printing an ep nov before normalisation:  46.463263904639625
maxi score, test score, baseline:  -0.1228600000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.758295993205053
actor:  1 policy actor:  1  step number:  66 total reward:  0.19333333333333258  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.1228600000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.35017496698758
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]] [[47.604]
 [47.604]
 [47.604]
 [47.604]
 [47.604]
 [47.604]
 [47.604]] [[0.85]
 [0.85]
 [0.85]
 [0.85]
 [0.85]
 [0.85]
 [0.85]]
printing an ep nov before normalisation:  40.09648805227846
actor:  0 policy actor:  0  step number:  53 total reward:  0.18666666666666598  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.12048666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.806207413160216
Printing some Q and Qe and total Qs values:  [[0.722]
 [0.732]
 [0.696]
 [0.742]
 [0.696]
 [0.696]
 [0.693]] [[36.412]
 [35.607]
 [36.61 ]
 [35.954]
 [36.433]
 [36.436]
 [36.597]] [[0.722]
 [0.732]
 [0.696]
 [0.742]
 [0.696]
 [0.696]
 [0.693]]
maxi score, test score, baseline:  -0.12048666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.676860440834066
actor:  1 policy actor:  1  step number:  59 total reward:  0.1999999999999994  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.12048666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.67249664727774
printing an ep nov before normalisation:  48.42044968843593
maxi score, test score, baseline:  -0.12048666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.12048666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  48 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.11799333333333342 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.11799333333333342 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.849958860050435
maxi score, test score, baseline:  -0.11799333333333342 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.019]
 [ 0.006]
 [-0.002]
 [ 0.008]
 [-0.002]
 [ 0.001]
 [-0.015]] [[46.756]
 [43.936]
 [49.484]
 [49.416]
 [53.198]
 [48.632]
 [48.391]] [[0.703]
 [0.657]
 [0.788]
 [0.796]
 [0.881]
 [0.77 ]
 [0.747]]
maxi score, test score, baseline:  -0.11799333333333342 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  46 total reward:  0.3266666666666661  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.084]
 [0.036]
 [0.036]
 [0.037]
 [0.037]
 [0.037]] [[24.924]
 [41.988]
 [26.142]
 [26.073]
 [25.919]
 [25.733]
 [25.801]] [[0.412]
 [1.112]
 [0.457]
 [0.454]
 [0.449]
 [0.442]
 [0.445]]
maxi score, test score, baseline:  -0.1153400000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]
 [0.099]] [[60.373]
 [60.373]
 [60.373]
 [60.373]
 [60.373]
 [60.373]
 [60.373]] [[1.072]
 [1.072]
 [1.072]
 [1.072]
 [1.072]
 [1.072]
 [1.072]]
printing an ep nov before normalisation:  43.63617477070803
maxi score, test score, baseline:  -0.1153400000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1153400000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1153400000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1153400000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.126826355622995
maxi score, test score, baseline:  -0.1153400000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.340121621029823
printing an ep nov before normalisation:  53.564673452362605
using explorer policy with actor:  1
siam score:  -0.84569114
siam score:  -0.8478836
actor:  1 policy actor:  1  step number:  54 total reward:  0.43333333333333346  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.1153400000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.1153400000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1153400000000001 0.6916666666666669 0.6916666666666669
actor:  1 policy actor:  1  step number:  56 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  53.61419424070189
maxi score, test score, baseline:  -0.1153400000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.43421912326142
printing an ep nov before normalisation:  48.51811106000286
printing an ep nov before normalisation:  41.2997895976933
maxi score, test score, baseline:  -0.1153400000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.28484450043198
printing an ep nov before normalisation:  47.83862596974504
maxi score, test score, baseline:  -0.1153400000000001 0.6916666666666669 0.6916666666666669
printing an ep nov before normalisation:  27.637600898742676
Printing some Q and Qe and total Qs values:  [[ 0.318]
 [ 0.364]
 [-0.062]
 [ 0.124]
 [ 0.123]
 [ 0.318]
 [ 0.166]] [[38.527]
 [41.701]
 [36.198]
 [42.776]
 [43.654]
 [38.527]
 [39.731]] [[0.676]
 [0.78 ]
 [0.253]
 [0.56 ]
 [0.575]
 [0.676]
 [0.546]]
maxi score, test score, baseline:  -0.1153400000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.23990440368652
printing an ep nov before normalisation:  54.005799272335196
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.574]
 [0.427]
 [0.426]
 [0.441]
 [0.427]
 [0.565]] [[31.97 ]
 [32.92 ]
 [32.928]
 [33.017]
 [32.646]
 [32.712]
 [33.654]] [[0.799]
 [0.97 ]
 [0.823]
 [0.825]
 [0.831]
 [0.819]
 [0.979]]
line 256 mcts: sample exp_bonus 36.8626433300297
printing an ep nov before normalisation:  39.646337111896706
line 256 mcts: sample exp_bonus 29.995644731739944
printing an ep nov before normalisation:  0.37141316795441526
actor:  1 policy actor:  1  step number:  57 total reward:  0.25333333333333274  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  43.487993137391804
actor:  1 policy actor:  1  step number:  74 total reward:  0.03333333333333266  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  62.564217836274075
maxi score, test score, baseline:  -0.1153400000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]
 [-0.041]] [[39.763]
 [39.763]
 [39.763]
 [39.763]
 [39.763]
 [39.763]
 [39.763]] [[0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]]
maxi score, test score, baseline:  -0.1153400000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.199793934822083
actions average: 
K:  1  action  0 :  tensor([0.4592, 0.1434, 0.0634, 0.0713, 0.0988, 0.0905, 0.0735],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0019, 0.9729, 0.0027, 0.0078, 0.0011, 0.0019, 0.0116],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0900, 0.0173, 0.4363, 0.1039, 0.1089, 0.1547, 0.0889],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0982, 0.0355, 0.0916, 0.4174, 0.1068, 0.1379, 0.1126],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1282, 0.0027, 0.0807, 0.0942, 0.5132, 0.1014, 0.0796],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0928, 0.0064, 0.2068, 0.0957, 0.1104, 0.3986, 0.0893],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1330, 0.0301, 0.1198, 0.1057, 0.0960, 0.1269, 0.3885],
       grad_fn=<DivBackward0>)
siam score:  -0.8434467
maxi score, test score, baseline:  -0.1153400000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.624545621183294
printing an ep nov before normalisation:  40.783600351549104
printing an ep nov before normalisation:  35.18079525407585
printing an ep nov before normalisation:  41.22335240546076
maxi score, test score, baseline:  -0.1153400000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.112101662827044
printing an ep nov before normalisation:  33.36682639279914
maxi score, test score, baseline:  -0.1153400000000001 0.6916666666666669 0.6916666666666669
printing an ep nov before normalisation:  56.57066802421872
maxi score, test score, baseline:  -0.1153400000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  49.385735091143616
actor:  0 policy actor:  0  step number:  44 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  41.08671834499553
printing an ep nov before normalisation:  49.95824376352548
Printing some Q and Qe and total Qs values:  [[ 0.075]
 [ 0.311]
 [-0.026]
 [ 0.093]
 [ 0.121]
 [ 0.067]
 [ 0.248]] [[38.092]
 [40.215]
 [52.536]
 [48.87 ]
 [46.66 ]
 [49.233]
 [54.895]] [[0.582]
 [0.872]
 [0.858]
 [0.881]
 [0.851]
 [0.864]
 [1.193]]
maxi score, test score, baseline:  -0.1125800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.91495684797245
printing an ep nov before normalisation:  52.83053569013351
maxi score, test score, baseline:  -0.1125800000000001 0.6916666666666669 0.6916666666666669
printing an ep nov before normalisation:  8.94531368600255
actor:  1 policy actor:  1  step number:  73 total reward:  0.09333333333333305  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  52 total reward:  0.5400000000000003  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.1125800000000001 0.6916666666666669 0.6916666666666669
printing an ep nov before normalisation:  53.40901308464703
maxi score, test score, baseline:  -0.1125800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1125800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1125800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.572]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]] [[37.784]
 [47.117]
 [37.784]
 [37.784]
 [37.784]
 [37.784]
 [37.784]] [[1.117]
 [1.485]
 [1.117]
 [1.117]
 [1.117]
 [1.117]
 [1.117]]
printing an ep nov before normalisation:  48.505427800131656
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  0.4199999999999995  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  47.04310885904788
printing an ep nov before normalisation:  41.49607470340529
maxi score, test score, baseline:  -0.1125800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.88587262327909
printing an ep nov before normalisation:  64.38841583103007
maxi score, test score, baseline:  -0.1125800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1125800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.560326907234625
maxi score, test score, baseline:  -0.1125800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.2594996808702
maxi score, test score, baseline:  -0.1125800000000001 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.1125800000000001 0.6916666666666669 0.6916666666666669
printing an ep nov before normalisation:  41.122930591809386
actor:  1 policy actor:  1  step number:  52 total reward:  0.27333333333333265  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  63 total reward:  0.14666666666666617  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.1125800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.171252309804224
line 256 mcts: sample exp_bonus 33.2206718823781
Printing some Q and Qe and total Qs values:  [[-0.011]
 [ 0.053]
 [-0.011]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.007]] [[44.193]
 [36.276]
 [44.788]
 [43.866]
 [44.272]
 [44.591]
 [40.389]] [[1.32 ]
 [0.902]
 [1.356]
 [1.299]
 [1.323]
 [1.342]
 [1.092]]
Printing some Q and Qe and total Qs values:  [[0.752]
 [0.852]
 [0.752]
 [0.722]
 [0.752]
 [0.752]
 [0.799]] [[45.499]
 [43.866]
 [45.499]
 [41.906]
 [45.499]
 [45.499]
 [43.679]] [[0.752]
 [0.852]
 [0.752]
 [0.722]
 [0.752]
 [0.752]
 [0.799]]
actor:  0 policy actor:  0  step number:  41 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  45 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  39.39442289635792
printing an ep nov before normalisation:  41.88812193240899
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.388]
 [0.312]
 [0.312]] [[33.831]
 [33.831]
 [33.831]
 [33.831]
 [39.456]
 [33.831]
 [33.831]] [[1.427]
 [1.427]
 [1.427]
 [1.427]
 [1.863]
 [1.427]
 [1.427]]
maxi score, test score, baseline:  -0.10706000000000009 0.6916666666666669 0.6916666666666669
printing an ep nov before normalisation:  42.43487571811036
maxi score, test score, baseline:  -0.10706000000000009 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.720872087645784
maxi score, test score, baseline:  -0.10706000000000009 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10706000000000009 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.2684, 0.0598, 0.1300, 0.1158, 0.1666, 0.1479, 0.1115],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0081, 0.9371, 0.0073, 0.0111, 0.0031, 0.0043, 0.0289],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1578, 0.0041, 0.2610, 0.1373, 0.1482, 0.1647, 0.1270],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1199, 0.0077, 0.1220, 0.4163, 0.1024, 0.1197, 0.1120],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1164, 0.0006, 0.1008, 0.1087, 0.4799, 0.1097, 0.0840],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1291, 0.0031, 0.1451, 0.1181, 0.1016, 0.3969, 0.1060],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1027, 0.2247, 0.1018, 0.1363, 0.1552, 0.1188, 0.1606],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.10706000000000009 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10706000000000009 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  46 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.066450425202376
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5266666666666668  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  17.927158707242405
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.649]
 [0.384]
 [0.391]
 [0.352]
 [0.358]
 [0.357]] [[13.604]
 [10.708]
 [13.703]
 [13.491]
 [13.49 ]
 [13.572]
 [13.608]] [[1.714]
 [1.696]
 [1.724]
 [1.711]
 [1.671]
 [1.685]
 [1.687]]
actor:  1 policy actor:  1  step number:  66 total reward:  0.17999999999999905  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.02354583024183
actor:  1 policy actor:  1  step number:  67 total reward:  0.02666666666666584  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8464002
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.006261686377556
actor:  1 policy actor:  1  step number:  50 total reward:  0.35333333333333283  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  40.89768409729004
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.31575559596261
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.3181, 0.0043, 0.1298, 0.1133, 0.1481, 0.1396, 0.1468],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0056, 0.9110, 0.0064, 0.0165, 0.0026, 0.0024, 0.0554],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1576, 0.0015, 0.2643, 0.1108, 0.1356, 0.1808, 0.1494],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1323, 0.0005, 0.1396, 0.2914, 0.1798, 0.1261, 0.1303],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0625, 0.0172, 0.0466, 0.0698, 0.6864, 0.0686, 0.0489],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1167, 0.0069, 0.1683, 0.0812, 0.1031, 0.3988, 0.1250],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1244, 0.1581, 0.0778, 0.1000, 0.1073, 0.0846, 0.3477],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  37.0152490513227
printing an ep nov before normalisation:  33.523229540441086
actor:  1 policy actor:  1  step number:  59 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]] [[44.378]
 [44.378]
 [44.378]
 [44.378]
 [44.378]
 [44.378]
 [44.378]] [[1.82]
 [1.82]
 [1.82]
 [1.82]
 [1.82]
 [1.82]
 [1.82]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.007]
 [-0.003]
 [-0.008]
 [-0.008]
 [-0.009]
 [-0.009]
 [-0.006]] [[49.029]
 [43.865]
 [49.129]
 [49.692]
 [54.604]
 [53.902]
 [48.559]] [[0.711]
 [0.57 ]
 [0.713]
 [0.729]
 [0.866]
 [0.846]
 [0.699]]
printing an ep nov before normalisation:  46.38393805534367
actor:  1 policy actor:  1  step number:  56 total reward:  0.39333333333333276  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.94904402515073
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.413476019816756
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  0.28666666666666596  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.09104663263075
printing an ep nov before normalisation:  47.209962664548826
actor:  1 policy actor:  1  step number:  57 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.238]
 [0.467]
 [0.095]
 [0.17 ]
 [0.153]
 [0.065]
 [0.151]] [[40.546]
 [40.476]
 [47.495]
 [52.696]
 [53.377]
 [53.918]
 [45.125]] [[1.208]
 [1.434]
 [1.385]
 [1.699]
 [1.713]
 [1.65 ]
 [1.332]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.28666666666666607  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.13534778870418
line 256 mcts: sample exp_bonus 40.29963615384947
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.43635736987646
using explorer policy with actor:  1
printing an ep nov before normalisation:  64.8389008454752
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 45.94855325747202
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.97709852713072
actor:  1 policy actor:  1  step number:  46 total reward:  0.4333333333333329  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  46.00946863147431
printing an ep nov before normalisation:  40.886395819890225
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.10448666666666678 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.454]
 [0.538]
 [0.258]
 [0.274]
 [0.239]
 [0.543]] [[17.611]
 [21.981]
 [19.954]
 [22.963]
 [25.365]
 [26.231]
 [20.351]] [[0.781]
 [0.848]
 [0.844]
 [0.695]
 [0.813]
 [0.816]
 [0.866]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.37999999999999934  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.428545654285735
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.85980221512673
printing an ep nov before normalisation:  33.97181777120455
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  57 total reward:  0.18666666666666631  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  53.00200152022714
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.577535605916275
maxi score, test score, baseline:  -0.10448666666666676 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  47 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.10163333333333342 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.102 0.551 0.041 0.041 0.061 0.163 0.041]
maxi score, test score, baseline:  -0.10163333333333342 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.10163333333333342 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  34 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.0985800000000001 0.6916666666666669 0.6916666666666669
UNIT TEST: sample policy line 217 mcts : [0.02  0.51  0.061 0.041 0.306 0.02  0.041]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.06021694175973
printing an ep nov before normalisation:  34.80496759689119
maxi score, test score, baseline:  -0.0985800000000001 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.0985800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0985800000000001 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.0985800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.025]
 [-0.059]
 [-0.046]
 [-0.029]
 [-0.057]
 [-0.029]
 [-0.046]] [[17.769]
 [19.706]
 [17.127]
 [17.041]
 [18.699]
 [17.272]
 [17.127]] [[1.08 ]
 [1.169]
 [1.018]
 [1.03 ]
 [1.107]
 [1.045]
 [1.018]]
Printing some Q and Qe and total Qs values:  [[-0.017]
 [ 0.022]
 [-0.004]
 [-0.01 ]
 [-0.013]
 [-0.012]
 [-0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.017]
 [ 0.022]
 [-0.004]
 [-0.01 ]
 [-0.013]
 [-0.012]
 [-0.01 ]]
siam score:  -0.8344288
maxi score, test score, baseline:  -0.10099333333333345 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.151617643618863
actor:  1 policy actor:  1  step number:  49 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.79092487723925
actor:  1 policy actor:  1  step number:  44 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.10099333333333343 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10099333333333343 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.152523785658662
printing an ep nov before normalisation:  31.15162661398806
maxi score, test score, baseline:  -0.10307333333333343 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10307333333333343 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.98077577381243
printing an ep nov before normalisation:  40.5771640532319
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  52 total reward:  0.3133333333333327  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.10044666666666678 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.10044666666666678 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.399]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]] [[33.329]
 [35.138]
 [33.329]
 [33.329]
 [33.329]
 [33.329]
 [33.329]] [[0.403]
 [0.578]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]]
maxi score, test score, baseline:  -0.10044666666666678 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.10044666666666678 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10044666666666678 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.849245012560715
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.268]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]] [[36.606]
 [38.997]
 [36.606]
 [36.606]
 [36.606]
 [36.606]
 [36.606]] [[0.857]
 [0.963]
 [0.857]
 [0.857]
 [0.857]
 [0.857]
 [0.857]]
maxi score, test score, baseline:  -0.10044666666666678 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.10044666666666678 0.6916666666666669 0.6916666666666669
printing an ep nov before normalisation:  33.24255457709914
maxi score, test score, baseline:  -0.10044666666666678 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  46 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  52 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.10044666666666678 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10044666666666678 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.20910066111423
printing an ep nov before normalisation:  34.340172540602715
maxi score, test score, baseline:  -0.10044666666666678 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.008]
 [-0.001]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]] [[47.316]
 [50.859]
 [47.316]
 [47.316]
 [47.316]
 [47.316]
 [47.316]] [[0.976]
 [1.135]
 [0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]]
siam score:  -0.840965
maxi score, test score, baseline:  -0.10044666666666678 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.45333333333333303  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.10044666666666678 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10044666666666678 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.62297413720079
maxi score, test score, baseline:  -0.10044666666666678 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10044666666666678 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3199999999999992  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.10044666666666678 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.27794593947128
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.10044666666666678 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.10044666666666678 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.975806300054735
maxi score, test score, baseline:  -0.10044666666666678 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.10044666666666678 0.6916666666666669 0.6916666666666669
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.1004466666666668 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.1004466666666668 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  51.560110192458346
maxi score, test score, baseline:  -0.1004466666666668 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.1004466666666668 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.53904284415519
maxi score, test score, baseline:  -0.1004466666666668 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actor:  0 policy actor:  0  step number:  48 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[ 0.087]
 [ 0.087]
 [-0.025]
 [ 0.087]
 [-0.029]
 [ 0.087]
 [ 0.087]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.087]
 [ 0.087]
 [-0.025]
 [ 0.087]
 [-0.029]
 [ 0.087]
 [ 0.087]]
printing an ep nov before normalisation:  55.99540074666341
printing an ep nov before normalisation:  42.262072222140574
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.64183540101506
maxi score, test score, baseline:  -0.10000666666666679 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  38 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  40.461225171488444
maxi score, test score, baseline:  -0.09687333333333341 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.025633139313996
actor:  1 policy actor:  1  step number:  39 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.09687333333333341 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.821821218596824
maxi score, test score, baseline:  -0.09687333333333341 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.9635681365088
maxi score, test score, baseline:  -0.09687333333333341 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.545]
 [0.449]
 [0.458]
 [0.457]
 [0.451]
 [0.459]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.458]
 [0.545]
 [0.449]
 [0.458]
 [0.457]
 [0.451]
 [0.459]]
maxi score, test score, baseline:  -0.09687333333333341 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.66149243832437
actor:  1 policy actor:  1  step number:  58 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.09687333333333341 0.6916666666666669 0.6916666666666669
siam score:  -0.84910494
actions average: 
K:  1  action  0 :  tensor([0.2851, 0.1215, 0.0950, 0.1042, 0.2169, 0.0905, 0.0868],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0071, 0.9239, 0.0112, 0.0049, 0.0013, 0.0015, 0.0501],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1132, 0.0044, 0.3710, 0.1533, 0.1119, 0.1308, 0.1152],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1066, 0.1218, 0.0983, 0.3659, 0.1085, 0.0946, 0.1043],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1363, 0.0072, 0.1010, 0.0986, 0.4786, 0.0978, 0.0806],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1193, 0.0823, 0.1184, 0.1392, 0.1457, 0.2834, 0.1116],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1742, 0.0055, 0.1439, 0.1680, 0.1709, 0.1705, 0.1669],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.761]
 [0.82 ]
 [0.778]
 [0.762]
 [0.762]
 [0.762]
 [0.762]] [[15.898]
 [29.841]
 [25.751]
 [15.675]
 [15.705]
 [15.707]
 [15.498]] [[0.761]
 [0.82 ]
 [0.778]
 [0.762]
 [0.762]
 [0.762]
 [0.762]]
maxi score, test score, baseline:  -0.09687333333333341 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.3796, 0.1008, 0.0827, 0.1122, 0.1270, 0.1020, 0.0956],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0054, 0.9673, 0.0039, 0.0033, 0.0026, 0.0033, 0.0142],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0860, 0.0099, 0.5116, 0.0715, 0.0885, 0.1336, 0.0990],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0596, 0.0431, 0.0581, 0.5840, 0.0599, 0.0695, 0.1259],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1093, 0.0330, 0.0673, 0.1090, 0.5443, 0.0795, 0.0577],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0952, 0.0209, 0.1354, 0.0987, 0.1030, 0.4456, 0.1011],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1231, 0.3030, 0.1139, 0.0990, 0.1044, 0.1045, 0.1522],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.043925285339355
maxi score, test score, baseline:  -0.09687333333333341 0.6916666666666669 0.6916666666666669
actor:  0 policy actor:  0  step number:  47 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  64 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.09182000000000011 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.09182000000000011 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.09182000000000011 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.638571740391164
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.85353183059338
maxi score, test score, baseline:  -0.09182000000000011 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.09182000000000011 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.6880257898556
printing an ep nov before normalisation:  38.781914207501565
printing an ep nov before normalisation:  61.207517756884144
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.06054592314072
printing an ep nov before normalisation:  40.514291305405784
actor:  1 policy actor:  1  step number:  60 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  2  action  0 :  tensor([0.4074, 0.0146, 0.0900, 0.1116, 0.1762, 0.0888, 0.1114],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0067, 0.9408, 0.0070, 0.0055, 0.0049, 0.0052, 0.0298],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1196, 0.0843, 0.2312, 0.1464, 0.1099, 0.1557, 0.1529],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0738, 0.0113, 0.1274, 0.2662, 0.0784, 0.1761, 0.2668],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1498, 0.0643, 0.0653, 0.0792, 0.4918, 0.0781, 0.0715],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0968, 0.0009, 0.1773, 0.1242, 0.1022, 0.3861, 0.1124],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1119, 0.0885, 0.0923, 0.1588, 0.0909, 0.1011, 0.3565],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.09182000000000011 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8511732
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.09182000000000011 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.09182000000000011 0.6916666666666669 0.6916666666666669
actor:  1 policy actor:  1  step number:  70 total reward:  0.1266666666666656  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  37.39275932312012
maxi score, test score, baseline:  -0.09182000000000011 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.698]
 [0.547]
 [0.593]
 [0.567]
 [0.552]
 [0.558]] [[37.54 ]
 [37.74 ]
 [36.56 ]
 [35.647]
 [36.65 ]
 [37.141]
 [36.732]] [[1.829]
 [1.978]
 [1.754]
 [1.743]
 [1.78 ]
 [1.795]
 [1.776]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  25.11018440207971
printing an ep nov before normalisation:  39.1097214724327
maxi score, test score, baseline:  -0.09182000000000011 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.09182000000000011 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.09182000000000011 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.09182000000000011 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.09182000000000011 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.00369832645514
actor:  1 policy actor:  1  step number:  55 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8457144
actions average: 
K:  2  action  0 :  tensor([0.1312, 0.0648, 0.1330, 0.1627, 0.2421, 0.1514, 0.1149],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0109, 0.9477, 0.0076, 0.0103, 0.0024, 0.0014, 0.0197],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0802, 0.0571, 0.3470, 0.1263, 0.1051, 0.1990, 0.0852],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0915, 0.1146, 0.1017, 0.3741, 0.1035, 0.1026, 0.1119],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1148, 0.0031, 0.0731, 0.1609, 0.4633, 0.0812, 0.1034],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0690, 0.0296, 0.1344, 0.1178, 0.0897, 0.4855, 0.0739],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1320, 0.0102, 0.1414, 0.1901, 0.2226, 0.1757, 0.1280],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.09182000000000011 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.71271037725826
actor:  1 policy actor:  1  step number:  49 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.618]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]] [[26.352]
 [29.541]
 [26.352]
 [26.352]
 [26.352]
 [26.352]
 [26.352]] [[0.49 ]
 [0.618]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]]
printing an ep nov before normalisation:  46.61271169356636
UNIT TEST: sample policy line 217 mcts : [0.143 0.082 0.184 0.102 0.061 0.388 0.041]
line 256 mcts: sample exp_bonus 35.752153396606445
printing an ep nov before normalisation:  36.88709071655836
maxi score, test score, baseline:  -0.09182000000000011 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.09182000000000011 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  51 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  1.667
siam score:  -0.84016013
maxi score, test score, baseline:  -0.08931333333333343 0.6916666666666669 0.6916666666666669
printing an ep nov before normalisation:  38.66287338591152
Printing some Q and Qe and total Qs values:  [[0.507]
 [0.482]
 [0.576]
 [0.572]
 [0.565]
 [0.56 ]
 [0.59 ]] [[38.002]
 [37.803]
 [35.197]
 [35.408]
 [35.158]
 [35.223]
 [34.307]] [[1.323]
 [1.293]
 [1.33 ]
 [1.331]
 [1.318]
 [1.315]
 [1.324]]
printing an ep nov before normalisation:  50.902618924543866
printing an ep nov before normalisation:  33.15071933934343
maxi score, test score, baseline:  -0.08931333333333343 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.08931333333333343 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.447]
 [0.364]
 [0.364]
 [0.364]
 [0.364]
 [0.364]] [[28.575]
 [29.15 ]
 [28.575]
 [28.575]
 [28.575]
 [28.575]
 [28.575]] [[1.289]
 [1.409]
 [1.289]
 [1.289]
 [1.289]
 [1.289]
 [1.289]]
maxi score, test score, baseline:  -0.08931333333333343 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.77318691417676
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.632]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]] [[38.248]
 [39.666]
 [38.248]
 [38.248]
 [38.248]
 [38.248]
 [38.248]] [[1.449]
 [1.664]
 [1.449]
 [1.449]
 [1.449]
 [1.449]
 [1.449]]
Printing some Q and Qe and total Qs values:  [[0.189]
 [0.235]
 [0.137]
 [0.153]
 [0.143]
 [0.143]
 [0.14 ]] [[46.824]
 [44.444]
 [47.85 ]
 [48.056]
 [46.545]
 [48.996]
 [48.947]] [[1.408]
 [1.328]
 [1.41 ]
 [1.437]
 [1.347]
 [1.476]
 [1.47 ]]
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.54 ]
 [0.292]
 [0.302]
 [0.292]
 [0.294]
 [0.293]] [[34.829]
 [31.647]
 [39.389]
 [37.551]
 [39.438]
 [39.507]
 [39.5  ]] [[0.607]
 [0.755]
 [0.623]
 [0.606]
 [0.624]
 [0.627]
 [0.626]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.7437],
        [ 0.0358],
        [ 0.3728],
        [ 0.1696],
        [-0.0528],
        [-0.0258],
        [-0.0000],
        [-0.0591],
        [-0.0112],
        [-0.0250]], dtype=torch.float64)
-0.071422513866 0.6722317968032798
-0.070771701198 -0.03496369731261678
-0.070771701198 0.30205108374384326
-0.032346567066 0.13726145523389477
-0.032346567066 -0.0851248651827401
-0.032346567066 -0.058141785159148626
0.9120460964999999 0.9120460964999999
-0.032346567066 -0.09144547706386708
-0.032346567066 -0.04358728932597858
-0.032346567066 -0.057321142187618454
printing an ep nov before normalisation:  51.237161713202944
actor:  0 policy actor:  0  step number:  32 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  39.44140672683716
line 256 mcts: sample exp_bonus 32.41913236297475
actor:  1 policy actor:  1  step number:  38 total reward:  0.5  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.133]
 [0.215]
 [0.133]
 [0.133]
 [0.133]
 [0.133]
 [0.133]] [[32.41 ]
 [39.185]
 [32.41 ]
 [32.41 ]
 [32.41 ]
 [32.41 ]
 [32.41 ]] [[0.303]
 [0.454]
 [0.303]
 [0.303]
 [0.303]
 [0.303]
 [0.303]]
siam score:  -0.84240675
maxi score, test score, baseline:  -0.08618000000000009 0.6916666666666669 0.6916666666666669
printing an ep nov before normalisation:  48.11247219602374
printing an ep nov before normalisation:  44.55026434498456
maxi score, test score, baseline:  -0.08618000000000009 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.526]
 [0.396]
 [0.396]
 [0.396]
 [0.402]
 [0.396]] [[47.136]
 [44.343]
 [47.136]
 [47.136]
 [47.136]
 [48.285]
 [47.136]] [[1.924]
 [1.914]
 [1.924]
 [1.924]
 [1.924]
 [1.987]
 [1.924]]
printing an ep nov before normalisation:  29.517476026888502
printing an ep nov before normalisation:  34.56182160231053
maxi score, test score, baseline:  -0.08618000000000009 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.137955366183064
maxi score, test score, baseline:  -0.08618000000000009 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.25333333333333274  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  33.268498787694526
Printing some Q and Qe and total Qs values:  [[0.62 ]
 [0.784]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.597]
 [0.62 ]] [[31.123]
 [35.235]
 [31.123]
 [31.123]
 [31.123]
 [33.839]
 [31.123]] [[1.109]
 [1.338]
 [1.109]
 [1.109]
 [1.109]
 [1.129]
 [1.109]]
actor:  1 policy actor:  1  step number:  57 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.08618000000000009 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.08618000000000009 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.08618000000000009 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.019]
 [-0.004]
 [-0.015]
 [-0.013]
 [-0.007]
 [-0.01 ]
 [-0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.019]
 [-0.004]
 [-0.015]
 [-0.013]
 [-0.007]
 [-0.01 ]
 [-0.01 ]]
line 256 mcts: sample exp_bonus 45.50534860056757
maxi score, test score, baseline:  -0.0861800000000001 0.6916666666666669 0.6916666666666669
printing an ep nov before normalisation:  48.75167051075526
maxi score, test score, baseline:  -0.0861800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.462]
 [0.473]
 [0.473]
 [0.478]
 [0.473]
 [0.473]] [[42.354]
 [44.617]
 [42.354]
 [42.354]
 [43.191]
 [42.354]
 [42.354]] [[0.98 ]
 [1.021]
 [0.98 ]
 [0.98 ]
 [1.004]
 [0.98 ]
 [0.98 ]]
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([0.1336, 0.4619, 0.0709, 0.1018, 0.0807, 0.0553, 0.0959],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0190, 0.8840, 0.0159, 0.0234, 0.0073, 0.0082, 0.0422],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1788, 0.0415, 0.1865, 0.1314, 0.1631, 0.1430, 0.1558],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1356, 0.1039, 0.1174, 0.2968, 0.1128, 0.1019, 0.1316],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2071, 0.0609, 0.0741, 0.0810, 0.4143, 0.0819, 0.0807],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1286, 0.0055, 0.1658, 0.1436, 0.1240, 0.3364, 0.0961],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2052, 0.2044, 0.1071, 0.1258, 0.1010, 0.0960, 0.1606],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.0861800000000001 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.0861800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.45545054404058
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.0861800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  55.62319755554199
actor:  0 policy actor:  0  step number:  56 total reward:  0.09999999999999942  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  31.339598068435983
printing an ep nov before normalisation:  34.46734501024955
maxi score, test score, baseline:  -0.08398000000000012 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.323]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]] [[37.222]
 [45.917]
 [37.222]
 [37.222]
 [37.222]
 [37.222]
 [37.222]] [[0.586]
 [0.814]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]]
maxi score, test score, baseline:  -0.0839800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.61671733856201
maxi score, test score, baseline:  -0.0839800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.88768055643185
printing an ep nov before normalisation:  14.690914151928496
actor:  1 policy actor:  1  step number:  56 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.368]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]] [[34.342]
 [44.609]
 [34.342]
 [34.342]
 [34.342]
 [34.342]
 [34.342]] [[0.88 ]
 [1.287]
 [0.88 ]
 [0.88 ]
 [0.88 ]
 [0.88 ]
 [0.88 ]]
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  -0.0839800000000001 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.0839800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.271979216291605
maxi score, test score, baseline:  -0.0839800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.2133333333333326  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.0839800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.28666666666666607  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  48.44803100635382
printing an ep nov before normalisation:  45.8991217458374
maxi score, test score, baseline:  -0.0839800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.5599838575826
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4066666666666666  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  64 total reward:  0.15333333333333254  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.0839800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0839800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0839800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]] [[12.541]
 [12.541]
 [12.541]
 [12.541]
 [12.541]
 [12.541]
 [12.541]] [[0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]]
Printing some Q and Qe and total Qs values:  [[0.823]
 [0.552]
 [0.823]
 [0.823]
 [0.823]
 [0.823]
 [0.823]] [[33.741]
 [35.618]
 [33.741]
 [33.741]
 [33.741]
 [33.741]
 [33.741]] [[0.823]
 [0.552]
 [0.823]
 [0.823]
 [0.823]
 [0.823]
 [0.823]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.547872022000504
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]
 [0.668]] [[38.816]
 [38.816]
 [38.816]
 [38.816]
 [38.816]
 [38.816]
 [38.816]] [[2.158]
 [2.158]
 [2.158]
 [2.158]
 [2.158]
 [2.158]
 [2.158]]
printing an ep nov before normalisation:  40.74732550072762
line 256 mcts: sample exp_bonus 53.1320643410175
printing an ep nov before normalisation:  43.915350373613556
maxi score, test score, baseline:  -0.0839800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0839800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  31.581985920610915
actor:  1 policy actor:  1  step number:  51 total reward:  0.34666666666666657  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.423]
 [0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]] [[35.662]
 [32.32 ]
 [35.662]
 [35.662]
 [35.662]
 [35.662]
 [35.662]] [[0.501]
 [0.577]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.5713163333252
maxi score, test score, baseline:  -0.0839800000000001 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.0839800000000001 0.6916666666666669 0.6916666666666669
actor:  0 policy actor:  0  step number:  72 total reward:  0.23333333333333306  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  49.419970608429345
printing an ep nov before normalisation:  39.144106050224494
maxi score, test score, baseline:  -0.08151333333333342 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.10540232853432
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.01030445098877
maxi score, test score, baseline:  -0.08151333333333342 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.009]
 [-0.005]
 [-0.009]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.009]] [[65.553]
 [52.867]
 [65.553]
 [63.476]
 [61.74 ]
 [62.481]
 [65.553]] [[1.49 ]
 [1.033]
 [1.49 ]
 [1.417]
 [1.354]
 [1.381]
 [1.49 ]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.08151333333333342 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.77889760335287
maxi score, test score, baseline:  -0.08151333333333342 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.08412666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.675823288024645
siam score:  -0.83159804
printing an ep nov before normalisation:  50.361074817809765
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.82 ]
 [0.734]
 [0.751]
 [0.719]
 [0.727]
 [0.741]] [[35.333]
 [40.587]
 [42.808]
 [39.641]
 [35.333]
 [42.426]
 [40.869]] [[0.719]
 [0.82 ]
 [0.734]
 [0.751]
 [0.719]
 [0.727]
 [0.741]]
printing an ep nov before normalisation:  21.876640744810828
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.07874325739158
printing an ep nov before normalisation:  45.18864631652832
actor:  0 policy actor:  0  step number:  51 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.14 ]
 [0.171]
 [0.14 ]
 [0.14 ]
 [0.14 ]
 [0.14 ]
 [0.14 ]] [[36.264]
 [47.756]
 [36.264]
 [36.264]
 [36.264]
 [36.264]
 [36.264]] [[0.421]
 [0.684]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  56 total reward:  0.326666666666666  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.08164666666666677 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.08164666666666677 0.6916666666666669 0.6916666666666669
actions average: 
K:  0  action  0 :  tensor([0.3422, 0.0964, 0.0994, 0.1113, 0.1122, 0.1104, 0.1282],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0062, 0.9598, 0.0040, 0.0038, 0.0030, 0.0023, 0.0208],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1143, 0.0180, 0.3932, 0.1231, 0.1024, 0.1212, 0.1278],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1080, 0.0035, 0.1216, 0.3744, 0.1164, 0.1397, 0.1365],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1414, 0.0042, 0.1172, 0.1400, 0.3593, 0.1167, 0.1212],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1157, 0.0104, 0.1159, 0.1131, 0.1076, 0.4093, 0.1278],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1196, 0.2326, 0.1111, 0.1222, 0.1179, 0.1119, 0.1847],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.042]
 [0.104]
 [0.055]
 [0.045]
 [0.046]
 [0.045]
 [0.058]] [[56.747]
 [50.472]
 [55.134]
 [57.412]
 [57.303]
 [58.015]
 [55.09 ]] [[0.862]
 [0.754]
 [0.831]
 [0.883]
 [0.881]
 [0.899]
 [0.833]]
maxi score, test score, baseline:  -0.08164666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.83467562313982
actions average: 
K:  1  action  0 :  tensor([0.2764, 0.0156, 0.1600, 0.1443, 0.1333, 0.1540, 0.1164],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0083, 0.9380, 0.0061, 0.0122, 0.0039, 0.0044, 0.0271],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1084, 0.0173, 0.4129, 0.1149, 0.1081, 0.1123, 0.1262],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1533, 0.1182, 0.0902, 0.2573, 0.1372, 0.0902, 0.1536],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1497, 0.0561, 0.1027, 0.1108, 0.3367, 0.1126, 0.1314],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1622, 0.0071, 0.1471, 0.1364, 0.1437, 0.2423, 0.1612],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1137, 0.1238, 0.1175, 0.1386, 0.1275, 0.1342, 0.2448],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.08164666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.175490054262745
actions average: 
K:  3  action  0 :  tensor([0.2413, 0.0213, 0.1337, 0.1504, 0.1481, 0.1334, 0.1717],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0054,     0.9566,     0.0032,     0.0068,     0.0010,     0.0008,
            0.0262], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1227, 0.1118, 0.3019, 0.1086, 0.1114, 0.1072, 0.1364],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0793, 0.0781, 0.0829, 0.3973, 0.1180, 0.0937, 0.1507],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1113, 0.1782, 0.1014, 0.1148, 0.2906, 0.0912, 0.1126],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1291, 0.0383, 0.1687, 0.1372, 0.1346, 0.2427, 0.1492],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1392, 0.0544, 0.1430, 0.1535, 0.1382, 0.1289, 0.2428],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.461]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]] [[37.128]
 [40.463]
 [37.128]
 [37.128]
 [37.128]
 [37.128]
 [37.128]] [[1.144]
 [1.316]
 [1.144]
 [1.144]
 [1.144]
 [1.144]
 [1.144]]
maxi score, test score, baseline:  -0.08164666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  52 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.08164666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.25813627243042
maxi score, test score, baseline:  -0.08164666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.44142440730983
maxi score, test score, baseline:  -0.08164666666666677 0.6916666666666669 0.6916666666666669
maxi score, test score, baseline:  -0.08164666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.08164666666666677 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.591]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.562]
 [0.591]
 [0.562]
 [0.562]
 [0.562]
 [0.562]
 [0.562]]
printing an ep nov before normalisation:  39.17672872543335
printing an ep nov before normalisation:  55.848332070433514
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]] [[44.141]
 [44.141]
 [44.141]
 [44.141]
 [44.141]
 [44.141]
 [44.141]] [[0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]]
printing an ep nov before normalisation:  44.62135911489092
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.637]
 [0.535]
 [0.535]
 [0.543]
 [0.535]
 [0.532]] [[38.286]
 [33.443]
 [36.624]
 [36.624]
 [42.996]
 [36.624]
 [38.521]] [[0.528]
 [0.637]
 [0.535]
 [0.535]
 [0.543]
 [0.535]
 [0.532]]
printing an ep nov before normalisation:  40.15985043756847
printing an ep nov before normalisation:  46.067800201448435
printing an ep nov before normalisation:  46.151933670043945
printing an ep nov before normalisation:  37.76674062191333
printing an ep nov before normalisation:  51.58392525906337
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actor:  1 policy actor:  1  step number:  54 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  39.527854559207555
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  45 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  42.868261676802135
printing an ep nov before normalisation:  31.545889992214956
printing an ep nov before normalisation:  31.14313840866089
printing an ep nov before normalisation:  37.67213855018907
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
Printing some Q and Qe and total Qs values:  [[0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]] [[38.503]
 [ 9.762]
 [36.908]
 [36.977]
 [37.064]
 [36.847]
 [31.069]] [[0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]]
printing an ep nov before normalisation:  1.0202975317952223e-05
maxi score, test score, baseline:  -0.07887333333333342 0.6916666666666669 0.6916666666666669
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  45.449471087330814
Printing some Q and Qe and total Qs values:  [[0.105]
 [0.162]
 [0.105]
 [0.105]
 [0.105]
 [0.133]
 [0.105]] [[36.118]
 [44.66 ]
 [36.118]
 [36.118]
 [36.118]
 [42.35 ]
 [36.118]] [[0.968]
 [1.422]
 [0.968]
 [0.968]
 [0.968]
 [1.285]
 [0.968]]
printing an ep nov before normalisation:  37.11088578546175
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.186]
 [0.272]
 [0.186]] [[53.081]
 [53.081]
 [53.081]
 [53.081]
 [53.081]
 [41.559]
 [53.081]] [[1.85 ]
 [1.85 ]
 [1.85 ]
 [1.85 ]
 [1.85 ]
 [1.373]
 [1.85 ]]
printing an ep nov before normalisation:  39.328471907255974
actor:  1 policy actor:  1  step number:  54 total reward:  0.2466666666666658  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.02575333333333344 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.99797797513699
Printing some Q and Qe and total Qs values:  [[0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]
 [0.298]] [[52.194]
 [52.194]
 [52.194]
 [52.194]
 [52.194]
 [52.194]
 [52.194]] [[2.18]
 [2.18]
 [2.18]
 [2.18]
 [2.18]
 [2.18]
 [2.18]]
maxi score, test score, baseline:  -0.02575333333333344 0.6923333333333335 0.6923333333333335
siam score:  -0.8334375
maxi score, test score, baseline:  -0.02575333333333344 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]
 [0.169]]
siam score:  -0.833488
maxi score, test score, baseline:  -0.02575333333333344 0.6923333333333335 0.6923333333333335
actor:  1 policy actor:  1  step number:  63 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.306]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.297]
 [0.306]
 [0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]]
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]
 [0.441]] [[46.685]
 [46.685]
 [46.685]
 [46.685]
 [46.685]
 [46.685]
 [46.685]] [[1.774]
 [1.774]
 [1.774]
 [1.774]
 [1.774]
 [1.774]
 [1.774]]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.02575333333333344 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.468]
 [0.588]
 [0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.43 ]] [[22.219]
 [23.584]
 [24.077]
 [24.077]
 [24.077]
 [24.077]
 [21.421]] [[1.563]
 [1.751]
 [1.699]
 [1.699]
 [1.699]
 [1.699]
 [1.486]]
actor:  1 policy actor:  1  step number:  63 total reward:  0.23999999999999988  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  44 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.025366666666666756 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.025366666666666756 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.006]
 [-0.017]
 [-0.006]
 [-0.006]
 [-0.005]
 [-0.005]
 [-0.003]] [[39.205]
 [51.669]
 [46.489]
 [37.705]
 [37.734]
 [38.022]
 [37.313]] [[0.236]
 [0.461]
 [0.375]
 [0.208]
 [0.21 ]
 [0.216]
 [0.204]]
maxi score, test score, baseline:  -0.025366666666666756 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.061310874091255
maxi score, test score, baseline:  -0.025366666666666756 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.025366666666666756 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.104885014908895
maxi score, test score, baseline:  -0.025366666666666756 0.6923333333333335 0.6923333333333335
maxi score, test score, baseline:  -0.025366666666666756 0.6923333333333335 0.6923333333333335
printing an ep nov before normalisation:  36.35297473536636
actor:  1 policy actor:  1  step number:  58 total reward:  0.28666666666666596  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.025366666666666756 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8326504
maxi score, test score, baseline:  -0.025366666666666756 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.77870772204673
actor:  1 policy actor:  1  step number:  76 total reward:  0.019999999999999463  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  44.99607616676862
Printing some Q and Qe and total Qs values:  [[0.082]
 [0.064]
 [0.079]
 [0.079]
 [0.083]
 [0.076]
 [0.093]] [[35.628]
 [38.559]
 [35.284]
 [35.427]
 [34.828]
 [35.158]
 [34.775]] [[0.779]
 [0.87 ]
 [0.763]
 [0.767]
 [0.749]
 [0.755]
 [0.757]]
printing an ep nov before normalisation:  29.553661346435547
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.025366666666666756 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.17145837556133
maxi score, test score, baseline:  -0.025366666666666756 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.025366666666666756 0.6923333333333335 0.6923333333333335
actor:  0 policy actor:  0  step number:  51 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.026153333333333445 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.026153333333333445 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.95018720646577
actor:  1 policy actor:  1  step number:  70 total reward:  0.07333333333333258  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8306606
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.581]
 [0.517]
 [0.548]
 [0.544]
 [0.515]
 [0.528]] [[53.268]
 [49.324]
 [57.494]
 [53.878]
 [53.436]
 [54.771]
 [51.616]] [[1.535]
 [1.473]
 [1.706]
 [1.605]
 [1.585]
 [1.605]
 [1.504]]
printing an ep nov before normalisation:  38.20169834294835
siam score:  -0.8326462
maxi score, test score, baseline:  -0.026153333333333445 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.45 ]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.407]] [[38.052]
 [43.304]
 [38.052]
 [38.052]
 [38.052]
 [38.052]
 [38.052]] [[1.546]
 [1.889]
 [1.546]
 [1.546]
 [1.546]
 [1.546]
 [1.546]]
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]] [[39.049]
 [39.049]
 [39.049]
 [39.049]
 [39.049]
 [39.049]
 [39.049]] [[65.469]
 [65.469]
 [65.469]
 [65.469]
 [65.469]
 [65.469]
 [65.469]]
printing an ep nov before normalisation:  41.43938064575195
maxi score, test score, baseline:  -0.026153333333333445 0.6923333333333335 0.6923333333333335
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.026153333333333445 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.026153333333333445 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  48.18369059870822
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.68 ]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]] [[38.83]
 [37.  ]
 [38.83]
 [38.83]
 [38.83]
 [38.83]
 [38.83]] [[0.641]
 [0.68 ]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]]
maxi score, test score, baseline:  -0.029486666666666758 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.3139, 0.0451, 0.1261, 0.1150, 0.1338, 0.1171, 0.1490],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0043, 0.9603, 0.0040, 0.0045, 0.0022, 0.0028, 0.0219],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1450, 0.1448, 0.3213, 0.0936, 0.0765, 0.1277, 0.0912],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1441, 0.0464, 0.1403, 0.2322, 0.1297, 0.1519, 0.1554],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1157, 0.0018, 0.0654, 0.0693, 0.6091, 0.0662, 0.0724],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1074, 0.0073, 0.1047, 0.1229, 0.1065, 0.4332, 0.1180],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1134, 0.1149, 0.0899, 0.1728, 0.1210, 0.1439, 0.2442],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  13.218045392123763
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.029486666666666758 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.029486666666666758 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  37 total reward:  0.52  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.02978000000000008 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.47060270813526
maxi score, test score, baseline:  -0.02978000000000008 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.665]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]] [[31.391]
 [32.019]
 [31.391]
 [31.391]
 [31.391]
 [31.391]
 [31.391]] [[0.816]
 [0.996]
 [0.816]
 [0.816]
 [0.816]
 [0.816]
 [0.816]]
maxi score, test score, baseline:  -0.02978000000000008 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  77 total reward:  0.02666666666666606  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.91084376804741
printing an ep nov before normalisation:  51.40208714157916
printing an ep nov before normalisation:  46.73698508120096
maxi score, test score, baseline:  -0.02978000000000008 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.02978000000000008 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.654202691777165
maxi score, test score, baseline:  -0.02978000000000008 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.061]
 [-0.1  ]
 [-0.061]
 [-0.061]
 [-0.061]
 [-0.062]
 [-0.061]] [[38.852]
 [38.641]
 [38.852]
 [38.852]
 [38.852]
 [39.368]
 [38.852]] [[1.243]
 [1.192]
 [1.243]
 [1.243]
 [1.243]
 [1.271]
 [1.243]]
maxi score, test score, baseline:  -0.02978000000000008 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.02978000000000008 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.884800910949707
printing an ep nov before normalisation:  29.22323226928711
printing an ep nov before normalisation:  35.62009790264944
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  54.02243406435168
maxi score, test score, baseline:  -0.03311333333333342 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.40888126608673
maxi score, test score, baseline:  -0.03311333333333342 0.6923333333333335 0.6923333333333335
maxi score, test score, baseline:  -0.03311333333333342 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.03311333333333342 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.34681440663995
actor:  1 policy actor:  1  step number:  51 total reward:  0.37333333333333285  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.03311333333333342 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.53615519850636
using explorer policy with actor:  1
siam score:  -0.824973
maxi score, test score, baseline:  -0.036446666666666766 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.036446666666666766 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.511261923990844
printing an ep nov before normalisation:  45.08868217468262
maxi score, test score, baseline:  -0.036446666666666766 0.6923333333333335 0.6923333333333335
printing an ep nov before normalisation:  38.90491083863754
printing an ep nov before normalisation:  44.83535034932511
maxi score, test score, baseline:  -0.036446666666666766 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.36  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  32.68855102726064
actor:  1 policy actor:  1  step number:  67 total reward:  0.34666666666666657  reward:  1.0 rdn_beta:  1.667
siam score:  -0.827247
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.61180524570928
maxi score, test score, baseline:  -0.036446666666666766 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.036446666666666766 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.10752999593704
maxi score, test score, baseline:  -0.036446666666666766 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.036446666666666766 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.13416537199689
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.276]
 [0.074]
 [0.074]
 [0.074]
 [0.074]
 [0.074]] [[53.133]
 [55.897]
 [53.133]
 [53.133]
 [53.133]
 [53.133]
 [53.133]] [[1.606]
 [1.943]
 [1.606]
 [1.606]
 [1.606]
 [1.606]
 [1.606]]
siam score:  -0.8269922
printing an ep nov before normalisation:  48.740787284705654
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.382]
 [0.382]
 [0.291]
 [0.495]
 [0.382]
 [0.382]] [[42.673]
 [46.352]
 [46.352]
 [42.339]
 [45.417]
 [46.352]
 [46.352]] [[0.621]
 [0.667]
 [0.667]
 [0.538]
 [0.771]
 [0.667]
 [0.667]]
actions average: 
K:  4  action  0 :  tensor([0.3032, 0.0890, 0.1382, 0.1039, 0.1038, 0.1295, 0.1325],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0120, 0.9157, 0.0101, 0.0094, 0.0048, 0.0065, 0.0415],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1359, 0.1664, 0.4017, 0.0564, 0.0613, 0.1085, 0.0699],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1568, 0.0768, 0.1298, 0.2820, 0.1276, 0.1093, 0.1176],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1184, 0.0030, 0.0874, 0.0780, 0.5531, 0.0807, 0.0794],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0926, 0.0406, 0.1121, 0.1022, 0.0645, 0.4741, 0.1138],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.2043, 0.0102, 0.1620, 0.1075, 0.1797, 0.1826, 0.1538],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.718]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]] [[33.811]
 [51.581]
 [33.811]
 [33.811]
 [33.811]
 [33.811]
 [33.811]] [[1.012]
 [2.006]
 [1.012]
 [1.012]
 [1.012]
 [1.012]
 [1.012]]
printing an ep nov before normalisation:  48.54826636634914
maxi score, test score, baseline:  -0.036446666666666766 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.661236336552946
maxi score, test score, baseline:  -0.036446666666666766 0.6923333333333335 0.6923333333333335
maxi score, test score, baseline:  -0.036446666666666766 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.253823882057446
printing an ep nov before normalisation:  44.113178197699995
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  58.459290839520115
maxi score, test score, baseline:  -0.036446666666666766 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8268509
maxi score, test score, baseline:  -0.036446666666666766 0.6923333333333335 0.6923333333333335
actor:  0 policy actor:  0  step number:  40 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  55.76911106366377
actor:  1 policy actor:  1  step number:  52 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.550047822291255
maxi score, test score, baseline:  -0.036753333333333436 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.036753333333333436 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.10152172398163
maxi score, test score, baseline:  -0.036753333333333436 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.03821786657397297
actor:  0 policy actor:  0  step number:  55 total reward:  0.13333333333333275  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.037806666666666766 0.6923333333333335 0.6923333333333335
maxi score, test score, baseline:  -0.037806666666666766 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.151922846054745
Printing some Q and Qe and total Qs values:  [[0.785]
 [0.785]
 [0.785]
 [0.785]
 [0.785]
 [0.785]
 [0.785]] [[34.539]
 [34.539]
 [34.539]
 [34.539]
 [34.539]
 [34.539]
 [34.539]] [[1.68]
 [1.68]
 [1.68]
 [1.68]
 [1.68]
 [1.68]
 [1.68]]
maxi score, test score, baseline:  -0.037806666666666766 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.037806666666666766 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.037806666666666766 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.037806666666666766 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.150723584193486
maxi score, test score, baseline:  -0.037806666666666766 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.937236159678069
maxi score, test score, baseline:  -0.037806666666666766 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.83223295
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.451]
 [0.463]
 [0.335]
 [0.332]
 [0.334]
 [0.334]] [[32.694]
 [35.731]
 [33.042]
 [33.244]
 [32.958]
 [33.08 ]
 [33.085]] [[0.555]
 [0.723]
 [0.698]
 [0.573]
 [0.566]
 [0.569]
 [0.57 ]]
maxi score, test score, baseline:  -0.037806666666666766 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.817]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]] [[27.163]
 [38.332]
 [27.163]
 [27.163]
 [27.163]
 [27.163]
 [27.163]] [[0.729]
 [0.817]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.729]]
actor:  1 policy actor:  1  step number:  66 total reward:  0.20666666666666655  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  32.48479983829506
actor:  1 policy actor:  1  step number:  45 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.667]
 [0.854]
 [0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]] [[30.578]
 [44.6  ]
 [30.578]
 [30.578]
 [30.578]
 [30.578]
 [30.578]] [[0.667]
 [0.854]
 [0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]]
maxi score, test score, baseline:  -0.037806666666666766 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.037806666666666766 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.217972285405295
printing an ep nov before normalisation:  27.38494632460597
printing an ep nov before normalisation:  55.47800086459769
actor:  0 policy actor:  0  step number:  61 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.03896666666666675 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.07438144572842
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]] [[27.881]
 [27.881]
 [27.881]
 [27.881]
 [27.881]
 [27.881]
 [27.881]] [[0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.735]]
maxi score, test score, baseline:  -0.04226000000000008 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.04226000000000008 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.9723632893796
printing an ep nov before normalisation:  40.898332774664496
maxi score, test score, baseline:  -0.04226000000000008 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.04226000000000008 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.04226000000000008 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0010374346527441958
printing an ep nov before normalisation:  34.072582721710205
maxi score, test score, baseline:  -0.04226000000000008 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  59 total reward:  0.11999999999999955  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  36 total reward:  0.48666666666666647  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.043646666666666736 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.043646666666666736 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.02294997290627
printing an ep nov before normalisation:  48.27636082200366
actor:  0 policy actor:  0  step number:  48 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.04726000000000008 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.42033912908778
printing an ep nov before normalisation:  40.61379504665117
printing an ep nov before normalisation:  42.365767897982614
maxi score, test score, baseline:  -0.04726000000000008 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.96985093750816
printing an ep nov before normalisation:  46.94576932235939
printing an ep nov before normalisation:  37.81330742587963
maxi score, test score, baseline:  -0.04726000000000008 0.6923333333333335 0.6923333333333335
maxi score, test score, baseline:  -0.04726000000000008 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.2644, 0.0050, 0.1492, 0.1440, 0.1599, 0.1408, 0.1367],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0023,     0.9793,     0.0014,     0.0029,     0.0004,     0.0004,
            0.0133], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0960, 0.0755, 0.4050, 0.0908, 0.0946, 0.1096, 0.1284],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1452, 0.0427, 0.1135, 0.2788, 0.1356, 0.1294, 0.1547],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1076, 0.0243, 0.0879, 0.0815, 0.5319, 0.0801, 0.0868],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0632, 0.0118, 0.1269, 0.0739, 0.0675, 0.5904, 0.0663],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1320, 0.2163, 0.1327, 0.1101, 0.1148, 0.1110, 0.1831],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  47 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.048033333333333435 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.83621
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]
 [0.142]] [[34.545]
 [34.545]
 [34.545]
 [34.545]
 [34.545]
 [34.545]
 [34.545]] [[2.142]
 [2.142]
 [2.142]
 [2.142]
 [2.142]
 [2.142]
 [2.142]]
printing an ep nov before normalisation:  38.075173470058004
printing an ep nov before normalisation:  37.62945145116596
actor:  0 policy actor:  0  step number:  43 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  42.7357541213137
maxi score, test score, baseline:  -0.04860666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.38 ]
 [0.38 ]
 [0.383]
 [0.38 ]
 [0.38 ]
 [0.38 ]] [[22.242]
 [22.242]
 [22.242]
 [22.299]
 [22.242]
 [22.242]
 [22.242]] [[1.707]
 [1.707]
 [1.707]
 [1.716]
 [1.707]
 [1.707]
 [1.707]]
printing an ep nov before normalisation:  53.44480802478756
actor:  1 policy actor:  1  step number:  54 total reward:  0.3933333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.04860666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.49705380237185
actions average: 
K:  2  action  0 :  tensor([0.3704, 0.0240, 0.0890, 0.1099, 0.1775, 0.1078, 0.1214],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0047, 0.9459, 0.0054, 0.0062, 0.0012, 0.0014, 0.0352],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1316, 0.0311, 0.2482, 0.1449, 0.1375, 0.1458, 0.1610],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0888, 0.0378, 0.0933, 0.4324, 0.1157, 0.1012, 0.1308],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1196, 0.0069, 0.0915, 0.1082, 0.4469, 0.1111, 0.1159],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0889, 0.0014, 0.1260, 0.0753, 0.0884, 0.5332, 0.0868],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1095, 0.3038, 0.1007, 0.1168, 0.1072, 0.1050, 0.1570],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  54.76080962265179
printing an ep nov before normalisation:  47.58943620716904
maxi score, test score, baseline:  -0.04860666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.04860666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.50181472789079
printing an ep nov before normalisation:  27.213036498012148
maxi score, test score, baseline:  -0.04860666666666676 0.6923333333333335 0.6923333333333335
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.77107768435633
actor:  1 policy actor:  1  step number:  54 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 0.0
siam score:  -0.8294368
maxi score, test score, baseline:  -0.04860666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.04860666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.04860666666666676 0.6923333333333335 0.6923333333333335
printing an ep nov before normalisation:  49.140275677848905
maxi score, test score, baseline:  -0.04860666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.04860666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.04860666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.04860666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.04860666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  74 total reward:  0.07333333333333292  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.04968666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.04968666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.04968666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.04968666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  45 total reward:  0.2666666666666664  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.05030000000000009 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.05030000000000009 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.05030000000000009 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
actions average: 
K:  0  action  0 :  tensor([0.3282, 0.0058, 0.0898, 0.1223, 0.2416, 0.0881, 0.1242],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0064, 0.9582, 0.0050, 0.0054, 0.0036, 0.0039, 0.0175],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1123, 0.0011, 0.3461, 0.1310, 0.1185, 0.1794, 0.1117],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0950, 0.0099, 0.1074, 0.4027, 0.1372, 0.1318, 0.1159],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1009, 0.0019, 0.0976, 0.1427, 0.4585, 0.1105, 0.0879],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0817, 0.0039, 0.1290, 0.0923, 0.0773, 0.5344, 0.0813],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0759, 0.2382, 0.0825, 0.1212, 0.0865, 0.0810, 0.3147],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.92983021366628
siam score:  -0.8292648
actor:  0 policy actor:  0  step number:  57 total reward:  0.13333333333333286  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [ 0.023]
 [-0.005]
 [ 0.005]
 [-0.002]
 [-0.001]
 [ 0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.01 ]
 [ 0.023]
 [-0.005]
 [ 0.005]
 [-0.002]
 [-0.001]
 [ 0.002]]
maxi score, test score, baseline:  -0.05444666666666676 0.6923333333333335 0.6923333333333335
actions average: 
K:  2  action  0 :  tensor([0.2551, 0.0090, 0.1407, 0.1454, 0.1644, 0.1610, 0.1244],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0156, 0.9157, 0.0077, 0.0081, 0.0054, 0.0038, 0.0438],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1842, 0.0437, 0.1596, 0.1582, 0.1456, 0.1579, 0.1510],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1308, 0.1031, 0.1415, 0.2536, 0.1057, 0.1428, 0.1225],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0944, 0.0159, 0.0734, 0.0728, 0.5994, 0.0860, 0.0580],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0875, 0.0146, 0.1716, 0.0912, 0.0853, 0.4542, 0.0956],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1682, 0.3350, 0.0844, 0.0776, 0.0487, 0.0587, 0.2273],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.27992199783118
actor:  1 policy actor:  1  step number:  59 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.5  ]
 [0.469]
 [0.435]
 [0.469]
 [0.414]
 [0.469]] [[38.227]
 [39.832]
 [36.531]
 [38.25 ]
 [36.531]
 [37.457]
 [36.531]] [[1.198]
 [1.343]
 [1.184]
 [1.217]
 [1.184]
 [1.164]
 [1.184]]
maxi score, test score, baseline:  -0.05444666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.05444666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.37547737175229
maxi score, test score, baseline:  -0.05444666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.05444666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.117347202842474
Printing some Q and Qe and total Qs values:  [[-0.013]
 [-0.049]
 [-0.013]
 [-0.012]
 [-0.013]
 [-0.007]
 [-0.01 ]] [[34.562]
 [36.996]
 [35.494]
 [35.291]
 [34.524]
 [34.906]
 [35.877]] [[0.932]
 [1.035]
 [0.985]
 [0.974]
 [0.929]
 [0.957]
 [1.01 ]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  40.51923751831055
printing an ep nov before normalisation:  44.87567302025861
maxi score, test score, baseline:  -0.05444666666666676 0.6923333333333335 0.6923333333333335
Printing some Q and Qe and total Qs values:  [[0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]] [[43.516]
 [43.516]
 [43.516]
 [43.516]
 [43.516]
 [43.516]
 [43.516]] [[1.581]
 [1.581]
 [1.581]
 [1.581]
 [1.581]
 [1.581]
 [1.581]]
printing an ep nov before normalisation:  49.071347512390794
maxi score, test score, baseline:  -0.05444666666666676 0.6923333333333335 0.6923333333333335
printing an ep nov before normalisation:  37.88930281927711
siam score:  -0.8318147
maxi score, test score, baseline:  -0.05444666666666676 0.6923333333333335 0.6923333333333335
maxi score, test score, baseline:  -0.05444666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.05444666666666676 0.6923333333333335 0.6923333333333335
actor:  1 policy actor:  1  step number:  38 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  62 total reward:  0.11333333333333284  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.05444666666666676 0.6923333333333335 0.6923333333333335
actor:  1 policy actor:  1  step number:  54 total reward:  0.4866666666666669  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.05444666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.04794292101211
printing an ep nov before normalisation:  38.15053626048806
maxi score, test score, baseline:  -0.05444666666666676 0.6923333333333335 0.6923333333333335
maxi score, test score, baseline:  -0.05444666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.72 ]
 [0.803]
 [0.72 ]
 [0.72 ]
 [0.72 ]
 [0.72 ]
 [0.72 ]] [[33.404]
 [34.391]
 [33.404]
 [33.404]
 [33.404]
 [33.404]
 [33.404]] [[0.72 ]
 [0.803]
 [0.72 ]
 [0.72 ]
 [0.72 ]
 [0.72 ]
 [0.72 ]]
printing an ep nov before normalisation:  34.95738761774866
Printing some Q and Qe and total Qs values:  [[0.797]
 [0.941]
 [0.797]
 [0.797]
 [0.797]
 [0.797]
 [0.797]] [[28.912]
 [36.877]
 [28.912]
 [28.912]
 [28.912]
 [28.912]
 [28.912]] [[0.797]
 [0.941]
 [0.797]
 [0.797]
 [0.797]
 [0.797]
 [0.797]]
maxi score, test score, baseline:  -0.05444666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.1432877285928
printing an ep nov before normalisation:  35.059825215290736
actor:  1 policy actor:  1  step number:  52 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  66 total reward:  0.21999999999999975  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  44.71648097677601
maxi score, test score, baseline:  -0.05503333333333342 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.97722008449653
siam score:  -0.82971865
siam score:  -0.8302496
printing an ep nov before normalisation:  33.114977234422575
printing an ep nov before normalisation:  38.8299266289194
actions average: 
K:  3  action  0 :  tensor([0.1375, 0.0055, 0.1413, 0.1447, 0.2693, 0.1851, 0.1165],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0070, 0.9575, 0.0056, 0.0033, 0.0010, 0.0014, 0.0243],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0806, 0.0260, 0.3755, 0.1238, 0.0833, 0.2124, 0.0984],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0939, 0.0639, 0.1006, 0.3077, 0.1123, 0.1527, 0.1688],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1250, 0.0087, 0.0903, 0.1065, 0.4669, 0.1184, 0.0842],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1266, 0.0427, 0.1599, 0.1441, 0.1329, 0.2511, 0.1427],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1721, 0.0313, 0.1184, 0.1731, 0.1642, 0.1736, 0.1673],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.05503333333333342 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.343093165383664
printing an ep nov before normalisation:  31.873675363589687
printing an ep nov before normalisation:  46.62784272148489
printing an ep nov before normalisation:  34.744799265772706
maxi score, test score, baseline:  -0.05503333333333342 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.05503333333333342 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.41496713245617
actor:  0 policy actor:  0  step number:  50 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  38 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.0495400000000001 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.126]
 [0.132]
 [0.047]
 [0.049]
 [0.048]
 [0.046]
 [0.051]] [[41.546]
 [44.578]
 [31.063]
 [31.25 ]
 [30.808]
 [30.654]
 [31.439]] [[0.53 ]
 [0.592]
 [0.251]
 [0.257]
 [0.247]
 [0.242]
 [0.262]]
actions average: 
K:  3  action  0 :  tensor([0.3899, 0.0480, 0.1041, 0.0878, 0.1854, 0.1004, 0.0844],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0153, 0.9250, 0.0076, 0.0163, 0.0132, 0.0077, 0.0150],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1094, 0.0012, 0.4856, 0.1138, 0.0930, 0.1044, 0.0926],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1137, 0.0275, 0.1304, 0.3111, 0.1253, 0.1083, 0.1837],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1228, 0.0096, 0.1872, 0.1980, 0.1920, 0.1708, 0.1196],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1110, 0.0031, 0.1175, 0.1145, 0.1177, 0.4431, 0.0930],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1109, 0.0675, 0.1307, 0.1476, 0.1235, 0.1175, 0.3023],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  -0.0495400000000001 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  36 total reward:  0.54  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  29.97546271486187
Printing some Q and Qe and total Qs values:  [[0.746]
 [0.806]
 [0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.746]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.746]
 [0.806]
 [0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.746]]
printing an ep nov before normalisation:  22.666508131395446
maxi score, test score, baseline:  -0.046460000000000085 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.046460000000000085 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.2666666666666657  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  24.45910930633545
maxi score, test score, baseline:  -0.046460000000000085 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.939]
 [0.988]
 [0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]] [[44.335]
 [36.773]
 [44.335]
 [44.335]
 [44.335]
 [44.335]
 [44.335]] [[0.939]
 [0.988]
 [0.939]
 [0.939]
 [0.939]
 [0.939]
 [0.939]]
maxi score, test score, baseline:  -0.046460000000000085 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.290861682754276
printing an ep nov before normalisation:  34.355384607121145
maxi score, test score, baseline:  -0.046460000000000085 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.92642709533013
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.923]
 [0.988]
 [0.923]
 [0.989]
 [0.988]
 [0.923]
 [0.99 ]] [[ 8.113]
 [ 7.823]
 [ 8.113]
 [ 8.419]
 [16.486]
 [ 8.113]
 [ 5.282]] [[0.923]
 [0.988]
 [0.923]
 [0.989]
 [0.988]
 [0.923]
 [0.99 ]]
actor:  0 policy actor:  0  step number:  58 total reward:  0.28666666666666574  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  0.0
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.42 ]
 [0.423]
 [0.423]
 [0.29 ]
 [0.423]
 [0.423]] [[34.795]
 [40.901]
 [34.795]
 [34.795]
 [35.765]
 [34.795]
 [34.795]] [[0.648]
 [0.795]
 [0.648]
 [0.648]
 [0.539]
 [0.648]
 [0.648]]
maxi score, test score, baseline:  -0.04388666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.016]] [[53.855]
 [51.014]
 [53.855]
 [53.855]
 [53.855]
 [53.855]
 [51.155]] [[0.894]
 [0.8  ]
 [0.894]
 [0.894]
 [0.894]
 [0.894]
 [0.794]]
Printing some Q and Qe and total Qs values:  [[0.034]
 [0.213]
 [0.034]
 [0.034]
 [0.034]
 [0.034]
 [0.034]] [[38.697]
 [42.724]
 [38.697]
 [38.697]
 [38.697]
 [38.697]
 [38.697]] [[0.965]
 [1.33 ]
 [0.965]
 [0.965]
 [0.965]
 [0.965]
 [0.965]]
Printing some Q and Qe and total Qs values:  [[0.726]
 [0.786]
 [0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]] [[47.051]
 [38.11 ]
 [47.051]
 [47.051]
 [47.051]
 [47.051]
 [47.051]] [[1.355]
 [1.205]
 [1.355]
 [1.355]
 [1.355]
 [1.355]
 [1.355]]
UNIT TEST: sample policy line 217 mcts : [0.082 0.204 0.163 0.184 0.143 0.143 0.082]
actor:  1 policy actor:  1  step number:  47 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.407]
 [0.584]
 [0.407]
 [0.407]
 [0.407]
 [0.407]
 [0.414]] [[30.15 ]
 [32.436]
 [30.15 ]
 [30.15 ]
 [30.15 ]
 [30.15 ]
 [27.989]] [[1.199]
 [1.486]
 [1.199]
 [1.199]
 [1.199]
 [1.199]
 [1.101]]
maxi score, test score, baseline:  -0.04388666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.7538557087123
siam score:  -0.8187346
maxi score, test score, baseline:  -0.04388666666666676 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.04388666666666676 0.6923333333333335 0.6923333333333335
printing an ep nov before normalisation:  58.12552112227446
printing an ep nov before normalisation:  57.83482296308999
maxi score, test score, baseline:  -0.043886666666666754 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.043886666666666754 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.82404023
maxi score, test score, baseline:  -0.043886666666666754 0.6923333333333335 0.6923333333333335
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.043886666666666754 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.30993556976318
maxi score, test score, baseline:  -0.04388666666666675 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.98227882385254
printing an ep nov before normalisation:  38.222551268562285
printing an ep nov before normalisation:  35.78744377222706
printing an ep nov before normalisation:  39.80604826012535
printing an ep nov before normalisation:  47.96270427386735
actor:  1 policy actor:  1  step number:  49 total reward:  0.3866666666666667  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  56 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  19.15740861707183
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  41 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.04092666666666677 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.04092666666666677 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.079]
 [0.03 ]
 [0.036]
 [0.036]
 [0.032]
 [0.05 ]] [[44.244]
 [38.437]
 [49.667]
 [50.353]
 [50.467]
 [51.368]
 [47.027]] [[0.458]
 [0.373]
 [0.572]
 [0.593]
 [0.595]
 [0.611]
 [0.534]]
printing an ep nov before normalisation:  41.1867831350942
printing an ep nov before normalisation:  52.03722654599084
maxi score, test score, baseline:  -0.04092666666666677 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.26579543546537
maxi score, test score, baseline:  -0.04092666666666677 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.04092666666666677 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.04092666666666678 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.04092666666666678 0.6923333333333335 0.6923333333333335
maxi score, test score, baseline:  -0.04092666666666678 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.04092666666666678 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.04092666666666678 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.2999999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.04092666666666678 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.583258478980042
maxi score, test score, baseline:  -0.04092666666666678 0.6923333333333335 0.6923333333333335
printing an ep nov before normalisation:  66.23774424184124
maxi score, test score, baseline:  -0.04092666666666678 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  23.462602946400462
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.82 ]
 [0.866]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]] [[39.667]
 [37.091]
 [39.667]
 [39.667]
 [39.667]
 [39.667]
 [39.667]] [[0.82 ]
 [0.866]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]
 [0.82 ]]
maxi score, test score, baseline:  -0.043100000000000104 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2999999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  43.127901513742444
maxi score, test score, baseline:  -0.043100000000000104 0.6923333333333335 0.6923333333333335
printing an ep nov before normalisation:  36.07283115386963
actor:  0 policy actor:  0  step number:  59 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.043353333333333424 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.043353333333333424 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.043353333333333424 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.36154902519636
maxi score, test score, baseline:  -0.043353333333333424 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.043353333333333424 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.132539652138014
printing an ep nov before normalisation:  55.58271350553655
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.81319535
actor:  0 policy actor:  0  step number:  48 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.81320274
maxi score, test score, baseline:  -0.040620000000000094 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.30128477637454
actor:  0 policy actor:  0  step number:  55 total reward:  0.38666666666666605  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.037846666666666785 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.037846666666666785 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.783]
 [0.786]
 [0.685]
 [0.735]
 [0.68 ]
 [0.687]
 [0.756]] [[33.428]
 [35.926]
 [30.66 ]
 [34.101]
 [30.409]
 [30.095]
 [33.88 ]] [[0.783]
 [0.786]
 [0.685]
 [0.735]
 [0.68 ]
 [0.687]
 [0.756]]
maxi score, test score, baseline:  -0.037846666666666785 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  43 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.034966666666666764 0.6923333333333335 0.6923333333333335
line 256 mcts: sample exp_bonus 43.91970842236734
printing an ep nov before normalisation:  44.641220838945024
printing an ep nov before normalisation:  44.420391091870194
printing an ep nov before normalisation:  47.62745757378809
line 256 mcts: sample exp_bonus 30.951377213001248
printing an ep nov before normalisation:  0.1669963589370127
actor:  1 policy actor:  1  step number:  61 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.034966666666666764 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 43.55192051908382
siam score:  -0.8119309
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.272]
 [0.231]
 [0.231]
 [0.231]
 [0.231]
 [0.231]] [[38.31 ]
 [39.266]
 [38.31 ]
 [38.31 ]
 [38.31 ]
 [38.31 ]
 [38.31 ]] [[1.086]
 [1.178]
 [1.086]
 [1.086]
 [1.086]
 [1.086]
 [1.086]]
maxi score, test score, baseline:  -0.034966666666666764 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.756]
 [0.803]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]] [[31.17 ]
 [38.255]
 [31.17 ]
 [31.17 ]
 [31.17 ]
 [31.17 ]
 [31.17 ]] [[0.756]
 [0.803]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]]
maxi score, test score, baseline:  -0.034966666666666764 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.480435102202634
maxi score, test score, baseline:  -0.034966666666666764 0.6923333333333335 0.6923333333333335
maxi score, test score, baseline:  -0.034966666666666764 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.416810876711615
maxi score, test score, baseline:  -0.034966666666666764 0.6923333333333335 0.6923333333333335
printing an ep nov before normalisation:  40.07690598155613
printing an ep nov before normalisation:  31.4239501953125
siam score:  -0.81104034
maxi score, test score, baseline:  -0.034966666666666764 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.623256732818525
maxi score, test score, baseline:  -0.034966666666666764 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.034966666666666764 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  58 total reward:  0.17999999999999938  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.015]
 [0.013]
 [0.013]
 [0.011]
 [0.023]
 [0.025]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.028]
 [0.015]
 [0.013]
 [0.013]
 [0.011]
 [0.023]
 [0.025]]
Printing some Q and Qe and total Qs values:  [[-0.02 ]
 [-0.008]
 [-0.02 ]
 [-0.02 ]
 [-0.118]
 [-0.056]
 [-0.009]] [[52.416]
 [59.752]
 [52.416]
 [52.416]
 [28.715]
 [29.332]
 [39.787]] [[ 0.391]
 [ 0.522]
 [ 0.391]
 [ 0.391]
 [-0.09 ]
 [-0.019]
 [ 0.198]]
printing an ep nov before normalisation:  31.94735050201416
actor:  0 policy actor:  0  step number:  49 total reward:  0.3466666666666661  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.012]
 [-0.012]
 [-0.012]
 [-0.012]
 [-0.021]
 [-0.016]
 [-0.012]] [[68.124]
 [68.124]
 [68.124]
 [68.124]
 [63.251]
 [59.526]
 [68.124]] [[1.719]
 [1.719]
 [1.719]
 [1.719]
 [1.573]
 [1.473]
 [1.719]]
maxi score, test score, baseline:  -0.03199333333333343 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.03199333333333343 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.64670316140908
siam score:  -0.8099034
maxi score, test score, baseline:  -0.03199333333333343 0.6923333333333335 0.6923333333333335
maxi score, test score, baseline:  -0.03199333333333343 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.03199333333333343 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.185]
 [ 0.   ]
 [-0.185]
 [-0.185]
 [-0.185]
 [-0.185]
 [-0.185]] [[28.242]
 [ 0.043]
 [28.242]
 [28.242]
 [28.242]
 [28.242]
 [28.242]] [[0.045]
 [0.   ]
 [0.045]
 [0.045]
 [0.045]
 [0.045]
 [0.045]]
siam score:  -0.81082976
siam score:  -0.8110888
maxi score, test score, baseline:  -0.03199333333333343 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.652]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]] [[43.132]
 [40.048]
 [43.132]
 [43.132]
 [43.132]
 [43.132]
 [43.132]] [[2.625]
 [2.396]
 [2.625]
 [2.625]
 [2.625]
 [2.625]
 [2.625]]
printing an ep nov before normalisation:  43.31706137465645
siam score:  -0.8089746
printing an ep nov before normalisation:  46.82157052568834
siam score:  -0.81013894
printing an ep nov before normalisation:  41.45266175075816
printing an ep nov before normalisation:  47.56167901254424
maxi score, test score, baseline:  -0.03199333333333343 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.03199333333333343 0.6923333333333335 0.6923333333333335
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.621]
 [0.534]
 [0.497]
 [0.522]
 [0.522]
 [0.494]] [[11.518]
 [ 9.223]
 [12.385]
 [10.594]
 [11.691]
 [11.691]
 [12.215]] [[2.07 ]
 [1.841]
 [2.172]
 [1.898]
 [2.068]
 [2.068]
 [2.11 ]]
maxi score, test score, baseline:  -0.03199333333333343 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.03199333333333343 0.6923333333333335 0.6923333333333335
printing an ep nov before normalisation:  47.09489130244178
maxi score, test score, baseline:  -0.03199333333333343 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  53 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8070253
maxi score, test score, baseline:  -0.03199333333333343 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.003864378105618016
printing an ep nov before normalisation:  29.17578935623169
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.898]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.732]] [[46.779]
 [42.613]
 [46.779]
 [46.779]
 [46.779]
 [46.779]
 [47.556]] [[0.735]
 [0.898]
 [0.735]
 [0.735]
 [0.735]
 [0.735]
 [0.732]]
printing an ep nov before normalisation:  35.267415352726154
maxi score, test score, baseline:  -0.03199333333333343 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.33984208479687
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.7653752826517
actor:  0 policy actor:  0  step number:  52 total reward:  0.20666666666666633  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  64 total reward:  0.23333333333333262  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  47.82574653625488
printing an ep nov before normalisation:  43.117523193359375
maxi score, test score, baseline:  -0.029580000000000092 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  23.772573118629822
maxi score, test score, baseline:  -0.029580000000000092 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.216169056662025
printing an ep nov before normalisation:  32.130890422061384
maxi score, test score, baseline:  -0.029580000000000106 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.029580000000000106 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.029580000000000106 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.348127786896185
printing an ep nov before normalisation:  61.05125683368949
maxi score, test score, baseline:  -0.029580000000000106 0.6923333333333335 0.6923333333333335
maxi score, test score, baseline:  -0.029580000000000106 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.08470622838422
printing an ep nov before normalisation:  18.11369196367906
actor:  1 policy actor:  1  step number:  51 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.029580000000000106 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.029580000000000106 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.029580000000000106 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.029580000000000106 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.32 ]
 [0.281]
 [0.283]
 [0.269]
 [0.284]
 [0.279]] [[36.288]
 [45.039]
 [36.098]
 [36.06 ]
 [42.508]
 [36.182]
 [36.916]] [[0.851]
 [1.254]
 [0.843]
 [0.843]
 [1.098]
 [0.849]
 [0.875]]
actor:  0 policy actor:  0  step number:  54 total reward:  0.23333333333333262  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  23.051312194766822
printing an ep nov before normalisation:  32.751751071093004
maxi score, test score, baseline:  -0.027113333333333437 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.35333333333333283  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.027113333333333437 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.027113333333333437 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  51 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  -0.02455333333333345 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.519418356667195
printing an ep nov before normalisation:  76.93194305140801
printing an ep nov before normalisation:  48.66831698735321
printing an ep nov before normalisation:  64.02898534425319
printing an ep nov before normalisation:  57.98697670747674
printing an ep nov before normalisation:  42.1025276184082
maxi score, test score, baseline:  -0.02455333333333345 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.982624662670666
printing an ep nov before normalisation:  46.9414328724192
printing an ep nov before normalisation:  77.00890388715757
Printing some Q and Qe and total Qs values:  [[0.854]
 [0.903]
 [0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]] [[36.796]
 [34.981]
 [36.796]
 [36.796]
 [36.796]
 [36.796]
 [36.796]] [[0.854]
 [0.903]
 [0.854]
 [0.854]
 [0.854]
 [0.854]
 [0.854]]
printing an ep nov before normalisation:  64.31003732710302
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.619]
 [0.572]
 [0.574]
 [0.574]
 [0.574]
 [0.588]] [[48.539]
 [41.022]
 [44.89 ]
 [45.481]
 [45.16 ]
 [45.432]
 [48.539]] [[1.658]
 [1.366]
 [1.486]
 [1.513]
 [1.499]
 [1.511]
 [1.658]]
maxi score, test score, baseline:  -0.02455333333333345 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.48 ]
 [0.382]
 [0.404]
 [0.454]
 [0.39 ]
 [0.37 ]] [[47.353]
 [43.441]
 [43.715]
 [46.036]
 [43.885]
 [45.721]
 [46.866]] [[1.266]
 [1.29 ]
 [1.2  ]
 [1.294]
 [1.278]
 [1.271]
 [1.286]]
actor:  0 policy actor:  0  step number:  40 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.021633333333333442 0.6923333333333335 0.6923333333333335
maxi score, test score, baseline:  -0.021633333333333442 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  52.03046529248439
siam score:  -0.81478256
maxi score, test score, baseline:  -0.021633333333333442 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.12524223327637
printing an ep nov before normalisation:  44.112958908081055
maxi score, test score, baseline:  -0.021633333333333442 0.6923333333333335 0.6923333333333335
actor:  1 policy actor:  1  step number:  64 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 38.88895638469032
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.4696164987805
printing an ep nov before normalisation:  33.69960308074951
maxi score, test score, baseline:  -0.021633333333333442 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.021633333333333442 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  62 total reward:  0.206666666666666  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.021633333333333442 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  47.83524426887944
Printing some Q and Qe and total Qs values:  [[0.165]
 [0.171]
 [0.172]
 [0.139]
 [0.14 ]
 [0.142]
 [0.165]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.165]
 [0.171]
 [0.172]
 [0.139]
 [0.14 ]
 [0.142]
 [0.165]]
printing an ep nov before normalisation:  29.84858088480606
actor:  0 policy actor:  0  step number:  39 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 56.048514502262655
actor:  1 policy actor:  1  step number:  51 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[ 0.042]
 [-0.002]
 [ 0.06 ]
 [ 0.069]
 [ 0.069]
 [ 0.067]
 [ 0.066]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.042]
 [-0.002]
 [ 0.06 ]
 [ 0.069]
 [ 0.069]
 [ 0.067]
 [ 0.066]]
siam score:  -0.8119417
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.004]
 [0.038]
 [0.063]
 [0.023]
 [0.055]
 [0.035]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.043]
 [0.004]
 [0.038]
 [0.063]
 [0.023]
 [0.055]
 [0.035]]
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.89785008640087
siam score:  -0.8135596
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.199]
 [0.17 ]
 [0.199]
 [0.199]
 [0.199]
 [0.199]
 [0.199]] [[37.384]
 [49.109]
 [37.384]
 [37.384]
 [37.384]
 [37.384]
 [37.384]] [[0.716]
 [1.004]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]]
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.465]
 [0.435]
 [0.378]
 [0.34 ]
 [0.335]
 [0.367]] [[30.799]
 [44.378]
 [43.579]
 [28.182]
 [28.067]
 [28.279]
 [28.47 ]] [[0.621]
 [0.969]
 [0.925]
 [0.618]
 [0.578]
 [0.577]
 [0.612]]
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.4751714709268
printing an ep nov before normalisation:  41.39676767639416
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.516]
 [0.089]
 [0.242]
 [0.237]
 [0.216]
 [0.337]] [[36.706]
 [47.761]
 [36.857]
 [40.655]
 [40.255]
 [39.637]
 [36.885]] [[0.495]
 [0.929]
 [0.361]
 [0.564]
 [0.553]
 [0.524]
 [0.609]]
siam score:  -0.8129695
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.90792269168205
actor:  1 policy actor:  1  step number:  55 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  44.38235847172993
actor:  1 policy actor:  1  step number:  39 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  38.198166905118654
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.34850700449245
printing an ep nov before normalisation:  23.535107367457883
printing an ep nov before normalisation:  37.78326860771034
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4066666666666666  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.01237762401172
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.21333333333333304  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  75 total reward:  0.06666666666666565  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  39.43455053656127
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.05628717202853
printing an ep nov before normalisation:  40.41535861538079
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.3599999999999999  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  69 total reward:  0.11999999999999988  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  44.560794830322266
actions average: 
K:  2  action  0 :  tensor([0.1788, 0.0041, 0.1190, 0.1941, 0.1864, 0.1870, 0.1305],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0132, 0.9180, 0.0112, 0.0141, 0.0061, 0.0114, 0.0261],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1500, 0.0053, 0.3626, 0.1211, 0.1080, 0.1215, 0.1315],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1291, 0.0404, 0.1453, 0.1635, 0.1172, 0.1546, 0.2499],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1431, 0.0048, 0.1009, 0.1048, 0.4302, 0.0978, 0.1184],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1540, 0.0038, 0.1426, 0.1451, 0.1298, 0.2876, 0.1371],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1139, 0.1721, 0.0825, 0.1010, 0.0619, 0.0816, 0.3871],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.208600878902196
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.32813634876395
actor:  1 policy actor:  1  step number:  57 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.448138647754334
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.67 ]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]] [[41.372]
 [41.565]
 [41.372]
 [41.372]
 [41.372]
 [41.372]
 [41.372]] [[2.634]
 [2.639]
 [2.634]
 [2.634]
 [2.634]
 [2.634]
 [2.634]]
Printing some Q and Qe and total Qs values:  [[0.867]
 [0.909]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]] [[36.876]
 [38.846]
 [36.876]
 [36.876]
 [36.876]
 [36.876]
 [36.876]] [[0.867]
 [0.909]
 [0.867]
 [0.867]
 [0.867]
 [0.867]
 [0.867]]
printing an ep nov before normalisation:  58.80841255062603
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.68 ]
 [0.701]
 [0.718]
 [0.701]
 [0.701]
 [0.701]
 [0.701]] [[41.252]
 [40.96 ]
 [44.037]
 [40.96 ]
 [40.96 ]
 [40.96 ]
 [40.96 ]] [[2.478]
 [2.478]
 [2.718]
 [2.478]
 [2.478]
 [2.478]
 [2.478]]
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.018700000000000105 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.81539947
printing an ep nov before normalisation:  58.449587384462006
actor:  0 policy actor:  0  step number:  55 total reward:  0.3333333333333328  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.018300000000000104 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.61480939433463
maxi score, test score, baseline:  -0.018300000000000104 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.346666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.018300000000000104 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4533333333333335  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.018300000000000104 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.814833109484
maxi score, test score, baseline:  -0.018300000000000104 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.44300026837305
maxi score, test score, baseline:  -0.018300000000000104 0.6923333333333335 0.6923333333333335
actor:  1 policy actor:  1  step number:  58 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  60.230802687922434
printing an ep nov before normalisation:  15.345239311247308
actions average: 
K:  4  action  0 :  tensor([0.2954, 0.1286, 0.0831, 0.0840, 0.2314, 0.0756, 0.1019],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0175, 0.8939, 0.0143, 0.0186, 0.0149, 0.0095, 0.0313],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1491, 0.0500, 0.2688, 0.1094, 0.1444, 0.1320, 0.1464],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1043, 0.0327, 0.1067, 0.3403, 0.1217, 0.1171, 0.1772],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1288, 0.0030, 0.0958, 0.0862, 0.5121, 0.1035, 0.0707],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0774, 0.0516, 0.1586, 0.1178, 0.1445, 0.3812, 0.0690],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1557, 0.1361, 0.1509, 0.1243, 0.1445, 0.1288, 0.1597],
       grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8032512
maxi score, test score, baseline:  -0.018300000000000104 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.018300000000000104 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.86508269615446
maxi score, test score, baseline:  -0.018300000000000104 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.80600715
printing an ep nov before normalisation:  33.030085355803536
maxi score, test score, baseline:  -0.018300000000000104 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.495]
 [0.441]
 [0.493]
 [0.493]
 [0.493]
 [0.493]] [[35.043]
 [37.676]
 [35.694]
 [35.043]
 [35.043]
 [35.043]
 [35.043]] [[1.858]
 [2.083]
 [1.862]
 [1.858]
 [1.858]
 [1.858]
 [1.858]]
maxi score, test score, baseline:  -0.018300000000000104 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.19999999999999962  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.018300000000000118 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.018300000000000118 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.3399999999999994  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.018300000000000118 0.6923333333333335 0.6923333333333335
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.16263955491352
siam score:  -0.81158996
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.018300000000000118 0.6923333333333335 0.6923333333333335
printing an ep nov before normalisation:  38.53624921516019
maxi score, test score, baseline:  -0.018300000000000118 0.6923333333333335 0.6923333333333335
maxi score, test score, baseline:  -0.018300000000000118 0.6923333333333335 0.6923333333333335
maxi score, test score, baseline:  -0.018300000000000118 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.018300000000000118 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.018300000000000118 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.44666666666666677  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  -0.018300000000000118 0.6923333333333335 0.6923333333333335
maxi score, test score, baseline:  -0.018300000000000118 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.018300000000000118 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.800825119018555
using explorer policy with actor:  1
maxi score, test score, baseline:  -0.018300000000000118 0.6923333333333335 0.6923333333333335
Printing some Q and Qe and total Qs values:  [[0.756]
 [0.745]
 [0.749]
 [0.752]
 [0.638]
 [0.675]
 [0.759]] [[40.054]
 [40.729]
 [39.682]
 [41.862]
 [41.627]
 [41.108]
 [40.828]] [[1.634]
 [1.638]
 [1.619]
 [1.67 ]
 [1.551]
 [1.576]
 [1.654]]
maxi score, test score, baseline:  -0.018300000000000118 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  64 total reward:  0.0733333333333327  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  48.17459135923506
maxi score, test score, baseline:  -0.018300000000000118 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.043]
 [0.132]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.132]
 [0.043]
 [0.132]]
maxi score, test score, baseline:  -0.01830000000000009 0.6923333333333335 0.6923333333333335
maxi score, test score, baseline:  -0.01830000000000009 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.01830000000000009 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  -0.01830000000000009 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.01830000000000009 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]] [[34.909]
 [34.909]
 [34.909]
 [34.909]
 [34.909]
 [34.909]
 [34.909]] [[1.985]
 [1.985]
 [1.985]
 [1.985]
 [1.985]
 [1.985]
 [1.985]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.2133333333333326  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  50 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  66 total reward:  0.07333333333333247  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  -0.01830000000000009 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.01830000000000009 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.09253978026654
maxi score, test score, baseline:  -0.01830000000000009 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.81430185
siam score:  -0.81446344
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.688]
 [0.568]
 [0.568]
 [0.568]
 [0.568]
 [0.568]] [[35.526]
 [47.221]
 [35.526]
 [35.526]
 [35.526]
 [35.526]
 [35.526]] [[0.864]
 [1.197]
 [0.864]
 [0.864]
 [0.864]
 [0.864]
 [0.864]]
UNIT TEST: sample policy line 217 mcts : [0.041 0.286 0.122 0.041 0.041 0.224 0.245]
maxi score, test score, baseline:  -0.01830000000000009 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]] [[41.483]
 [41.483]
 [41.483]
 [41.483]
 [41.483]
 [41.483]
 [41.483]] [[1.213]
 [1.213]
 [1.213]
 [1.213]
 [1.213]
 [1.213]
 [1.213]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  0.667
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  47 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  -0.01830000000000009 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8196909
printing an ep nov before normalisation:  37.52624616618334
maxi score, test score, baseline:  -0.01574000000000011 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.07255354570723
maxi score, test score, baseline:  -0.01574000000000011 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.60663465158753
printing an ep nov before normalisation:  53.965156728867235
printing an ep nov before normalisation:  42.938852310180664
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]] [[38.872]
 [38.872]
 [38.872]
 [38.872]
 [38.872]
 [38.872]
 [38.872]] [[77.861]
 [77.861]
 [77.861]
 [77.861]
 [77.861]
 [77.861]
 [77.861]]
maxi score, test score, baseline:  -0.01574000000000011 0.6923333333333335 0.6923333333333335
maxi score, test score, baseline:  -0.01574000000000011 0.6923333333333335 0.6923333333333335
printing an ep nov before normalisation:  30.856725260287558
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  -0.01574000000000011 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  -0.015740000000000105 0.6923333333333335 0.6923333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.234737407132755
printing an ep nov before normalisation:  36.72583728115597
Printing some Q and Qe and total Qs values:  [[0.941]
 [0.923]
 [0.942]
 [0.941]
 [0.94 ]
 [0.935]
 [0.942]] [[41.908]
 [10.685]
 [40.725]
 [40.7  ]
 [41.409]
 [ 8.302]
 [13.669]] [[0.941]
 [0.923]
 [0.942]
 [0.941]
 [0.94 ]
 [0.935]
 [0.942]]
printing an ep nov before normalisation:  0.02703397185683798
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  39.591217041015625
actor:  0 policy actor:  0  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  36.4717435836792
maxi score, test score, baseline:  0.0418999999999999 0.6883333333333335 0.6883333333333335
maxi score, test score, baseline:  0.0418999999999999 0.6883333333333335 0.6883333333333335
printing an ep nov before normalisation:  45.7836839934294
maxi score, test score, baseline:  0.0418999999999999 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.511]
 [0.597]
 [0.511]
 [0.513]
 [0.518]
 [0.52 ]
 [0.504]] [[42.233]
 [39.045]
 [42.182]
 [43.244]
 [42.355]
 [42.319]
 [43.349]] [[1.971]
 [1.835]
 [1.967]
 [2.043]
 [1.986]
 [1.986]
 [2.042]]
Printing some Q and Qe and total Qs values:  [[0.071]
 [0.196]
 [0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.071]] [[22.213]
 [43.114]
 [22.213]
 [22.213]
 [22.213]
 [22.213]
 [22.213]] [[0.155]
 [0.44 ]
 [0.155]
 [0.155]
 [0.155]
 [0.155]
 [0.155]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0418999999999999 0.6883333333333335 0.6883333333333335
actor:  1 policy actor:  1  step number:  55 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8161661
printing an ep nov before normalisation:  30.80539100591337
printing an ep nov before normalisation:  46.7327880859375
printing an ep nov before normalisation:  33.55138467688266
printing an ep nov before normalisation:  36.94246730833632
printing an ep nov before normalisation:  23.078971454781907
actions average: 
K:  2  action  0 :  tensor([0.4391, 0.0667, 0.0727, 0.0857, 0.1574, 0.0915, 0.0869],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0101, 0.9227, 0.0110, 0.0086, 0.0063, 0.0091, 0.0321],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1293, 0.0076, 0.3794, 0.1231, 0.1057, 0.1534, 0.1016],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0485, 0.2894, 0.0415, 0.4495, 0.0365, 0.0807, 0.0540],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1380, 0.0068, 0.0781, 0.0794, 0.5035, 0.1033, 0.0910],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0897, 0.0348, 0.1513, 0.0914, 0.0521, 0.4821, 0.0987],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1092, 0.2344, 0.0887, 0.1481, 0.0724, 0.1201, 0.2271],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.681]
 [0.786]
 [0.546]
 [0.634]
 [0.701]
 [0.674]
 [0.683]] [[14.986]
 [10.505]
 [17.199]
 [14.113]
 [14.51 ]
 [14.421]
 [14.83 ]] [[1.513]
 [1.368]
 [1.5  ]
 [1.417]
 [1.506]
 [1.474]
 [1.506]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.40686140465638
printing an ep nov before normalisation:  49.828520526958975
maxi score, test score, baseline:  0.0418999999999999 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0418999999999999 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.27591225797744
actor:  1 policy actor:  1  step number:  55 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  48.01753039966731
printing an ep nov before normalisation:  44.629437611648584
printing an ep nov before normalisation:  48.21118235363365
maxi score, test score, baseline:  0.0418999999999999 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.371540380697425
printing an ep nov before normalisation:  39.02296752958062
actor:  1 policy actor:  1  step number:  49 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.56985098819783
printing an ep nov before normalisation:  35.84198146696277
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.759870264984876
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  0.0418999999999999 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0418999999999999 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.3397, 0.0053, 0.1144, 0.1405, 0.1415, 0.1326, 0.1260],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0049, 0.9573, 0.0054, 0.0054, 0.0015, 0.0012, 0.0243],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0980, 0.0497, 0.3459, 0.1239, 0.0915, 0.1843, 0.1068],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0932, 0.0483, 0.0798, 0.4775, 0.0982, 0.1015, 0.1015],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1042, 0.0053, 0.0708, 0.0967, 0.5572, 0.0933, 0.0725],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0858, 0.0097, 0.1034, 0.0963, 0.0934, 0.5327, 0.0788],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1591, 0.2128, 0.1103, 0.1388, 0.1165, 0.1379, 0.1246],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0418999999999999 0.6883333333333335 0.6883333333333335
line 256 mcts: sample exp_bonus 31.712561944406538
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.52 ]
 [0.52 ]
 [0.527]
 [0.526]
 [0.52 ]
 [0.52 ]] [[33.118]
 [33.118]
 [33.118]
 [31.197]
 [31.289]
 [33.118]
 [33.118]] [[2.004]
 [2.004]
 [2.004]
 [1.852]
 [1.859]
 [2.004]
 [2.004]]
maxi score, test score, baseline:  0.04189999999999989 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  44.32365228945926
printing an ep nov before normalisation:  39.07641294583866
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.649]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]] [[38.152]
 [39.2  ]
 [38.152]
 [38.152]
 [38.152]
 [38.152]
 [38.152]] [[1.9  ]
 [2.118]
 [1.9  ]
 [1.9  ]
 [1.9  ]
 [1.9  ]
 [1.9  ]]
maxi score, test score, baseline:  0.04189999999999989 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03973999999999989 0.6883333333333335 0.6883333333333335
maxi score, test score, baseline:  0.03973999999999989 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.03973999999999989 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03973999999999989 0.6883333333333335 0.6883333333333335
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.813]
 [0.762]
 [0.762]
 [0.762]
 [0.762]
 [0.749]] [[37.303]
 [47.17 ]
 [37.303]
 [37.303]
 [37.303]
 [37.303]
 [37.558]] [[0.762]
 [0.813]
 [0.762]
 [0.762]
 [0.762]
 [0.762]
 [0.749]]
printing an ep nov before normalisation:  44.321626107289994
actor:  1 policy actor:  1  step number:  53 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  36.785543248647414
maxi score, test score, baseline:  0.03973999999999989 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.791224237102924
siam score:  -0.8000327
printing an ep nov before normalisation:  31.619376069931953
maxi score, test score, baseline:  0.03973999999999989 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  69 total reward:  0.02666666666666584  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.04179333333333323 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.558549880981445
actor:  1 policy actor:  1  step number:  58 total reward:  0.3266666666666661  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.785]
 [0.811]
 [0.785]
 [0.785]
 [0.785]
 [0.785]
 [0.785]] [[35.843]
 [37.054]
 [35.843]
 [35.843]
 [35.843]
 [35.843]
 [35.843]] [[0.785]
 [0.811]
 [0.785]
 [0.785]
 [0.785]
 [0.785]
 [0.785]]
maxi score, test score, baseline:  0.04179333333333323 0.6883333333333335 0.6883333333333335
printing an ep nov before normalisation:  32.9069709777832
printing an ep nov before normalisation:  32.517361640930176
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actor:  1 policy actor:  1  step number:  57 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.04179333333333323 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.15796345831045
actor:  0 policy actor:  0  step number:  51 total reward:  0.306666666666666  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  41.53060362202288
maxi score, test score, baseline:  0.04440666666666656 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.71355612361143
maxi score, test score, baseline:  0.04440666666666656 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04440666666666656 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.381295013036095
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.53 ]
 [0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]] [[37.124]
 [37.955]
 [37.124]
 [37.124]
 [37.124]
 [37.124]
 [37.124]] [[0.95 ]
 [1.002]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.95 ]]
actor:  1 policy actor:  1  step number:  73 total reward:  0.0933333333333326  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  0.0011027010879161026
actor:  1 policy actor:  1  step number:  52 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04440666666666656 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.76991500151792
maxi score, test score, baseline:  0.04440666666666656 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.03393558428013
maxi score, test score, baseline:  0.04440666666666656 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.94553688748688
printing an ep nov before normalisation:  40.317043561794705
actions average: 
K:  4  action  0 :  tensor([0.2776, 0.0557, 0.1285, 0.1487, 0.1206, 0.1426, 0.1264],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0097, 0.9154, 0.0116, 0.0172, 0.0023, 0.0025, 0.0413],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1381, 0.0091, 0.3276, 0.1324, 0.1166, 0.1243, 0.1519],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1351, 0.0086, 0.1157, 0.3757, 0.1201, 0.1167, 0.1281],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1720, 0.0102, 0.0960, 0.1042, 0.4163, 0.0973, 0.1040],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1147, 0.0311, 0.2050, 0.1330, 0.1040, 0.3129, 0.0992],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1109, 0.1396, 0.1128, 0.1061, 0.1033, 0.1361, 0.2912],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  34 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  63.89028330082795
printing an ep nov before normalisation:  12.63270616531372
printing an ep nov before normalisation:  50.10641803091527
Printing some Q and Qe and total Qs values:  [[ 0.084]
 [-0.082]
 [ 0.066]
 [ 0.212]
 [ 0.07 ]
 [ 0.059]
 [ 0.075]] [[17.314]
 [22.456]
 [17.205]
 [12.774]
 [18.657]
 [18.551]
 [18.279]] [[0.428]
 [0.368]
 [0.408]
 [0.464]
 [0.442]
 [0.429]
 [0.439]]
printing an ep nov before normalisation:  42.48202323913574
siam score:  -0.81417227
Printing some Q and Qe and total Qs values:  [[0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]
 [0.761]] [[30.936]
 [30.936]
 [30.936]
 [30.936]
 [30.936]
 [30.936]
 [30.936]] [[0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.897]]
maxi score, test score, baseline:  0.044806666666666564 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.015853008790145395
actor:  1 policy actor:  1  step number:  75 total reward:  0.02666666666666584  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.044806666666666564 0.6883333333333335 0.6883333333333335
maxi score, test score, baseline:  0.044806666666666564 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3333333333333327  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.044806666666666564 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.52 ]
 [0.505]
 [0.419]
 [0.398]
 [0.505]
 [0.432]] [[38.93 ]
 [36.055]
 [38.93 ]
 [40.338]
 [40.129]
 [38.93 ]
 [40.885]] [[2.338]
 [2.106]
 [2.338]
 [2.372]
 [2.333]
 [2.338]
 [2.432]]
printing an ep nov before normalisation:  34.86184462174252
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.044806666666666564 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.044806666666666564 0.6883333333333335 0.6883333333333335
line 256 mcts: sample exp_bonus 41.020603389453825
maxi score, test score, baseline:  0.044806666666666564 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.35333333333333283  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.044806666666666564 0.6883333333333335 0.6883333333333335
printing an ep nov before normalisation:  38.41700471841627
maxi score, test score, baseline:  0.044806666666666564 0.6883333333333335 0.6883333333333335
maxi score, test score, baseline:  0.044806666666666564 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.125]
 [0.279]
 [0.125]
 [0.125]
 [0.125]
 [0.125]
 [0.125]] [[34.075]
 [40.158]
 [34.075]
 [34.075]
 [34.075]
 [34.075]
 [34.075]] [[0.66 ]
 [1.037]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]
 [0.66 ]]
printing an ep nov before normalisation:  58.9078713852027
maxi score, test score, baseline:  0.044806666666666564 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.157]
 [0.273]
 [0.157]
 [0.157]
 [0.193]
 [0.139]
 [0.157]] [[43.594]
 [45.441]
 [43.594]
 [43.594]
 [46.604]
 [46.822]
 [43.594]] [[0.866]
 [1.05 ]
 [0.866]
 [0.866]
 [1.013]
 [0.967]
 [0.866]]
maxi score, test score, baseline:  0.044806666666666564 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.044806666666666564 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.044806666666666564 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.044806666666666564 0.6883333333333335 0.6883333333333335
actor:  1 policy actor:  1  step number:  52 total reward:  0.21999999999999964  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  53.46635294598463
maxi score, test score, baseline:  0.044806666666666564 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.54 ]
 [0.681]
 [0.62 ]
 [0.54 ]
 [0.657]
 [0.623]
 [0.54 ]] [[41.32 ]
 [40.025]
 [47.919]
 [41.32 ]
 [46.397]
 [48.235]
 [41.32 ]] [[1.057]
 [1.154]
 [1.362]
 [1.057]
 [1.348]
 [1.376]
 [1.057]]
maxi score, test score, baseline:  0.044806666666666564 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.044806666666666564 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.044806666666666564 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.19515641850027
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.542]
 [0.401]
 [0.47 ]
 [0.47 ]
 [0.4  ]
 [0.396]] [[33.76 ]
 [39.416]
 [33.903]
 [40.081]
 [40.081]
 [33.021]
 [33.412]] [[0.746]
 [1.008]
 [0.748]
 [0.95 ]
 [0.95 ]
 [0.729]
 [0.733]]
printing an ep nov before normalisation:  51.83349640897198
using explorer policy with actor:  1
siam score:  -0.8030319
printing an ep nov before normalisation:  40.93102349105314
maxi score, test score, baseline:  0.044806666666666564 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.13333333333333264  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  0.01315073008427703
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  67 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.18738131156592
printing an ep nov before normalisation:  44.2256498336792
maxi score, test score, baseline:  0.04145999999999989 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04145999999999989 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.41547393798828
printing an ep nov before normalisation:  43.579096931101375
printing an ep nov before normalisation:  49.92431956558135
printing an ep nov before normalisation:  49.52884512256178
UNIT TEST: sample policy line 217 mcts : [0.041 0.224 0.143 0.102 0.306 0.102 0.082]
printing an ep nov before normalisation:  45.592250142778674
using explorer policy with actor:  1
printing an ep nov before normalisation:  68.61289872668516
maxi score, test score, baseline:  0.04145999999999989 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.24666666666666637  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  62.97842742811639
actor:  0 policy actor:  0  step number:  51 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
siam score:  -0.79390913
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.574]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]] [[45.74]
 [43.45]
 [45.74]
 [45.74]
 [45.74]
 [45.74]
 [45.74]] [[0.905]
 [1.003]
 [0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.905]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.517041381794797
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.33333333333333326  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.82622126690986
siam score:  -0.79715925
printing an ep nov before normalisation:  36.61716884180289
printing an ep nov before normalisation:  36.16360760175242
printing an ep nov before normalisation:  44.2362117767334
maxi score, test score, baseline:  0.040619999999999906 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.768]
 [0.696]
 [0.688]
 [0.696]
 [0.696]
 [0.685]] [[48.157]
 [39.536]
 [48.157]
 [44.123]
 [48.157]
 [48.157]
 [38.15 ]] [[0.696]
 [0.768]
 [0.696]
 [0.688]
 [0.696]
 [0.696]
 [0.685]]
actions average: 
K:  0  action  0 :  tensor([0.3549, 0.0176, 0.1415, 0.1213, 0.1444, 0.1156, 0.1046],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0010,     0.9646,     0.0033,     0.0005,     0.0000,     0.0001,
            0.0304], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0781, 0.0110, 0.4844, 0.1209, 0.0817, 0.1059, 0.1179],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1295, 0.0160, 0.1342, 0.3642, 0.1136, 0.1239, 0.1187],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1065, 0.0119, 0.0799, 0.1010, 0.5350, 0.0867, 0.0790],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0740, 0.0058, 0.1071, 0.1135, 0.0803, 0.5298, 0.0894],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1074, 0.0623, 0.1492, 0.1133, 0.0872, 0.1090, 0.3717],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  45 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  47.52539487726052
actor:  0 policy actor:  0  step number:  54 total reward:  0.11333333333333273  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  37.16386255685589
printing an ep nov before normalisation:  47.911038822694096
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.025]
 [0.024]
 [0.024]
 [0.024]
 [0.017]
 [0.024]] [[10.982]
 [27.254]
 [10.982]
 [10.982]
 [10.982]
 [10.686]
 [10.982]] [[0.093]
 [0.292]
 [0.093]
 [0.093]
 [0.093]
 [0.082]
 [0.093]]
maxi score, test score, baseline:  0.03881999999999989 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.910331051460396
printing an ep nov before normalisation:  57.05574253625727
printing an ep nov before normalisation:  49.89381466928224
maxi score, test score, baseline:  0.03881999999999989 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  42 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  31.51362631639544
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.50888913045003
maxi score, test score, baseline:  0.03832666666666655 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.01729370752323
maxi score, test score, baseline:  0.03832666666666655 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.608245059003
printing an ep nov before normalisation:  51.62014914923443
printing an ep nov before normalisation:  32.0744533791548
maxi score, test score, baseline:  0.03832666666666655 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  44 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  1  action  0 :  tensor([0.3657, 0.0062, 0.1119, 0.1151, 0.1900, 0.0975, 0.1136],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0052, 0.9671, 0.0033, 0.0058, 0.0034, 0.0033, 0.0118],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1382, 0.1083, 0.2950, 0.0973, 0.0965, 0.1584, 0.1063],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0754, 0.0791, 0.0967, 0.3926, 0.0959, 0.1090, 0.1513],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1181, 0.0218, 0.1108, 0.1136, 0.4255, 0.1018, 0.1083],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0853, 0.0492, 0.1421, 0.0772, 0.0774, 0.4829, 0.0860],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1218, 0.0219, 0.1668, 0.1473, 0.1475, 0.1413, 0.2535],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.03777999999999989 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.396]
 [0.447]
 [0.397]
 [0.397]
 [0.397]
 [0.474]] [[47.662]
 [62.496]
 [59.962]
 [47.662]
 [47.662]
 [47.662]
 [58.213]] [[1.357]
 [1.812]
 [1.785]
 [1.357]
 [1.357]
 [1.357]
 [1.759]]
maxi score, test score, baseline:  0.03777999999999989 0.6883333333333335 0.6883333333333335
printing an ep nov before normalisation:  29.621216454722408
actor:  1 policy actor:  1  step number:  47 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.03447333333333322 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03447333333333322 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.58964669533142
maxi score, test score, baseline:  0.03447333333333322 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.899807929992676
actor:  1 policy actor:  1  step number:  57 total reward:  0.42666666666666675  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.03447333333333322 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]] [[34.856]
 [34.856]
 [34.856]
 [34.856]
 [34.856]
 [34.856]
 [34.856]] [[1.565]
 [1.565]
 [1.565]
 [1.565]
 [1.565]
 [1.565]
 [1.565]]
maxi score, test score, baseline:  0.03447333333333322 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03447333333333322 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  1.333
siam score:  -0.79426897
actor:  1 policy actor:  1  step number:  49 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03447333333333322 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03447333333333322 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.52041622432926
printing an ep nov before normalisation:  36.73727989196777
maxi score, test score, baseline:  0.03447333333333322 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.44697712206014
maxi score, test score, baseline:  0.03447333333333322 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.97231433732925
Printing some Q and Qe and total Qs values:  [[0.893]
 [0.975]
 [0.889]
 [0.89 ]
 [0.889]
 [0.885]
 [0.898]] [[30.586]
 [31.372]
 [32.614]
 [33.338]
 [33.791]
 [33.998]
 [32.476]] [[0.893]
 [0.975]
 [0.889]
 [0.89 ]
 [0.889]
 [0.885]
 [0.898]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.031113333333333215 0.6883333333333335 0.6883333333333335
actor:  1 policy actor:  1  step number:  70 total reward:  0.05999999999999983  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.031113333333333215 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  43 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  50.87237358093262
printing an ep nov before normalisation:  40.32520771026611
maxi score, test score, baseline:  0.03081999999999988 0.6883333333333335 0.6883333333333335
Printing some Q and Qe and total Qs values:  [[0.833]
 [0.897]
 [0.833]
 [0.804]
 [0.833]
 [0.833]
 [0.83 ]] [[37.31 ]
 [40.566]
 [37.31 ]
 [39.144]
 [37.31 ]
 [37.31 ]
 [45.749]] [[0.833]
 [0.897]
 [0.833]
 [0.804]
 [0.833]
 [0.833]
 [0.83 ]]
line 256 mcts: sample exp_bonus 39.254465103149414
maxi score, test score, baseline:  0.03081999999999988 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.583]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]] [[46.092]
 [42.663]
 [46.092]
 [46.092]
 [46.092]
 [46.092]
 [46.092]] [[1.334]
 [1.296]
 [1.334]
 [1.334]
 [1.334]
 [1.334]
 [1.334]]
printing an ep nov before normalisation:  45.483682870689485
printing an ep nov before normalisation:  34.54725545620741
actor:  0 policy actor:  0  step number:  51 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.93603041320933
printing an ep nov before normalisation:  43.83656870107091
maxi score, test score, baseline:  0.02999333333333321 0.6883333333333335 0.6883333333333335
maxi score, test score, baseline:  0.02999333333333321 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.87472057342529
maxi score, test score, baseline:  0.02999333333333321 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.37217781733897
printing an ep nov before normalisation:  17.73395211281013
actor:  1 policy actor:  1  step number:  52 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.026673333333333216 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.026673333333333216 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.112]
 [0.188]
 [0.133]
 [0.142]
 [0.143]
 [0.144]
 [0.136]] [[59.234]
 [55.67 ]
 [59.048]
 [58.778]
 [59.785]
 [59.46 ]
 [57.914]] [[1.857]
 [1.752]
 [1.869]
 [1.864]
 [1.917]
 [1.901]
 [1.815]]
printing an ep nov before normalisation:  49.52129367626125
maxi score, test score, baseline:  0.026673333333333216 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.075]
 [ 0.102]
 [-0.006]
 [ 0.12 ]
 [ 0.094]
 [ 0.103]
 [ 0.085]] [[42.117]
 [45.075]
 [48.25 ]
 [46.216]
 [46.098]
 [45.883]
 [43.456]] [[1.112]
 [1.298]
 [1.36 ]
 [1.376]
 [1.344]
 [1.342]
 [1.194]]
actor:  0 policy actor:  0  step number:  48 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.02608666666666655 0.6883333333333335 0.6883333333333335
Printing some Q and Qe and total Qs values:  [[ 0.345]
 [ 0.371]
 [ 0.328]
 [-0.007]
 [ 0.328]
 [ 0.328]
 [ 0.328]] [[38.212]
 [37.791]
 [37.883]
 [35.854]
 [37.883]
 [37.883]
 [37.883]] [[1.683]
 [1.683]
 [1.645]
 [1.182]
 [1.645]
 [1.645]
 [1.645]]
maxi score, test score, baseline:  0.02608666666666655 0.6883333333333335 0.6883333333333335
maxi score, test score, baseline:  0.02608666666666655 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.358]
 [0.423]
 [0.423]
 [0.288]
 [0.423]
 [0.423]] [[46.595]
 [50.412]
 [46.595]
 [46.595]
 [46.668]
 [46.595]
 [46.595]] [[1.648]
 [1.767]
 [1.648]
 [1.648]
 [1.516]
 [1.648]
 [1.648]]
printing an ep nov before normalisation:  26.410673874811668
actor:  1 policy actor:  1  step number:  48 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  1.667
siam score:  -0.80020857
UNIT TEST: sample policy line 217 mcts : [0.061 0.776 0.02  0.02  0.082 0.    0.041]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.02608666666666655 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.26314749454787
printing an ep nov before normalisation:  38.03767570172281
printing an ep nov before normalisation:  31.588007162154195
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.624]
 [0.331]
 [0.48 ]
 [0.255]
 [0.515]
 [0.539]] [[38.852]
 [34.422]
 [32.218]
 [32.778]
 [39.743]
 [35.955]
 [34.759]] [[0.445]
 [0.624]
 [0.331]
 [0.48 ]
 [0.255]
 [0.515]
 [0.539]]
maxi score, test score, baseline:  0.02608666666666655 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.297]
 [0.239]
 [0.239]
 [0.239]
 [0.239]
 [0.239]] [[32.181]
 [44.272]
 [32.181]
 [32.181]
 [32.181]
 [32.181]
 [32.181]] [[0.325]
 [0.481]
 [0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.325]]
printing an ep nov before normalisation:  41.84945354483113
printing an ep nov before normalisation:  50.05819495109405
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.58680011129052
actor:  0 policy actor:  0  step number:  41 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.02551333333333321 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.2773089198746
maxi score, test score, baseline:  0.02551333333333321 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.02551333333333321 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  51.99595181109905
actor:  1 policy actor:  1  step number:  57 total reward:  0.37333333333333285  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.02551333333333321 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.399421215057373
maxi score, test score, baseline:  0.02551333333333321 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.02551333333333321 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.02551333333333321 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.3615, 0.0115, 0.1119, 0.1173, 0.1454, 0.1267, 0.1257],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0048, 0.9563, 0.0039, 0.0022, 0.0016, 0.0013, 0.0299],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1195, 0.0139, 0.2896, 0.1310, 0.1219, 0.1776, 0.1466],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0924, 0.0160, 0.1170, 0.3331, 0.1635, 0.1439, 0.1341],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1305, 0.0558, 0.1093, 0.1024, 0.3924, 0.1040, 0.1056],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0639, 0.0067, 0.1022, 0.0729, 0.0854, 0.5884, 0.0804],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0697, 0.2122, 0.0869, 0.1147, 0.0879, 0.0807, 0.3477],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[ 0.226]
 [ 0.149]
 [-0.007]
 [ 0.157]
 [ 0.174]
 [-0.029]
 [ 0.11 ]] [[45.682]
 [46.918]
 [43.799]
 [49.178]
 [52.721]
 [48.795]
 [48.762]] [[0.995]
 [0.964]
 [0.69 ]
 [1.058]
 [1.21 ]
 [0.858]
 [0.996]]
Printing some Q and Qe and total Qs values:  [[0.684]
 [0.684]
 [0.684]
 [0.598]
 [0.684]
 [0.593]
 [0.684]] [[43.816]
 [43.816]
 [43.816]
 [42.691]
 [43.816]
 [42.264]
 [43.816]] [[1.627]
 [1.627]
 [1.627]
 [1.503]
 [1.627]
 [1.484]
 [1.627]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.02551333333333321 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.02551333333333321 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  68 total reward:  0.086666666666666  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  50 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  60 total reward:  0.29999999999999993  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.02551333333333321 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.10077972244254
Printing some Q and Qe and total Qs values:  [[0.245]
 [0.275]
 [0.186]
 [0.21 ]
 [0.21 ]
 [0.212]
 [0.21 ]] [[38.762]
 [40.161]
 [38.124]
 [40.086]
 [40.086]
 [44.215]
 [40.086]] [[1.188]
 [1.289]
 [1.096]
 [1.22 ]
 [1.22 ]
 [1.433]
 [1.22 ]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.98292591928443
maxi score, test score, baseline:  0.02551333333333321 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.02551333333333321 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.02551333333333321 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.53562990967036
maxi score, test score, baseline:  0.02551333333333321 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.42000000000000015  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  61 total reward:  0.14666666666666595  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.02551333333333321 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.02551333333333321 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.02551333333333321 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.02551333333333321 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  34 total reward:  0.4999999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.02520666666666654 0.6883333333333335 0.6883333333333335
maxi score, test score, baseline:  0.02520666666666654 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.55812173132658
Printing some Q and Qe and total Qs values:  [[0.131]
 [0.294]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]] [[48.798]
 [41.324]
 [48.798]
 [48.798]
 [48.798]
 [48.798]
 [48.798]] [[1.471]
 [1.239]
 [1.471]
 [1.471]
 [1.471]
 [1.471]
 [1.471]]
printing an ep nov before normalisation:  62.1866297869062
actions average: 
K:  4  action  0 :  tensor([0.1624, 0.0407, 0.1682, 0.1052, 0.1971, 0.2190, 0.1074],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0089, 0.8930, 0.0113, 0.0182, 0.0064, 0.0033, 0.0588],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1290, 0.0534, 0.2477, 0.1356, 0.1456, 0.1197, 0.1691],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1415, 0.0021, 0.1334, 0.1167, 0.2894, 0.1653, 0.1518],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1339, 0.0268, 0.0577, 0.0673, 0.5720, 0.0579, 0.0845],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0593, 0.0293, 0.1337, 0.0835, 0.0751, 0.5473, 0.0717],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1511, 0.0862, 0.1185, 0.1214, 0.1570, 0.1312, 0.2346],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.02520666666666654 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.02520666666666654 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.9491106216795
printing an ep nov before normalisation:  41.551932529435796
printing an ep nov before normalisation:  33.89388497127807
actor:  1 policy actor:  1  step number:  71 total reward:  0.026666666666665617  reward:  1.0 rdn_beta:  0.667
siam score:  -0.790274
maxi score, test score, baseline:  0.02520666666666654 0.6883333333333335 0.6883333333333335
maxi score, test score, baseline:  0.02520666666666654 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 37.04485578441244
printing an ep nov before normalisation:  36.36862538658756
maxi score, test score, baseline:  0.02520666666666654 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  40 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.02477999999999988 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.1029607476192
Printing some Q and Qe and total Qs values:  [[0.215]
 [0.302]
 [0.285]
 [0.215]
 [0.215]
 [0.236]
 [0.215]] [[56.419]
 [49.283]
 [55.697]
 [56.419]
 [56.419]
 [62.803]
 [56.419]] [[1.833]
 [1.574]
 [1.868]
 [1.833]
 [1.833]
 [2.164]
 [1.833]]
siam score:  -0.7906058
actor:  1 policy actor:  1  step number:  57 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  58.428245217085326
printing an ep nov before normalisation:  36.6273832321167
maxi score, test score, baseline:  0.02145999999999987 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.001]
 [ 0.001]
 [ 0.001]
 [ 0.001]
 [-0.006]
 [ 0.001]
 [ 0.001]] [[44.812]
 [44.812]
 [44.812]
 [44.812]
 [40.31 ]
 [44.812]
 [44.812]] [[2.001]
 [2.001]
 [2.001]
 [2.001]
 [1.602]
 [2.001]
 [2.001]]
maxi score, test score, baseline:  0.02145999999999987 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.02145999999999987 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.225090926627225
maxi score, test score, baseline:  0.02145999999999987 0.6883333333333335 0.6883333333333335
actor:  1 policy actor:  1  step number:  39 total reward:  0.6000000000000002  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.02145999999999987 0.6883333333333335 0.6883333333333335
maxi score, test score, baseline:  0.02145999999999987 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.02145999999999987 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.008]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.003]
 [-0.008]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  55.50447891629751
printing an ep nov before normalisation:  44.8282989836804
maxi score, test score, baseline:  0.02145999999999987 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.199]
 [0.385]
 [0.199]
 [0.305]
 [0.273]
 [0.028]
 [0.332]] [[54.636]
 [40.462]
 [54.636]
 [50.161]
 [60.802]
 [54.424]
 [53.857]] [[0.838]
 [0.703]
 [0.838]
 [0.842]
 [1.051]
 [0.662]
 [0.953]]
maxi score, test score, baseline:  0.02145999999999987 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.79493433
printing an ep nov before normalisation:  44.22444430820229
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.916]
 [0.749]
 [0.749]
 [0.749]
 [0.749]
 [0.749]] [[40.382]
 [47.834]
 [40.382]
 [40.382]
 [40.382]
 [40.382]
 [40.382]] [[1.42 ]
 [1.827]
 [1.42 ]
 [1.42 ]
 [1.42 ]
 [1.42 ]
 [1.42 ]]
maxi score, test score, baseline:  0.02145999999999987 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.43990898132324
printing an ep nov before normalisation:  36.330116899226795
printing an ep nov before normalisation:  31.088461061801453
maxi score, test score, baseline:  0.02145999999999987 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  34.745373557141555
actor:  1 policy actor:  1  step number:  47 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  0.02884037372950843
actor:  0 policy actor:  0  step number:  49 total reward:  0.34666666666666657  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  61.949939831560215
maxi score, test score, baseline:  0.020886666666666547 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.40677839335442
printing an ep nov before normalisation:  43.665326700155774
line 256 mcts: sample exp_bonus 33.992187676426944
maxi score, test score, baseline:  0.020886666666666547 0.6883333333333335 0.6883333333333335
maxi score, test score, baseline:  0.020886666666666547 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.79100436
maxi score, test score, baseline:  0.020886666666666547 0.6883333333333335 0.6883333333333335
maxi score, test score, baseline:  0.020886666666666547 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.020886666666666547 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  70 total reward:  0.07333333333333292  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.020886666666666547 0.6883333333333335 0.6883333333333335
maxi score, test score, baseline:  0.020886666666666547 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  73 total reward:  0.11999999999999933  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.020886666666666547 0.6883333333333335 0.6883333333333335
printing an ep nov before normalisation:  55.268290004679955
maxi score, test score, baseline:  0.020886666666666547 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.26397276860714
maxi score, test score, baseline:  0.020886666666666547 0.6883333333333335 0.6883333333333335
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.020886666666666547 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.22766673788881
actor:  1 policy actor:  1  step number:  48 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.33137403157066
maxi score, test score, baseline:  0.020886666666666547 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  45.511649967128655
maxi score, test score, baseline:  0.020886666666666547 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.020886666666666547 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.3225838455347
actor:  1 policy actor:  1  step number:  46 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  68 total reward:  0.0733333333333328  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.020886666666666547 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3866666666666667  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.020886666666666547 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.047804599689414
maxi score, test score, baseline:  0.020886666666666547 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.81545666711183
actor:  0 policy actor:  0  step number:  38 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.020593333333333224 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.020593333333333224 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.358]
 [0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.378]] [[35.195]
 [40.011]
 [35.195]
 [35.195]
 [35.195]
 [35.195]
 [37.983]] [[0.841]
 [0.982]
 [0.841]
 [0.841]
 [0.841]
 [0.841]
 [0.947]]
printing an ep nov before normalisation:  27.47600153039188
actor:  0 policy actor:  0  step number:  38 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  21.225013732910156
siam score:  -0.79127765
printing an ep nov before normalisation:  40.10032057104742
printing an ep nov before normalisation:  49.28331644799199
maxi score, test score, baseline:  0.020286666666666554 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.5156119934415
maxi score, test score, baseline:  0.020286666666666554 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.25073164432177
maxi score, test score, baseline:  0.020286666666666554 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.7909772
actor:  0 policy actor:  0  step number:  60 total reward:  0.2599999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.019579999999999882 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.27789418663879
line 256 mcts: sample exp_bonus 46.26644750359887
printing an ep nov before normalisation:  55.74300415721355
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.304]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]] [[37.801]
 [35.415]
 [37.801]
 [37.801]
 [37.801]
 [37.801]
 [37.801]] [[2.168]
 [1.947]
 [2.168]
 [2.168]
 [2.168]
 [2.168]
 [2.168]]
maxi score, test score, baseline:  0.019579999999999882 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.019579999999999882 0.6883333333333335 0.6883333333333335
maxi score, test score, baseline:  0.019579999999999882 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.4415780293194
maxi score, test score, baseline:  0.019579999999999882 0.6883333333333335 0.6883333333333335
maxi score, test score, baseline:  0.019579999999999882 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.235035928439736
printing an ep nov before normalisation:  43.28699392838519
Printing some Q and Qe and total Qs values:  [[0.212]
 [0.216]
 [0.212]
 [0.212]
 [0.196]
 [0.212]
 [0.212]] [[38.633]
 [32.932]
 [38.633]
 [38.633]
 [41.275]
 [38.633]
 [38.633]] [[0.817]
 [0.686]
 [0.817]
 [0.817]
 [0.863]
 [0.817]
 [0.817]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.41333333333333344  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  2  action  0 :  tensor([0.2933, 0.1711, 0.1068, 0.0903, 0.1121, 0.1166, 0.1098],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0074,     0.9343,     0.0128,     0.0023,     0.0007,     0.0009,
            0.0418], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0878, 0.0517, 0.4557, 0.0999, 0.0756, 0.0986, 0.1306],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0872, 0.1031, 0.1123, 0.2975, 0.1010, 0.1315, 0.1674],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0808, 0.0183, 0.0758, 0.0717, 0.5941, 0.0850, 0.0743],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0639, 0.0129, 0.1488, 0.0915, 0.0909, 0.4960, 0.0961],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1205, 0.0766, 0.1701, 0.1346, 0.1208, 0.1061, 0.2714],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  41.06201093614862
printing an ep nov before normalisation:  43.79525399244393
maxi score, test score, baseline:  0.019579999999999882 0.6883333333333335 0.6883333333333335
actor:  1 policy actor:  1  step number:  72 total reward:  0.17999999999999916  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.019579999999999882 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.019579999999999882 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.844]
 [0.844]
 [0.844]
 [0.844]
 [0.844]
 [0.844]
 [0.844]] [[42.117]
 [42.117]
 [42.117]
 [42.117]
 [42.117]
 [42.117]
 [42.117]] [[1.386]
 [1.386]
 [1.386]
 [1.386]
 [1.386]
 [1.386]
 [1.386]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.25333333333333263  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.019579999999999882 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.2801900324009
maxi score, test score, baseline:  0.019579999999999882 0.6883333333333335 0.6883333333333335
maxi score, test score, baseline:  0.019579999999999882 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.019579999999999882 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.019579999999999882 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.650450229644775
printing an ep nov before normalisation:  36.42026901245117
actor:  0 policy actor:  0  step number:  55 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  56 total reward:  0.5133333333333335  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  52.435751072889204
siam score:  -0.78378326
line 256 mcts: sample exp_bonus 12.677654027938843
maxi score, test score, baseline:  0.02213999999999988 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.602]
 [0.583]
 [0.549]
 [0.594]
 [0.577]
 [0.437]] [[32.214]
 [37.545]
 [33.172]
 [37.307]
 [33.266]
 [34.066]
 [32.56 ]] [[1.277]
 [1.677]
 [1.412]
 [1.611]
 [1.428]
 [1.456]
 [1.232]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.10946846008301
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.027]
 [0.997]
 [0.997]
 [0.003]
 [0.008]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.027]
 [0.997]
 [0.997]
 [0.003]
 [0.008]
 [0.005]]
maxi score, test score, baseline:  0.02213999999999988 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.926668274198136
printing an ep nov before normalisation:  46.76787904100835
printing an ep nov before normalisation:  42.11392328456337
printing an ep nov before normalisation:  37.07279620931534
printing an ep nov before normalisation:  34.85306612607101
printing an ep nov before normalisation:  45.20851102710578
actions average: 
K:  0  action  0 :  tensor([0.6710, 0.0127, 0.0445, 0.0547, 0.1103, 0.0510, 0.0558],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0024, 0.9792, 0.0021, 0.0019, 0.0014, 0.0016, 0.0114],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0613, 0.0449, 0.5467, 0.0725, 0.0751, 0.1084, 0.0911],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1073, 0.0323, 0.0955, 0.3908, 0.1135, 0.1094, 0.1511],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0951, 0.0029, 0.0581, 0.0993, 0.5864, 0.0850, 0.0732],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0890, 0.0104, 0.1387, 0.1036, 0.0973, 0.4550, 0.1060],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0613, 0.3734, 0.0992, 0.0793, 0.0728, 0.0785, 0.2354],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.02213999999999988 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.02213999999999988 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actor:  1 policy actor:  1  step number:  55 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  46 total reward:  0.4466666666666663  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  46 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.02213999999999988 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.96406641744642
printing an ep nov before normalisation:  42.72994428642422
maxi score, test score, baseline:  0.02213999999999988 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.2004, 0.0158, 0.1388, 0.1877, 0.1596, 0.1605, 0.1372],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0066, 0.9198, 0.0045, 0.0280, 0.0040, 0.0034, 0.0337],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1760, 0.0068, 0.2980, 0.1112, 0.1302, 0.1489, 0.1289],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1127, 0.0035, 0.0859, 0.4580, 0.1391, 0.1290, 0.0717],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1216, 0.0039, 0.0886, 0.0976, 0.5157, 0.0843, 0.0884],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1174, 0.0362, 0.1344, 0.1200, 0.0776, 0.4308, 0.0837],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1946, 0.0762, 0.1287, 0.1428, 0.1653, 0.1464, 0.1461],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.02213999999999988 0.6883333333333335 0.6883333333333335
printing an ep nov before normalisation:  38.39018786963436
maxi score, test score, baseline:  0.02213999999999988 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  51 total reward:  0.27999999999999936  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  40.01979524198485
actor:  0 policy actor:  0  step number:  67 total reward:  0.06666666666666565  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  0.05340179300787895
actor:  1 policy actor:  1  step number:  60 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  50.292975531229125
printing an ep nov before normalisation:  55.76516011428566
printing an ep nov before normalisation:  34.077439308166504
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.61800404300954
actor:  0 policy actor:  0  step number:  41 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  28.010458946228027
printing an ep nov before normalisation:  26.384446620941162
maxi score, test score, baseline:  0.029766666666666542 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]] [[48.187]
 [48.187]
 [48.187]
 [48.187]
 [48.187]
 [48.187]
 [48.187]] [[1.461]
 [1.461]
 [1.461]
 [1.461]
 [1.461]
 [1.461]
 [1.461]]
printing an ep nov before normalisation:  58.86218735039823
maxi score, test score, baseline:  0.029766666666666542 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.029766666666666542 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.811]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.668]] [[44.875]
 [43.888]
 [44.875]
 [44.875]
 [44.875]
 [44.875]
 [44.585]] [[0.729]
 [0.811]
 [0.729]
 [0.729]
 [0.729]
 [0.729]
 [0.668]]
maxi score, test score, baseline:  0.029766666666666542 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.21716322747737
maxi score, test score, baseline:  0.029766666666666542 0.6883333333333335 0.6883333333333335
actor:  1 policy actor:  1  step number:  50 total reward:  0.31333333333333324  reward:  1.0 rdn_beta:  1.0
siam score:  -0.78337157
Printing some Q and Qe and total Qs values:  [[-0.008]
 [-0.011]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.008]
 [-0.011]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]]
printing an ep nov before normalisation:  42.4507769053841
siam score:  -0.78216344
printing an ep nov before normalisation:  35.80438330364875
actor:  0 policy actor:  0  step number:  52 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  45.46730135789937
printing an ep nov before normalisation:  45.283372053434135
maxi score, test score, baseline:  0.029833333333333212 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.029833333333333212 0.6883333333333335 0.6883333333333335
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.649]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]] [[26.498]
 [37.075]
 [26.498]
 [26.498]
 [26.498]
 [26.498]
 [26.498]] [[1.098]
 [1.58 ]
 [1.098]
 [1.098]
 [1.098]
 [1.098]
 [1.098]]
printing an ep nov before normalisation:  53.145156572329945
printing an ep nov before normalisation:  30.37253473070578
printing an ep nov before normalisation:  51.109439150695316
printing an ep nov before normalisation:  50.35009270614075
actor:  1 policy actor:  1  step number:  48 total reward:  0.33999999999999986  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.029833333333333212 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]
 [-0.008]] [[56.132]
 [56.132]
 [56.132]
 [56.132]
 [56.132]
 [56.132]
 [56.132]] [[0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]
 [0.659]]
printing an ep nov before normalisation:  39.69335276342606
maxi score, test score, baseline:  0.029833333333333212 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.50339149428692
maxi score, test score, baseline:  0.029833333333333212 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.029833333333333212 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4133333333333328  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.029833333333333212 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.029833333333333212 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.029833333333333212 0.6883333333333335 0.6883333333333335
maxi score, test score, baseline:  0.029833333333333212 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  68 total reward:  0.033333333333332105  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  60 total reward:  0.13999999999999935  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  34.8635547954499
Printing some Q and Qe and total Qs values:  [[-0.01]
 [-0.  ]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.01]
 [-0.  ]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]
 [-0.01]]
printing an ep nov before normalisation:  50.91956081911917
printing an ep nov before normalisation:  37.00691650652559
maxi score, test score, baseline:  0.03211333333333322 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.78014356
actor:  0 policy actor:  0  step number:  53 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.034566666666666544 0.6883333333333335 0.6883333333333335
printing an ep nov before normalisation:  44.57051073920283
maxi score, test score, baseline:  0.034566666666666544 0.6883333333333335 0.6883333333333335
maxi score, test score, baseline:  0.034566666666666544 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
siam score:  -0.7784273
maxi score, test score, baseline:  0.034566666666666544 0.6883333333333335 0.6883333333333335
actions average: 
K:  0  action  0 :  tensor([0.4046, 0.0141, 0.1281, 0.1046, 0.1281, 0.1001, 0.1204],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0062, 0.9660, 0.0050, 0.0031, 0.0024, 0.0018, 0.0155],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1028, 0.0288, 0.4623, 0.0923, 0.1051, 0.1092, 0.0996],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0949, 0.0464, 0.1219, 0.3112, 0.1162, 0.0927, 0.2166],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1278, 0.0018, 0.1003, 0.1027, 0.4744, 0.0974, 0.0956],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1010, 0.0013, 0.1750, 0.0826, 0.1061, 0.4469, 0.0872],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1069, 0.1159, 0.1254, 0.1510, 0.1232, 0.1077, 0.2698],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  58.79205092503163
maxi score, test score, baseline:  0.034566666666666544 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.1794, 0.0184, 0.1647, 0.1307, 0.1955, 0.1588, 0.1525],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0108, 0.9326, 0.0103, 0.0100, 0.0035, 0.0023, 0.0304],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1508, 0.0615, 0.2576, 0.1728, 0.1206, 0.1272, 0.1094],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1128, 0.0494, 0.1115, 0.2821, 0.1583, 0.1348, 0.1512],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1146, 0.0107, 0.1306, 0.0967, 0.4274, 0.1141, 0.1058],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0800, 0.0023, 0.1165, 0.0667, 0.1059, 0.5515, 0.0771],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1607, 0.1812, 0.1464, 0.1169, 0.1258, 0.1038, 0.1652],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.034566666666666544 0.6883333333333335 0.6883333333333335
printing an ep nov before normalisation:  30.66957712173462
maxi score, test score, baseline:  0.034566666666666544 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  43 total reward:  0.5600000000000002  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.034566666666666544 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.034566666666666544 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.034566666666666544 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  47 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.88623285293579
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.087846092253415
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
printing an ep nov before normalisation:  45.71037244889699
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
actor:  1 policy actor:  1  step number:  58 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.081]
 [0.02 ]
 [0.02 ]
 [0.02 ]
 [0.02 ]
 [0.02 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.02 ]
 [0.081]
 [0.02 ]
 [0.02 ]
 [0.02 ]
 [0.02 ]
 [0.02 ]]
printing an ep nov before normalisation:  29.68635320663452
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.511]
 [0.387]
 [0.387]
 [0.391]
 [0.354]
 [0.379]] [[42.569]
 [40.622]
 [42.569]
 [42.569]
 [44.721]
 [49.499]
 [44.719]] [[1.143]
 [1.198]
 [1.143]
 [1.143]
 [1.223]
 [1.354]
 [1.211]]
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]] [[53.275]
 [53.275]
 [53.275]
 [53.275]
 [53.275]
 [53.275]
 [53.275]] [[1.291]
 [1.291]
 [1.291]
 [1.291]
 [1.291]
 [1.291]
 [1.291]]
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]] [[43.109]
 [43.109]
 [43.109]
 [43.109]
 [43.109]
 [43.109]
 [43.109]] [[1.312]
 [1.312]
 [1.312]
 [1.312]
 [1.312]
 [1.312]
 [1.312]]
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]] [[35.847]
 [35.847]
 [35.847]
 [35.847]
 [35.847]
 [35.847]
 [35.847]] [[1.308]
 [1.308]
 [1.308]
 [1.308]
 [1.308]
 [1.308]
 [1.308]]
actor:  1 policy actor:  1  step number:  77 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.619470954861065
printing an ep nov before normalisation:  30.848808138441353
actor:  1 policy actor:  1  step number:  64 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  49.98756862161143
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [0.1  ]
 [0.1  ]
 [0.1  ]
 [0.132]
 [0.1  ]
 [0.1  ]] [[57.518]
 [57.518]
 [57.518]
 [57.518]
 [64.935]
 [57.518]
 [57.518]] [[1.194]
 [1.194]
 [1.194]
 [1.194]
 [1.512]
 [1.194]
 [1.194]]
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.2331, 0.0699, 0.1157, 0.1743, 0.1283, 0.1301, 0.1486],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0132, 0.9152, 0.0121, 0.0105, 0.0063, 0.0075, 0.0352],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1692, 0.0228, 0.2334, 0.1337, 0.1283, 0.1664, 0.1462],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1125, 0.0036, 0.1221, 0.4127, 0.0986, 0.1844, 0.0662],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1430, 0.0118, 0.0805, 0.0851, 0.5091, 0.0998, 0.0708],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0967, 0.0135, 0.1850, 0.0936, 0.0677, 0.4614, 0.0821],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1479, 0.0029, 0.1947, 0.1701, 0.1238, 0.1690, 0.1917],
       grad_fn=<DivBackward0>)
siam score:  -0.78303576
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.57653891624524
printing an ep nov before normalisation:  32.05344451709033
actor:  1 policy actor:  1  step number:  61 total reward:  0.18666666666666587  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  49.767068639980465
printing an ep nov before normalisation:  28.2206153545039
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.2134942340182
actor:  1 policy actor:  1  step number:  63 total reward:  0.15999999999999925  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
actor:  1 policy actor:  1  step number:  70 total reward:  0.059999999999999276  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.446]
 [0.446]
 [0.446]
 [0.468]
 [0.446]
 [0.446]] [[30.928]
 [30.928]
 [30.928]
 [30.928]
 [42.011]
 [30.928]
 [30.928]] [[1.298]
 [1.298]
 [1.298]
 [1.298]
 [1.915]
 [1.298]
 [1.298]]
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  27.388762055072604
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]] [[30.939]
 [30.939]
 [30.939]
 [30.939]
 [30.939]
 [30.939]
 [30.939]] [[0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]]
printing an ep nov before normalisation:  22.193680018692135
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  11.199170351028442
printing an ep nov before normalisation:  0.04026046700801089
actor:  1 policy actor:  1  step number:  69 total reward:  0.03999999999999948  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  68 total reward:  0.006666666666665599  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.435334272756016
maxi score, test score, baseline:  0.034779999999999894 0.6883333333333335 0.6883333333333335
printing an ep nov before normalisation:  31.879150687606224
printing an ep nov before normalisation:  28.832527782873033
actor:  0 policy actor:  0  step number:  44 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.037513333333333225 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.035446756920614
maxi score, test score, baseline:  0.03751333333333322 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03751333333333322 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03751333333333322 0.6883333333333335 0.6883333333333335
maxi score, test score, baseline:  0.03751333333333322 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.41821564872398
maxi score, test score, baseline:  0.03751333333333322 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.36404319139119
actor:  1 policy actor:  1  step number:  59 total reward:  0.23999999999999977  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.544]
 [0.481]
 [0.486]
 [0.487]
 [0.493]
 [0.48 ]] [[40.302]
 [38.694]
 [42.509]
 [42.74 ]
 [42.824]
 [43.326]
 [43.2  ]] [[1.27 ]
 [1.276]
 [1.357]
 [1.37 ]
 [1.374]
 [1.398]
 [1.381]]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  42.45845141366345
Printing some Q and Qe and total Qs values:  [[-0.006]
 [ 0.01 ]
 [-0.005]
 [-0.006]
 [-0.006]
 [-0.006]
 [-0.006]] [[11.953]
 [36.23 ]
 [39.196]
 [11.857]
 [11.853]
 [11.985]
 [12.205]] [[0.066]
 [0.368]
 [0.388]
 [0.066]
 [0.065]
 [0.067]
 [0.07 ]]
maxi score, test score, baseline:  0.03751333333333322 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.19 ]
 [0.19 ]
 [0.19 ]
 [0.19 ]
 [0.205]
 [0.19 ]
 [0.19 ]] [[36.173]
 [36.173]
 [36.173]
 [36.173]
 [41.31 ]
 [36.173]
 [36.173]] [[0.695]
 [0.695]
 [0.695]
 [0.695]
 [0.872]
 [0.695]
 [0.695]]
maxi score, test score, baseline:  0.03751333333333322 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.74634953038788
printing an ep nov before normalisation:  36.49417266845631
actor:  0 policy actor:  0  step number:  65 total reward:  0.10666666666666602  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  27.89250373840332
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.445665821818
actor:  1 policy actor:  1  step number:  73 total reward:  0.14666666666666628  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.03972666666666655 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03972666666666654 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7767337
printing an ep nov before normalisation:  38.52067902552936
maxi score, test score, baseline:  0.03972666666666654 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.03972666666666655 0.6883333333333335 0.6883333333333335
maxi score, test score, baseline:  0.03972666666666655 0.6883333333333335 0.6883333333333335
maxi score, test score, baseline:  0.03972666666666655 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.649]
 [0.571]
 [0.571]
 [0.496]
 [0.573]
 [0.559]] [[29.451]
 [31.703]
 [30.794]
 [33.402]
 [33.36 ]
 [35.851]
 [32.681]] [[1.055]
 [1.305]
 [1.189]
 [1.297]
 [1.22 ]
 [1.4  ]
 [1.255]]
maxi score, test score, baseline:  0.03972666666666655 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03972666666666655 0.6883333333333335 0.6883333333333335
printing an ep nov before normalisation:  44.42913669968721
actor:  1 policy actor:  1  step number:  69 total reward:  0.10666666666666613  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.03972666666666655 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03972666666666655 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77826416
maxi score, test score, baseline:  0.03972666666666655 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.418]
 [0.385]
 [0.385]
 [0.385]
 [0.385]
 [0.385]] [[47.468]
 [44.393]
 [47.468]
 [47.468]
 [47.468]
 [47.468]
 [47.468]] [[1.049]
 [1.004]
 [1.049]
 [1.049]
 [1.049]
 [1.049]
 [1.049]]
maxi score, test score, baseline:  0.039726666666666556 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.86349263670795
printing an ep nov before normalisation:  0.0077891186219858355
actor:  1 policy actor:  1  step number:  49 total reward:  0.3733333333333333  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.039726666666666556 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.58385848870413
Printing some Q and Qe and total Qs values:  [[0.633]
 [0.55 ]
 [0.548]
 [0.548]
 [0.549]
 [0.548]
 [0.523]] [[51.38 ]
 [66.643]
 [57.039]
 [57.039]
 [60.977]
 [57.039]
 [62.31 ]] [[0.633]
 [0.55 ]
 [0.548]
 [0.548]
 [0.549]
 [0.548]
 [0.523]]
maxi score, test score, baseline:  0.039726666666666556 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.50663148006076
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  59 total reward:  0.18666666666666631  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  55 total reward:  0.2533333333333331  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.03937999999999989 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03937999999999989 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.03937999999999989 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.005]
 [-0.021]
 [-0.005]] [[60.449]
 [60.449]
 [60.449]
 [60.449]
 [60.449]
 [61.112]
 [60.449]] [[1.958]
 [1.958]
 [1.958]
 [1.958]
 [1.958]
 [1.979]
 [1.958]]
using explorer policy with actor:  1
siam score:  -0.77814466
maxi score, test score, baseline:  0.03937999999999989 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7773761
maxi score, test score, baseline:  0.03937999999999989 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  57 total reward:  0.173333333333333  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
siam score:  -0.77565813
printing an ep nov before normalisation:  33.23995351791382
maxi score, test score, baseline:  0.04172666666666654 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.24002610987609
actor:  1 policy actor:  1  step number:  53 total reward:  0.37333333333333274  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  36.766209481844754
maxi score, test score, baseline:  0.04172666666666654 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.30365985266906
printing an ep nov before normalisation:  39.52933210182148
maxi score, test score, baseline:  0.04172666666666654 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04172666666666654 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.89752197265625
line 256 mcts: sample exp_bonus 20.208325940594474
printing an ep nov before normalisation:  0.21746982359161393
actor:  1 policy actor:  1  step number:  58 total reward:  0.2733333333333332  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.04172666666666654 0.6883333333333335 0.6883333333333335
printing an ep nov before normalisation:  62.38641583203244
maxi score, test score, baseline:  0.04172666666666654 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.85050248731535
printing an ep nov before normalisation:  49.57853546152234
maxi score, test score, baseline:  0.04172666666666654 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.67430766984185
maxi score, test score, baseline:  0.04172666666666654 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.61528664569931
maxi score, test score, baseline:  0.04172666666666654 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  66 total reward:  0.17999999999999938  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.04172666666666654 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04172666666666654 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.4094, 0.0355, 0.1370, 0.0861, 0.1026, 0.0918, 0.1376],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0071, 0.9617, 0.0075, 0.0043, 0.0022, 0.0023, 0.0148],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1334, 0.0035, 0.3004, 0.1370, 0.1281, 0.1737, 0.1237],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1549, 0.0136, 0.1358, 0.2733, 0.1292, 0.1395, 0.1537],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1417, 0.0081, 0.0684, 0.0868, 0.5532, 0.0712, 0.0705],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1157, 0.0150, 0.1382, 0.1123, 0.1015, 0.4058, 0.1115],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0795, 0.1501, 0.0943, 0.0885, 0.0698, 0.0733, 0.4445],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  37.024132773428896
printing an ep nov before normalisation:  38.18098361734261
siam score:  -0.7828162
maxi score, test score, baseline:  0.04172666666666654 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04172666666666654 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  50 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  2.0
Starting evaluation
maxi score, test score, baseline:  0.04125999999999987 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.623]
 [0.581]
 [0.596]
 [0.569]
 [0.557]
 [0.543]] [[24.925]
 [30.044]
 [24.877]
 [29.044]
 [28.375]
 [24.686]
 [24.632]] [[0.282]
 [0.623]
 [0.581]
 [0.596]
 [0.569]
 [0.557]
 [0.543]]
maxi score, test score, baseline:  0.04125999999999987 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.04125999999999987 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.145245279584614
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.731]
 [0.6  ]
 [0.575]
 [0.574]
 [0.568]
 [0.575]] [[32.609]
 [38.996]
 [36.848]
 [35.145]
 [35.653]
 [34.985]
 [34.307]] [[0.567]
 [0.731]
 [0.6  ]
 [0.575]
 [0.574]
 [0.568]
 [0.575]]
printing an ep nov before normalisation:  32.96087003749757
printing an ep nov before normalisation:  29.72900743773503
maxi score, test score, baseline:  0.04125999999999987 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.863]
 [0.906]
 [0.863]
 [0.863]
 [0.812]
 [0.863]
 [0.869]] [[28.697]
 [39.69 ]
 [28.697]
 [28.697]
 [37.634]
 [28.697]
 [38.434]] [[0.863]
 [0.906]
 [0.863]
 [0.863]
 [0.812]
 [0.863]
 [0.869]]
actor:  0 policy actor:  0  step number:  38 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  46.77357454092947
printing an ep nov before normalisation:  39.46811163389677
maxi score, test score, baseline:  0.04412666666666653 0.6883333333333335 0.6883333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7826625
printing an ep nov before normalisation:  35.38328388191979
Printing some Q and Qe and total Qs values:  [[0.789]
 [0.851]
 [0.835]
 [0.827]
 [0.772]
 [0.804]
 [0.837]] [[41.378]
 [38.749]
 [38.962]
 [38.923]
 [42.707]
 [42.098]
 [39.766]] [[0.789]
 [0.851]
 [0.835]
 [0.827]
 [0.772]
 [0.804]
 [0.837]]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  38.68412494659424
printing an ep nov before normalisation:  35.324699594310786
Printing some Q and Qe and total Qs values:  [[0.95 ]
 [0.977]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.881]] [[38.241]
 [33.359]
 [38.241]
 [38.241]
 [38.241]
 [38.241]
 [33.897]] [[0.95 ]
 [0.977]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.95 ]
 [0.881]]
Printing some Q and Qe and total Qs values:  [[0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]] [[30.104]
 [30.104]
 [30.104]
 [30.104]
 [30.104]
 [30.104]
 [30.104]] [[0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]
 [0.953]]
printing an ep nov before normalisation:  50.74316384347451
siam score:  -0.7825197
printing an ep nov before normalisation:  31.292213123913935
printing an ep nov before normalisation:  37.502291938425174
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.534]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]] [[20.629]
 [25.046]
 [20.629]
 [20.629]
 [20.629]
 [20.629]
 [20.629]] [[0.648]
 [0.534]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]]
printing an ep nov before normalisation:  36.82297119536666
printing an ep nov before normalisation:  38.92676014424581
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  28.262246765292502
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
using explorer policy with actor:  1
siam score:  -0.78253824
printing an ep nov before normalisation:  44.124616426415635
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10664666666666654 0.6893333333333335 0.6893333333333335
Printing some Q and Qe and total Qs values:  [[0.861]
 [0.875]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]] [[37.759]
 [40.117]
 [37.759]
 [37.759]
 [37.759]
 [37.759]
 [37.759]] [[1.543]
 [1.643]
 [1.543]
 [1.543]
 [1.543]
 [1.543]
 [1.543]]
actor:  0 policy actor:  0  step number:  52 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10913999999999986 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  0.10913999999999986 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.10913999999999986 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10913999999999986 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.56955352012465
maxi score, test score, baseline:  0.10913999999999986 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.51504707703681
printing an ep nov before normalisation:  36.60432815551758
printing an ep nov before normalisation:  10.477221012115479
printing an ep nov before normalisation:  40.63007831573486
maxi score, test score, baseline:  0.10913999999999986 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.29333333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10913999999999986 0.6893333333333335 0.6893333333333335
printing an ep nov before normalisation:  0.036040267576709084
actor:  0 policy actor:  0  step number:  66 total reward:  0.006666666666665599  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10889999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.11265665403722
printing an ep nov before normalisation:  40.70910872604338
maxi score, test score, baseline:  0.10889999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3333333333333328  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  43.19489209728744
printing an ep nov before normalisation:  38.977308785284976
printing an ep nov before normalisation:  54.33778749403707
actions average: 
K:  2  action  0 :  tensor([0.2760, 0.0018, 0.1327, 0.1528, 0.1290, 0.1497, 0.1581],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0223, 0.8868, 0.0140, 0.0100, 0.0088, 0.0133, 0.0449],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1996, 0.0067, 0.2357, 0.1169, 0.1278, 0.1725, 0.1407],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1905, 0.0179, 0.1541, 0.1438, 0.1705, 0.1908, 0.1323],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1480, 0.0043, 0.1355, 0.1635, 0.2756, 0.1567, 0.1162],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1207, 0.0064, 0.1274, 0.1016, 0.0950, 0.4383, 0.1105],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0990, 0.0137, 0.1211, 0.1316, 0.0902, 0.1104, 0.4340],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  35.90794950360377
maxi score, test score, baseline:  0.10889999999999987 0.6893333333333335 0.6893333333333335
actor:  1 policy actor:  1  step number:  43 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10889999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.10889999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10889999999999987 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  0.10889999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.69907175890144
maxi score, test score, baseline:  0.10889999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.10889999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.73859032954599
maxi score, test score, baseline:  0.10889999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.10889999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.46188259124756
printing an ep nov before normalisation:  31.29118260463544
actor:  0 policy actor:  0  step number:  48 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  44 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.11217999999999988 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.05362998359325
maxi score, test score, baseline:  0.11217999999999988 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11217999999999988 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  0.11217999999999988 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11217999999999988 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11217999999999988 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  0.11217999999999988 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11217999999999988 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11217999999999988 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  0.11217999999999988 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  47 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  56 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11167333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11167333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11167333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.578]
 [0.595]
 [0.614]
 [0.613]
 [0.598]
 [0.603]] [[34.377]
 [36.168]
 [34.583]
 [29.595]
 [32.473]
 [32.441]
 [33.526]] [[0.79 ]
 [0.811]
 [0.812]
 [0.776]
 [0.806]
 [0.791]
 [0.807]]
printing an ep nov before normalisation:  41.747219596550195
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.11167333333333321 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  0.11167333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.50192671583482
line 256 mcts: sample exp_bonus 37.7245250312011
maxi score, test score, baseline:  0.11167333333333321 0.6893333333333335 0.6893333333333335
actions average: 
K:  0  action  0 :  tensor([0.2979, 0.0047, 0.1439, 0.1242, 0.1260, 0.1591, 0.1442],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0048, 0.9453, 0.0071, 0.0055, 0.0014, 0.0019, 0.0339],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1181, 0.0170, 0.4313, 0.1051, 0.0883, 0.1309, 0.1093],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1290, 0.0025, 0.1268, 0.3408, 0.0939, 0.1317, 0.1753],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1067, 0.0016, 0.0782, 0.0780, 0.5903, 0.0839, 0.0613],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1299, 0.0050, 0.1359, 0.1226, 0.1181, 0.3612, 0.1272],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1117, 0.2547, 0.1098, 0.1033, 0.0902, 0.1173, 0.2130],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  34.957826137542725
printing an ep nov before normalisation:  34.668709871922644
maxi score, test score, baseline:  0.11167333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  1.333
UNIT TEST: sample policy line 217 mcts : [0.061 0.633 0.041 0.041 0.102 0.061 0.061]
actor:  1 policy actor:  1  step number:  60 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11167333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11167333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.290159157053644
maxi score, test score, baseline:  0.11167333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.774068
printing an ep nov before normalisation:  43.7724520149857
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  75 total reward:  0.09333333333333249  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  39.04335226666376
printing an ep nov before normalisation:  45.387249096594445
maxi score, test score, baseline:  0.11047333333333322 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  52.1964450858272
actor:  0 policy actor:  0  step number:  41 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[-0.008]
 [-0.004]
 [-0.015]
 [-0.013]
 [-0.026]
 [-0.011]
 [-0.007]] [[45.669]
 [41.815]
 [45.319]
 [45.936]
 [43.355]
 [40.867]
 [43.131]] [[0.596]
 [0.52 ]
 [0.582]
 [0.597]
 [0.53 ]
 [0.492]
 [0.545]]
maxi score, test score, baseline:  0.10908666666666654 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7698791
maxi score, test score, baseline:  0.10908666666666654 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.714]
 [0.731]
 [0.714]
 [0.714]
 [0.714]
 [0.714]
 [0.714]] [[23.887]
 [26.052]
 [23.887]
 [23.887]
 [23.887]
 [23.887]
 [23.887]] [[0.714]
 [0.731]
 [0.714]
 [0.714]
 [0.714]
 [0.714]
 [0.714]]
maxi score, test score, baseline:  0.10908666666666654 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.376368743166474
actor:  0 policy actor:  0  step number:  51 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  33.992498127965554
maxi score, test score, baseline:  0.10817999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1047933333333332 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.43911587602765
actor:  0 policy actor:  0  step number:  39 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.601]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]] [[39.069]
 [42.883]
 [39.069]
 [39.069]
 [39.069]
 [39.069]
 [39.069]] [[1.21 ]
 [1.515]
 [1.21 ]
 [1.21 ]
 [1.21 ]
 [1.21 ]
 [1.21 ]]
siam score:  -0.7653171
printing an ep nov before normalisation:  55.42975902557373
siam score:  -0.76530373
printing an ep nov before normalisation:  40.445768221394445
maxi score, test score, baseline:  0.10448666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  57.09277232265984
actor:  1 policy actor:  1  step number:  61 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10448666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10448666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.237610483416994
actor:  1 policy actor:  1  step number:  74 total reward:  0.07333333333333258  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.083]
 [0.122]
 [0.108]
 [0.023]
 [0.083]
 [0.007]
 [0.032]] [[34.449]
 [32.992]
 [33.668]
 [34.342]
 [34.449]
 [35.311]
 [34.521]] [[1.478]
 [1.457]
 [1.471]
 [1.413]
 [1.478]
 [1.437]
 [1.43 ]]
maxi score, test score, baseline:  0.10448666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10448666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.073809003163106
printing an ep nov before normalisation:  44.65734004974365
siam score:  -0.7695157
maxi score, test score, baseline:  0.10448666666666655 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  0.10448666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.486]
 [0.087]
 [0.154]
 [0.567]
 [0.486]
 [0.486]] [[40.355]
 [40.355]
 [38.464]
 [36.139]
 [46.891]
 [40.355]
 [40.355]] [[0.893]
 [0.893]
 [0.449]
 [0.462]
 [1.127]
 [0.893]
 [0.893]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.10448666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.611]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]] [[32.582]
 [37.855]
 [32.582]
 [32.582]
 [32.582]
 [32.582]
 [32.582]] [[0.898]
 [1.139]
 [0.898]
 [0.898]
 [0.898]
 [0.898]
 [0.898]]
maxi score, test score, baseline:  0.10448666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.10448666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  0.15333333333333288  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10448666666666655 0.6893333333333335 0.6893333333333335
actor:  1 policy actor:  1  step number:  68 total reward:  0.03333333333333266  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  58.36639656275625
printing an ep nov before normalisation:  40.890552703518914
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.10448666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.95065386224317
maxi score, test score, baseline:  0.10448666666666655 0.6893333333333335 0.6893333333333335
Printing some Q and Qe and total Qs values:  [[0.086]
 [0.112]
 [0.171]
 [0.171]
 [0.152]
 [0.141]
 [0.173]] [[50.033]
 [48.997]
 [39.294]
 [35.977]
 [35.647]
 [36.716]
 [39.737]] [[1.003]
 [0.993]
 [0.717]
 [0.603]
 [0.573]
 [0.599]
 [0.735]]
siam score:  -0.76470613
maxi score, test score, baseline:  0.10448666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76416
printing an ep nov before normalisation:  35.42885780334473
printing an ep nov before normalisation:  15.702955722808838
actor:  1 policy actor:  1  step number:  51 total reward:  0.33333333333333326  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10448666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.10448666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.10111333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.10111333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.10111333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.10111333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.10111333333333321 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  0.10111333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  0.13999999999999946  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  49 total reward:  0.38666666666666616  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  44.0527237095445
maxi score, test score, baseline:  0.10111333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.235]
 [0.317]
 [0.235]
 [0.235]
 [0.235]
 [0.182]
 [0.235]] [[46.436]
 [42.138]
 [46.436]
 [46.436]
 [46.436]
 [38.373]
 [46.436]] [[0.855]
 [0.823]
 [0.855]
 [0.855]
 [0.855]
 [0.588]
 [0.855]]
Printing some Q and Qe and total Qs values:  [[0.488]
 [0.549]
 [0.488]
 [0.488]
 [0.488]
 [0.488]
 [0.488]] [[35.905]
 [39.371]
 [35.905]
 [35.905]
 [35.905]
 [35.905]
 [35.905]] [[1.128]
 [1.322]
 [1.128]
 [1.128]
 [1.128]
 [1.128]
 [1.128]]
actions average: 
K:  3  action  0 :  tensor([0.2600, 0.0054, 0.1448, 0.1280, 0.1560, 0.1706, 0.1353],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0171, 0.9067, 0.0067, 0.0173, 0.0162, 0.0042, 0.0318],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1213, 0.0015, 0.3414, 0.1285, 0.1344, 0.1553, 0.1177],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1248, 0.1594, 0.1239, 0.1206, 0.1411, 0.1488, 0.1814],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1284, 0.1497, 0.1044, 0.0668, 0.3285, 0.0959, 0.1264],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1425, 0.0097, 0.1625, 0.1552, 0.1580, 0.2372, 0.1347],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1051, 0.1939, 0.0924, 0.1203, 0.0985, 0.1044, 0.2853],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  33.03381020857538
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.10111333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77193457
maxi score, test score, baseline:  0.10111333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10111333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.2533333333333331  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10111333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.10111333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.011]
 [ 0.063]
 [-0.006]
 [-0.002]
 [-0.003]
 [-0.001]
 [-0.004]] [[44.895]
 [40.865]
 [44.431]
 [44.942]
 [45.031]
 [44.836]
 [44.408]] [[1.984]
 [1.69 ]
 [1.928]
 [1.975]
 [1.982]
 [1.967]
 [1.928]]
actor:  0 policy actor:  0  step number:  53 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0970333333333332 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.003]
 [-0.189]
 [ 0.003]
 [-0.147]
 [ 0.012]
 [-0.   ]
 [ 0.019]] [[14.116]
 [22.796]
 [14.027]
 [20.582]
 [14.023]
 [13.817]
 [13.64 ]] [[0.589]
 [0.768]
 [0.592]
 [0.717]
 [0.601]
 [0.58 ]
 [0.592]]
maxi score, test score, baseline:  0.0970333333333332 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09365999999999988 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09365999999999988 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09365999999999988 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09365999999999988 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09365999999999988 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09365999999999988 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.55521202087402
line 256 mcts: sample exp_bonus 29.66403248346199
maxi score, test score, baseline:  0.09365999999999988 0.6893333333333335 0.6893333333333335
printing an ep nov before normalisation:  37.46692895889282
printing an ep nov before normalisation:  54.12367009985315
printing an ep nov before normalisation:  55.427363387361865
printing an ep nov before normalisation:  44.74586885547112
maxi score, test score, baseline:  0.0903133333333332 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.621698495267566
actor:  0 policy actor:  0  step number:  61 total reward:  0.13333333333333242  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  2.70312455931105
actor:  1 policy actor:  1  step number:  57 total reward:  0.41333333333333333  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  40.32509822785513
printing an ep nov before normalisation:  37.05458072215249
printing an ep nov before normalisation:  46.93499653830081
printing an ep nov before normalisation:  67.33675491065354
maxi score, test score, baseline:  0.08920666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.58684992097277
printing an ep nov before normalisation:  54.63100186049925
maxi score, test score, baseline:  0.08920666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.062734498201515
maxi score, test score, baseline:  0.08920666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08920666666666655 0.6893333333333335 0.6893333333333335
printing an ep nov before normalisation:  45.21964640541979
printing an ep nov before normalisation:  40.11633760032146
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08920666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  0.12666666666666626  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]
 [0.694]] [[39.925]
 [39.925]
 [39.925]
 [39.925]
 [39.925]
 [39.925]
 [39.925]] [[2.093]
 [2.093]
 [2.093]
 [2.093]
 [2.093]
 [2.093]
 [2.093]]
printing an ep nov before normalisation:  40.17662020305719
actor:  1 policy actor:  1  step number:  51 total reward:  0.4533333333333335  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  74 total reward:  0.16666666666666596  reward:  1.0 rdn_beta:  1.0
siam score:  -0.7667168
maxi score, test score, baseline:  0.08920666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.548]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]] [[42.94 ]
 [44.331]
 [42.94 ]
 [42.94 ]
 [42.94 ]
 [42.94 ]
 [42.94 ]] [[2.291]
 [2.384]
 [2.291]
 [2.291]
 [2.291]
 [2.291]
 [2.291]]
maxi score, test score, baseline:  0.08920666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08920666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.64095870893744
maxi score, test score, baseline:  0.08920666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08920666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  67 total reward:  0.07999999999999974  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.041 0.49  0.184 0.061 0.102 0.041 0.082]
printing an ep nov before normalisation:  38.5005029124999
printing an ep nov before normalisation:  23.666719512729824
printing an ep nov before normalisation:  14.548640251159668
maxi score, test score, baseline:  0.08920666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.678999251936201
maxi score, test score, baseline:  0.08920666666666655 0.6893333333333335 0.6893333333333335
actor:  1 policy actor:  1  step number:  56 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[ 0.004]
 [ 0.047]
 [-0.01 ]
 [-0.003]
 [-0.004]
 [-0.005]
 [ 0.008]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.004]
 [ 0.047]
 [-0.01 ]
 [-0.003]
 [-0.004]
 [-0.005]
 [ 0.008]]
actions average: 
K:  3  action  0 :  tensor([0.3603, 0.0089, 0.1045, 0.1039, 0.1986, 0.1232, 0.1006],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0084, 0.9153, 0.0144, 0.0082, 0.0031, 0.0079, 0.0427],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0883, 0.0850, 0.1844, 0.1289, 0.1652, 0.2387, 0.1093],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1353, 0.1389, 0.1219, 0.1914, 0.1057, 0.1348, 0.1720],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1265, 0.0112, 0.0989, 0.0738, 0.4890, 0.1242, 0.0764],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1094, 0.0499, 0.1711, 0.1238, 0.1245, 0.3081, 0.1130],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0924, 0.2765, 0.0807, 0.0745, 0.0704, 0.0736, 0.3321],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.08920666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08920666666666655 0.6893333333333335 0.6893333333333335
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.678]
 [0.569]
 [0.534]
 [0.501]
 [0.593]
 [0.584]] [[43.272]
 [36.209]
 [45.42 ]
 [47.193]
 [47.812]
 [41.411]
 [41.218]] [[0.993]
 [1.005]
 [1.075]
 [1.075]
 [1.053]
 [1.021]
 [1.009]]
maxi score, test score, baseline:  0.08920666666666655 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.149]
 [0.221]
 [0.149]
 [0.149]
 [0.149]
 [0.149]
 [0.149]] [[34.683]
 [40.03 ]
 [34.683]
 [34.683]
 [34.683]
 [34.683]
 [34.683]] [[0.565]
 [0.746]
 [0.565]
 [0.565]
 [0.565]
 [0.565]
 [0.565]]
actor:  0 policy actor:  0  step number:  53 total reward:  0.5200000000000005  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.08888666666666652 0.6893333333333335 0.6893333333333335
actor:  1 policy actor:  1  step number:  42 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.270668501420616
maxi score, test score, baseline:  0.08888666666666652 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.06862490218035
line 256 mcts: sample exp_bonus 42.33202653451276
maxi score, test score, baseline:  0.08888666666666652 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  0.08888666666666652 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  0.667
UNIT TEST: sample policy line 217 mcts : [0.286 0.265 0.082 0.082 0.082 0.122 0.082]
actor:  0 policy actor:  0  step number:  48 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.08809999999999986 0.6893333333333335 0.6893333333333335
printing an ep nov before normalisation:  36.248841285705566
maxi score, test score, baseline:  0.08809999999999986 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  0.08809999999999986 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08809999999999986 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  76 total reward:  0.11333333333333273  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  30.729816693854325
maxi score, test score, baseline:  0.08809999999999986 0.6893333333333335 0.6893333333333335
printing an ep nov before normalisation:  0.4658994275206396
actor:  0 policy actor:  0  step number:  57 total reward:  0.17333333333333267  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  48.172996874564106
maxi score, test score, baseline:  0.08708666666666653 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.23061378036942
printing an ep nov before normalisation:  30.663391260398978
maxi score, test score, baseline:  0.08708666666666653 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.49]
 [0.49]
 [0.49]
 [0.49]
 [0.49]
 [0.49]
 [0.49]] [[36.522]
 [36.522]
 [36.522]
 [36.522]
 [36.522]
 [36.522]
 [36.522]] [[2.157]
 [2.157]
 [2.157]
 [2.157]
 [2.157]
 [2.157]
 [2.157]]
printing an ep nov before normalisation:  0.0006431489384794986
printing an ep nov before normalisation:  36.79079026923374
printing an ep nov before normalisation:  50.85215091181382
maxi score, test score, baseline:  0.08708666666666653 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  0.08708666666666653 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.23423770304361824
actor:  1 policy actor:  1  step number:  52 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.91082122075764
maxi score, test score, baseline:  0.08708666666666653 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.73446962767104
printing an ep nov before normalisation:  47.928786520794695
printing an ep nov before normalisation:  31.389059908529013
printing an ep nov before normalisation:  40.39055400421827
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.502]
 [0.425]
 [0.425]
 [0.425]
 [0.402]
 [0.425]] [[54.64 ]
 [50.847]
 [54.64 ]
 [54.64 ]
 [54.64 ]
 [55.266]
 [54.64 ]] [[1.08 ]
 [1.088]
 [1.08 ]
 [1.08 ]
 [1.08 ]
 [1.069]
 [1.08 ]]
siam score:  -0.7677585
printing an ep nov before normalisation:  35.03259695214677
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  46.862162953344516
maxi score, test score, baseline:  0.08708666666666653 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.17592204630656
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  55 total reward:  0.4399999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.08708666666666653 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.26535763077149
maxi score, test score, baseline:  0.08708666666666653 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.38107892113156
maxi score, test score, baseline:  0.08708666666666653 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.464494655561396
maxi score, test score, baseline:  0.08708666666666653 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.134442501958546
actor:  0 policy actor:  0  step number:  42 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.08651333333333319 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.25136278766985
printing an ep nov before normalisation:  55.85695372740752
actor:  1 policy actor:  1  step number:  50 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.08651333333333319 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.94339630595949
actor:  1 policy actor:  1  step number:  52 total reward:  0.32666666666666666  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.08651333333333319 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]]
printing an ep nov before normalisation:  33.56163035846242
maxi score, test score, baseline:  0.08651333333333319 0.6893333333333335 0.6893333333333335
printing an ep nov before normalisation:  41.384153304431976
printing an ep nov before normalisation:  40.36312267478145
maxi score, test score, baseline:  0.08651333333333319 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.59 ]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]] [[31.175]
 [37.831]
 [31.175]
 [31.175]
 [31.175]
 [31.175]
 [31.175]] [[0.601]
 [0.784]
 [0.601]
 [0.601]
 [0.601]
 [0.601]
 [0.601]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.795]
 [0.697]
 [0.697]] [[46.283]
 [46.283]
 [46.283]
 [46.283]
 [48.256]
 [46.283]
 [46.283]] [[0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.795]
 [0.697]
 [0.697]]
maxi score, test score, baseline:  0.08651333333333319 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08651333333333319 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  42 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08604666666666654 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08604666666666654 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.33333333333333326  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.08604666666666654 0.6893333333333335 0.6893333333333335
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  0.08272666666666653 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.29740141718639
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  53 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 43.996627481153915
printing an ep nov before normalisation:  47.60468942946895
printing an ep nov before normalisation:  34.101653778165186
actor:  1 policy actor:  1  step number:  57 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  50.05532741546631
printing an ep nov before normalisation:  12.183703184127808
maxi score, test score, baseline:  0.08195333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.96320388675001
maxi score, test score, baseline:  0.08195333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.102]
 [0.155]
 [0.102]
 [0.033]
 [0.102]
 [0.102]
 [0.021]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.102]
 [0.155]
 [0.102]
 [0.033]
 [0.102]
 [0.102]
 [0.021]]
printing an ep nov before normalisation:  32.58573764191402
maxi score, test score, baseline:  0.07975333333333319 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.1306857477582
maxi score, test score, baseline:  0.07975333333333319 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  68 total reward:  0.03333333333333233  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  30.240602019854165
maxi score, test score, baseline:  0.07975333333333319 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.009]
 [ 0.078]
 [ 0.024]
 [ 0.045]
 [-0.004]
 [ 0.011]
 [ 0.152]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.009]
 [ 0.078]
 [ 0.024]
 [ 0.045]
 [-0.004]
 [ 0.011]
 [ 0.152]]
actor:  1 policy actor:  1  step number:  70 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.07975333333333319 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.944048557671806
maxi score, test score, baseline:  0.07975333333333319 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  44 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 57.65075910009742
printing an ep nov before normalisation:  57.64890165458342
printing an ep nov before normalisation:  37.74221711443677
maxi score, test score, baseline:  0.08256666666666652 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.03631782531738
actor:  1 policy actor:  1  step number:  48 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  44.60493340139078
Printing some Q and Qe and total Qs values:  [[0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]] [[69.319]
 [69.319]
 [69.319]
 [69.319]
 [69.319]
 [69.319]
 [69.319]] [[1.067]
 [1.067]
 [1.067]
 [1.067]
 [1.067]
 [1.067]
 [1.067]]
maxi score, test score, baseline:  0.08256666666666652 0.6893333333333335 0.6893333333333335
printing an ep nov before normalisation:  31.513845759487683
maxi score, test score, baseline:  0.08256666666666652 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  59 total reward:  0.13333333333333264  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.08255333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  0.3466666666666667  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  52.95318100671901
maxi score, test score, baseline:  0.08255333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08255333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08255333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08255333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.03717565986073
Printing some Q and Qe and total Qs values:  [[0.938]
 [1.064]
 [0.938]
 [0.938]
 [0.938]
 [0.938]
 [0.938]] [[46.521]
 [37.328]
 [46.521]
 [46.521]
 [46.521]
 [46.521]
 [46.521]] [[1.198]
 [1.233]
 [1.198]
 [1.198]
 [1.198]
 [1.198]
 [1.198]]
maxi score, test score, baseline:  0.08255333333333321 0.6893333333333335 0.6893333333333335
printing an ep nov before normalisation:  15.779979654640073
actor:  1 policy actor:  1  step number:  50 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([0.1563, 0.0055, 0.1384, 0.1943, 0.2138, 0.1397, 0.1520],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0248, 0.8317, 0.0199, 0.0359, 0.0098, 0.0127, 0.0652],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1547, 0.0054, 0.1810, 0.1507, 0.1995, 0.2072, 0.1015],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0989, 0.1567, 0.1241, 0.2471, 0.1137, 0.1279, 0.1316],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0919, 0.0122, 0.0683, 0.1486, 0.4873, 0.1101, 0.0815],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0693, 0.0029, 0.1301, 0.1621, 0.0925, 0.4608, 0.0822],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0624, 0.3106, 0.0915, 0.1798, 0.0753, 0.0829, 0.1975],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  17.28642987587961
printing an ep nov before normalisation:  10.762299193146498
printing an ep nov before normalisation:  50.497399339836285
actor:  1 policy actor:  1  step number:  59 total reward:  0.11999999999999933  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  45.563142928874846
maxi score, test score, baseline:  0.08255333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.4826, 0.0245, 0.0980, 0.1051, 0.1191, 0.0869, 0.0837],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0059, 0.9258, 0.0097, 0.0137, 0.0025, 0.0038, 0.0386],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1190, 0.0140, 0.3502, 0.1152, 0.1076, 0.1506, 0.1434],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0872, 0.0104, 0.0911, 0.4806, 0.1274, 0.1113, 0.0919],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1076, 0.0015, 0.0721, 0.1049, 0.5553, 0.0782, 0.0804],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1067, 0.0016, 0.1279, 0.1220, 0.1188, 0.4081, 0.1149],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1138, 0.0725, 0.1092, 0.1525, 0.1261, 0.1176, 0.3083],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.08255333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.58732175729284
siam score:  -0.76635116
actor:  0 policy actor:  0  step number:  47 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  46.64109895447886
maxi score, test score, baseline:  0.08297999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  0.56  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  46.94426453585271
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.389]
 [0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]] [[41.896]
 [47.789]
 [41.896]
 [41.896]
 [41.896]
 [41.896]
 [41.896]] [[1.585]
 [2.   ]
 [1.585]
 [1.585]
 [1.585]
 [1.585]
 [1.585]]
printing an ep nov before normalisation:  47.98984846929841
maxi score, test score, baseline:  0.08297999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.55326108061524
maxi score, test score, baseline:  0.08297999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08297999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.21680841187424
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  63 total reward:  0.346666666666666  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  77 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.82656760088728
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.324]
 [0.324]
 [0.208]
 [0.16 ]
 [0.086]
 [0.253]] [[56.699]
 [56.699]
 [56.699]
 [64.168]
 [63.665]
 [62.39 ]
 [57.656]] [[1.444]
 [1.444]
 [1.444]
 [1.593]
 [1.527]
 [1.407]
 [1.407]]
printing an ep nov before normalisation:  41.654705352880455
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.752]
 [0.659]
 [0.717]
 [0.726]
 [0.698]
 [0.767]] [[42.461]
 [50.139]
 [45.127]
 [43.863]
 [43.617]
 [45.105]
 [45.702]] [[1.504]
 [1.752]
 [1.489]
 [1.504]
 [1.505]
 [1.528]
 [1.617]]
printing an ep nov before normalisation:  46.54695739234938
actor:  1 policy actor:  1  step number:  56 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  1.0
siam score:  -0.7796214
maxi score, test score, baseline:  0.08297999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08297999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08297999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08297999999999987 0.6893333333333335 0.6893333333333335
actor:  1 policy actor:  1  step number:  54 total reward:  0.42000000000000004  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
siam score:  -0.7784258
maxi score, test score, baseline:  0.08297999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.38  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08297999999999987 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  0.08297999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08297999999999987 0.6893333333333335 0.6893333333333335
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  61 total reward:  0.06666666666666621  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.08511333333333321 0.6893333333333335 0.6893333333333335
printing an ep nov before normalisation:  0.004859263117964474
actor:  1 policy actor:  1  step number:  46 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[-0.017]
 [ 0.073]
 [-0.017]
 [ 0.024]
 [-0.011]
 [ 0.004]
 [ 0.054]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.017]
 [ 0.073]
 [-0.017]
 [ 0.024]
 [-0.011]
 [ 0.004]
 [ 0.054]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actions average: 
K:  1  action  0 :  tensor([0.4180, 0.0091, 0.1121, 0.1143, 0.1528, 0.0810, 0.1128],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0055, 0.9293, 0.0046, 0.0038, 0.0024, 0.0018, 0.0527],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1293, 0.0270, 0.2832, 0.1310, 0.1096, 0.2212, 0.0987],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1432, 0.0122, 0.1319, 0.3239, 0.1218, 0.1140, 0.1531],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1163, 0.1104, 0.0534, 0.0615, 0.5518, 0.0552, 0.0514],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0915, 0.0037, 0.1325, 0.1054, 0.0859, 0.4796, 0.1015],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1126, 0.1937, 0.1091, 0.1378, 0.0994, 0.1008, 0.2465],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.08511333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08511333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08511333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.88671221440083
actor:  1 policy actor:  1  step number:  48 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  33.04219666651748
maxi score, test score, baseline:  0.08511333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08511333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.681]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]] [[49.461]
 [45.756]
 [49.461]
 [49.461]
 [49.461]
 [49.461]
 [49.461]] [[0.654]
 [0.681]
 [0.654]
 [0.654]
 [0.654]
 [0.654]
 [0.654]]
printing an ep nov before normalisation:  40.64120933878782
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.554]
 [0.452]
 [0.533]
 [0.54 ]
 [0.514]
 [0.462]] [[39.385]
 [33.78 ]
 [39.385]
 [39.293]
 [39.058]
 [39.565]
 [37.804]] [[2.436]
 [2.042]
 [2.436]
 [2.509]
 [2.495]
 [2.514]
 [2.307]]
printing an ep nov before normalisation:  43.17007558364999
printing an ep nov before normalisation:  39.45196757043685
printing an ep nov before normalisation:  45.525123205596074
maxi score, test score, baseline:  0.08511333333333321 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  0.08511333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.08511333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08511333333333321 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.6551718711853
actor:  0 policy actor:  0  step number:  45 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  59 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  64 total reward:  0.20666666666666633  reward:  1.0 rdn_beta:  2.0
siam score:  -0.7686873
maxi score, test score, baseline:  0.08523333333333322 0.6893333333333335 0.6893333333333335
printing an ep nov before normalisation:  50.49360966692373
maxi score, test score, baseline:  0.08523333333333322 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.881884276390814
siam score:  -0.76866883
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  56 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.08523333333333322 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.014884193736293128
printing an ep nov before normalisation:  58.098384894080866
printing an ep nov before normalisation:  47.64298915863037
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.08523333333333322 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.736]
 [0.707]
 [0.67 ]
 [0.692]
 [0.685]
 [0.594]] [[42.292]
 [44.044]
 [46.94 ]
 [47.242]
 [46.283]
 [46.862]
 [48.344]] [[1.654]
 [1.697]
 [1.787]
 [1.763]
 [1.745]
 [1.762]
 [1.731]]
maxi score, test score, baseline:  0.08523333333333322 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.565911045508926
actor:  1 policy actor:  1  step number:  52 total reward:  0.4066666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.08523333333333322 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.1733333333333329  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.08523333333333322 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.024188498998682917
actor:  1 policy actor:  1  step number:  67 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  52.79354581493607
printing an ep nov before normalisation:  49.3943047473792
actor:  1 policy actor:  1  step number:  52 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 59.90207469785828
maxi score, test score, baseline:  0.08523333333333322 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.124]
 [0.151]
 [0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]] [[41.529]
 [48.061]
 [41.529]
 [41.529]
 [41.529]
 [41.529]
 [41.529]] [[1.074]
 [1.403]
 [1.074]
 [1.074]
 [1.074]
 [1.074]
 [1.074]]
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.867]
 [0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]] [[49.129]
 [47.154]
 [49.129]
 [49.129]
 [49.129]
 [49.129]
 [49.129]] [[2.002]
 [2.106]
 [2.002]
 [2.002]
 [2.002]
 [2.002]
 [2.002]]
printing an ep nov before normalisation:  50.21172144766533
maxi score, test score, baseline:  0.08523333333333322 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08523333333333322 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  69 total reward:  0.09333333333333238  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  59 total reward:  0.30666666666666664  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  70 total reward:  0.13999999999999901  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  32.80971050262451
siam score:  -0.7591574
actor:  1 policy actor:  1  step number:  39 total reward:  0.5466666666666667  reward:  1.0 rdn_beta:  2.0
siam score:  -0.7622005
maxi score, test score, baseline:  0.08523333333333322 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.26683072714886
printing an ep nov before normalisation:  39.984315580414105
maxi score, test score, baseline:  0.08523333333333322 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  0.08523333333333322 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08523333333333322 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08523333333333322 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08523333333333322 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.389]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.378]
 [0.389]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]]
printing an ep nov before normalisation:  54.61269165270792
printing an ep nov before normalisation:  51.959143791557764
Printing some Q and Qe and total Qs values:  [[0.061]
 [0.107]
 [0.061]
 [0.092]
 [0.093]
 [0.092]
 [0.061]] [[51.079]
 [39.794]
 [51.079]
 [50.631]
 [50.336]
 [50.283]
 [51.079]] [[1.794]
 [1.251]
 [1.794]
 [1.801]
 [1.787]
 [1.783]
 [1.794]]
printing an ep nov before normalisation:  45.133395765433654
maxi score, test score, baseline:  0.08519333333333319 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  8.304115794999234e-06
maxi score, test score, baseline:  0.08519333333333319 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  34.990290649242326
printing an ep nov before normalisation:  45.99924598142537
siam score:  -0.7630078
Printing some Q and Qe and total Qs values:  [[0.755]
 [0.877]
 [0.755]
 [0.766]
 [0.755]
 [0.755]
 [0.762]] [[34.707]
 [34.143]
 [34.707]
 [35.459]
 [34.707]
 [34.707]
 [32.916]] [[0.755]
 [0.877]
 [0.755]
 [0.766]
 [0.755]
 [0.755]
 [0.762]]
printing an ep nov before normalisation:  32.41126298904419
maxi score, test score, baseline:  0.08519333333333319 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.15333333333333277  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  44 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  47.07640003316341
printing an ep nov before normalisation:  35.79730663174736
printing an ep nov before normalisation:  39.52489403539448
maxi score, test score, baseline:  0.08581999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.385593249110464
maxi score, test score, baseline:  0.08581999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  0.1933333333333327  reward:  1.0 rdn_beta:  1.667
UNIT TEST: sample policy line 217 mcts : [0.204 0.694 0.02  0.02  0.02  0.02  0.02 ]
maxi score, test score, baseline:  0.08581999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.221]
 [0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]] [[48.453]
 [33.585]
 [48.453]
 [48.453]
 [48.453]
 [48.453]
 [48.453]] [[0.439]
 [0.362]
 [0.439]
 [0.439]
 [0.439]
 [0.439]
 [0.439]]
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.76 ]
 [0.322]
 [0.719]
 [0.719]
 [0.719]
 [0.719]] [[33.546]
 [33.449]
 [28.564]
 [33.546]
 [33.546]
 [33.546]
 [33.546]] [[0.719]
 [0.76 ]
 [0.322]
 [0.719]
 [0.719]
 [0.719]
 [0.719]]
maxi score, test score, baseline:  0.08581999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.08581999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.15349398854334
maxi score, test score, baseline:  0.08581999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.465026665056556
actor:  1 policy actor:  1  step number:  56 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  34 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  67 total reward:  0.1999999999999994  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.12 ]
 [0.222]
 [0.207]
 [0.203]
 [0.179]
 [0.263]
 [0.186]] [[42.534]
 [41.394]
 [40.055]
 [42.833]
 [43.528]
 [40.878]
 [42.098]] [[0.925]
 [0.984]
 [0.918]
 [1.019]
 [1.021]
 [1.005]
 [0.975]]
maxi score, test score, baseline:  0.08887333333333322 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.43979260726907
actor:  0 policy actor:  0  step number:  44 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09157999999999987 0.6893333333333335 0.6893333333333335
actor:  1 policy actor:  1  step number:  56 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  39.12559072470876
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 42.55817051100634
printing an ep nov before normalisation:  43.200998836093476
printing an ep nov before normalisation:  57.17591610769993
printing an ep nov before normalisation:  36.751508246785335
line 256 mcts: sample exp_bonus 37.146568039162275
maxi score, test score, baseline:  0.09157999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.551]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]] [[48.131]
 [45.815]
 [48.131]
 [48.131]
 [48.131]
 [48.131]
 [48.131]] [[1.498]
 [1.466]
 [1.498]
 [1.498]
 [1.498]
 [1.498]
 [1.498]]
printing an ep nov before normalisation:  28.57100754653286
actor:  1 policy actor:  1  step number:  61 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  50.63768758565392
maxi score, test score, baseline:  0.09157999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09157999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.90030695226933
printing an ep nov before normalisation:  55.154896097820654
maxi score, test score, baseline:  0.09157999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.165]
 [0.195]
 [0.182]
 [0.187]
 [0.19 ]
 [0.189]
 [0.196]] [[28.278]
 [31.545]
 [18.344]
 [18.197]
 [18.218]
 [19.452]
 [20.142]] [[1.035]
 [1.212]
 [0.608]
 [0.606]
 [0.61 ]
 [0.664]
 [0.702]]
actor:  0 policy actor:  0  step number:  56 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.955]
 [0.971]
 [0.955]
 [0.955]
 [0.955]
 [0.955]
 [0.97 ]] [[35.076]
 [34.44 ]
 [35.076]
 [35.076]
 [35.076]
 [35.076]
 [33.124]] [[0.955]
 [0.971]
 [0.955]
 [0.955]
 [0.955]
 [0.955]
 [0.97 ]]
printing an ep nov before normalisation:  27.52218566039709
printing an ep nov before normalisation:  41.38390970993768
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  36.50792006112081
actor:  0 policy actor:  0  step number:  46 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  19.570851020633675
maxi score, test score, baseline:  0.09693999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  78 total reward:  0.019999999999998685  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09693999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.875]
 [0.878]
 [0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]] [[46.204]
 [46.573]
 [46.204]
 [46.204]
 [46.204]
 [46.204]
 [46.204]] [[0.875]
 [0.878]
 [0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]]
maxi score, test score, baseline:  0.09693999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.27938712887445
printing an ep nov before normalisation:  39.03984069824219
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.282]
 [0.282]
 [0.282]
 [0.273]
 [0.282]
 [0.282]] [[46.298]
 [46.298]
 [46.298]
 [46.298]
 [48.069]
 [46.298]
 [46.298]] [[1.101]
 [1.101]
 [1.101]
 [1.101]
 [1.144]
 [1.101]
 [1.101]]
maxi score, test score, baseline:  0.09693999999999987 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.351816177368164
printing an ep nov before normalisation:  26.697588803757196
siam score:  -0.7561117
actor:  0 policy actor:  0  step number:  47 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.63987636566162
using explorer policy with actor:  1
maxi score, test score, baseline:  0.09957999999999986 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  0.09957999999999986 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.265800777397274
printing an ep nov before normalisation:  63.70154291718821
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.806]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.691]] [[51.528]
 [41.096]
 [50.302]
 [49.807]
 [49.699]
 [50.206]
 [47.596]] [[0.682]
 [0.806]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.691]]
actor:  1 policy actor:  1  step number:  75 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.09957999999999986 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  37 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  56 total reward:  0.3800000000000001  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  35.06349937395659
siam score:  -0.7548352
maxi score, test score, baseline:  0.10000666666666652 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.188]
 [0.08 ]
 [0.252]] [[41.239]
 [41.239]
 [41.239]
 [41.239]
 [53.038]
 [45.983]
 [41.239]] [[0.946]
 [0.946]
 [0.946]
 [0.946]
 [1.269]
 [0.929]
 [0.946]]
siam score:  -0.76032495
maxi score, test score, baseline:  0.10000666666666652 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  0.10000666666666652 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.08 ]
 [-0.113]
 [-0.048]
 [-0.049]
 [-0.08 ]
 [-0.043]
 [-0.087]] [[14.884]
 [17.102]
 [12.391]
 [12.321]
 [14.884]
 [12.105]
 [19.325]] [[0.489]
 [0.54 ]
 [0.425]
 [0.421]
 [0.489]
 [0.419]
 [0.65 ]]
maxi score, test score, baseline:  0.10000666666666652 0.6893333333333335 0.6893333333333335
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.2809508537216
maxi score, test score, baseline:  0.10000666666666652 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  35.274560342502966
printing an ep nov before normalisation:  29.514832496643066
actor:  1 policy actor:  1  step number:  57 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10000666666666652 0.6893333333333335 0.6893333333333335
siam score:  -0.7646646
printing an ep nov before normalisation:  43.56672568252751
maxi score, test score, baseline:  0.10000666666666652 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.024]
 [1.024]
 [1.024]
 [1.024]
 [1.024]
 [1.024]
 [1.024]] [[46.235]
 [46.235]
 [46.235]
 [46.235]
 [46.235]
 [46.235]
 [46.235]] [[1.024]
 [1.024]
 [1.024]
 [1.024]
 [1.024]
 [1.024]
 [1.024]]
actor:  0 policy actor:  0  step number:  35 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1004333333333332 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.08928152510239
maxi score, test score, baseline:  0.1004333333333332 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [0.14 ]
 [0.099]
 [0.095]
 [0.08 ]
 [0.075]
 [0.079]] [[35.72 ]
 [28.57 ]
 [36.831]
 [37.288]
 [37.302]
 [38.089]
 [37.705]] [[1.071]
 [0.759]
 [1.125]
 [1.144]
 [1.129]
 [1.164]
 [1.149]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.13558588633277
printing an ep nov before normalisation:  32.46511936187744
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  0.5800000000000003  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  64 total reward:  0.11333333333333273  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  0  action  0 :  tensor([0.5185, 0.0384, 0.0842, 0.0858, 0.0943, 0.0820, 0.0968],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0072, 0.9608, 0.0053, 0.0064, 0.0040, 0.0033, 0.0130],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0738, 0.0278, 0.5649, 0.0752, 0.0762, 0.1031, 0.0791],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0874, 0.1866, 0.0631, 0.4067, 0.0698, 0.0609, 0.1255],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1315, 0.0049, 0.0594, 0.0569, 0.6286, 0.0600, 0.0587],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1063, 0.0069, 0.1294, 0.0962, 0.0896, 0.4847, 0.0870],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1636, 0.1212, 0.1459, 0.1266, 0.1292, 0.1313, 0.1821],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  34.04552217973786
actions average: 
K:  3  action  0 :  tensor([0.2115, 0.0072, 0.1716, 0.1478, 0.1537, 0.1485, 0.1597],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0065, 0.9457, 0.0039, 0.0190, 0.0016, 0.0018, 0.0214],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0665, 0.1205, 0.5690, 0.0342, 0.0339, 0.1082, 0.0677],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1425, 0.0429, 0.0897, 0.1829, 0.3450, 0.0826, 0.1143],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1181, 0.0010, 0.0593, 0.0526, 0.6527, 0.0692, 0.0471],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1011, 0.0029, 0.1520, 0.0944, 0.1084, 0.4459, 0.0953],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0804, 0.3341, 0.0549, 0.1115, 0.1026, 0.0488, 0.2677],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1004333333333332 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.242]
 [0.182]
 [0.182]
 [0.182]
 [0.182]
 [0.182]] [[28.834]
 [31.648]
 [28.834]
 [28.834]
 [28.834]
 [28.834]
 [28.834]] [[1.862]
 [2.242]
 [1.862]
 [1.862]
 [1.862]
 [1.862]
 [1.862]]
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.211]
 [0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.225]] [[42.942]
 [53.121]
 [42.942]
 [42.942]
 [42.942]
 [42.942]
 [47.538]] [[1.259]
 [1.552]
 [1.259]
 [1.259]
 [1.259]
 [1.259]
 [1.374]]
maxi score, test score, baseline:  0.1004333333333332 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1004333333333332 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1004333333333332 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.968144861977095
actions average: 
K:  4  action  0 :  tensor([0.3416, 0.1176, 0.0901, 0.1086, 0.1461, 0.0832, 0.1127],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0129, 0.8921, 0.0089, 0.0131, 0.0081, 0.0063, 0.0585],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1566, 0.0481, 0.1346, 0.1979, 0.1940, 0.1621, 0.1067],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1126, 0.0263, 0.1167, 0.3210, 0.1522, 0.1543, 0.1169],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1440, 0.0093, 0.1039, 0.2220, 0.2925, 0.1117, 0.1166],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.2105, 0.0122, 0.1754, 0.1415, 0.1550, 0.1555, 0.1499],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1576, 0.0304, 0.1343, 0.1924, 0.1365, 0.1177, 0.2311],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.09793999999999986 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  2.0
siam score:  -0.75413436
maxi score, test score, baseline:  0.09793999999999986 0.6893333333333335 0.6893333333333335
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.499]
 [0.398]
 [0.398]
 [0.398]
 [0.398]
 [0.398]] [[29.712]
 [43.635]
 [29.712]
 [29.712]
 [29.712]
 [29.712]
 [29.712]] [[0.845]
 [1.253]
 [0.845]
 [0.845]
 [0.845]
 [0.845]
 [0.845]]
maxi score, test score, baseline:  0.09793999999999986 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.396]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]] [[37.985]
 [45.214]
 [37.985]
 [37.985]
 [37.985]
 [37.985]
 [37.985]] [[1.11 ]
 [1.473]
 [1.11 ]
 [1.11 ]
 [1.11 ]
 [1.11 ]
 [1.11 ]]
printing an ep nov before normalisation:  42.19602624153813
maxi score, test score, baseline:  0.09793999999999986 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  67 total reward:  0.15999999999999914  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09793999999999986 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.267]
 [0.389]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]] [[29.221]
 [48.514]
 [29.221]
 [29.221]
 [29.221]
 [29.221]
 [29.221]] [[0.463]
 [0.838]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]]
Printing some Q and Qe and total Qs values:  [[0.151]
 [0.236]
 [0.151]
 [0.151]
 [0.151]
 [0.151]
 [0.151]] [[34.842]
 [41.186]
 [34.842]
 [34.842]
 [34.842]
 [34.842]
 [34.842]] [[0.641]
 [0.89 ]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]]
maxi score, test score, baseline:  0.09793999999999986 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.08912997007855
printing an ep nov before normalisation:  54.72079903171865
printing an ep nov before normalisation:  46.76545396505389
printing an ep nov before normalisation:  44.80584598812426
printing an ep nov before normalisation:  44.09569827342668
printing an ep nov before normalisation:  52.9271742383105
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.337]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.519]] [[40.627]
 [45.744]
 [43.372]
 [43.372]
 [43.372]
 [43.372]
 [42.893]] [[1.237]
 [1.196]
 [1.2  ]
 [1.2  ]
 [1.2  ]
 [1.2  ]
 [1.285]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  0.667
siam score:  -0.75549155
siam score:  -0.7571045
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.33999999999999986  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09793999999999986 0.6893333333333335 0.6893333333333335
printing an ep nov before normalisation:  44.22182555502285
printing an ep nov before normalisation:  41.74461364746094
maxi score, test score, baseline:  0.09793999999999986 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.09793999999999986 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.11333333333333273  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.09793999999999986 0.6893333333333335 0.6893333333333335
actions average: 
K:  4  action  0 :  tensor([0.1539, 0.0137, 0.1320, 0.2047, 0.1360, 0.1904, 0.1693],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0169, 0.9106, 0.0142, 0.0147, 0.0065, 0.0076, 0.0296],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0881, 0.0488, 0.5869, 0.0463, 0.0389, 0.0774, 0.1135],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0741, 0.0506, 0.0895, 0.4990, 0.0631, 0.1155, 0.1082],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1443, 0.0196, 0.0872, 0.1119, 0.4407, 0.1157, 0.0807],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1273, 0.0799, 0.1417, 0.1757, 0.1258, 0.2048, 0.1447],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1935, 0.0339, 0.1242, 0.1921, 0.1502, 0.1425, 0.1636],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.09793999999999986 0.6893333333333335 0.6893333333333335
using explorer policy with actor:  1
printing an ep nov before normalisation:  22.389658560012485
actor:  1 policy actor:  1  step number:  62 total reward:  0.16666666666666585  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  52 total reward:  0.3399999999999994  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  53.85969607782177
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.671]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]] [[32.135]
 [37.598]
 [32.135]
 [32.135]
 [32.135]
 [32.135]
 [32.135]] [[0.863]
 [1.231]
 [0.863]
 [0.863]
 [0.863]
 [0.863]
 [0.863]]
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([0.3092, 0.0305, 0.0994, 0.1341, 0.1921, 0.1262, 0.1085],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0024, 0.9576, 0.0034, 0.0051, 0.0014, 0.0016, 0.0284],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1078, 0.0481, 0.3279, 0.1276, 0.1083, 0.1537, 0.1265],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0998, 0.0230, 0.1009, 0.4327, 0.1145, 0.1326, 0.0965],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0984, 0.0651, 0.0772, 0.1296, 0.4751, 0.0765, 0.0782],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0999, 0.0766, 0.2071, 0.1750, 0.1357, 0.1613, 0.1444],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0889, 0.0691, 0.1021, 0.1738, 0.0866, 0.0922, 0.3873],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  43.96981030206957
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.627]
 [0.618]
 [0.623]
 [0.623]
 [0.623]
 [0.634]] [[38.266]
 [45.94 ]
 [44.569]
 [38.266]
 [38.266]
 [38.266]
 [46.093]] [[1.009]
 [1.164]
 [1.127]
 [1.009]
 [1.009]
 [1.009]
 [1.174]]
Printing some Q and Qe and total Qs values:  [[-0.232]
 [-0.   ]
 [-0.232]
 [-0.   ]
 [-0.232]
 [-0.   ]
 [-0.232]] [[38.678]
 [ 0.025]
 [38.678]
 [ 0.021]
 [38.678]
 [ 0.021]
 [38.678]] [[0.849]
 [0.001]
 [0.849]
 [0.001]
 [0.849]
 [0.001]
 [0.849]]
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.722]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]] [[35.622]
 [42.52 ]
 [35.622]
 [35.622]
 [35.622]
 [35.622]
 [35.622]] [[1.037]
 [1.254]
 [1.037]
 [1.037]
 [1.037]
 [1.037]
 [1.037]]
maxi score, test score, baseline:  0.09793999999999986 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.41 ]
 [0.268]
 [0.268]
 [0.193]
 [0.268]
 [0.25 ]] [[43.441]
 [45.323]
 [43.441]
 [43.441]
 [46.28 ]
 [43.441]
 [46.318]] [[0.427]
 [0.58 ]
 [0.427]
 [0.427]
 [0.369]
 [0.427]
 [0.427]]
maxi score, test score, baseline:  0.09793999999999986 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.22300642092506
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.651]
 [0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]] [[35.031]
 [33.604]
 [35.031]
 [35.031]
 [35.031]
 [35.031]
 [35.031]] [[0.687]
 [0.868]
 [0.687]
 [0.687]
 [0.687]
 [0.687]
 [0.687]]
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]] [[40.518]
 [40.518]
 [40.518]
 [40.518]
 [40.518]
 [40.518]
 [40.518]] [[1.679]
 [1.679]
 [1.679]
 [1.679]
 [1.679]
 [1.679]
 [1.679]]
maxi score, test score, baseline:  0.09793999999999986 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.516]
 [0.666]
 [0.534]
 [0.534]
 [0.527]
 [0.534]
 [0.534]] [[39.408]
 [39.335]
 [40.521]
 [40.521]
 [40.93 ]
 [40.521]
 [40.521]] [[0.734]
 [0.883]
 [0.764]
 [0.764]
 [0.762]
 [0.764]
 [0.764]]
printing an ep nov before normalisation:  67.57213856963321
maxi score, test score, baseline:  0.09793999999999986 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.24388660669137
actor:  1 policy actor:  1  step number:  44 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
siam score:  -0.7631977
maxi score, test score, baseline:  0.09793999999999986 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.00295162200928
maxi score, test score, baseline:  0.09793999999999986 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.568]
 [0.554]
 [0.528]
 [0.528]
 [0.555]
 [0.543]] [[33.462]
 [40.21 ]
 [36.294]
 [36.313]
 [36.313]
 [33.537]
 [37.272]] [[0.745]
 [0.836]
 [0.778]
 [0.752]
 [0.752]
 [0.748]
 [0.778]]
actor:  1 policy actor:  1  step number:  68 total reward:  0.2599999999999989  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  43.86628533269257
Printing some Q and Qe and total Qs values:  [[0.071]
 [0.077]
 [0.064]
 [0.064]
 [0.066]
 [0.066]
 [0.065]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.071]
 [0.077]
 [0.064]
 [0.064]
 [0.066]
 [0.066]
 [0.065]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4333333333333329  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.486]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]] [[31.769]
 [36.711]
 [31.769]
 [31.769]
 [31.769]
 [31.769]
 [31.769]] [[0.429]
 [0.486]
 [0.429]
 [0.429]
 [0.429]
 [0.429]
 [0.429]]
actions average: 
K:  3  action  0 :  tensor([0.4499, 0.0015, 0.1106, 0.0976, 0.1020, 0.0982, 0.1402],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0100, 0.9365, 0.0080, 0.0088, 0.0063, 0.0049, 0.0255],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1138, 0.0468, 0.3989, 0.0966, 0.1047, 0.1393, 0.0999],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0728, 0.1512, 0.0620, 0.3834, 0.1326, 0.0643, 0.1337],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1222, 0.0093, 0.1036, 0.1265, 0.4167, 0.1184, 0.1033],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1317, 0.0167, 0.1510, 0.1305, 0.1406, 0.3157, 0.1139],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1127, 0.1802, 0.1122, 0.1271, 0.1090, 0.0976, 0.2613],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  48.51901968247184
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.349]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]] [[39.624]
 [36.237]
 [39.624]
 [39.624]
 [39.624]
 [39.624]
 [39.624]] [[0.661]
 [0.635]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]]
printing an ep nov before normalisation:  36.818251609802246
printing an ep nov before normalisation:  44.89247200170649
printing an ep nov before normalisation:  43.33088078458083
printing an ep nov before normalisation:  53.10005039257443
maxi score, test score, baseline:  0.09793999999999986 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.77795157751248
printing an ep nov before normalisation:  35.31185466320164
maxi score, test score, baseline:  0.09793999999999986 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  36 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.09824666666666654 0.6893333333333335 0.6893333333333335
actions average: 
K:  3  action  0 :  tensor([0.2680, 0.1353, 0.1046, 0.1126, 0.1340, 0.0991, 0.1464],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0126, 0.9063, 0.0129, 0.0151, 0.0086, 0.0077, 0.0369],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1229, 0.0371, 0.3484, 0.1105, 0.1142, 0.1300, 0.1369],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1105, 0.0211, 0.0894, 0.3491, 0.1580, 0.1131, 0.1588],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1041, 0.0139, 0.0129, 0.0129, 0.8320, 0.0166, 0.0076],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0985, 0.0076, 0.1592, 0.1021, 0.1004, 0.4275, 0.1047],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1286, 0.0681, 0.1380, 0.1463, 0.1473, 0.1386, 0.2332],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.09824666666666654 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.814094021419336
maxi score, test score, baseline:  0.09824666666666654 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  64 total reward:  0.0599999999999995  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  42.75856426043092
maxi score, test score, baseline:  0.10036666666666653 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.68 ]
 [0.595]
 [0.621]
 [0.619]
 [0.619]
 [0.6  ]] [[40.51 ]
 [36.09 ]
 [40.603]
 [41.563]
 [40.852]
 [40.852]
 [40.667]] [[2.454]
 [2.158]
 [2.485]
 [2.599]
 [2.531]
 [2.531]
 [2.496]]
printing an ep nov before normalisation:  41.92758560180664
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
line 256 mcts: sample exp_bonus 33.87131316452993
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  56 total reward:  0.2599999999999999  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  53 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10036666666666653 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.99344259470987
siam score:  -0.7565634
actor:  1 policy actor:  1  step number:  42 total reward:  0.54  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.10036666666666653 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.55 ]
 [0.383]
 [0.397]
 [0.38 ]
 [0.34 ]
 [0.384]] [[30.067]
 [35.56 ]
 [31.249]
 [32.011]
 [31.466]
 [31.049]
 [29.552]] [[0.877]
 [1.21 ]
 [0.907]
 [0.944]
 [0.911]
 [0.857]
 [0.854]]
printing an ep nov before normalisation:  47.82777238840319
printing an ep nov before normalisation:  26.002354621887207
maxi score, test score, baseline:  0.10036666666666653 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.102]
 [-0.24 ]
 [-0.242]
 [-0.237]
 [-0.234]
 [-0.24 ]
 [ 0.102]] [[42.404]
 [50.93 ]
 [42.273]
 [34.395]
 [36.738]
 [37.019]
 [42.404]] [[1.767]
 [1.76 ]
 [1.418]
 [1.114]
 [1.209]
 [1.214]
 [1.767]]
maxi score, test score, baseline:  0.10036666666666653 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.10036666666666653 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.10036666666666653 0.6893333333333335 0.6893333333333335
maxi score, test score, baseline:  0.10036666666666653 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.1999999999999993  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.10036666666666653 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.01 ]
 [-0.004]
 [-0.026]
 [-0.01 ]
 [-0.024]
 [-0.01 ]
 [-0.01 ]] [[36.124]
 [48.673]
 [45.177]
 [36.124]
 [44.288]
 [36.124]
 [36.124]] [[0.898]
 [1.519]
 [1.326]
 [0.898]
 [1.284]
 [0.898]
 [0.898]]
maxi score, test score, baseline:  0.10036666666666653 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.213]
 [0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[28.  ]
 [42.51]
 [28.  ]
 [28.  ]
 [28.  ]
 [28.  ]
 [28.  ]] [[0.244]
 [0.693]
 [0.244]
 [0.244]
 [0.244]
 [0.244]
 [0.244]]
actor:  1 policy actor:  1  step number:  64 total reward:  0.23333333333333306  reward:  1.0 rdn_beta:  0.667
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.746]
 [0.634]
 [0.66 ]
 [0.653]
 [0.635]
 [0.674]] [[30.308]
 [34.512]
 [33.86 ]
 [36.172]
 [38.364]
 [38.75 ]
 [34.131]] [[0.653]
 [0.746]
 [0.634]
 [0.66 ]
 [0.653]
 [0.635]
 [0.674]]
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.499]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]] [[28.393]
 [33.368]
 [28.393]
 [28.393]
 [28.393]
 [28.393]
 [28.393]] [[0.372]
 [0.499]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]]
printing an ep nov before normalisation:  34.971396128336586
printing an ep nov before normalisation:  41.49046963457719
printing an ep nov before normalisation:  33.90706150027441
maxi score, test score, baseline:  0.10036666666666653 0.6893333333333335 0.6893333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.730977946596234
printing an ep nov before normalisation:  46.258593932808616
printing an ep nov before normalisation:  43.960461622818116
printing an ep nov before normalisation:  35.89491605758667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  37.80078794428776
printing an ep nov before normalisation:  43.390532512730594
printing an ep nov before normalisation:  38.10145129073237
printing an ep nov before normalisation:  42.50331561914959
Printing some Q and Qe and total Qs values:  [[1.027]
 [1.034]
 [1.027]
 [1.027]
 [1.027]
 [1.027]
 [1.027]] [[42.452]
 [42.101]
 [42.452]
 [42.452]
 [42.452]
 [42.452]
 [42.452]] [[1.027]
 [1.034]
 [1.027]
 [1.027]
 [1.027]
 [1.027]
 [1.027]]
printing an ep nov before normalisation:  33.57616966733999
printing an ep nov before normalisation:  44.02479141129546
printing an ep nov before normalisation:  42.4687385559082
Printing some Q and Qe and total Qs values:  [[0.673]
 [0.789]
 [0.719]
 [0.704]
 [0.702]
 [0.673]
 [0.673]] [[35.963]
 [33.977]
 [38.582]
 [42.744]
 [42.281]
 [35.963]
 [42.469]] [[0.673]
 [0.789]
 [0.719]
 [0.704]
 [0.702]
 [0.673]
 [0.673]]
line 256 mcts: sample exp_bonus 37.19109448275918
printing an ep nov before normalisation:  0.01734728692099452
printing an ep nov before normalisation:  31.339831352233887
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  28 total reward:  0.6333333333333334  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  20.34285647730627
actor:  1 policy actor:  1  step number:  57 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.14224666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]]
maxi score, test score, baseline:  0.14224666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.454]
 [0.415]
 [0.414]
 [0.414]
 [0.447]
 [0.414]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.423]
 [0.454]
 [0.415]
 [0.414]
 [0.414]
 [0.447]
 [0.414]]
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.057]
 [0.07 ]
 [0.041]
 [0.04 ]
 [0.048]
 [0.054]] [[42.624]
 [42.086]
 [38.748]
 [41.439]
 [41.7  ]
 [42.372]
 [42.023]] [[1.384]
 [1.361]
 [1.171]
 [1.306]
 [1.32 ]
 [1.369]
 [1.354]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.2933333333333331  reward:  1.0 rdn_beta:  1.0
siam score:  -0.7518907
maxi score, test score, baseline:  0.14224666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.50195935471139
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  46.97641271336879
maxi score, test score, baseline:  0.14224666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14224666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.259]
 [0.501]
 [0.259]
 [0.259]
 [0.259]
 [0.259]
 [0.259]] [[33.581]
 [40.089]
 [33.581]
 [33.581]
 [33.581]
 [33.581]
 [33.581]] [[0.799]
 [1.274]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]]
maxi score, test score, baseline:  0.14224666666666652 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  42.185110698763545
printing an ep nov before normalisation:  42.80726454597252
actor:  0 policy actor:  0  step number:  51 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.14464666666666656 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.78209415018405
maxi score, test score, baseline:  0.14464666666666656 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  32.93399487438258
printing an ep nov before normalisation:  49.49831298566103
actor:  1 policy actor:  1  step number:  56 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  57 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  39.67124196670009
Printing some Q and Qe and total Qs values:  [[-0.026]
 [ 0.063]
 [-0.01 ]
 [ 0.032]
 [-0.017]
 [ 0.042]
 [ 0.063]] [[35.328]
 [29.399]
 [38.776]
 [34.531]
 [42.327]
 [35.004]
 [35.382]] [[0.206]
 [0.243]
 [0.252]
 [0.257]
 [0.277]
 [0.271]
 [0.295]]
printing an ep nov before normalisation:  43.990513945537145
Printing some Q and Qe and total Qs values:  [[0.314]
 [0.556]
 [0.431]
 [0.345]
 [0.431]
 [0.431]
 [0.336]] [[33.324]
 [36.432]
 [36.713]
 [32.697]
 [36.713]
 [36.713]
 [30.739]] [[0.504]
 [0.778]
 [0.656]
 [0.53 ]
 [0.656]
 [0.656]
 [0.5  ]]
maxi score, test score, baseline:  0.14464666666666656 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14464666666666656 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14464666666666656 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.171112112164785
printing an ep nov before normalisation:  39.62013483047485
siam score:  -0.7581292
maxi score, test score, baseline:  0.14464666666666656 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14464666666666656 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.093]
 [-0.093]
 [-0.093]
 [-0.093]
 [-0.093]
 [-0.093]
 [-0.093]] [[52.378]
 [52.378]
 [52.378]
 [52.378]
 [52.378]
 [52.378]
 [52.378]] [[0.11]
 [0.11]
 [0.11]
 [0.11]
 [0.11]
 [0.11]
 [0.11]]
maxi score, test score, baseline:  0.14464666666666656 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.979333097822014
actor:  1 policy actor:  1  step number:  58 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  55.82115151655079
maxi score, test score, baseline:  0.14464666666666656 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  77.69266215565241
maxi score, test score, baseline:  0.14464666666666656 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.443745631791614
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.302]
 [0.453]
 [0.453]] [[36.533]
 [36.533]
 [36.533]
 [36.533]
 [42.144]
 [36.533]
 [36.533]] [[1.643]
 [1.643]
 [1.643]
 [1.643]
 [1.848]
 [1.643]
 [1.643]]
Printing some Q and Qe and total Qs values:  [[0.116]
 [0.37 ]
 [0.341]
 [0.291]
 [0.221]
 [0.284]
 [0.31 ]] [[49.818]
 [47.555]
 [46.486]
 [50.485]
 [50.239]
 [49.251]
 [49.354]] [[1.698]
 [1.81 ]
 [1.715]
 [1.914]
 [1.829]
 [1.83 ]
 [1.863]]
maxi score, test score, baseline:  0.14464666666666656 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7584463
actor:  1 policy actor:  1  step number:  52 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  0.005502739418261626
actions average: 
K:  0  action  0 :  tensor([0.3915, 0.0145, 0.1090, 0.1038, 0.1556, 0.0983, 0.1273],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0125, 0.9213, 0.0097, 0.0101, 0.0046, 0.0031, 0.0386],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0672, 0.0422, 0.5954, 0.0553, 0.0654, 0.0862, 0.0883],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0959, 0.0680, 0.0940, 0.3527, 0.1299, 0.1154, 0.1442],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1377, 0.0030, 0.1408, 0.1082, 0.3381, 0.1283, 0.1439],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1107, 0.0297, 0.1534, 0.0956, 0.1273, 0.3570, 0.1263],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0937, 0.3414, 0.0926, 0.1013, 0.0868, 0.0830, 0.2011],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.14464666666666656 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.6902870945148
Printing some Q and Qe and total Qs values:  [[0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]] [[31.535]
 [31.535]
 [31.535]
 [31.535]
 [31.535]
 [31.535]
 [31.535]] [[0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]
 [0.775]]
printing an ep nov before normalisation:  54.61021896019459
maxi score, test score, baseline:  0.14464666666666656 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.874403953552246
printing an ep nov before normalisation:  41.70185425075275
maxi score, test score, baseline:  0.14464666666666656 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  50.7521659334198
printing an ep nov before normalisation:  32.04384185570108
actor:  0 policy actor:  0  step number:  54 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.57120487826457
printing an ep nov before normalisation:  42.199372346128094
actor:  1 policy actor:  1  step number:  69 total reward:  0.18666666666666587  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  42.07558723811452
Printing some Q and Qe and total Qs values:  [[0.895]
 [0.978]
 [0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]] [[42.211]
 [44.66 ]
 [42.211]
 [42.211]
 [42.211]
 [42.211]
 [42.211]] [[0.895]
 [0.978]
 [0.895]
 [0.895]
 [0.895]
 [0.895]
 [0.895]]
maxi score, test score, baseline:  0.1470066666666665 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.08504312816003
maxi score, test score, baseline:  0.1470066666666665 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  59 total reward:  0.02666666666666606  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.14905999999999986 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14905999999999986 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3466666666666659  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.14905999999999986 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14905999999999986 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.256]
 [0.369]
 [0.256]
 [0.256]
 [0.256]
 [0.256]
 [0.256]] [[35.153]
 [40.262]
 [35.153]
 [35.153]
 [35.153]
 [35.153]
 [35.153]] [[0.641]
 [0.852]
 [0.641]
 [0.641]
 [0.641]
 [0.641]
 [0.641]]
maxi score, test score, baseline:  0.14905999999999986 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.14905999999999986 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.20668366299167
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]] [[39.924]
 [39.924]
 [39.924]
 [39.924]
 [39.924]
 [39.924]
 [39.924]] [[0.956]
 [0.956]
 [0.956]
 [0.956]
 [0.956]
 [0.956]
 [0.956]]
printing an ep nov before normalisation:  42.354874641930486
actor:  1 policy actor:  1  step number:  70 total reward:  0.08666666666666589  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.04]
 [-0.04]
 [-0.04]
 [-0.04]
 [-0.04]
 [-0.04]
 [-0.04]] [[70.466]
 [70.466]
 [70.466]
 [70.466]
 [70.466]
 [70.466]
 [70.466]] [[1.627]
 [1.627]
 [1.627]
 [1.627]
 [1.627]
 [1.627]
 [1.627]]
actor:  0 policy actor:  0  step number:  57 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.14948666666666652 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.14948666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.614]
 [0.471]
 [0.525]
 [0.522]
 [0.481]
 [0.527]] [[38.475]
 [30.362]
 [35.663]
 [36.145]
 [37.728]
 [34.953]
 [37.22 ]] [[1.182]
 [1.096]
 [1.037]
 [1.099]
 [1.121]
 [1.036]
 [1.118]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.14948666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.213]
 [0.251]
 [0.203]
 [0.222]
 [0.222]
 [0.222]
 [0.213]] [[46.216]
 [55.034]
 [46.78 ]
 [55.22 ]
 [55.22 ]
 [55.22 ]
 [46.046]] [[1.532]
 [2.067]
 [1.554]
 [2.048]
 [2.048]
 [2.048]
 [1.523]]
printing an ep nov before normalisation:  45.05208732381835
printing an ep nov before normalisation:  47.45894432067871
maxi score, test score, baseline:  0.14948666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14948666666666652 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.14948666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  47.986597753554236
actor:  1 policy actor:  1  step number:  53 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  53 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 52.622902568809465
maxi score, test score, baseline:  0.14948666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.700007432105124
printing an ep nov before normalisation:  33.15228573913159
maxi score, test score, baseline:  0.14948666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.94275787059059
maxi score, test score, baseline:  0.14948666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14948666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.14948666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.3118782043457
maxi score, test score, baseline:  0.14948666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.41930136067978
maxi score, test score, baseline:  0.14948666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14948666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.711]
 [0.802]
 [0.711]
 [0.583]
 [0.711]
 [0.711]
 [0.689]] [[34.459]
 [38.613]
 [34.459]
 [31.67 ]
 [34.459]
 [34.459]
 [29.392]] [[0.711]
 [0.802]
 [0.711]
 [0.583]
 [0.711]
 [0.711]
 [0.689]]
actor:  1 policy actor:  1  step number:  68 total reward:  0.1133333333333324  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.14948666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14948666666666652 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.14948666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.28522271087162
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.118]
 [0.102]
 [0.104]
 [0.103]
 [0.091]
 [0.058]] [[48.276]
 [51.764]
 [48.195]
 [48.138]
 [47.407]
 [48.147]
 [46.718]] [[0.314]
 [0.398]
 [0.347]
 [0.348]
 [0.339]
 [0.335]
 [0.288]]
maxi score, test score, baseline:  0.14948666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.414]
 [0.657]
 [0.414]
 [0.414]
 [0.303]
 [0.414]
 [0.414]] [[45.399]
 [37.514]
 [45.399]
 [45.399]
 [47.635]
 [45.399]
 [45.399]] [[0.72 ]
 [0.888]
 [0.72 ]
 [0.72 ]
 [0.629]
 [0.72 ]
 [0.72 ]]
actor:  0 policy actor:  0  step number:  38 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.49]
 [0.49]
 [0.49]
 [0.49]
 [0.49]
 [0.49]
 [0.49]] [[42.474]
 [42.474]
 [42.474]
 [42.474]
 [42.474]
 [42.474]
 [42.474]] [[1.48]
 [1.48]
 [1.48]
 [1.48]
 [1.48]
 [1.48]
 [1.48]]
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]] [[64.778]
 [64.778]
 [64.778]
 [64.778]
 [64.778]
 [64.778]
 [64.778]] [[1.452]
 [1.452]
 [1.452]
 [1.452]
 [1.452]
 [1.452]
 [1.452]]
printing an ep nov before normalisation:  29.33431520567104
siam score:  -0.7652865
siam score:  -0.7664287
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.614]
 [0.553]
 [0.509]
 [0.458]
 [0.458]
 [0.456]] [[18.334]
 [44.498]
 [42.116]
 [36.264]
 [15.939]
 [15.865]
 [15.303]] [[0.532]
 [0.873]
 [0.796]
 [0.71 ]
 [0.517]
 [0.516]
 [0.51 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14948666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14948666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14948666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7678369
maxi score, test score, baseline:  0.14948666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  71 total reward:  0.13333333333333242  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]
 [0.387]] [[42.426]
 [42.426]
 [42.426]
 [42.426]
 [42.426]
 [42.426]
 [42.426]] [[2.379]
 [2.379]
 [2.379]
 [2.379]
 [2.379]
 [2.379]
 [2.379]]
Printing some Q and Qe and total Qs values:  [[0.222]
 [0.441]
 [0.222]
 [0.222]
 [0.222]
 [0.222]
 [0.222]] [[53.286]
 [49.171]
 [53.286]
 [53.286]
 [53.286]
 [53.286]
 [53.286]] [[2.027]
 [2.019]
 [2.027]
 [2.027]
 [2.027]
 [2.027]
 [2.027]]
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.735]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]] [[40.761]
 [42.084]
 [40.761]
 [40.761]
 [40.761]
 [40.761]
 [40.761]] [[1.779]
 [1.902]
 [1.779]
 [1.779]
 [1.779]
 [1.779]
 [1.779]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.683]
 [0.606]
 [0.606]
 [0.606]
 [0.606]
 [0.606]] [[37.357]
 [42.998]
 [37.357]
 [37.357]
 [37.357]
 [37.357]
 [37.357]] [[1.809]
 [2.239]
 [1.809]
 [1.809]
 [1.809]
 [1.809]
 [1.809]]
printing an ep nov before normalisation:  42.08997216816708
maxi score, test score, baseline:  0.14708666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14708666666666653 0.6900000000000002 0.6900000000000002
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.614]
 [0.614]
 [0.613]
 [0.613]
 [0.613]
 [0.614]] [[4.893]
 [4.541]
 [4.053]
 [4.752]
 [4.787]
 [4.023]
 [3.444]] [[0.993]
 [0.964]
 [0.926]
 [0.979]
 [0.982]
 [0.923]
 [0.879]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.14708666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.68029864237366
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.28 ]
 [0.261]
 [0.257]
 [0.261]
 [0.269]
 [0.274]] [[37.059]
 [47.084]
 [37.059]
 [41.419]
 [37.059]
 [45.212]
 [44.865]] [[1.197]
 [1.768]
 [1.197]
 [1.434]
 [1.197]
 [1.654]
 [1.64 ]]
printing an ep nov before normalisation:  41.45230085130392
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.852]
 [0.832]
 [0.865]
 [0.865]
 [0.865]
 [0.865]
 [0.93 ]] [[39.122]
 [39.133]
 [41.333]
 [41.333]
 [41.333]
 [41.333]
 [40.217]] [[2.364]
 [2.345]
 [2.532]
 [2.532]
 [2.532]
 [2.532]
 [2.519]]
siam score:  -0.7699701
maxi score, test score, baseline:  0.14708666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.14666666666666583  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.14708666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14708666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14708666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14708666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  73 total reward:  0.1599999999999987  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.14708666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1442333333333332 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  51.17655466922811
printing an ep nov before normalisation:  43.09704229175852
printing an ep nov before normalisation:  49.656455107126085
maxi score, test score, baseline:  0.1420333333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.2483973753901
maxi score, test score, baseline:  0.1420333333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.192150068841656
maxi score, test score, baseline:  0.1420333333333332 0.6900000000000002 0.6900000000000002
actor:  0 policy actor:  0  step number:  46 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1445533333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1445533333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  59 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1445533333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.747431726690536
siam score:  -0.7710481
maxi score, test score, baseline:  0.1445533333333332 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.1445533333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1445533333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1445533333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1445533333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.36838348551022
maxi score, test score, baseline:  0.1445533333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1445533333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.660538644237407
maxi score, test score, baseline:  0.1445533333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.837815702528587
actor:  1 policy actor:  1  step number:  59 total reward:  0.14666666666666606  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1445533333333332 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  47.513810442453234
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actor:  1 policy actor:  1  step number:  57 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1445533333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.761]
 [0.672]
 [0.67 ]
 [0.669]
 [0.687]
 [0.674]] [[28.868]
 [37.137]
 [32.018]
 [31.708]
 [31.188]
 [28.878]
 [31.688]] [[0.671]
 [0.761]
 [0.672]
 [0.67 ]
 [0.669]
 [0.687]
 [0.674]]
maxi score, test score, baseline:  0.1445533333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 39.34961336706257
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  47.79279851236285
actor:  0 policy actor:  0  step number:  45 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.14409999999999987 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.14409999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.022752826167874
maxi score, test score, baseline:  0.1407133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1407133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.361924171447754
actor:  1 policy actor:  1  step number:  50 total reward:  0.43333333333333335  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  37 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1403133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.19290119715017
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.093]
 [0.078]] [[52.341]
 [48.761]
 [48.761]
 [48.761]
 [48.761]
 [49.481]
 [48.761]] [[0.665]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.607]
 [0.576]]
maxi score, test score, baseline:  0.1403133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.87768488489142
printing an ep nov before normalisation:  53.64745596305866
maxi score, test score, baseline:  0.1403133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.839837074279785
printing an ep nov before normalisation:  37.27244914019788
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.502]
 [0.456]
 [0.331]
 [0.436]
 [0.347]
 [0.323]] [[46.119]
 [41.241]
 [41.674]
 [44.781]
 [41.637]
 [44.283]
 [43.48 ]] [[0.878]
 [0.908]
 [0.872]
 [0.816]
 [0.851]
 [0.821]
 [0.779]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.273]
 [0.197]
 [0.273]
 [0.273]
 [0.273]
 [0.273]
 [0.273]] [[36.24 ]
 [46.684]
 [36.24 ]
 [36.24 ]
 [36.24 ]
 [36.24 ]
 [36.24 ]] [[1.14 ]
 [1.534]
 [1.14 ]
 [1.14 ]
 [1.14 ]
 [1.14 ]
 [1.14 ]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3133333333333326  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  43.54612359348964
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]
 [0.648]] [[34.01]
 [34.01]
 [34.01]
 [34.01]
 [34.01]
 [34.01]
 [34.01]] [[1.168]
 [1.168]
 [1.168]
 [1.168]
 [1.168]
 [1.168]
 [1.168]]
printing an ep nov before normalisation:  53.05990580265436
actor:  1 policy actor:  1  step number:  56 total reward:  0.2599999999999998  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.1403133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.07223675698369
maxi score, test score, baseline:  0.13692666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.815446376800537
actor:  1 policy actor:  1  step number:  54 total reward:  0.2866666666666665  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.13692666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13692666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13692666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13692666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  70 total reward:  0.23333333333333262  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.13692666666666653 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.13692666666666653 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.13692666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13692666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4866666666666668  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  50 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.505]
 [0.457]
 [0.457]
 [0.457]
 [0.457]
 [0.457]] [[32.848]
 [35.683]
 [32.848]
 [32.848]
 [32.848]
 [32.848]
 [32.848]] [[1.059]
 [1.221]
 [1.059]
 [1.059]
 [1.059]
 [1.059]
 [1.059]]
printing an ep nov before normalisation:  43.06601046649338
printing an ep nov before normalisation:  29.648487731407247
maxi score, test score, baseline:  0.13692666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]] [[45.288]
 [45.288]
 [45.288]
 [45.288]
 [45.288]
 [45.288]
 [45.288]] [[2.113]
 [2.113]
 [2.113]
 [2.113]
 [2.113]
 [2.113]
 [2.113]]
printing an ep nov before normalisation:  49.140864093716864
actor:  1 policy actor:  1  step number:  64 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  47.82799378323337
maxi score, test score, baseline:  0.13692666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13692666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4866666666666668  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  35.45305252075195
maxi score, test score, baseline:  0.13692666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.773]
 [0.831]
 [0.773]
 [0.773]
 [0.773]
 [0.773]
 [0.773]] [[36.376]
 [36.043]
 [36.376]
 [36.376]
 [36.376]
 [36.376]
 [36.376]] [[0.773]
 [0.831]
 [0.773]
 [0.773]
 [0.773]
 [0.773]
 [0.773]]
maxi score, test score, baseline:  0.13692666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  42 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.777]
 [0.792]
 [0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.777]] [[51.553]
 [37.103]
 [51.553]
 [51.553]
 [51.553]
 [51.553]
 [51.553]] [[0.777]
 [0.792]
 [0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.777]]
printing an ep nov before normalisation:  33.66183860821309
maxi score, test score, baseline:  0.13629999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13629999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13291333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.38645087068966
maxi score, test score, baseline:  0.13291333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13291333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13291333333333322 0.6900000000000002 0.6900000000000002
line 256 mcts: sample exp_bonus 45.98337511757772
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.651]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]] [[41.501]
 [40.213]
 [41.501]
 [41.501]
 [41.501]
 [41.501]
 [41.501]] [[1.122]
 [1.185]
 [1.122]
 [1.122]
 [1.122]
 [1.122]
 [1.122]]
maxi score, test score, baseline:  0.13291333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 39.56297444123833
maxi score, test score, baseline:  0.13291333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.81971970693771
Printing some Q and Qe and total Qs values:  [[0.728]
 [0.721]
 [0.728]
 [0.728]
 [0.727]
 [0.725]
 [0.727]] [[5.107]
 [4.098]
 [4.607]
 [5.791]
 [6.153]
 [6.158]
 [4.976]] [[1.004]
 [0.943]
 [0.977]
 [1.041]
 [1.06 ]
 [1.058]
 [0.996]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.4600000000000002  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  47.94515023215846
maxi score, test score, baseline:  0.13291333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13291333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13291333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  55.80305501365119
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.026]
 [ 0.006]
 [-0.024]
 [ 0.01 ]
 [-0.017]
 [-0.136]
 [ 0.024]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.026]
 [ 0.006]
 [-0.024]
 [ 0.01 ]
 [-0.017]
 [-0.136]
 [ 0.024]]
maxi score, test score, baseline:  0.13291333333333322 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.13291333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13291333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13291333333333322 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.13291333333333322 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.13291333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13291333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13291333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.61401421426834
actions average: 
K:  3  action  0 :  tensor([0.2728, 0.0137, 0.1253, 0.1224, 0.2394, 0.1120, 0.1144],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0102, 0.9380, 0.0069, 0.0117, 0.0012, 0.0013, 0.0306],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1193, 0.1649, 0.1278, 0.1293, 0.1288, 0.2082, 0.1217],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0837, 0.2081, 0.1294, 0.2982, 0.0720, 0.1071, 0.1016],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1507, 0.0846, 0.1243, 0.1103, 0.2966, 0.1204, 0.1131],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0652, 0.0050, 0.1411, 0.1199, 0.0956, 0.5189, 0.0542],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1180, 0.1393, 0.1184, 0.1656, 0.1343, 0.1231, 0.2014],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  30.290963479895858
actor:  0 policy actor:  0  step number:  45 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  43 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  36.55678512291218
maxi score, test score, baseline:  0.13231333333333317 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.23533844734356
maxi score, test score, baseline:  0.13231333333333317 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.552]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]] [[37.222]
 [44.054]
 [37.222]
 [37.222]
 [37.222]
 [37.222]
 [37.222]] [[1.363]
 [1.761]
 [1.363]
 [1.363]
 [1.363]
 [1.363]
 [1.363]]
actor:  0 policy actor:  0  step number:  55 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  55 total reward:  0.35999999999999943  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  60 total reward:  0.0733333333333327  reward:  1.0 rdn_beta:  0.667
siam score:  -0.75340027
printing an ep nov before normalisation:  39.44443585835542
maxi score, test score, baseline:  0.13125999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13125999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13125999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13125999999999988 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  39.183621406555176
printing an ep nov before normalisation:  56.32256749855086
maxi score, test score, baseline:  0.13125999999999988 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.13125999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13125999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13125999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13125999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13125999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  56 total reward:  0.15333333333333266  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.13021999999999986 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13021999999999986 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.87864875793457
maxi score, test score, baseline:  0.13021999999999986 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.38483344777566
actor:  1 policy actor:  1  step number:  55 total reward:  0.3466666666666661  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.13021999999999986 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.57805140093895
printing an ep nov before normalisation:  33.694798946380615
printing an ep nov before normalisation:  48.40557221832489
maxi score, test score, baseline:  0.13021999999999986 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  40 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12973999999999986 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  37.3417879815936
maxi score, test score, baseline:  0.12973999999999986 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.6389497399589
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.501]] [[44.419]
 [44.419]
 [44.419]
 [44.419]
 [44.419]
 [44.419]
 [44.419]] [[0.834]
 [0.834]
 [0.834]
 [0.834]
 [0.834]
 [0.834]
 [0.834]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  44.583072055427216
UNIT TEST: sample policy line 217 mcts : [0.061 0.408 0.061 0.061 0.041 0.041 0.327]
maxi score, test score, baseline:  0.12973999999999986 0.6900000000000002 0.6900000000000002
Printing some Q and Qe and total Qs values:  [[0.509]
 [0.661]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]] [[43.876]
 [49.726]
 [43.876]
 [43.876]
 [43.876]
 [43.876]
 [43.876]] [[0.509]
 [0.661]
 [0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]]
actor:  1 policy actor:  1  step number:  41 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12973999999999986 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.12973999999999986 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.862640670495516
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]] [[29.635]
 [29.635]
 [29.635]
 [29.635]
 [29.635]
 [29.635]
 [29.635]] [[0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]]
printing an ep nov before normalisation:  37.203322779011124
maxi score, test score, baseline:  0.12973999999999986 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  44 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.265]
 [0.177]
 [0.146]
 [0.177]
 [0.164]
 [0.143]] [[48.824]
 [39.649]
 [48.545]
 [51.191]
 [50.133]
 [50.056]
 [52.682]] [[1.412]
 [1.064]
 [1.399]
 [1.495]
 [1.475]
 [1.459]
 [1.563]]
maxi score, test score, baseline:  0.12909999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.06541664637197
maxi score, test score, baseline:  0.12909999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.01979231062242
maxi score, test score, baseline:  0.12909999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.23486867958655
maxi score, test score, baseline:  0.12909999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12909999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  54 total reward:  0.28666666666666607  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1283266666666665 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1283266666666665 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.39333333333333276  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  49.500806171166104
maxi score, test score, baseline:  0.1283266666666665 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1249933333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.622417932849565
Printing some Q and Qe and total Qs values:  [[0.19]
 [0.19]
 [0.19]
 [0.19]
 [0.19]
 [0.19]
 [0.19]] [[72.249]
 [72.249]
 [72.249]
 [72.249]
 [72.249]
 [72.249]
 [72.249]] [[1.821]
 [1.821]
 [1.821]
 [1.821]
 [1.821]
 [1.821]
 [1.821]]
printing an ep nov before normalisation:  44.21898370582506
maxi score, test score, baseline:  0.1249933333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1249933333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1249933333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1249933333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.269]
 [0.284]
 [0.269]
 [0.248]
 [0.269]
 [0.184]
 [0.269]] [[41.932]
 [33.42 ]
 [41.932]
 [43.347]
 [41.932]
 [45.165]
 [41.932]] [[1.763]
 [1.355]
 [1.763]
 [1.813]
 [1.763]
 [1.839]
 [1.763]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  34.42340090205741
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.678]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]] [[41.553]
 [38.447]
 [41.553]
 [41.553]
 [41.553]
 [41.553]
 [41.553]] [[1.082]
 [1.102]
 [1.082]
 [1.082]
 [1.082]
 [1.082]
 [1.082]]
printing an ep nov before normalisation:  0.004415561458870343
actor:  1 policy actor:  1  step number:  41 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  53 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  56 total reward:  0.23333333333333273  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  45.019535035036164
maxi score, test score, baseline:  0.1249933333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.98646056184625
printing an ep nov before normalisation:  46.20468040426586
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1249933333333332 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  43.84546250705067
printing an ep nov before normalisation:  52.52450156099824
maxi score, test score, baseline:  0.1249933333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1249933333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.01212470079172
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.649]
 [0.787]
 [0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]] [[43.946]
 [46.893]
 [43.946]
 [43.946]
 [43.946]
 [43.946]
 [43.946]] [[1.734]
 [2.014]
 [1.734]
 [1.734]
 [1.734]
 [1.734]
 [1.734]]
printing an ep nov before normalisation:  47.75119616794649
line 256 mcts: sample exp_bonus 42.72072993551868
maxi score, test score, baseline:  0.1249933333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.47692102608186
actor:  1 policy actor:  1  step number:  62 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  50 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1249933333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.905186650799116
actor:  0 policy actor:  0  step number:  37 total reward:  0.56  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  43.808749431501845
printing an ep nov before normalisation:  30.55924884777394
maxi score, test score, baseline:  0.12477999999999986 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.75090512993693
maxi score, test score, baseline:  0.12477999999999986 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12141999999999985 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12141999999999985 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12141999999999985 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12141999999999985 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.809624224096176
actor:  1 policy actor:  1  step number:  69 total reward:  0.30666666666666587  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12141999999999985 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  36.53475701620118
printing an ep nov before normalisation:  39.18287817727646
printing an ep nov before normalisation:  44.25866463360421
printing an ep nov before normalisation:  26.387553238089012
printing an ep nov before normalisation:  25.85627555847168
actor:  1 policy actor:  1  step number:  40 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  43.307809517290266
maxi score, test score, baseline:  0.12141999999999985 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12141999999999985 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12141999999999985 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11805999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11805999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11805999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.2999999999999998  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.92299953035337
printing an ep nov before normalisation:  42.0167068200095
maxi score, test score, baseline:  0.11805999999999987 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.36447339526819
actor:  0 policy actor:  0  step number:  51 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  50 total reward:  0.5400000000000005  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.248875794255014
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
Printing some Q and Qe and total Qs values:  [[0.211]
 [0.342]
 [0.221]
 [0.208]
 [0.182]
 [0.21 ]
 [0.221]] [[35.619]
 [38.649]
 [37.223]
 [36.225]
 [35.173]
 [36.501]
 [37.223]] [[0.36 ]
 [0.517]
 [0.383]
 [0.361]
 [0.327]
 [0.366]
 [0.383]]
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  67 total reward:  0.13333333333333264  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.523]
 [0.419]
 [0.4  ]
 [0.437]
 [0.437]
 [0.398]] [[43.261]
 [45.803]
 [44.942]
 [42.604]
 [47.903]
 [47.903]
 [42.951]] [[0.85 ]
 [1.026]
 [0.903]
 [0.834]
 [0.985]
 [0.985]
 [0.84 ]]
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.2615909576416
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.126]
 [0.166]
 [0.126]
 [0.126]
 [0.126]
 [0.127]
 [0.126]] [[54.579]
 [42.263]
 [54.579]
 [54.579]
 [54.579]
 [53.892]
 [54.579]] [[0.451]
 [0.35 ]
 [0.451]
 [0.451]
 [0.451]
 [0.445]
 [0.451]]
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.708951942863706
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.51 ]
 [0.462]
 [0.458]
 [0.462]
 [0.462]
 [0.462]] [[41.405]
 [41.161]
 [41.405]
 [40.914]
 [41.405]
 [41.405]
 [41.405]] [[0.768]
 [0.813]
 [0.768]
 [0.757]
 [0.768]
 [0.768]
 [0.768]]
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.25999999999999923  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  47.15820780122704
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.659]
 [0.782]
 [0.683]
 [0.693]
 [0.697]
 [0.695]
 [0.691]] [[46.656]
 [39.896]
 [47.545]
 [47.391]
 [47.711]
 [48.437]
 [48.998]] [[1.885]
 [1.701]
 [1.95 ]
 [1.953]
 [1.972]
 [2.002]
 [2.024]]
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]
 [0.517]] [[40.082]
 [40.082]
 [40.082]
 [40.082]
 [40.082]
 [40.082]
 [40.082]] [[0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
actor:  1 policy actor:  1  step number:  47 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.3369, 0.0218, 0.1007, 0.0834, 0.2121, 0.1026, 0.1426],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0123, 0.9049, 0.0138, 0.0111, 0.0076, 0.0074, 0.0429],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0863, 0.0285, 0.4363, 0.0831, 0.0976, 0.1574, 0.1107],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1589, 0.0104, 0.1319, 0.2583, 0.1440, 0.1346, 0.1619],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1266, 0.0022, 0.0594, 0.0968, 0.5740, 0.0717, 0.0693],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0714, 0.1016, 0.0775, 0.0544, 0.0828, 0.5411, 0.0712],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1301, 0.1062, 0.1086, 0.1043, 0.1247, 0.1079, 0.3183],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  32.86851787326024
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.43333333333333346  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.751]
 [0.697]
 [0.697]] [[39.599]
 [39.599]
 [39.599]
 [39.599]
 [47.601]
 [39.599]
 [39.599]] [[1.484]
 [1.484]
 [1.484]
 [1.484]
 [1.816]
 [1.484]
 [1.484]]
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.782091866638616
printing an ep nov before normalisation:  46.255993063844535
printing an ep nov before normalisation:  0.03593626511246839
Printing some Q and Qe and total Qs values:  [[0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.755]
 [0.74 ]] [[32.404]
 [32.404]
 [32.404]
 [32.404]
 [32.404]
 [38.386]
 [32.404]] [[1.638]
 [1.638]
 [1.638]
 [1.638]
 [1.638]
 [1.819]
 [1.638]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  63.099140474786346
maxi score, test score, baseline:  0.11725999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.84900535187739
printing an ep nov before normalisation:  34.58257798365586
printing an ep nov before normalisation:  41.79535914951677
actor:  0 policy actor:  0  step number:  46 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7672342
maxi score, test score, baseline:  0.11671333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.61408238783927
maxi score, test score, baseline:  0.11671333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.131]
 [0.32 ]
 [0.054]
 [0.27 ]
 [0.228]
 [0.195]
 [0.016]] [[43.635]
 [43.94 ]
 [49.209]
 [46.004]
 [48.104]
 [49.411]
 [49.599]] [[0.909]
 [1.111]
 [1.074]
 [1.15 ]
 [1.2  ]
 [1.223]
 [1.053]]
printing an ep nov before normalisation:  36.31053341530891
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11671333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.622950535133246
printing an ep nov before normalisation:  0.0008388997554220623
actor:  1 policy actor:  1  step number:  78 total reward:  0.059999999999999165  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.56222245584928
maxi score, test score, baseline:  0.11671333333333321 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.11671333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11671333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.22999647142671
Printing some Q and Qe and total Qs values:  [[0.872]
 [0.924]
 [0.872]
 [0.872]
 [0.872]
 [0.872]
 [0.872]] [[42.609]
 [40.482]
 [42.609]
 [42.609]
 [42.609]
 [42.609]
 [42.609]] [[0.872]
 [0.924]
 [0.872]
 [0.872]
 [0.872]
 [0.872]
 [0.872]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.14666666666666606  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11671333333333321 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.63753754819678
actor:  0 policy actor:  0  step number:  46 total reward:  0.37999999999999945  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  35 total reward:  0.5866666666666667  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1161133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.43812482451226
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.477]
 [0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]] [[41.749]
 [40.558]
 [41.749]
 [41.749]
 [41.749]
 [41.749]
 [41.749]] [[1.413]
 [1.416]
 [1.413]
 [1.413]
 [1.413]
 [1.413]
 [1.413]]
maxi score, test score, baseline:  0.1161133333333332 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.1161133333333332 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.1161133333333332 0.6900000000000002 0.6900000000000002
actions average: 
K:  2  action  0 :  tensor([0.3863, 0.0243, 0.0900, 0.0959, 0.1934, 0.1024, 0.1077],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0013,     0.9761,     0.0015,     0.0016,     0.0002,     0.0001,
            0.0193], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0753, 0.0383, 0.5980, 0.0499, 0.1017, 0.0858, 0.0510],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1556, 0.0919, 0.1480, 0.1308, 0.1359, 0.1721, 0.1657],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1014, 0.0046, 0.0836, 0.0845, 0.4924, 0.1096, 0.1240],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1006, 0.0180, 0.1178, 0.0881, 0.0676, 0.4996, 0.1083],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1125, 0.2208, 0.1091, 0.1078, 0.0880, 0.1113, 0.2506],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  57 total reward:  0.1999999999999993  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1161133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.542279875575872
actor:  1 policy actor:  1  step number:  49 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  62.12698538639449
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1161133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 39.533107941175054
printing an ep nov before normalisation:  38.744468688964844
maxi score, test score, baseline:  0.1161133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1161133333333332 0.6900000000000002 0.6900000000000002
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1161133333333332 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.1161133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.42666666666666675  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.252]
 [0.216]
 [0.218]
 [0.229]
 [0.22 ]
 [0.214]] [[46.656]
 [48.498]
 [46.893]
 [46.96 ]
 [46.656]
 [47.667]
 [47.62 ]] [[1.359]
 [1.47 ]
 [1.358]
 [1.363]
 [1.359]
 [1.398]
 [1.39 ]]
maxi score, test score, baseline:  0.1161133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1161133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.572]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]] [[41.287]
 [43.421]
 [41.287]
 [41.287]
 [41.287]
 [41.287]
 [41.287]] [[1.548]
 [1.68 ]
 [1.548]
 [1.548]
 [1.548]
 [1.548]
 [1.548]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.1161133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1161133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1161133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.634511646444246
maxi score, test score, baseline:  0.1161133333333332 0.6900000000000002 0.6900000000000002
actor:  1 policy actor:  1  step number:  53 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1161133333333332 0.6900000000000002 0.6900000000000002
siam score:  -0.75501955
actor:  1 policy actor:  1  step number:  51 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1161133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1161133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]] [[68.543]
 [68.543]
 [68.543]
 [68.543]
 [68.543]
 [68.543]
 [68.543]] [[0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]]
printing an ep nov before normalisation:  31.67000346449278
actor:  1 policy actor:  1  step number:  61 total reward:  0.06666666666666621  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1161133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1161133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.190481662750244
printing an ep nov before normalisation:  51.408006507443844
maxi score, test score, baseline:  0.11611333333333318 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11611333333333318 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.910913895223345
line 256 mcts: sample exp_bonus 27.066860752657277
Printing some Q and Qe and total Qs values:  [[0.239]
 [0.331]
 [0.239]
 [0.239]
 [0.161]
 [0.239]
 [0.178]] [[47.16 ]
 [37.74 ]
 [47.16 ]
 [47.16 ]
 [50.687]
 [47.16 ]
 [41.293]] [[0.523]
 [0.535]
 [0.523]
 [0.523]
 [0.475]
 [0.523]
 [0.412]]
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.343]
 [0.283]
 [0.283]
 [0.283]
 [0.283]
 [0.283]] [[39.609]
 [42.684]
 [39.609]
 [39.609]
 [39.609]
 [39.609]
 [39.609]] [[0.468]
 [0.557]
 [0.468]
 [0.468]
 [0.468]
 [0.468]
 [0.468]]
printing an ep nov before normalisation:  36.76335072140177
maxi score, test score, baseline:  0.11611333333333318 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  46 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.632260875928225
maxi score, test score, baseline:  0.11879333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11879333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.12 ]
 [0.15 ]
 [0.107]
 [0.107]
 [0.107]
 [0.108]
 [0.109]] [[49.458]
 [39.861]
 [49.72 ]
 [50.029]
 [49.973]
 [50.1  ]
 [49.809]] [[1.396]
 [0.932]
 [1.396]
 [1.412]
 [1.41 ]
 [1.417]
 [1.403]]
printing an ep nov before normalisation:  54.4061210832221
printing an ep nov before normalisation:  28.500158099199858
printing an ep nov before normalisation:  50.308868675681516
actor:  1 policy actor:  1  step number:  56 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.01897785646802
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.604]
 [0.524]
 [0.524]
 [0.524]
 [0.524]
 [0.524]] [[42.86 ]
 [43.622]
 [42.86 ]
 [42.86 ]
 [42.86 ]
 [42.86 ]
 [42.86 ]] [[1.411]
 [1.522]
 [1.411]
 [1.411]
 [1.411]
 [1.411]
 [1.411]]
maxi score, test score, baseline:  0.11879333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11879333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  69 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11879333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.997374901556775
maxi score, test score, baseline:  0.11879333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75939286
printing an ep nov before normalisation:  65.55322773639251
Printing some Q and Qe and total Qs values:  [[0.902]
 [0.942]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]] [[50.329]
 [41.998]
 [50.329]
 [50.329]
 [50.329]
 [50.329]
 [50.329]] [[0.902]
 [0.942]
 [0.902]
 [0.902]
 [0.902]
 [0.902]
 [0.902]]
maxi score, test score, baseline:  0.11879333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11879333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11879333333333322 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  54 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1189533333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.4415, 0.0700, 0.0877, 0.0872, 0.1025, 0.0920, 0.1191],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0072, 0.9320, 0.0067, 0.0211, 0.0084, 0.0043, 0.0203],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0813, 0.0480, 0.4435, 0.1288, 0.0893, 0.1094, 0.0996],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1600, 0.0015, 0.1369, 0.2629, 0.1384, 0.1439, 0.1563],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1494, 0.0123, 0.1345, 0.1663, 0.2488, 0.1357, 0.1530],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0901, 0.0049, 0.1157, 0.1021, 0.1085, 0.4711, 0.1076],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1204, 0.0499, 0.1226, 0.1148, 0.1197, 0.1206, 0.3519],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.751]
 [0.768]
 [0.72 ]
 [0.718]
 [0.709]
 [0.693]
 [0.749]] [[34.871]
 [30.81 ]
 [33.723]
 [32.276]
 [31.644]
 [32.475]
 [31.932]] [[0.751]
 [0.768]
 [0.72 ]
 [0.718]
 [0.709]
 [0.693]
 [0.749]]
actor:  0 policy actor:  0  step number:  41 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  48.720125935448685
maxi score, test score, baseline:  0.12196666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12196666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76056695
maxi score, test score, baseline:  0.12196666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12196666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.536]
 [0.374]
 [0.331]
 [0.435]
 [0.421]
 [0.491]] [[33.192]
 [26.012]
 [32.153]
 [27.429]
 [30.039]
 [30.282]
 [31.918]] [[0.773]
 [0.777]
 [0.672]
 [0.585]
 [0.714]
 [0.702]
 [0.786]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12196666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.044]
 [0.06 ]
 [0.053]
 [0.056]
 [0.053]
 [0.066]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.065]
 [0.044]
 [0.06 ]
 [0.053]
 [0.056]
 [0.053]
 [0.066]]
actions average: 
K:  1  action  0 :  tensor([0.5057, 0.0279, 0.0779, 0.0776, 0.1251, 0.0957, 0.0902],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0064, 0.9615, 0.0052, 0.0039, 0.0023, 0.0030, 0.0177],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0493, 0.0769, 0.6602, 0.0448, 0.0404, 0.0782, 0.0502],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1171, 0.0369, 0.0920, 0.3876, 0.1254, 0.1017, 0.1393],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1669, 0.0169, 0.1255, 0.1157, 0.3373, 0.1191, 0.1187],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0501, 0.0013, 0.0913, 0.0565, 0.0445, 0.6946, 0.0617],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1383, 0.0968, 0.1549, 0.1226, 0.1335, 0.1757, 0.1782],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  56 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12196666666666653 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  59.82490129770015
maxi score, test score, baseline:  0.12196666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12196666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.716]
 [0.783]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]] [[35.977]
 [31.574]
 [35.977]
 [35.977]
 [35.977]
 [35.977]
 [35.977]] [[0.716]
 [0.783]
 [0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]]
maxi score, test score, baseline:  0.12196666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.75974464416504
printing an ep nov before normalisation:  40.443925857543945
Printing some Q and Qe and total Qs values:  [[0.677]
 [0.638]
 [0.575]
 [0.583]
 [0.595]
 [0.569]
 [0.623]] [[40.302]
 [40.357]
 [40.52 ]
 [42.309]
 [41.655]
 [43.438]
 [40.928]] [[1.971]
 [1.936]
 [1.883]
 [1.998]
 [1.97 ]
 [2.051]
 [1.954]]
printing an ep nov before normalisation:  43.231681017150905
printing an ep nov before normalisation:  42.58242130279541
actor:  1 policy actor:  1  step number:  62 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12196666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12196666666666653 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.12196666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  50 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  54 total reward:  0.32666666666666666  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1244333333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.62677709102196
maxi score, test score, baseline:  0.1244333333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.545932736921294
printing an ep nov before normalisation:  45.51489341994585
maxi score, test score, baseline:  0.1244333333333332 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.1244333333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.466]
 [0.411]
 [0.381]
 [0.427]
 [0.457]
 [0.363]] [[23.702]
 [35.655]
 [33.872]
 [24.41 ]
 [32.314]
 [34.138]
 [24.463]] [[0.491]
 [0.706]
 [0.634]
 [0.508]
 [0.634]
 [0.682]
 [0.491]]
printing an ep nov before normalisation:  54.09223788574804
actions average: 
K:  0  action  0 :  tensor([0.2575, 0.0029, 0.1386, 0.1398, 0.1669, 0.1610, 0.1333],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0022,     0.9826,     0.0018,     0.0005,     0.0002,     0.0003,
            0.0125], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1080, 0.0050, 0.4710, 0.0915, 0.1089, 0.1148, 0.1008],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1313, 0.0068, 0.1150, 0.3643, 0.1364, 0.1243, 0.1219],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0850, 0.0026, 0.0697, 0.0874, 0.5678, 0.0875, 0.1000],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0746, 0.0075, 0.1820, 0.0939, 0.0830, 0.4570, 0.1021],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1760, 0.0072, 0.1523, 0.1451, 0.1603, 0.1599, 0.1992],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1244333333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.1866666666666662  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  51 total reward:  0.39999999999999947  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1244333333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.7267686854085
printing an ep nov before normalisation:  36.20656848923452
printing an ep nov before normalisation:  46.818794794472076
Printing some Q and Qe and total Qs values:  [[0.27 ]
 [0.336]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]] [[39.856]
 [38.885]
 [39.856]
 [39.856]
 [39.856]
 [39.856]
 [39.856]] [[1.786]
 [1.782]
 [1.786]
 [1.786]
 [1.786]
 [1.786]
 [1.786]]
actor:  1 policy actor:  1  step number:  69 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.432]
 [0.432]
 [0.432]
 [0.518]
 [0.432]
 [0.069]] [[40.059]
 [41.201]
 [41.201]
 [41.201]
 [39.981]
 [41.201]
 [39.603]] [[2.001]
 [1.993]
 [1.993]
 [1.993]
 [1.993]
 [1.993]
 [1.517]]
printing an ep nov before normalisation:  49.04183150788348
printing an ep nov before normalisation:  45.721920062225074
printing an ep nov before normalisation:  58.25161731613128
actor:  1 policy actor:  1  step number:  46 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.79310395800683
maxi score, test score, baseline:  0.1244333333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.33999999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  48.30264103462983
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.495]
 [0.495]
 [0.495]
 [0.583]
 [0.553]
 [0.628]] [[42.976]
 [39.699]
 [39.699]
 [39.699]
 [44.914]
 [47.025]
 [36.167]] [[1.345]
 [1.112]
 [1.112]
 [1.112]
 [1.379]
 [1.422]
 [1.122]]
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.625]
 [0.635]
 [0.634]
 [0.635]
 [0.635]
 [0.635]] [[3.438]
 [4.816]
 [5.022]
 [5.977]
 [4.858]
 [5.805]
 [6.992]] [[0.744]
 [0.781]
 [0.798]
 [0.828]
 [0.792]
 [0.823]
 [0.862]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1244333333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.76906181810728
printing an ep nov before normalisation:  0.0007699939246208487
printing an ep nov before normalisation:  39.66062784194946
maxi score, test score, baseline:  0.1216333333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.89016325253532
printing an ep nov before normalisation:  40.766029285338085
maxi score, test score, baseline:  0.1191133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1191133333333332 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  41 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  55 total reward:  0.40000000000000013  reward:  1.0 rdn_beta:  0.333
siam score:  -0.75253975
maxi score, test score, baseline:  0.12188666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.98423719406128
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]] [[25.984]
 [25.984]
 [25.984]
 [25.984]
 [25.984]
 [25.984]
 [25.984]] [[0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]]
printing an ep nov before normalisation:  44.0356497567265
line 256 mcts: sample exp_bonus 44.03190612792969
Printing some Q and Qe and total Qs values:  [[0.832]
 [0.938]
 [0.783]
 [0.741]
 [0.76 ]
 [0.722]
 [0.729]] [[42.052]
 [38.854]
 [41.123]
 [42.523]
 [42.945]
 [42.512]
 [40.378]] [[1.544]
 [1.553]
 [1.466]
 [1.467]
 [1.499]
 [1.448]
 [1.39 ]]
maxi score, test score, baseline:  0.12188666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.684771227961974
maxi score, test score, baseline:  0.12188666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.38272173530186
maxi score, test score, baseline:  0.12188666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.2933333333333328  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  61 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12188666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12188666666666652 0.6900000000000002 0.6900000000000002
siam score:  -0.7485975
maxi score, test score, baseline:  0.12188666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12188666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.50674819597141
actor:  1 policy actor:  1  step number:  74 total reward:  0.12666666666666604  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  60.30097806988132
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.12188666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  45.65268852726457
maxi score, test score, baseline:  0.12188666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7458061
maxi score, test score, baseline:  0.12188666666666652 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.12188666666666652 0.6900000000000002 0.6900000000000002
maxi score, test score, baseline:  0.12188666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4266666666666663  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  51 total reward:  0.4133333333333328  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.204]
 [0.245]
 [0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]] [[34.367]
 [47.79 ]
 [34.367]
 [34.367]
 [34.367]
 [34.367]
 [34.367]] [[0.805]
 [1.29 ]
 [0.805]
 [0.805]
 [0.805]
 [0.805]
 [0.805]]
printing an ep nov before normalisation:  40.84193283027673
siam score:  -0.75251627
maxi score, test score, baseline:  0.12188666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12188666666666652 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  53 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12436666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12436666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.03314539147398
printing an ep nov before normalisation:  46.1780899078491
printing an ep nov before normalisation:  38.167698710856804
printing an ep nov before normalisation:  0.00036210364100952575
actor:  1 policy actor:  1  step number:  56 total reward:  0.17999999999999927  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.12436666666666653 0.6900000000000002 0.6900000000000002
siam score:  -0.74717593
maxi score, test score, baseline:  0.12436666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.96580436984537
maxi score, test score, baseline:  0.12436666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12436666666666653 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  43.21591763353265
printing an ep nov before normalisation:  28.93698164334578
maxi score, test score, baseline:  0.12436666666666653 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.3679, 0.0445, 0.1370, 0.0983, 0.1116, 0.1158, 0.1249],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0143, 0.9101, 0.0131, 0.0093, 0.0071, 0.0071, 0.0390],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0886, 0.0401, 0.4702, 0.1019, 0.0878, 0.1116, 0.0997],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1180, 0.0069, 0.1390, 0.2996, 0.1558, 0.1270, 0.1536],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1316, 0.0100, 0.1088, 0.0878, 0.4609, 0.0985, 0.1023],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0599, 0.0798, 0.1778, 0.0585, 0.0537, 0.4802, 0.0901],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1782, 0.1158, 0.1234, 0.1105, 0.1049, 0.1199, 0.2472],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.12229999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12229999999999988 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.656503440055154
maxi score, test score, baseline:  0.11960666666666656 0.6900000000000002 0.6900000000000002
printing an ep nov before normalisation:  41.11329617136589
printing an ep nov before normalisation:  25.822463035583496
maxi score, test score, baseline:  0.11960666666666656 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  52.49391762684807
maxi score, test score, baseline:  0.11960666666666654 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.55240258510483
maxi score, test score, baseline:  0.11960666666666654 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11960666666666654 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11960666666666654 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.19735223167459
printing an ep nov before normalisation:  38.11324577553623
maxi score, test score, baseline:  0.11960666666666654 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.353076849887614
printing an ep nov before normalisation:  38.4123101550137
maxi score, test score, baseline:  0.11960666666666654 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.31461048126221
maxi score, test score, baseline:  0.11960666666666654 0.6900000000000002 0.6900000000000002
actor:  1 policy actor:  1  step number:  53 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.11960666666666654 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  1.0
Starting evaluation
maxi score, test score, baseline:  0.11960666666666654 0.6900000000000002 0.6900000000000002
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.262]
 [0.255]
 [0.211]
 [0.171]
 [0.255]
 [0.226]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.255]
 [0.262]
 [0.255]
 [0.211]
 [0.171]
 [0.255]
 [0.226]]
printing an ep nov before normalisation:  72.23816374039198
maxi score, test score, baseline:  0.11960666666666654 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.4  ]
 [0.318]
 [0.32 ]
 [0.319]
 [0.321]
 [0.312]] [[ 0.   ]
 [36.371]
 [29.235]
 [35.043]
 [30.224]
 [30.11 ]
 [ 0.   ]] [[0.312]
 [0.4  ]
 [0.318]
 [0.32 ]
 [0.319]
 [0.321]
 [0.312]]
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.67 ]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]] [[51.55 ]
 [56.429]
 [51.55 ]
 [51.55 ]
 [51.55 ]
 [51.55 ]
 [51.55 ]] [[0.595]
 [0.67 ]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]]
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.512]
 [0.374]
 [0.374]
 [0.35 ]
 [0.374]
 [0.374]] [[39.011]
 [43.385]
 [34.859]
 [34.859]
 [43.991]
 [34.859]
 [34.859]] [[0.359]
 [0.512]
 [0.374]
 [0.374]
 [0.35 ]
 [0.374]
 [0.374]]
line 256 mcts: sample exp_bonus 33.38578261435032
printing an ep nov before normalisation:  42.26140081882477
printing an ep nov before normalisation:  52.64667175383576
printing an ep nov before normalisation:  44.246153419334924
Printing some Q and Qe and total Qs values:  [[0.238]
 [0.239]
 [0.238]
 [0.238]
 [0.191]
 [0.238]
 [0.238]] [[47.697]
 [41.387]
 [47.697]
 [47.697]
 [57.281]
 [47.697]
 [47.697]] [[0.875]
 [0.735]
 [0.875]
 [0.875]
 [1.042]
 [0.875]
 [0.875]]
maxi score, test score, baseline:  0.11960666666666654 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.487441949458606
printing an ep nov before normalisation:  46.0316952350742
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  37.3667349958361
printing an ep nov before normalisation:  36.0073190748237
printing an ep nov before normalisation:  43.41582567575874
printing an ep nov before normalisation:  40.0044847658962
maxi score, test score, baseline:  0.11960666666666654 0.6900000000000002 0.6900000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.2809053411802
printing an ep nov before normalisation:  41.82909103443849
printing an ep nov before normalisation:  28.930466200518968
Printing some Q and Qe and total Qs values:  [[0.99 ]
 [1.008]
 [0.99 ]
 [0.99 ]
 [0.99 ]
 [0.99 ]
 [0.993]] [[33.165]
 [30.092]
 [33.165]
 [33.165]
 [33.165]
 [33.165]
 [30.731]] [[0.99 ]
 [1.008]
 [0.99 ]
 [0.99 ]
 [0.99 ]
 [0.99 ]
 [0.993]]
printing an ep nov before normalisation:  31.19669688826025
printing an ep nov before normalisation:  33.537822903682766
actor:  0 policy actor:  0  step number:  54 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.218]
 [0.301]
 [0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]] [[31.761]
 [42.605]
 [31.761]
 [31.761]
 [31.761]
 [31.761]
 [31.761]] [[0.658]
 [1.067]
 [0.658]
 [0.658]
 [0.658]
 [0.658]
 [0.658]]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  0.13108666666666652 0.6900000000000002 0.6900000000000002
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  35.50845146179199
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14983333333333324 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.78358019844067
maxi score, test score, baseline:  0.14983333333333324 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14983333333333324 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14983333333333324 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.74039125442505
actor:  1 policy actor:  1  step number:  59 total reward:  0.19999999999999962  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.14983333333333324 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  62 total reward:  0.11333333333333295  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.14957999999999985 0.6913333333333336 0.6913333333333336
Printing some Q and Qe and total Qs values:  [[ 0.247]
 [ 0.29 ]
 [ 0.355]
 [ 0.355]
 [ 0.27 ]
 [-0.029]
 [ 0.355]] [[25.783]
 [27.094]
 [38.627]
 [38.627]
 [25.179]
 [26.179]
 [38.627]] [[1.454]
 [1.623]
 [2.795]
 [2.795]
 [1.42 ]
 [1.216]
 [2.795]]
printing an ep nov before normalisation:  39.95772817653358
printing an ep nov before normalisation:  44.10840211852041
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14957999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.043834398666036
printing an ep nov before normalisation:  34.78423504133053
maxi score, test score, baseline:  0.14957999999999985 0.6913333333333336 0.6913333333333336
actor:  1 policy actor:  1  step number:  51 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  53.45819919926124
printing an ep nov before normalisation:  0.031185108751969892
actor:  1 policy actor:  1  step number:  56 total reward:  0.37999999999999934  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.97339166103395
printing an ep nov before normalisation:  17.49008510270812
actor:  1 policy actor:  1  step number:  79 total reward:  0.026666666666665506  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  42.76207555655138
printing an ep nov before normalisation:  46.00801373650278
maxi score, test score, baseline:  0.14957999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.184]
 [0.184]
 [0.2  ]
 [0.184]
 [0.184]
 [0.184]
 [0.184]] [[40.631]
 [40.631]
 [30.746]
 [40.631]
 [40.631]
 [40.631]
 [40.631]] [[2.354]
 [2.354]
 [1.533]
 [2.354]
 [2.354]
 [2.354]
 [2.354]]
Printing some Q and Qe and total Qs values:  [[0.28 ]
 [0.44 ]
 [0.392]
 [0.384]
 [0.406]
 [0.369]
 [0.39 ]] [[27.533]
 [27.951]
 [31.29 ]
 [29.779]
 [31.118]
 [31.742]
 [30.426]] [[1.14 ]
 [1.332]
 [1.534]
 [1.413]
 [1.535]
 [1.545]
 [1.468]]
siam score:  -0.74416596
printing an ep nov before normalisation:  53.42693392129658
actor:  1 policy actor:  1  step number:  49 total reward:  0.4666666666666668  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  11.61536939989638
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[ 0.01 ]
 [ 0.038]
 [-0.143]
 [ 0.032]
 [ 0.028]
 [-0.006]
 [ 0.022]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.01 ]
 [ 0.038]
 [-0.143]
 [ 0.032]
 [ 0.028]
 [-0.006]
 [ 0.022]]
actor:  1 policy actor:  1  step number:  65 total reward:  0.09333333333333282  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  49 total reward:  0.3599999999999999  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.063]
 [0.053]
 [0.051]
 [0.048]
 [0.049]
 [0.037]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.035]
 [0.063]
 [0.053]
 [0.051]
 [0.048]
 [0.049]
 [0.037]]
printing an ep nov before normalisation:  47.60748139145086
printing an ep nov before normalisation:  34.64346647262573
maxi score, test score, baseline:  0.14957999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.70124101638794
printing an ep nov before normalisation:  33.41142179035978
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.448]
 [0.326]
 [0.326]
 [0.262]
 [0.326]
 [0.326]] [[33.996]
 [39.393]
 [33.996]
 [33.996]
 [33.296]
 [33.996]
 [33.996]] [[1.292]
 [1.682]
 [1.292]
 [1.292]
 [1.194]
 [1.292]
 [1.292]]
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.617]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]] [[31.566]
 [35.775]
 [31.566]
 [31.566]
 [31.566]
 [31.566]
 [31.566]] [[1.197]
 [1.555]
 [1.197]
 [1.197]
 [1.197]
 [1.197]
 [1.197]]
maxi score, test score, baseline:  0.14957999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.37895271486698
actor:  1 policy actor:  1  step number:  47 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.14957999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.72860771051857
printing an ep nov before normalisation:  42.32752075288109
actor:  1 policy actor:  1  step number:  58 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.14957999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  43.83745188493237
actor:  1 policy actor:  1  step number:  64 total reward:  0.2199999999999992  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.14957999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.17 ]
 [0.241]
 [0.164]
 [0.163]
 [0.196]
 [0.238]
 [0.222]] [[38.767]
 [42.055]
 [40.165]
 [40.137]
 [42.171]
 [41.987]
 [41.038]] [[1.1  ]
 [1.325]
 [1.16 ]
 [1.158]
 [1.285]
 [1.319]
 [1.259]]
printing an ep nov before normalisation:  37.09257395637985
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.87587547302246
printing an ep nov before normalisation:  38.80103661136601
maxi score, test score, baseline:  0.14957999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14957999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.1866666666666661  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.14957999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.032248529315396
actor:  1 policy actor:  1  step number:  56 total reward:  0.28666666666666596  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  38.95218458998667
maxi score, test score, baseline:  0.14957999999999985 0.6913333333333336 0.6913333333333336
siam score:  -0.74382424
printing an ep nov before normalisation:  38.264012609416
maxi score, test score, baseline:  0.14957999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.675752367443287
printing an ep nov before normalisation:  43.522630366427926
siam score:  -0.74439543
maxi score, test score, baseline:  0.14957999999999985 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.14957999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.32690974920929
actor:  0 policy actor:  0  step number:  58 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  64 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  1.0
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.14965999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14965999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14965999999999988 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  37.47083559114005
maxi score, test score, baseline:  0.14965999999999988 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7419479
printing an ep nov before normalisation:  36.90638470542052
Printing some Q and Qe and total Qs values:  [[0.181]
 [0.334]
 [0.271]
 [0.169]
 [0.271]
 [0.208]
 [0.181]] [[47.939]
 [41.806]
 [40.472]
 [50.475]
 [40.472]
 [43.709]
 [50.362]] [[0.477]
 [0.564]
 [0.486]
 [0.492]
 [0.486]
 [0.459]
 [0.503]]
actor:  0 policy actor:  0  step number:  57 total reward:  0.23999999999999977  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15004666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15004666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15004666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15004666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.58385758729237
line 256 mcts: sample exp_bonus 29.348022937774658
maxi score, test score, baseline:  0.15004666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.68428325653076
maxi score, test score, baseline:  0.15004666666666652 0.6913333333333336 0.6913333333333336
Printing some Q and Qe and total Qs values:  [[0.8]
 [0.8]
 [0.8]
 [0.8]
 [0.8]
 [0.8]
 [0.8]] [[39.333]
 [39.333]
 [39.333]
 [39.333]
 [39.333]
 [39.333]
 [39.333]] [[0.8]
 [0.8]
 [0.8]
 [0.8]
 [0.8]
 [0.8]
 [0.8]]
maxi score, test score, baseline:  0.15004666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  40 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.55020400208969
maxi score, test score, baseline:  0.15052666666666656 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.30029296875
actor:  0 policy actor:  0  step number:  40 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  2.0
siam score:  -0.7475011
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.559]
 [0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]] [[41.149]
 [37.479]
 [41.149]
 [41.149]
 [41.149]
 [41.149]
 [41.149]] [[1.611]
 [1.59 ]
 [1.611]
 [1.611]
 [1.611]
 [1.611]
 [1.611]]
Printing some Q and Qe and total Qs values:  [[0.422]
 [0.467]
 [0.424]
 [0.424]
 [0.401]
 [0.426]
 [0.401]] [[40.116]
 [34.024]
 [39.196]
 [40.02 ]
 [42.736]
 [40.416]
 [42.736]] [[2.397]
 [1.93 ]
 [2.321]
 [2.391]
 [2.595]
 [2.426]
 [2.595]]
maxi score, test score, baseline:  0.1506333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.3864, 0.0083, 0.0735, 0.1062, 0.1659, 0.0854, 0.1742],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0049, 0.9565, 0.0028, 0.0029, 0.0015, 0.0013, 0.0301],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0661, 0.0124, 0.6060, 0.0644, 0.0566, 0.1131, 0.0814],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1253, 0.0058, 0.1147, 0.2335, 0.1743, 0.1732, 0.1732],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1413, 0.0023, 0.1104, 0.1364, 0.3513, 0.1065, 0.1518],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1061, 0.0015, 0.0961, 0.1063, 0.1160, 0.4548, 0.1191],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1432, 0.1336, 0.0774, 0.1157, 0.1194, 0.0768, 0.3338],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  43.77225392526146
printing an ep nov before normalisation:  40.57227611541748
maxi score, test score, baseline:  0.1506333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.110162462950846
actor:  1 policy actor:  1  step number:  50 total reward:  0.44666666666666677  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  51 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1506333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.69856797139666
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.16211980452954
printing an ep nov before normalisation:  35.99177572332999
actor:  1 policy actor:  1  step number:  64 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1506333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.634099184034135
printing an ep nov before normalisation:  27.024005073147325
maxi score, test score, baseline:  0.1506333333333332 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  39.120427369178216
maxi score, test score, baseline:  0.1506333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1506333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.86958637844817
maxi score, test score, baseline:  0.1506333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.14768666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  52 total reward:  0.23333333333333273  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.737]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.652]
 [0.737]
 [0.652]
 [0.652]
 [0.652]
 [0.652]
 [0.652]]
actions average: 
K:  1  action  0 :  tensor([0.3238, 0.0611, 0.1106, 0.1247, 0.1364, 0.1000, 0.1433],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0169, 0.9153, 0.0105, 0.0148, 0.0115, 0.0126, 0.0184],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1428, 0.0056, 0.2590, 0.1772, 0.1534, 0.1399, 0.1222],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1047, 0.0253, 0.1132, 0.3719, 0.1076, 0.1202, 0.1571],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1520, 0.0046, 0.1179, 0.1279, 0.3631, 0.1239, 0.1105],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0501, 0.0458, 0.1214, 0.0654, 0.0498, 0.6048, 0.0627],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1595, 0.0232, 0.1146, 0.1202, 0.1056, 0.0956, 0.3814],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.14676666666666655 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.61311996912918
printing an ep nov before normalisation:  43.201966790954685
printing an ep nov before normalisation:  42.170889969731
Printing some Q and Qe and total Qs values:  [[0.824]
 [0.844]
 [0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.752]] [[38.133]
 [28.912]
 [38.133]
 [38.133]
 [38.133]
 [38.133]
 [38.158]] [[0.824]
 [0.844]
 [0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.752]]
printing an ep nov before normalisation:  34.84342686665025
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  53 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  34.333035194849536
maxi score, test score, baseline:  0.14569999999999989 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14569999999999989 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.4632, 0.0120, 0.0737, 0.0757, 0.1565, 0.1001, 0.1188],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0043, 0.9393, 0.0133, 0.0059, 0.0019, 0.0081, 0.0272],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0804, 0.0047, 0.5414, 0.0637, 0.0887, 0.1246, 0.0967],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0966, 0.0034, 0.1008, 0.4246, 0.1343, 0.1205, 0.1197],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0813, 0.0027, 0.0512, 0.0545, 0.6399, 0.0861, 0.0842],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1214, 0.0039, 0.1372, 0.1068, 0.1556, 0.3365, 0.1385],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1008, 0.1303, 0.0870, 0.0929, 0.0913, 0.0939, 0.4039],
       grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14569999999999989 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.009723164761624
actor:  0 policy actor:  0  step number:  47 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  34 total reward:  0.5533333333333333  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  39.333609075348846
printing an ep nov before normalisation:  36.79175007936952
maxi score, test score, baseline:  0.1446733333333332 0.6913333333333336 0.6913333333333336
line 256 mcts: sample exp_bonus 41.195535687832184
printing an ep nov before normalisation:  43.49064003823571
printing an ep nov before normalisation:  24.713893835035464
actions average: 
K:  2  action  0 :  tensor([0.2993, 0.0064, 0.1256, 0.1265, 0.1525, 0.1422, 0.1475],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0022,     0.9731,     0.0013,     0.0011,     0.0002,     0.0003,
            0.0218], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0898, 0.0107, 0.3770, 0.1243, 0.1235, 0.1434, 0.1313],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0881, 0.0027, 0.0985, 0.3911, 0.1236, 0.1712, 0.1248],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1269, 0.0037, 0.0795, 0.1222, 0.3786, 0.1140, 0.1751],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0746, 0.0198, 0.1110, 0.1225, 0.0904, 0.4634, 0.1183],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1625, 0.1107, 0.1036, 0.0951, 0.0897, 0.0837, 0.3547],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.14128666666666653 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.65343533614326
maxi score, test score, baseline:  0.14128666666666653 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.910400404435553
siam score:  -0.755273
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  54 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  65 total reward:  0.05333333333333268  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.13908666666666653 0.6913333333333336 0.6913333333333336
actor:  0 policy actor:  0  step number:  55 total reward:  0.41333333333333333  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  2  action  0 :  tensor([0.3159, 0.0752, 0.1204, 0.1001, 0.1221, 0.1264, 0.1400],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0173, 0.8720, 0.0248, 0.0051, 0.0055, 0.0065, 0.0688],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1466, 0.0145, 0.2786, 0.1109, 0.1281, 0.1797, 0.1416],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1648, 0.0024, 0.1531, 0.1649, 0.1798, 0.1716, 0.1634],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1349, 0.0316, 0.0810, 0.0849, 0.4865, 0.0894, 0.0916],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0944, 0.0021, 0.1305, 0.1082, 0.0884, 0.4906, 0.0858],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1185, 0.2279, 0.0972, 0.0842, 0.0872, 0.1014, 0.2837],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  43.52752208709717
maxi score, test score, baseline:  0.13852666666666655 0.6913333333333336 0.6913333333333336
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  45.50267883672355
printing an ep nov before normalisation:  45.31970928690523
maxi score, test score, baseline:  0.13852666666666655 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13852666666666655 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.13564109802246
printing an ep nov before normalisation:  50.66886901855299
actor:  1 policy actor:  1  step number:  52 total reward:  0.40666666666666673  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.13852666666666655 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.232043769180194
maxi score, test score, baseline:  0.13852666666666655 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  55 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.13785999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13785999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.13785999999999987 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  44.520639320996835
printing an ep nov before normalisation:  37.80324725091711
maxi score, test score, baseline:  0.13785999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  35 total reward:  0.5866666666666668  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  36.06599569320679
actor:  0 policy actor:  0  step number:  44 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.62203193927202
printing an ep nov before normalisation:  18.32387924194336
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.169]
 [0.169]
 [0.192]
 [0.125]
 [0.342]
 [0.169]] [[34.699]
 [34.699]
 [34.699]
 [31.493]
 [29.553]
 [45.851]
 [34.699]] [[0.599]
 [0.599]
 [0.599]
 [0.565]
 [0.464]
 [0.969]
 [0.599]]
printing an ep nov before normalisation:  53.30659477621414
printing an ep nov before normalisation:  40.48909562373229
printing an ep nov before normalisation:  18.534087171282888
actor:  1 policy actor:  1  step number:  67 total reward:  0.3466666666666659  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  39.416207822167905
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.63018253344785
maxi score, test score, baseline:  0.1369933333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1369933333333332 0.6913333333333336 0.6913333333333336
actor:  1 policy actor:  1  step number:  57 total reward:  0.15999999999999914  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1369933333333332 0.6913333333333336 0.6913333333333336
actor:  1 policy actor:  1  step number:  68 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.14 ]
 [0.325]
 [0.141]
 [0.167]
 [0.152]
 [0.131]
 [0.251]] [[32.189]
 [36.467]
 [40.111]
 [29.768]
 [36.2  ]
 [28.125]
 [29.943]] [[0.409]
 [0.67 ]
 [0.549]
 [0.394]
 [0.492]
 [0.328]
 [0.481]]
maxi score, test score, baseline:  0.1369933333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.75718874
Printing some Q and Qe and total Qs values:  [[0.492]
 [0.533]
 [0.492]
 [0.492]
 [0.492]
 [0.492]
 [0.492]] [[35.545]
 [39.531]
 [35.545]
 [35.545]
 [35.545]
 [35.545]
 [35.545]] [[0.719]
 [0.809]
 [0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]]
printing an ep nov before normalisation:  38.99316940150548
siam score:  -0.75502974
maxi score, test score, baseline:  0.13360666666666654 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.13360666666666654 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13360666666666654 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.580417694816106
actor:  1 policy actor:  1  step number:  55 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.484]
 [0.48 ]
 [0.474]
 [0.474]
 [0.48 ]
 [0.478]] [[47.374]
 [43.872]
 [46.486]
 [45.962]
 [46.045]
 [46.675]
 [46.867]] [[1.069]
 [1.01 ]
 [1.057]
 [1.041]
 [1.043]
 [1.061]
 [1.064]]
Printing some Q and Qe and total Qs values:  [[0.078]
 [0.243]
 [0.078]
 [0.078]
 [0.078]
 [0.078]
 [0.078]] [[ 0.   ]
 [34.238]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.873]
 [ 1.08 ]
 [-0.873]
 [-0.873]
 [-0.873]
 [-0.873]
 [-0.873]]
maxi score, test score, baseline:  0.13360666666666654 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.5425727524492
siam score:  -0.7482477
actor:  1 policy actor:  1  step number:  53 total reward:  0.48  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.13360666666666654 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.444]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.376]] [[38.251]
 [43.746]
 [38.251]
 [38.251]
 [38.251]
 [38.251]
 [38.251]] [[1.19 ]
 [1.488]
 [1.19 ]
 [1.19 ]
 [1.19 ]
 [1.19 ]
 [1.19 ]]
maxi score, test score, baseline:  0.13360666666666654 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.13360666666666654 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.13360666666666654 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.769]
 [0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]] [[34.187]
 [27.704]
 [34.187]
 [34.187]
 [34.187]
 [34.187]
 [34.187]] [[1.925]
 [1.754]
 [1.925]
 [1.925]
 [1.925]
 [1.925]
 [1.925]]
line 256 mcts: sample exp_bonus 32.24156758973885
actor:  1 policy actor:  1  step number:  67 total reward:  0.19999999999999907  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  62 total reward:  0.29999999999999927  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.459]
 [0.451]
 [0.626]
 [0.626]
 [0.474]
 [0.626]] [[35.287]
 [40.295]
 [37.995]
 [35.287]
 [35.287]
 [36.866]
 [35.287]] [[1.453]
 [1.505]
 [1.397]
 [1.453]
 [1.453]
 [1.371]
 [1.453]]
siam score:  -0.75488126
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([0.3880, 0.0340, 0.1036, 0.0988, 0.1128, 0.1072, 0.1556],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0011,     0.9700,     0.0011,     0.0018,     0.0002,     0.0002,
            0.0257], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1087, 0.0911, 0.2921, 0.1346, 0.1100, 0.1467, 0.1168],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1478, 0.0814, 0.1236, 0.2172, 0.1444, 0.1343, 0.1512],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1019, 0.0021, 0.0674, 0.0610, 0.6217, 0.0682, 0.0776],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0678, 0.0016, 0.1080, 0.0661, 0.0595, 0.6319, 0.0652],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1600, 0.0861, 0.1130, 0.1127, 0.1131, 0.1203, 0.2949],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.13360666666666654 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  58 total reward:  0.23333333333333273  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13360666666666654 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.492]
 [0.417]
 [0.417]
 [0.417]
 [0.417]
 [0.417]] [[22.689]
 [42.814]
 [22.689]
 [22.689]
 [22.689]
 [22.689]
 [22.689]] [[0.507]
 [0.743]
 [0.507]
 [0.507]
 [0.507]
 [0.507]
 [0.507]]
actor:  1 policy actor:  1  step number:  65 total reward:  0.0933333333333326  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.13360666666666654 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.13360666666666654 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.127726554870605
actions average: 
K:  1  action  0 :  tensor([0.3859, 0.0062, 0.0962, 0.1192, 0.1868, 0.1002, 0.1055],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0095, 0.9335, 0.0058, 0.0082, 0.0045, 0.0046, 0.0339],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0738, 0.0309, 0.4617, 0.0880, 0.0836, 0.1645, 0.0976],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0950, 0.0123, 0.0733, 0.4601, 0.1702, 0.0847, 0.1044],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1404, 0.0025, 0.0922, 0.1130, 0.4608, 0.0918, 0.0993],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1084, 0.0011, 0.1212, 0.1172, 0.1194, 0.4166, 0.1162],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1331, 0.0667, 0.1177, 0.1381, 0.1624, 0.1349, 0.2471],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.13360666666666654 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.13360666666666654 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  3  action  0 :  tensor([0.3817, 0.0018, 0.0979, 0.1201, 0.1534, 0.1297, 0.1155],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0068, 0.9535, 0.0051, 0.0082, 0.0030, 0.0035, 0.0200],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1070, 0.0036, 0.2959, 0.1270, 0.1010, 0.2466, 0.1190],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1072, 0.1054, 0.0746, 0.2788, 0.1573, 0.0992, 0.1776],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1791, 0.0077, 0.0664, 0.0660, 0.5008, 0.0949, 0.0851],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1178, 0.0299, 0.1330, 0.1183, 0.1482, 0.3336, 0.1191],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1938, 0.2481, 0.0726, 0.0755, 0.1074, 0.0733, 0.2294],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  54 total reward:  0.2999999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.13360666666666654 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.13360666666666654 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13360666666666654 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.13360666666666654 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13360666666666654 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.77740582845357
printing an ep nov before normalisation:  31.56516287678477
printing an ep nov before normalisation:  54.42729369713283
maxi score, test score, baseline:  0.13360666666666654 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.0395592368617
actor:  1 policy actor:  1  step number:  42 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  37.118148940381424
actor:  0 policy actor:  0  step number:  44 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.13313999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.98557000572299
Printing some Q and Qe and total Qs values:  [[0.21 ]
 [0.211]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.21 ]
 [0.21 ]] [[42.008]
 [44.135]
 [42.008]
 [42.008]
 [42.008]
 [42.008]
 [42.008]] [[1.308]
 [1.418]
 [1.308]
 [1.308]
 [1.308]
 [1.308]
 [1.308]]
printing an ep nov before normalisation:  40.22584778350204
actor:  1 policy actor:  1  step number:  42 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  42.74489651976691
printing an ep nov before normalisation:  53.687560825064075
maxi score, test score, baseline:  0.13313999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13313999999999987 0.6913333333333336 0.6913333333333336
actions average: 
K:  0  action  0 :  tensor([0.5759, 0.0027, 0.0538, 0.0769, 0.1465, 0.0619, 0.0823],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0086, 0.9408, 0.0058, 0.0067, 0.0032, 0.0038, 0.0312],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1433, 0.0041, 0.3361, 0.1349, 0.1161, 0.1233, 0.1421],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0968, 0.0110, 0.0814, 0.4875, 0.1072, 0.0942, 0.1219],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1121, 0.0083, 0.0903, 0.1074, 0.5046, 0.0800, 0.0973],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0836, 0.0064, 0.0855, 0.0827, 0.0717, 0.5918, 0.0783],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1224, 0.0942, 0.1232, 0.1339, 0.1116, 0.1102, 0.3046],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.13313999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.964870033310405
maxi score, test score, baseline:  0.13313999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.42000000000000015  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.338]
 [0.257]
 [0.246]
 [0.257]
 [0.246]
 [0.246]] [[37.177]
 [35.001]
 [33.754]
 [37.177]
 [34.409]
 [37.177]
 [37.177]] [[1.428]
 [1.396]
 [1.244]
 [1.428]
 [1.282]
 [1.428]
 [1.428]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  2.0
siam score:  -0.76271147
printing an ep nov before normalisation:  44.18473720550537
maxi score, test score, baseline:  0.13313999999999987 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.13313999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.105555775528245
actor:  1 policy actor:  1  step number:  64 total reward:  0.4066666666666662  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.13313999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13313999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.38649456411361
printing an ep nov before normalisation:  47.92453706296935
printing an ep nov before normalisation:  30.456698306332186
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.075]
 [0.042]
 [0.037]
 [0.047]
 [0.036]
 [0.04 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.036]
 [0.075]
 [0.042]
 [0.037]
 [0.047]
 [0.036]
 [0.04 ]]
printing an ep nov before normalisation:  37.17112983974241
maxi score, test score, baseline:  0.13313999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  32 total reward:  0.5533333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.13285999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.23982141974983
actions average: 
K:  0  action  0 :  tensor([0.3680, 0.0048, 0.1220, 0.1194, 0.1125, 0.1583, 0.1150],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0078, 0.9677, 0.0014, 0.0034, 0.0030, 0.0012, 0.0156],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0682, 0.1303, 0.4388, 0.0859, 0.0717, 0.1269, 0.0782],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1115, 0.0169, 0.0953, 0.4490, 0.0902, 0.1373, 0.0998],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1130, 0.0042, 0.0946, 0.1126, 0.4808, 0.1042, 0.0906],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0557, 0.0740, 0.1805, 0.0633, 0.0495, 0.5213, 0.0557],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1872, 0.0175, 0.1043, 0.1066, 0.1066, 0.1413, 0.3365],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  71 total reward:  0.19999999999999885  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.13285999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.46789011306518
printing an ep nov before normalisation:  44.340407807168056
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13285999999999987 0.6913333333333336 0.6913333333333336
actor:  1 policy actor:  1  step number:  53 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  38.6835626917903
maxi score, test score, baseline:  0.13285999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13285999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.91322336263819
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13285999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.565]
 [0.569]
 [0.538]
 [0.498]
 [0.458]
 [0.53 ]] [[38.289]
 [46.966]
 [45.067]
 [33.309]
 [40.029]
 [42.433]
 [39.541]] [[0.74 ]
 [0.839]
 [0.832]
 [0.733]
 [0.732]
 [0.705]
 [0.76 ]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  64 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7579416
maxi score, test score, baseline:  0.13285999999999987 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.13285999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7592643
printing an ep nov before normalisation:  49.68965111810144
maxi score, test score, baseline:  0.13285999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.4666666666666668  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.13285999999999987 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.13285999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.46912432727088
maxi score, test score, baseline:  0.13285999999999987 0.6913333333333336 0.6913333333333336
Printing some Q and Qe and total Qs values:  [[1.003]
 [0.997]
 [0.974]
 [0.974]
 [0.974]
 [0.974]
 [0.984]] [[14.907]
 [17.361]
 [14.789]
 [14.789]
 [14.789]
 [14.789]
 [14.96 ]] [[2.257]
 [2.458]
 [2.219]
 [2.219]
 [2.219]
 [2.219]
 [2.243]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2733333333333332  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.13285999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.45181411702184
actor:  1 policy actor:  1  step number:  74 total reward:  0.2733333333333322  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.13285999999999987 0.6913333333333336 0.6913333333333336
actor:  1 policy actor:  1  step number:  48 total reward:  0.4866666666666669  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  52.1204555663726
printing an ep nov before normalisation:  59.64328949706341
maxi score, test score, baseline:  0.13285999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13285999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.506]
 [0.409]
 [0.405]
 [0.454]
 [0.407]
 [0.454]] [[32.267]
 [37.416]
 [34.674]
 [32.392]
 [36.402]
 [32.503]
 [36.402]] [[0.639]
 [0.821]
 [0.684]
 [0.647]
 [0.754]
 [0.65 ]
 [0.754]]
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.74 ]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]] [[36.665]
 [38.638]
 [36.665]
 [36.665]
 [36.665]
 [36.665]
 [36.665]] [[0.651]
 [0.74 ]
 [0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]]
printing an ep nov before normalisation:  27.83391262069359
siam score:  -0.7593062
actor:  0 policy actor:  0  step number:  66 total reward:  0.08666666666666611  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 52.66736905734256
maxi score, test score, baseline:  0.12825999999999987 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  40.23410032240284
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]
 [0.408]] [[42.04]
 [42.04]
 [42.04]
 [42.04]
 [42.04]
 [42.04]
 [42.04]] [[1.914]
 [1.914]
 [1.914]
 [1.914]
 [1.914]
 [1.914]
 [1.914]]
maxi score, test score, baseline:  0.12825999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12825999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  0.12666666666666604  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  38.75035570171782
printing an ep nov before normalisation:  40.16549065697159
printing an ep nov before normalisation:  50.62391375047436
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12825999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12825999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.486379443983374
printing an ep nov before normalisation:  38.45986298951896
actor:  0 policy actor:  0  step number:  43 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.12764666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12764666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.60602002438225
maxi score, test score, baseline:  0.12764666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12764666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.187286923182622
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  42 total reward:  0.4066666666666662  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.12708666666666651 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.96846072494456
actor:  0 policy actor:  0  step number:  41 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.12655333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.631]
 [0.376]
 [0.379]
 [0.381]
 [0.374]
 [0.377]] [[34.561]
 [46.555]
 [34.49 ]
 [34.919]
 [34.842]
 [34.878]
 [34.415]] [[0.631]
 [1.017]
 [0.603]
 [0.613]
 [0.613]
 [0.607]
 [0.604]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12655333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.3730, 0.0088, 0.1038, 0.1156, 0.1552, 0.1261, 0.1174],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0065, 0.9447, 0.0080, 0.0051, 0.0046, 0.0034, 0.0277],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0881, 0.0291, 0.5312, 0.0826, 0.0811, 0.0839, 0.1040],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1033, 0.0416, 0.1432, 0.2309, 0.1616, 0.1237, 0.1957],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1084, 0.0044, 0.0926, 0.0896, 0.5018, 0.0877, 0.1156],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0958, 0.0445, 0.1348, 0.1100, 0.1040, 0.3920, 0.1188],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1284, 0.1886, 0.1096, 0.0676, 0.0743, 0.0866, 0.3450],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.12655333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12655333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.8  ]
 [0.719]
 [0.719]
 [0.676]
 [0.677]
 [0.67 ]] [[26.105]
 [31.405]
 [30.807]
 [30.807]
 [26.462]
 [26.52 ]
 [27.434]] [[1.827]
 [2.412]
 [2.278]
 [2.278]
 [1.857]
 [1.863]
 [1.936]]
printing an ep nov before normalisation:  17.95529545128202
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.2799999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12655333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12655333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.17333333333333312  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  40.67747921153481
printing an ep nov before normalisation:  43.99774172644015
printing an ep nov before normalisation:  40.570194839430265
maxi score, test score, baseline:  0.12655333333333318 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  48.99634954818423
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12655333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  2  action  0 :  tensor([0.2969, 0.0093, 0.1116, 0.1626, 0.1412, 0.1594, 0.1190],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0083, 0.9362, 0.0102, 0.0059, 0.0026, 0.0030, 0.0339],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1367, 0.0240, 0.3694, 0.0974, 0.1070, 0.1537, 0.1118],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1915, 0.0130, 0.1315, 0.1562, 0.1218, 0.1913, 0.1948],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1478, 0.0051, 0.0948, 0.0940, 0.4291, 0.1275, 0.1017],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0857, 0.0050, 0.1176, 0.0564, 0.0567, 0.6170, 0.0616],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1552, 0.1115, 0.0747, 0.1007, 0.0758, 0.0904, 0.3916],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  55 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.12655333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12655333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12655333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.05759896493466
actor:  1 policy actor:  1  step number:  47 total reward:  0.4133333333333329  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12655333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12655333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.32940174196804
actor:  1 policy actor:  1  step number:  47 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.839]
 [0.876]
 [0.839]
 [0.839]
 [0.839]
 [0.839]
 [0.767]] [[38.012]
 [39.878]
 [38.012]
 [38.012]
 [38.012]
 [38.012]
 [31.553]] [[0.839]
 [0.876]
 [0.839]
 [0.839]
 [0.839]
 [0.839]
 [0.767]]
maxi score, test score, baseline:  0.12655333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12655333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.21361491749869
maxi score, test score, baseline:  0.12655333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.855952354637296
maxi score, test score, baseline:  0.12655333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12655333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.419]
 [0.758]
 [0.544]
 [0.38 ]
 [0.438]
 [0.544]
 [0.419]] [[26.762]
 [36.146]
 [36.416]
 [25.492]
 [28.048]
 [36.416]
 [25.722]] [[0.548]
 [0.976]
 [0.766]
 [0.497]
 [0.579]
 [0.766]
 [0.537]]
line 256 mcts: sample exp_bonus 45.288026496773206
printing an ep nov before normalisation:  56.84306829465668
actor:  0 policy actor:  0  step number:  42 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.12601999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.592874005982644
siam score:  -0.75082666
printing an ep nov before normalisation:  39.18492026522592
maxi score, test score, baseline:  0.12601999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  41.40810487791258
maxi score, test score, baseline:  0.12601999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  58 total reward:  0.17999999999999927  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.393587195703375
maxi score, test score, baseline:  0.12601999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12601999999999985 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.12601999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12601999999999985 0.6913333333333336 0.6913333333333336
actions average: 
K:  1  action  0 :  tensor([0.3672, 0.0058, 0.1273, 0.1187, 0.1330, 0.1241, 0.1240],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0109, 0.9441, 0.0096, 0.0039, 0.0028, 0.0037, 0.0250],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0681, 0.0191, 0.6201, 0.0604, 0.0648, 0.0960, 0.0715],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1294, 0.0289, 0.1312, 0.3386, 0.1129, 0.1312, 0.1277],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1490, 0.0019, 0.1046, 0.0921, 0.4403, 0.1014, 0.1108],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0895, 0.0071, 0.1492, 0.1013, 0.0893, 0.4646, 0.0989],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1172, 0.1024, 0.1278, 0.1200, 0.1144, 0.1176, 0.3007],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.12601999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.2133333333333326  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  46 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.12601999999999985 0.6913333333333336 0.6913333333333336
line 256 mcts: sample exp_bonus 38.37811439206396
printing an ep nov before normalisation:  10.865388825132086
maxi score, test score, baseline:  0.12601999999999985 0.6913333333333336 0.6913333333333336
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12601999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12601999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.78441427138294
actions average: 
K:  1  action  0 :  tensor([0.2274, 0.0082, 0.1462, 0.1541, 0.1483, 0.1428, 0.1729],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0043, 0.9546, 0.0033, 0.0035, 0.0019, 0.0023, 0.0302],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0856, 0.0750, 0.3482, 0.1186, 0.1223, 0.1380, 0.1124],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0798, 0.0757, 0.0973, 0.4408, 0.0981, 0.1014, 0.1069],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0900, 0.0286, 0.0564, 0.0709, 0.6488, 0.0455, 0.0598],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0780, 0.0628, 0.1130, 0.1116, 0.0999, 0.4510, 0.0837],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1268, 0.0419, 0.1241, 0.1333, 0.1407, 0.1138, 0.3195],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.12601999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
Printing some Q and Qe and total Qs values:  [[-0.018]
 [-0.002]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]
 [-0.018]] [[ 0.   ]
 [50.641]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.455]
 [ 0.467]
 [-0.455]
 [-0.455]
 [-0.455]
 [-0.455]
 [-0.455]]
printing an ep nov before normalisation:  48.54547932847141
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  34.6329402923584
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12601999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.12601999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  43 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  45.19864314154218
siam score:  -0.75610083
maxi score, test score, baseline:  0.12636666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.62192837309504
actor:  0 policy actor:  0  step number:  52 total reward:  0.23333333333333273  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12620666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12620666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.952190709806395
Printing some Q and Qe and total Qs values:  [[0.72 ]
 [0.771]
 [0.72 ]
 [0.72 ]
 [0.72 ]
 [0.72 ]
 [0.72 ]] [[44.092]
 [51.872]
 [44.092]
 [44.092]
 [44.092]
 [44.092]
 [44.092]] [[0.72 ]
 [0.771]
 [0.72 ]
 [0.72 ]
 [0.72 ]
 [0.72 ]
 [0.72 ]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7527101
maxi score, test score, baseline:  0.12620666666666652 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.12620666666666652 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  37.90724953802693
printing an ep nov before normalisation:  40.13443713420677
line 256 mcts: sample exp_bonus 37.081465108240174
printing an ep nov before normalisation:  39.91661599947376
actor:  0 policy actor:  0  step number:  40 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.29333333333333333  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.007]
 [ 0.013]
 [-0.007]
 [ 0.023]
 [ 0.044]
 [-0.007]
 [-0.007]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.007]
 [ 0.013]
 [-0.007]
 [ 0.023]
 [ 0.044]
 [-0.007]
 [-0.007]]
printing an ep nov before normalisation:  39.86519084101635
printing an ep nov before normalisation:  34.16925590300499
using explorer policy with actor:  1
siam score:  -0.7560864
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  43.219274757050044
printing an ep nov before normalisation:  25.946250838342795
printing an ep nov before normalisation:  39.24592186319499
printing an ep nov before normalisation:  36.52726483417561
actor:  0 policy actor:  0  step number:  38 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  28 total reward:  0.6333333333333334  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.13036666666666652 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.13036666666666652 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  53.73333317130545
actor:  1 policy actor:  1  step number:  53 total reward:  0.38666666666666605  reward:  1.0 rdn_beta:  1.333
siam score:  -0.7631552
maxi score, test score, baseline:  0.13036666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13036666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.572891974029215
maxi score, test score, baseline:  0.13036666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13036666666666652 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.13036666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13036666666666652 0.6913333333333336 0.6913333333333336
Printing some Q and Qe and total Qs values:  [[0.26 ]
 [0.417]
 [0.232]
 [0.232]
 [0.259]
 [0.257]
 [0.25 ]] [[35.132]
 [41.153]
 [40.945]
 [39.381]
 [37.214]
 [37.734]
 [38.053]] [[0.924]
 [1.307]
 [1.113]
 [1.055]
 [1.001]
 [1.018]
 [1.023]]
UNIT TEST: sample policy line 217 mcts : [0.184 0.224 0.082 0.184 0.122 0.082 0.122]
actor:  0 policy actor:  0  step number:  50 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  59.32037394276262
actor:  1 policy actor:  1  step number:  50 total reward:  0.36666666666666603  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  53.4443391500053
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  39.134307711603896
printing an ep nov before normalisation:  54.97392369457999
printing an ep nov before normalisation:  40.219583180550636
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.426]
 [0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]] [[35.029]
 [43.025]
 [35.029]
 [35.029]
 [35.029]
 [35.029]
 [35.029]] [[0.705]
 [0.953]
 [0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]]
maxi score, test score, baseline:  0.13080666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.426]
 [0.353]
 [0.353]] [[39.816]
 [39.816]
 [39.816]
 [39.816]
 [43.967]
 [39.816]
 [39.816]] [[0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.927]
 [0.777]
 [0.777]]
maxi score, test score, baseline:  0.13080666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  60.99272581749165
maxi score, test score, baseline:  0.13080666666666652 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.13080666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0012340758189566259
actor:  1 policy actor:  1  step number:  57 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.13080666666666652 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  61.05956240028856
maxi score, test score, baseline:  0.13080666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13080666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.13080666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.418]
 [0.344]
 [0.344]] [[38.189]
 [38.189]
 [38.189]
 [38.189]
 [38.972]
 [38.189]
 [38.189]] [[0.984]
 [0.984]
 [0.984]
 [0.984]
 [1.083]
 [0.984]
 [0.984]]
printing an ep nov before normalisation:  43.70787545565967
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]] [[35.599]
 [35.599]
 [35.599]
 [35.599]
 [35.599]
 [35.599]
 [35.599]] [[0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]
 [0.539]]
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]] [[34.224]
 [34.224]
 [33.068]
 [34.224]
 [34.224]
 [34.224]
 [33.241]] [[1.424]
 [1.424]
 [1.364]
 [1.424]
 [1.424]
 [1.424]
 [1.372]]
maxi score, test score, baseline:  0.13080666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.46480997930371
printing an ep nov before normalisation:  17.9297713951325
actor:  1 policy actor:  1  step number:  55 total reward:  0.37333333333333285  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.67092201523252
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  37.18739803363178
maxi score, test score, baseline:  0.13080666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13080666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1285533333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.518]
 [0.522]
 [0.503]
 [0.511]
 [0.499]
 [0.502]] [[38.668]
 [38.899]
 [38.266]
 [39.001]
 [39.493]
 [40.228]
 [40.472]] [[0.498]
 [0.518]
 [0.522]
 [0.503]
 [0.511]
 [0.499]
 [0.502]]
printing an ep nov before normalisation:  51.8777394009133
maxi score, test score, baseline:  0.1258733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7580401
printing an ep nov before normalisation:  56.489411744586626
maxi score, test score, baseline:  0.1258733333333332 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.1258733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1258733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5200000000000001  reward:  1.0 rdn_beta:  1.667
siam score:  -0.7508532
actor:  1 policy actor:  1  step number:  47 total reward:  0.4933333333333335  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1258733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1258733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1258733333333332 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  34.22515153884888
printing an ep nov before normalisation:  31.878284889779103
actor:  1 policy actor:  1  step number:  65 total reward:  0.09333333333333271  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.09 ]
 [0.138]
 [0.109]
 [0.088]
 [0.109]
 [0.109]
 [0.146]] [[38.731]
 [56.454]
 [46.212]
 [45.925]
 [46.212]
 [46.212]
 [51.763]] [[0.528]
 [1.142]
 [0.786]
 [0.756]
 [0.786]
 [0.786]
 [1.   ]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.243]
 [0.24 ]
 [0.232]
 [0.271]
 [0.229]
 [0.246]] [[32.487]
 [40.332]
 [33.145]
 [33.583]
 [41.395]
 [32.91 ]
 [32.417]] [[0.904]
 [1.201]
 [0.912]
 [0.921]
 [1.271]
 [0.891]
 [0.889]]
printing an ep nov before normalisation:  45.66816040293145
Printing some Q and Qe and total Qs values:  [[-0.042]
 [-0.049]
 [ 0.244]
 [ 0.244]
 [ 0.244]
 [ 0.244]
 [-0.089]] [[47.375]
 [44.962]
 [38.75 ]
 [38.75 ]
 [38.75 ]
 [38.75 ]
 [46.555]] [[1.476]
 [1.321]
 [1.234]
 [1.234]
 [1.234]
 [1.234]
 [1.378]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1258733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1258733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  0.37333333333333274  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1258733333333332 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  35.97386162663887
maxi score, test score, baseline:  0.1258733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.373]
 [0.49 ]
 [0.398]
 [0.39 ]
 [0.392]
 [0.41 ]
 [0.377]] [[43.363]
 [45.237]
 [43.736]
 [44.081]
 [44.048]
 [45.367]
 [42.695]] [[1.811]
 [2.032]
 [1.857]
 [1.868]
 [1.868]
 [1.96 ]
 [1.778]]
maxi score, test score, baseline:  0.1258733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.59175418350286
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.441272379842616
printing an ep nov before normalisation:  40.27364772768995
printing an ep nov before normalisation:  42.11042725520758
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.38  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1258733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1258733333333332 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  45.23684908735607
Printing some Q and Qe and total Qs values:  [[0.734]
 [0.828]
 [0.734]
 [0.734]
 [0.783]
 [0.734]
 [0.821]] [[38.218]
 [35.625]
 [38.218]
 [38.218]
 [42.009]
 [38.218]
 [40.189]] [[0.734]
 [0.828]
 [0.734]
 [0.734]
 [0.783]
 [0.734]
 [0.821]]
actor:  1 policy actor:  1  step number:  63 total reward:  0.1999999999999994  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  51.58242053647472
Printing some Q and Qe and total Qs values:  [[0.767]
 [0.755]
 [0.767]
 [0.767]
 [0.767]
 [0.767]
 [0.767]] [[34.82]
 [41.77]
 [34.82]
 [34.82]
 [34.82]
 [34.82]
 [34.82]] [[0.767]
 [0.755]
 [0.767]
 [0.767]
 [0.767]
 [0.767]
 [0.767]]
actions average: 
K:  1  action  0 :  tensor([0.2882, 0.0115, 0.1176, 0.1249, 0.2016, 0.1229, 0.1332],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0126, 0.9205, 0.0159, 0.0102, 0.0032, 0.0041, 0.0334],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1228, 0.0096, 0.2580, 0.1217, 0.1156, 0.2398, 0.1325],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1437, 0.0027, 0.1521, 0.2650, 0.1552, 0.1493, 0.1320],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0956, 0.0023, 0.0948, 0.0910, 0.5224, 0.0965, 0.0973],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1269, 0.0109, 0.1890, 0.1366, 0.1444, 0.2734, 0.1190],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1209, 0.1647, 0.0898, 0.0742, 0.0855, 0.0852, 0.3797],
       grad_fn=<DivBackward0>)
siam score:  -0.76371866
maxi score, test score, baseline:  0.1258733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.208141803741455
maxi score, test score, baseline:  0.1258733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  56 total reward:  0.13999999999999957  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  46.749929886003706
printing an ep nov before normalisation:  36.90419312130354
printing an ep nov before normalisation:  63.38726878166199
maxi score, test score, baseline:  0.12603333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.23596580681144
siam score:  -0.76419973
actor:  0 policy actor:  0  step number:  42 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  65 total reward:  0.11999999999999977  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1263533333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.48293504483587
actor:  0 policy actor:  0  step number:  49 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  50.11523055452237
actor:  0 policy actor:  0  step number:  40 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12699333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.68942849824313
line 256 mcts: sample exp_bonus 25.190413745072792
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.95602492446147
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
Printing some Q and Qe and total Qs values:  [[0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.255]
 [0.255]] [[66.141]
 [66.141]
 [66.141]
 [60.922]
 [66.141]
 [66.141]
 [66.141]] [[1.464]
 [1.464]
 [1.464]
 [1.318]
 [1.464]
 [1.464]
 [1.464]]
maxi score, test score, baseline:  0.12699333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.71320343017578
actor:  1 policy actor:  1  step number:  73 total reward:  0.03999999999999926  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  50.04168633211703
printing an ep nov before normalisation:  20.069272920538044
actor:  1 policy actor:  1  step number:  58 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12699333333333318 0.6913333333333336 0.6913333333333336
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.443]
 [0.301]
 [0.389]
 [0.485]
 [0.39 ]
 [0.475]] [[35.71 ]
 [37.632]
 [35.195]
 [33.231]
 [34.574]
 [33.586]
 [34.59 ]] [[1.643]
 [1.761]
 [1.46 ]
 [1.421]
 [1.604]
 [1.445]
 [1.595]]
printing an ep nov before normalisation:  46.34877840677897
actor:  1 policy actor:  1  step number:  47 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  44.330092477140056
printing an ep nov before normalisation:  41.23639919309911
siam score:  -0.76526904
printing an ep nov before normalisation:  40.39724558503947
maxi score, test score, baseline:  0.12699333333333318 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  46.19108245817388
maxi score, test score, baseline:  0.12699333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  67 total reward:  0.37333333333333285  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  45.827091042221696
maxi score, test score, baseline:  0.12699333333333318 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  44.71549987792969
printing an ep nov before normalisation:  39.47208830524828
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12699333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  48.4956290064125
maxi score, test score, baseline:  0.12699333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12699333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.375115411968345
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.54 ]
 [0.376]
 [0.366]
 [0.37 ]
 [0.375]
 [0.374]] [[42.179]
 [39.128]
 [39.845]
 [42.519]
 [41.862]
 [41.497]
 [42.004]] [[2.277]
 [2.16 ]
 [2.06 ]
 [2.291]
 [2.236]
 [2.208]
 [2.253]]
maxi score, test score, baseline:  0.12699333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.649131298065186
actions average: 
K:  4  action  0 :  tensor([0.1461, 0.0126, 0.1215, 0.2257, 0.1685, 0.1786, 0.1471],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0305, 0.7895, 0.0254, 0.0407, 0.0209, 0.0184, 0.0745],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1055, 0.0123, 0.3417, 0.1591, 0.0620, 0.2052, 0.1143],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0889, 0.1354, 0.0986, 0.2915, 0.1190, 0.1340, 0.1325],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1459, 0.0049, 0.1162, 0.1658, 0.2768, 0.1529, 0.1375],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0937, 0.0320, 0.1237, 0.1552, 0.0963, 0.3375, 0.1616],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1604, 0.0055, 0.1659, 0.1093, 0.1101, 0.1647, 0.2842],
       grad_fn=<DivBackward0>)
siam score:  -0.75998116
actor:  1 policy actor:  1  step number:  69 total reward:  0.06666666666666643  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12699333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12699333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.332]
 [0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.325]] [[40.005]
 [45.23 ]
 [40.005]
 [40.005]
 [40.005]
 [40.005]
 [40.005]] [[1.344]
 [1.608]
 [1.344]
 [1.344]
 [1.344]
 [1.344]
 [1.344]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3866666666666667  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.066]
 [0.078]
 [0.019]] [[39.339]
 [39.339]
 [39.339]
 [39.339]
 [43.523]
 [48.835]
 [39.339]] [[0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.718]
 [0.885]
 [0.548]]
maxi score, test score, baseline:  0.12699333333333318 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.12699333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12699333333333318 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.12699333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.231777761872422
actor:  1 policy actor:  1  step number:  39 total reward:  0.52  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.665]
 [0.681]
 [0.667]
 [0.665]
 [0.669]
 [0.665]] [[22.616]
 [22.616]
 [22.199]
 [22.389]
 [22.616]
 [21.858]
 [22.616]] [[0.665]
 [0.665]
 [0.681]
 [0.667]
 [0.665]
 [0.669]
 [0.665]]
maxi score, test score, baseline:  0.12699333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.475]
 [0.469]
 [0.459]
 [0.46 ]
 [0.459]
 [0.454]] [[62.679]
 [59.153]
 [60.823]
 [64.934]
 [64.496]
 [63.71 ]
 [62.927]] [[0.97 ]
 [0.976]
 [0.991]
 [1.032]
 [1.028]
 [1.017]
 [1.002]]
printing an ep nov before normalisation:  48.01872622312651
maxi score, test score, baseline:  0.12449999999999987 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.12449999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.459]
 [0.537]
 [0.349]
 [0.423]
 [0.358]
 [0.524]] [[38.84 ]
 [34.551]
 [34.389]
 [29.915]
 [42.52 ]
 [31.073]
 [36.362]] [[0.938]
 [0.848]
 [0.923]
 [0.659]
 [0.947]
 [0.688]
 [0.944]]
maxi score, test score, baseline:  0.12449999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  0.4333333333333331  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.479]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.421]
 [0.479]
 [0.421]
 [0.421]
 [0.421]
 [0.421]
 [0.421]]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  46 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.066251176567945
using explorer policy with actor:  1
printing an ep nov before normalisation:  68.51496617380154
printing an ep nov before normalisation:  33.68415214629208
Printing some Q and Qe and total Qs values:  [[0.23 ]
 [0.286]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]
 [0.23 ]] [[31.452]
 [34.831]
 [31.452]
 [31.452]
 [31.452]
 [31.452]
 [31.452]] [[1.765]
 [2.143]
 [1.765]
 [1.765]
 [1.765]
 [1.765]
 [1.765]]
printing an ep nov before normalisation:  47.39662194258848
maxi score, test score, baseline:  0.12155333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12155333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12155333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12155333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.23435446028816
Printing some Q and Qe and total Qs values:  [[0.667]
 [0.697]
 [0.697]
 [0.697]
 [0.697]
 [0.701]
 [0.699]] [[43.093]
 [39.831]
 [39.831]
 [39.831]
 [39.831]
 [40.827]
 [41.12 ]] [[1.983]
 [1.914]
 [1.914]
 [1.914]
 [1.914]
 [1.948]
 [1.955]]
printing an ep nov before normalisation:  21.004600524902344
actor:  1 policy actor:  1  step number:  54 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12155333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.016172719729183882
maxi score, test score, baseline:  0.12155333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.14666666666666628  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  45.79230474497389
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.2  ]
 [0.178]
 [0.18 ]
 [0.181]
 [0.181]
 [0.19 ]] [[57.372]
 [49.871]
 [56.515]
 [56.014]
 [56.713]
 [56.28 ]
 [54.407]] [[1.114]
 [0.977]
 [1.104]
 [1.095]
 [1.112]
 [1.102]
 [1.068]]
maxi score, test score, baseline:  0.12155333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12155333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12155333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.4808222575627
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12155333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  1.0
UNIT TEST: sample policy line 217 mcts : [0.082 0.571 0.122 0.02  0.184 0.    0.02 ]
maxi score, test score, baseline:  0.12155333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12155333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.89696286080368
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.55 ]
 [0.563]
 [0.528]
 [0.526]
 [0.539]
 [0.537]] [[48.185]
 [47.651]
 [43.367]
 [45.377]
 [46.396]
 [46.133]
 [46.068]] [[1.741]
 [1.601]
 [1.446]
 [1.49 ]
 [1.528]
 [1.53 ]
 [1.526]]
printing an ep nov before normalisation:  34.99291331977231
actor:  1 policy actor:  1  step number:  46 total reward:  0.4066666666666666  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]
 [0.779]] [[40.966]
 [40.966]
 [40.966]
 [40.966]
 [40.966]
 [40.966]
 [40.966]] [[1.391]
 [1.391]
 [1.391]
 [1.391]
 [1.391]
 [1.391]
 [1.391]]
maxi score, test score, baseline:  0.12155333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.12155333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12155333333333321 0.6913333333333336 0.6913333333333336
Printing some Q and Qe and total Qs values:  [[ 0.317]
 [ 0.383]
 [-0.078]
 [ 0.317]
 [ 0.317]
 [ 0.317]
 [ 0.366]] [[36.594]
 [38.731]
 [37.828]
 [36.594]
 [36.594]
 [36.594]
 [40.424]] [[1.201]
 [1.366]
 [0.863]
 [1.201]
 [1.201]
 [1.201]
 [1.427]]
maxi score, test score, baseline:  0.12155333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  38 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12195333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12195333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12195333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.820884704589844
printing an ep nov before normalisation:  41.57934676288121
actor:  1 policy actor:  1  step number:  52 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  55.65280201563587
actor:  1 policy actor:  1  step number:  55 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12195333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12195333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.25937532761152
Printing some Q and Qe and total Qs values:  [[0.213]
 [0.245]
 [0.213]
 [0.213]
 [0.213]
 [0.213]
 [0.213]] [[47.338]
 [56.276]
 [47.338]
 [47.338]
 [47.338]
 [47.338]
 [47.338]] [[1.134]
 [1.505]
 [1.134]
 [1.134]
 [1.134]
 [1.134]
 [1.134]]
Printing some Q and Qe and total Qs values:  [[0.188]
 [0.226]
 [0.188]
 [0.188]
 [0.188]
 [0.188]
 [0.09 ]] [[51.636]
 [56.268]
 [51.636]
 [51.636]
 [51.636]
 [51.636]
 [47.923]] [[1.323]
 [1.563]
 [1.323]
 [1.323]
 [1.323]
 [1.323]
 [1.062]]
printing an ep nov before normalisation:  49.328854803192556
maxi score, test score, baseline:  0.12195333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12195333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.12195333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.145]
 [0.252]
 [0.145]
 [0.145]
 [0.145]
 [0.145]
 [0.145]] [[43.742]
 [45.054]
 [43.742]
 [43.742]
 [43.742]
 [43.742]
 [43.742]] [[1.534]
 [1.722]
 [1.534]
 [1.534]
 [1.534]
 [1.534]
 [1.534]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.12195333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.348]
 [0.258]
 [0.258]
 [0.201]
 [0.258]
 [0.222]] [[44.201]
 [46.662]
 [44.201]
 [44.201]
 [42.429]
 [44.201]
 [43.735]] [[0.647]
 [0.779]
 [0.647]
 [0.647]
 [0.56 ]
 [0.647]
 [0.603]]
maxi score, test score, baseline:  0.12195333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.416]
 [0.362]
 [0.362]
 [0.362]
 [0.362]
 [0.362]] [[37.687]
 [38.927]
 [37.687]
 [37.687]
 [37.687]
 [37.687]
 [37.687]] [[1.362]
 [1.483]
 [1.362]
 [1.362]
 [1.362]
 [1.362]
 [1.362]]
maxi score, test score, baseline:  0.12195333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12195333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.895926599172824
printing an ep nov before normalisation:  40.73157255306201
actions average: 
K:  4  action  0 :  tensor([0.3417, 0.0200, 0.1396, 0.1033, 0.1655, 0.1073, 0.1225],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0215, 0.8279, 0.0207, 0.0358, 0.0172, 0.0180, 0.0590],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1474, 0.0189, 0.1873, 0.1454, 0.1258, 0.2286, 0.1465],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1508, 0.0869, 0.1270, 0.2133, 0.1266, 0.1431, 0.1522],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0839, 0.0020, 0.0524, 0.0654, 0.6588, 0.0840, 0.0536],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1457, 0.0026, 0.2109, 0.1119, 0.1488, 0.2584, 0.1217],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1344, 0.0265, 0.1188, 0.2314, 0.0998, 0.1087, 0.2804],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.739]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]] [[40.196]
 [37.408]
 [40.196]
 [40.196]
 [40.196]
 [40.196]
 [40.196]] [[1.282]
 [1.312]
 [1.282]
 [1.282]
 [1.282]
 [1.282]
 [1.282]]
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.593]
 [0.5  ]
 [0.5  ]
 [0.507]
 [0.5  ]
 [0.5  ]] [[40.037]
 [37.956]
 [40.037]
 [40.037]
 [38.527]
 [40.037]
 [40.037]] [[1.973]
 [1.918]
 [1.973]
 [1.973]
 [1.873]
 [1.973]
 [1.973]]
actions average: 
K:  3  action  0 :  tensor([0.3029, 0.0039, 0.1012, 0.1811, 0.2085, 0.0890, 0.1135],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0171, 0.8971, 0.0109, 0.0153, 0.0245, 0.0095, 0.0255],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0999, 0.0104, 0.3760, 0.1142, 0.0939, 0.1948, 0.1108],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1361, 0.0685, 0.1433, 0.2262, 0.1277, 0.1317, 0.1664],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1117, 0.0046, 0.0738, 0.0798, 0.4725, 0.0987, 0.1588],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0848, 0.0193, 0.1210, 0.0663, 0.1154, 0.5248, 0.0683],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1902, 0.0071, 0.1804, 0.1454, 0.1476, 0.1568, 0.1724],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  31.922683715820312
actor:  1 policy actor:  1  step number:  53 total reward:  0.3333333333333328  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  53 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.616]
 [0.685]
 [0.625]
 [0.625]
 [0.626]
 [0.63 ]
 [0.61 ]] [[15.813]
 [10.314]
 [14.962]
 [14.656]
 [14.798]
 [14.694]
 [15.429]] [[2.026]
 [1.604]
 [1.959]
 [1.933]
 [1.946]
 [1.941]
 [1.986]]
actor:  1 policy actor:  1  step number:  68 total reward:  0.1266666666666656  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.039]
 [0.02 ]
 [0.019]
 [0.016]
 [0.011]
 [0.012]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.015]
 [0.039]
 [0.02 ]
 [0.019]
 [0.016]
 [0.011]
 [0.012]]
maxi score, test score, baseline:  0.12195333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12195333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.62342937577691
maxi score, test score, baseline:  0.12195333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76044726
maxi score, test score, baseline:  0.12195333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12195333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.27184890529074
printing an ep nov before normalisation:  48.393748322290854
actor:  1 policy actor:  1  step number:  59 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.12195333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.12195333333333322 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.12195333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.40779621752851
printing an ep nov before normalisation:  5.7211758530684165e-05
actor:  0 policy actor:  0  step number:  52 total reward:  0.206666666666666  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.12139333333333321 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  37.130093574523926
maxi score, test score, baseline:  0.12139333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.29706345039805
actor:  0 policy actor:  0  step number:  38 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  51 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.465]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.233]] [[36.724]
 [50.217]
 [36.724]
 [36.724]
 [36.724]
 [36.724]
 [49.11 ]] [[0.604]
 [0.978]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.729]]
actor:  0 policy actor:  0  step number:  44 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.12224666666666655 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12224666666666655 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  0.5666666666666668  reward:  1.0 rdn_beta:  0.667
Starting evaluation
maxi score, test score, baseline:  0.12224666666666655 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.417]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.345]
 [0.417]
 [0.345]
 [0.345]
 [0.345]
 [0.345]
 [0.345]]
printing an ep nov before normalisation:  33.685487819241665
printing an ep nov before normalisation:  53.56076633163687
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.464]
 [0.464]
 [0.464]
 [0.573]
 [0.464]
 [0.464]] [[46.501]
 [53.561]
 [53.561]
 [53.561]
 [54.898]
 [53.561]
 [53.561]] [[0.499]
 [0.464]
 [0.464]
 [0.464]
 [0.573]
 [0.464]
 [0.464]]
maxi score, test score, baseline:  0.12224666666666655 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.21601607876286
printing an ep nov before normalisation:  35.32734192989599
Printing some Q and Qe and total Qs values:  [[0.908]
 [0.807]
 [0.807]
 [0.807]
 [0.874]
 [0.807]
 [0.807]] [[38.684]
 [38.268]
 [38.268]
 [38.268]
 [52.155]
 [38.268]
 [38.268]] [[0.908]
 [0.807]
 [0.807]
 [0.807]
 [0.874]
 [0.807]
 [0.807]]
line 256 mcts: sample exp_bonus 64.12766772825016
maxi score, test score, baseline:  0.12224666666666655 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.18332552661453
printing an ep nov before normalisation:  43.48349033769758
Printing some Q and Qe and total Qs values:  [[0.881]
 [0.952]
 [0.881]
 [0.881]
 [0.854]
 [0.881]
 [0.881]] [[45.804]
 [40.434]
 [45.804]
 [45.804]
 [46.189]
 [45.804]
 [45.804]] [[0.881]
 [0.952]
 [0.881]
 [0.881]
 [0.854]
 [0.881]
 [0.881]]
siam score:  -0.771539
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.854]
 [0.78 ]
 [0.793]
 [0.772]
 [0.84 ]
 [0.817]] [[48.734]
 [42.189]
 [47.202]
 [46.638]
 [48.174]
 [41.255]
 [43.137]] [[0.765]
 [0.854]
 [0.78 ]
 [0.793]
 [0.772]
 [0.84 ]
 [0.817]]
printing an ep nov before normalisation:  62.9499320157215
printing an ep nov before normalisation:  43.52974891662598
Printing some Q and Qe and total Qs values:  [[0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]] [[40.945]
 [40.945]
 [40.945]
 [40.945]
 [40.945]
 [40.945]
 [40.945]] [[0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]]
printing an ep nov before normalisation:  38.23376512650681
printing an ep nov before normalisation:  42.10817760096432
Printing some Q and Qe and total Qs values:  [[0.852]
 [0.93 ]
 [0.852]
 [0.852]
 [0.852]
 [0.852]
 [0.852]] [[45.976]
 [40.699]
 [45.976]
 [45.976]
 [45.976]
 [45.976]
 [45.976]] [[0.852]
 [0.93 ]
 [0.852]
 [0.852]
 [0.852]
 [0.852]
 [0.852]]
printing an ep nov before normalisation:  37.017754391598885
printing an ep nov before normalisation:  34.65566573389579
maxi score, test score, baseline:  0.12224666666666655 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.33966313848064
printing an ep nov before normalisation:  33.9198388219074
line 256 mcts: sample exp_bonus 36.6147564643348
printing an ep nov before normalisation:  34.89250481778157
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  0.23517801629395763
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  68 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.423]
 [0.378]
 [0.378]
 [0.378]
 [0.378]
 [0.378]] [[47.671]
 [41.226]
 [47.671]
 [47.671]
 [47.671]
 [47.671]
 [47.671]] [[0.704]
 [0.664]
 [0.704]
 [0.704]
 [0.704]
 [0.704]
 [0.704]]
maxi score, test score, baseline:  0.1515533333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3533333333333327  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1515533333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.011]
 [-0.032]
 [-0.005]
 [-0.009]
 [-0.008]
 [-0.015]
 [-0.002]] [[55.588]
 [55.39 ]
 [57.444]
 [56.777]
 [57.039]
 [58.83 ]
 [54.565]] [[1.428]
 [1.398]
 [1.513]
 [1.481]
 [1.492]
 [1.562]
 [1.393]]
printing an ep nov before normalisation:  32.68881928327289
actor:  1 policy actor:  1  step number:  68 total reward:  0.206666666666666  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1515533333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.20736734588424
actions average: 
K:  4  action  0 :  tensor([0.2835, 0.0268, 0.1283, 0.1356, 0.1586, 0.1207, 0.1465],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0096, 0.9132, 0.0083, 0.0233, 0.0096, 0.0087, 0.0272],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1089, 0.2021, 0.2690, 0.0983, 0.1037, 0.0959, 0.1222],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1148, 0.0062, 0.1281, 0.2639, 0.1540, 0.1159, 0.2171],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1071, 0.1023, 0.0831, 0.0828, 0.4657, 0.0651, 0.0940],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0767, 0.0995, 0.1168, 0.0781, 0.0783, 0.4846, 0.0661],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1303, 0.2226, 0.1393, 0.1141, 0.1034, 0.1129, 0.1774],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  52 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1515533333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1515533333333332 0.6816666666666666 0.6816666666666666
actor:  1 policy actor:  1  step number:  54 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[-0.038]
 [-0.039]
 [-0.038]
 [-0.037]
 [-0.039]
 [-0.036]
 [-0.033]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.038]
 [-0.039]
 [-0.038]
 [-0.037]
 [-0.039]
 [-0.036]
 [-0.033]]
maxi score, test score, baseline:  0.1515533333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.404739379882812
printing an ep nov before normalisation:  54.9606141118571
Printing some Q and Qe and total Qs values:  [[0.781]
 [0.845]
 [0.826]
 [0.782]
 [0.783]
 [0.782]
 [0.82 ]] [[35.989]
 [42.817]
 [40.813]
 [37.579]
 [36.937]
 [37.731]
 [40.431]] [[0.781]
 [0.845]
 [0.826]
 [0.782]
 [0.783]
 [0.782]
 [0.82 ]]
maxi score, test score, baseline:  0.1515533333333332 0.6816666666666666 0.6816666666666666
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.1515533333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.473493801626837
printing an ep nov before normalisation:  24.33367776285919
actor:  0 policy actor:  0  step number:  51 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
printing an ep nov before normalisation:  40.21145421269298
actor:  0 policy actor:  0  step number:  53 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.15124666666666656 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15124666666666656 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.34218917459423
Printing some Q and Qe and total Qs values:  [[-0.086]
 [-0.07 ]
 [-0.081]
 [-0.085]
 [-0.087]
 [-0.089]
 [-0.086]] [[20.331]
 [17.608]
 [20.514]
 [20.343]
 [20.44 ]
 [20.536]
 [20.799]] [[1.009]
 [0.862]
 [1.025]
 [1.011]
 [1.015]
 [1.019]
 [1.038]]
printing an ep nov before normalisation:  21.39002545236711
printing an ep nov before normalisation:  50.149076888147164
printing an ep nov before normalisation:  48.00514968542363
maxi score, test score, baseline:  0.15124666666666656 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.15124666666666656 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.8871921067061
actor:  1 policy actor:  1  step number:  48 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  53.981692017994035
Printing some Q and Qe and total Qs values:  [[-0.014]
 [-0.048]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]
 [-0.014]] [[51.737]
 [53.848]
 [51.737]
 [51.737]
 [51.737]
 [51.737]
 [51.737]] [[1.039]
 [1.089]
 [1.039]
 [1.039]
 [1.039]
 [1.039]
 [1.039]]
line 256 mcts: sample exp_bonus 39.05441684066503
maxi score, test score, baseline:  0.15124666666666656 0.6816666666666666 0.6816666666666666
maxi score, test score, baseline:  0.15124666666666656 0.6816666666666666 0.6816666666666666
actor:  1 policy actor:  1  step number:  54 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  50.56772424981906
maxi score, test score, baseline:  0.15124666666666656 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15124666666666656 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.12201835328845
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15124666666666656 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.117560079251525
printing an ep nov before normalisation:  46.38033267098087
line 256 mcts: sample exp_bonus 45.24123344819417
actor:  1 policy actor:  1  step number:  58 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  1.0
siam score:  -0.769431
maxi score, test score, baseline:  0.15124666666666656 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.207009411492734
printing an ep nov before normalisation:  43.619087630482824
maxi score, test score, baseline:  0.15124666666666656 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.06834560641779
printing an ep nov before normalisation:  54.946793726851716
actor:  0 policy actor:  0  step number:  48 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.95507467123454
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.306952897028
actor:  1 policy actor:  1  step number:  46 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  50.52801756042625
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.005]
 [ 0.027]
 [-0.005]
 [-0.005]
 [-0.078]
 [-0.005]
 [-0.005]] [[45.236]
 [47.135]
 [45.236]
 [45.236]
 [27.619]
 [45.236]
 [45.236]] [[0.83 ]
 [0.911]
 [0.83 ]
 [0.83 ]
 [0.294]
 [0.83 ]
 [0.83 ]]
printing an ep nov before normalisation:  76.0862535269829
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  80 total reward:  0.033333333333332216  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.58204879684148
siam score:  -0.76343566
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.874956130981445
printing an ep nov before normalisation:  46.21068893180967
Printing some Q and Qe and total Qs values:  [[-0.028]
 [ 0.084]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.023]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.028]
 [ 0.084]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.028]
 [-0.023]]
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.98487383433473
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.32770819100352
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
printing an ep nov before normalisation:  41.863349857259934
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  44.301918977296886
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
actor:  1 policy actor:  1  step number:  59 total reward:  0.23999999999999988  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  0.333
siam score:  -0.76382446
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.08891389374938
using explorer policy with actor:  1
printing an ep nov before normalisation:  17.43369499842326
actor:  1 policy actor:  1  step number:  60 total reward:  0.17999999999999938  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  44.54876203491698
actor:  1 policy actor:  1  step number:  48 total reward:  0.5800000000000003  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.47379544019609
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
line 256 mcts: sample exp_bonus 47.81483663704721
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.2999999999999998  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  49.35613853188237
actor:  1 policy actor:  1  step number:  42 total reward:  0.5533333333333333  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.61 ]
 [0.665]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]] [[50.517]
 [48.108]
 [50.517]
 [50.517]
 [50.517]
 [50.517]
 [50.517]] [[0.924]
 [0.955]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]]
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.288]
 [0.138]
 [0.093]
 [0.088]
 [0.076]
 [0.085]] [[43.592]
 [51.062]
 [44.429]
 [38.392]
 [38.674]
 [39.132]
 [38.614]] [[0.326]
 [0.487]
 [0.293]
 [0.209]
 [0.206]
 [0.197]
 [0.203]]
printing an ep nov before normalisation:  53.98414686936572
printing an ep nov before normalisation:  57.231488312018705
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.9437853108287
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.98507062113291
actor:  1 policy actor:  1  step number:  43 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.94805143146665
actor:  1 policy actor:  1  step number:  62 total reward:  0.2199999999999993  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.15140666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.766]
 [0.626]
 [0.633]
 [0.675]
 [0.675]
 [0.779]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.675]
 [0.766]
 [0.626]
 [0.633]
 [0.675]
 [0.675]
 [0.779]]
printing an ep nov before normalisation:  50.04391772158369
printing an ep nov before normalisation:  57.62256719858855
printing an ep nov before normalisation:  48.46637725830078
maxi score, test score, baseline:  0.14872666666666653 0.6816666666666666 0.6816666666666666
printing an ep nov before normalisation:  39.29896362066094
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14872666666666653 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  53 total reward:  0.13333333333333286  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.32423918236493
actor:  1 policy actor:  1  step number:  65 total reward:  0.22666666666666602  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1481799999999999 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.4503997857528
maxi score, test score, baseline:  0.14583333333333323 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14583333333333323 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14583333333333323 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14583333333333323 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  62 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.033]
 [0.033]
 [0.033]
 [0.033]
 [0.033]
 [0.033]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.033]
 [0.033]
 [0.033]
 [0.033]
 [0.033]
 [0.033]
 [0.033]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.   ]
 [0.982]
 [1.   ]
 [1.   ]
 [1.   ]
 [1.   ]
 [1.   ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.   ]
 [0.982]
 [1.   ]
 [1.   ]
 [1.   ]
 [1.   ]
 [1.   ]]
maxi score, test score, baseline:  0.14620666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.617765963308365
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.812]
 [0.93 ]
 [0.81 ]
 [0.832]
 [0.805]
 [0.832]
 [0.832]] [[54.172]
 [55.33 ]
 [56.06 ]
 [44.485]
 [56.185]
 [44.485]
 [44.485]] [[0.812]
 [0.93 ]
 [0.81 ]
 [0.832]
 [0.805]
 [0.832]
 [0.832]]
maxi score, test score, baseline:  0.14620666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.492]
 [0.366]
 [0.366]
 [0.366]
 [0.366]
 [0.366]] [[44.04 ]
 [47.708]
 [44.04 ]
 [44.04 ]
 [44.04 ]
 [44.04 ]
 [44.04 ]] [[0.931]
 [1.14 ]
 [0.931]
 [0.931]
 [0.931]
 [0.931]
 [0.931]]
printing an ep nov before normalisation:  38.44255357851009
printing an ep nov before normalisation:  43.84767945014257
line 256 mcts: sample exp_bonus 44.4578805433537
maxi score, test score, baseline:  0.14620666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  53 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  40.03000197710382
maxi score, test score, baseline:  0.14852666666666653 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.748]
 [0.821]
 [0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]] [[33.208]
 [48.044]
 [33.208]
 [33.208]
 [33.208]
 [33.208]
 [33.208]] [[0.748]
 [0.821]
 [0.748]
 [0.748]
 [0.748]
 [0.748]
 [0.748]]
maxi score, test score, baseline:  0.14852666666666653 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  41 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.14804666666666652 0.6816666666666666 0.6816666666666666
maxi score, test score, baseline:  0.14804666666666652 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.126667227659993
Printing some Q and Qe and total Qs values:  [[-0.043]
 [-0.011]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.043]
 [-0.011]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]
 [-0.043]]
actor:  0 policy actor:  0  step number:  41 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  59 total reward:  0.3466666666666668  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1474333333333332 0.6816666666666666 0.6816666666666666
maxi score, test score, baseline:  0.1474333333333332 0.6816666666666666 0.6816666666666666
printing an ep nov before normalisation:  29.611186981201172
maxi score, test score, baseline:  0.1474333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1474333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.50951179930755
maxi score, test score, baseline:  0.1474333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.30000000000000004  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1474333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1474333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1474333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1474333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1474333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.244282493032756
maxi score, test score, baseline:  0.1474333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1474333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.16194927730078
maxi score, test score, baseline:  0.1474333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1474333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.12238837676449066
actor:  1 policy actor:  1  step number:  50 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1474333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1474333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.62 ]
 [0.545]
 [0.54 ]
 [0.541]
 [0.533]
 [0.546]] [[49.993]
 [43.14 ]
 [55.574]
 [62.482]
 [63.076]
 [65.865]
 [61.857]] [[0.565]
 [0.62 ]
 [0.545]
 [0.54 ]
 [0.541]
 [0.533]
 [0.546]]
siam score:  -0.76159686
maxi score, test score, baseline:  0.1474333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.910520203786845
maxi score, test score, baseline:  0.1474333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1474333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.27984288476787
Printing some Q and Qe and total Qs values:  [[0.791]
 [0.891]
 [0.785]
 [0.842]
 [0.78 ]
 [0.788]
 [0.835]] [[33.715]
 [42.365]
 [28.253]
 [37.461]
 [28.345]
 [33.312]
 [36.723]] [[0.791]
 [0.891]
 [0.785]
 [0.842]
 [0.78 ]
 [0.788]
 [0.835]]
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.554]
 [0.675]
 [0.522]
 [0.686]
 [0.504]
 [0.559]] [[35.928]
 [35.737]
 [34.266]
 [35.533]
 [34.41 ]
 [36.116]
 [34.802]] [[2.229]
 [2.188]
 [2.241]
 [2.146]
 [2.259]
 [2.155]
 [2.15 ]]
maxi score, test score, baseline:  0.1474333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1474333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.24666666666666648  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  60 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  43.31922424125581
printing an ep nov before normalisation:  57.57777774245114
printing an ep nov before normalisation:  55.16229175430042
maxi score, test score, baseline:  0.14659333333333321 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.58743436126939
printing an ep nov before normalisation:  42.9362348027663
maxi score, test score, baseline:  0.14659333333333321 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  48 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.84837056783318
Printing some Q and Qe and total Qs values:  [[0.04]
 [0.04]
 [0.04]
 [0.04]
 [0.04]
 [0.04]
 [0.04]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.04]
 [0.04]
 [0.04]
 [0.04]
 [0.04]
 [0.04]
 [0.04]]
printing an ep nov before normalisation:  30.625168399892573
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.843848153399684
maxi score, test score, baseline:  0.1459133333333332 0.6816666666666666 0.6816666666666666
actor:  1 policy actor:  1  step number:  61 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  66 total reward:  0.19333333333333325  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1459133333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.059]
 [0.088]
 [0.04 ]
 [0.042]
 [0.047]
 [0.048]
 [0.045]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.059]
 [0.088]
 [0.04 ]
 [0.042]
 [0.047]
 [0.048]
 [0.045]]
printing an ep nov before normalisation:  33.846179725124586
printing an ep nov before normalisation:  48.493986220741014
maxi score, test score, baseline:  0.1459133333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.912086577467534
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 37.943692422590544
Printing some Q and Qe and total Qs values:  [[0.299]
 [0.362]
 [0.299]
 [0.299]
 [0.191]
 [0.299]
 [0.299]] [[44.69 ]
 [46.829]
 [44.69 ]
 [44.69 ]
 [45.452]
 [44.69 ]
 [44.69 ]] [[0.456]
 [0.534]
 [0.456]
 [0.456]
 [0.354]
 [0.456]
 [0.456]]
printing an ep nov before normalisation:  55.28674387169607
printing an ep nov before normalisation:  35.96509170360635
printing an ep nov before normalisation:  46.58596992492676
printing an ep nov before normalisation:  50.79200430955029
actions average: 
K:  4  action  0 :  tensor([0.2708, 0.1335, 0.1166, 0.1238, 0.1195, 0.1208, 0.1149],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0253, 0.9033, 0.0151, 0.0087, 0.0073, 0.0083, 0.0319],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1277, 0.0357, 0.3263, 0.1358, 0.1322, 0.1277, 0.1146],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0984, 0.1637, 0.1099, 0.2235, 0.1030, 0.1103, 0.1913],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0870, 0.0091, 0.0736, 0.0696, 0.5730, 0.1130, 0.0747],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1034, 0.0042, 0.1275, 0.1415, 0.0892, 0.4306, 0.1035],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0871, 0.2029, 0.0796, 0.1299, 0.0786, 0.0678, 0.3540],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  56 total reward:  0.33999999999999986  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1459133333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.23730723777515
printing an ep nov before normalisation:  41.76574100907221
maxi score, test score, baseline:  0.1459133333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.671]
 [0.569]
 [0.586]
 [0.586]
 [0.586]
 [0.586]] [[36.643]
 [36.409]
 [33.509]
 [38.856]
 [38.856]
 [38.856]
 [38.856]] [[0.606]
 [0.671]
 [0.569]
 [0.586]
 [0.586]
 [0.586]
 [0.586]]
Printing some Q and Qe and total Qs values:  [[0.848]
 [0.963]
 [0.841]
 [0.843]
 [0.919]
 [0.846]
 [0.919]] [[49.574]
 [42.525]
 [51.091]
 [51.397]
 [52.32 ]
 [50.929]
 [52.32 ]] [[0.848]
 [0.963]
 [0.841]
 [0.843]
 [0.919]
 [0.846]
 [0.919]]
maxi score, test score, baseline:  0.14252666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.50210788179475
actor:  1 policy actor:  1  step number:  71 total reward:  0.2133333333333325  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.14252666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  51 total reward:  0.346666666666666  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.14183333333333323 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  41 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  2.0
siam score:  -0.7596077
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.88015235054229
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.32779407894044
printing an ep nov before normalisation:  34.6875549580632
printing an ep nov before normalisation:  41.03646776016459
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.583]
 [0.542]
 [0.566]
 [0.571]
 [0.567]
 [0.566]] [[37.632]
 [39.986]
 [36.678]
 [38.614]
 [39.212]
 [39.041]
 [39.2  ]] [[0.956]
 [1.044]
 [0.934]
 [0.999]
 [1.016]
 [1.008]
 [1.01 ]]
printing an ep nov before normalisation:  44.07219580731514
printing an ep nov before normalisation:  40.62851820891178
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
printing an ep nov before normalisation:  47.18387409896118
printing an ep nov before normalisation:  29.094866041135887
printing an ep nov before normalisation:  3.891197252414713e-05
actor:  1 policy actor:  1  step number:  58 total reward:  0.31333333333333324  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.37 ]
 [0.328]
 [0.335]
 [0.332]
 [0.367]
 [0.33 ]] [[36.446]
 [43.274]
 [35.935]
 [38.523]
 [36.814]
 [37.961]
 [36.074]] [[1.153]
 [1.511]
 [1.148]
 [1.268]
 [1.19 ]
 [1.275]
 [1.156]]
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.906910064972536
line 256 mcts: sample exp_bonus 41.106309420718134
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.001914978027344
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.416]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.404]
 [0.416]
 [0.404]
 [0.404]
 [0.404]
 [0.404]
 [0.404]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
printing an ep nov before normalisation:  46.684996379351055
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.05305517508169
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.033695141419464
Printing some Q and Qe and total Qs values:  [[0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]
 [0.766]] [[46.578]
 [46.578]
 [46.578]
 [46.578]
 [46.578]
 [46.578]
 [46.578]] [[1.976]
 [1.976]
 [1.976]
 [1.976]
 [1.976]
 [1.976]
 [1.976]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  28.541820872080468
siam score:  -0.764668
printing an ep nov before normalisation:  18.226819075920744
printing an ep nov before normalisation:  44.77201441093795
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.01750794935814
actor:  1 policy actor:  1  step number:  47 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.333
siam score:  -0.7644031
printing an ep nov before normalisation:  47.422248906261245
actor:  1 policy actor:  1  step number:  53 total reward:  0.3333333333333328  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.356]
 [0.286]
 [0.286]
 [0.287]
 [0.289]
 [0.291]] [[31.883]
 [41.089]
 [30.605]
 [30.927]
 [30.84 ]
 [30.657]
 [31.144]] [[0.522]
 [0.731]
 [0.502]
 [0.507]
 [0.506]
 [0.506]
 [0.514]]
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.9935434984012
siam score:  -0.7604811
printing an ep nov before normalisation:  46.046398338063476
siam score:  -0.760064
Printing some Q and Qe and total Qs values:  [[0.989]
 [1.244]
 [0.989]
 [0.989]
 [0.989]
 [0.989]
 [0.989]] [[40.683]
 [43.763]
 [40.683]
 [40.683]
 [40.683]
 [40.683]
 [40.683]] [[1.434]
 [1.755]
 [1.434]
 [1.434]
 [1.434]
 [1.434]
 [1.434]]
printing an ep nov before normalisation:  40.94883194608017
actor:  1 policy actor:  1  step number:  46 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7603739
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
printing an ep nov before normalisation:  55.4239528727149
actor:  1 policy actor:  1  step number:  63 total reward:  0.2666666666666665  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.02783489227295
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.2473330509626
actor:  1 policy actor:  1  step number:  54 total reward:  0.2999999999999994  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  1  action  0 :  tensor([0.3668, 0.0073, 0.1176, 0.1058, 0.1398, 0.1379, 0.1248],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0080, 0.9254, 0.0176, 0.0037, 0.0043, 0.0044, 0.0365],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1349, 0.0116, 0.2461, 0.1451, 0.1186, 0.1755, 0.1682],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1148, 0.1557, 0.1094, 0.2904, 0.1050, 0.1100, 0.1148],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1537, 0.0024, 0.1091, 0.1263, 0.4029, 0.1222, 0.0834],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0627, 0.0022, 0.1304, 0.1252, 0.0853, 0.5026, 0.0915],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1103, 0.1455, 0.1207, 0.1483, 0.1236, 0.1395, 0.2121],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14124666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.762495
printing an ep nov before normalisation:  40.63950096641735
actor:  0 policy actor:  0  step number:  46 total reward:  0.3533333333333333  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  48 total reward:  0.43333333333333335  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[ 0.377]
 [ 0.38 ]
 [ 0.377]
 [ 0.327]
 [ 0.377]
 [-0.003]
 [ 0.338]] [[26.471]
 [40.849]
 [26.471]
 [17.424]
 [26.471]
 [18.738]
 [44.841]] [[ 0.377]
 [ 0.38 ]
 [ 0.377]
 [ 0.327]
 [ 0.377]
 [-0.003]
 [ 0.338]]
printing an ep nov before normalisation:  53.66112457074969
maxi score, test score, baseline:  0.14056666666666653 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14056666666666653 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.7730594630264
line 256 mcts: sample exp_bonus 53.72447458770565
maxi score, test score, baseline:  0.14056666666666653 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.756]
 [0.872]
 [0.752]
 [0.757]
 [0.752]
 [0.757]
 [0.758]] [[31.434]
 [38.622]
 [31.314]
 [31.188]
 [31.314]
 [31.202]
 [31.432]] [[0.756]
 [0.872]
 [0.752]
 [0.757]
 [0.752]
 [0.757]
 [0.758]]
printing an ep nov before normalisation:  37.93016566069203
siam score:  -0.7607529
printing an ep nov before normalisation:  47.96499190225034
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13717999999999986 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  50 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1363933333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 45.22436409302079
maxi score, test score, baseline:  0.1363933333333332 0.6816666666666666 0.6816666666666666
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1363933333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  1.333
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  52 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1363933333333332 0.6816666666666666 0.6816666666666666
printing an ep nov before normalisation:  45.39004475837323
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.1363933333333332 0.6816666666666666 0.6816666666666666
printing an ep nov before normalisation:  54.717323379054776
printing an ep nov before normalisation:  38.588528472558
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.945650629986496
maxi score, test score, baseline:  0.1363933333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  59.23837093489792
printing an ep nov before normalisation:  55.640463234315646
actor:  1 policy actor:  1  step number:  60 total reward:  0.09999999999999942  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  44.240304207506284
printing an ep nov before normalisation:  45.31518100666748
maxi score, test score, baseline:  0.13548666666666653 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13548666666666653 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.497745635483675
line 256 mcts: sample exp_bonus 46.32580757141113
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  41.3501388273119
actor:  1 policy actor:  1  step number:  59 total reward:  0.19999999999999918  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.13209999999999988 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.591368549672964
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]] [[68.62]
 [68.62]
 [68.62]
 [68.62]
 [68.62]
 [68.62]
 [68.62]] [[1.645]
 [1.645]
 [1.645]
 [1.645]
 [1.645]
 [1.645]
 [1.645]]
Printing some Q and Qe and total Qs values:  [[0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]
 [0.351]] [[52.447]
 [52.447]
 [52.447]
 [52.447]
 [52.447]
 [52.447]
 [52.447]] [[1.433]
 [1.433]
 [1.433]
 [1.433]
 [1.433]
 [1.433]
 [1.433]]
maxi score, test score, baseline:  0.13209999999999988 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13209999999999988 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.9091958455867
maxi score, test score, baseline:  0.13209999999999988 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.67599898482251
siam score:  -0.7693444
actor:  1 policy actor:  1  step number:  50 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.12871333333333318 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12532666666666653 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.784]
 [0.784]
 [0.784]
 [0.784]
 [0.853]
 [0.784]
 [0.784]] [[35.107]
 [35.107]
 [35.107]
 [35.107]
 [37.593]
 [35.107]
 [35.107]] [[0.784]
 [0.784]
 [0.784]
 [0.784]
 [0.853]
 [0.784]
 [0.784]]
printing an ep nov before normalisation:  26.62423849105835
printing an ep nov before normalisation:  42.06498473923442
maxi score, test score, baseline:  0.12532666666666653 0.6816666666666666 0.6816666666666666
UNIT TEST: sample policy line 217 mcts : [0.041 0.347 0.163 0.082 0.184 0.082 0.102]
actor:  0 policy actor:  0  step number:  36 total reward:  0.48666666666666647  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  59 total reward:  0.23999999999999977  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.12400666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.16563433726014
actor:  1 policy actor:  1  step number:  52 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.12400666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12400666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.87105613973477
Printing some Q and Qe and total Qs values:  [[0.328]
 [0.328]
 [0.328]
 [0.255]
 [0.328]
 [0.289]
 [0.326]] [[60.499]
 [60.499]
 [60.499]
 [59.55 ]
 [60.499]
 [59.623]
 [62.11 ]] [[1.29 ]
 [1.29 ]
 [1.29 ]
 [1.195]
 [1.29 ]
 [1.23 ]
 [1.326]]
actor:  0 policy actor:  0  step number:  44 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.12343333333333321 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.428]
 [0.262]
 [0.373]
 [0.236]
 [0.278]
 [0.347]] [[45.425]
 [45.787]
 [48.854]
 [43.645]
 [57.044]
 [47.55 ]
 [48.655]] [[1.11 ]
 [1.146]
 [1.059]
 [1.035]
 [1.245]
 [1.041]
 [1.139]]
siam score:  -0.7653537
maxi score, test score, baseline:  0.12343333333333321 0.6816666666666666 0.6816666666666666
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.12439821327627
maxi score, test score, baseline:  0.12343333333333321 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.4733333333333335  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.12343333333333321 0.6816666666666666 0.6816666666666666
maxi score, test score, baseline:  0.12343333333333321 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.912510720149214
maxi score, test score, baseline:  0.12343333333333321 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12343333333333321 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  0.16666666666666585  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.12343333333333321 0.6816666666666666 0.6816666666666666
printing an ep nov before normalisation:  72.853789372977
maxi score, test score, baseline:  0.12343333333333321 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  72 total reward:  0.11333333333333295  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12343333333333321 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.184]
 [0.228]
 [0.184]
 [0.184]
 [0.184]
 [0.184]
 [0.184]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.184]
 [0.228]
 [0.184]
 [0.184]
 [0.184]
 [0.184]
 [0.184]]
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.11 ]
 [0.335]
 [0.335]
 [0.191]
 [0.335]
 [0.335]] [[44.477]
 [43.482]
 [44.477]
 [44.477]
 [45.824]
 [44.477]
 [44.477]] [[0.96 ]
 [0.708]
 [0.96 ]
 [0.96 ]
 [0.852]
 [0.96 ]
 [0.96 ]]
line 256 mcts: sample exp_bonus 43.41723088838801
maxi score, test score, baseline:  0.12343333333333321 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.671603009189674
siam score:  -0.7687372
printing an ep nov before normalisation:  47.19411167751443
line 256 mcts: sample exp_bonus 41.60098740482313
printing an ep nov before normalisation:  42.72429422477079
printing an ep nov before normalisation:  46.73884691351629
maxi score, test score, baseline:  0.12004666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.302]
 [0.307]
 [0.301]
 [0.295]
 [0.302]
 [0.302]
 [0.298]] [[40.232]
 [36.99 ]
 [41.938]
 [37.436]
 [43.068]
 [42.503]
 [41.274]] [[1.1  ]
 [0.974]
 [1.169]
 [0.98 ]
 [1.215]
 [1.193]
 [1.139]]
printing an ep nov before normalisation:  44.64875607785907
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.327 0.143 0.102 0.061 0.061 0.245 0.061]
printing an ep nov before normalisation:  41.9672155380249
printing an ep nov before normalisation:  56.818738052656485
maxi score, test score, baseline:  0.12004666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.646715226787855
maxi score, test score, baseline:  0.12004666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.12004666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12004666666666655 0.6816666666666666 0.6816666666666666
actor:  1 policy actor:  1  step number:  59 total reward:  0.27999999999999936  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.12004666666666655 0.6816666666666666 0.6816666666666666
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.39988377260464
maxi score, test score, baseline:  0.12004666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76711917
printing an ep nov before normalisation:  54.62149443625848
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.686]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]] [[46.469]
 [50.178]
 [46.469]
 [46.469]
 [46.469]
 [46.469]
 [46.469]] [[2.125]
 [2.43 ]
 [2.125]
 [2.125]
 [2.125]
 [2.125]
 [2.125]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  44 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  52 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  48.70151902667624
maxi score, test score, baseline:  0.12004666666666655 0.6816666666666666 0.6816666666666666
printing an ep nov before normalisation:  36.64580064695294
Printing some Q and Qe and total Qs values:  [[0.608]
 [0.639]
 [0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]] [[42.642]
 [43.211]
 [42.642]
 [42.642]
 [42.642]
 [42.642]
 [42.642]] [[0.921]
 [0.958]
 [0.921]
 [0.921]
 [0.921]
 [0.921]
 [0.921]]
maxi score, test score, baseline:  0.12004666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.2466666666666658  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[ 0.342]
 [ 0.378]
 [-0.053]
 [ 0.342]
 [ 0.342]
 [ 0.342]
 [ 0.342]] [[35.724]
 [35.012]
 [35.003]
 [35.724]
 [35.724]
 [35.724]
 [35.724]] [[2.019]
 [1.989]
 [1.557]
 [2.019]
 [2.019]
 [2.019]
 [2.019]]
maxi score, test score, baseline:  0.12004666666666655 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  0.6400000000000001  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.11995333333333322 0.6816666666666666 0.6816666666666666
maxi score, test score, baseline:  0.11995333333333322 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.262]
 [0.269]
 [0.268]
 [0.241]
 [0.241]
 [0.238]] [[43.865]
 [43.178]
 [43.742]
 [43.568]
 [46.009]
 [46.073]
 [46.052]] [[1.639]
 [1.607]
 [1.649]
 [1.638]
 [1.765]
 [1.769]
 [1.764]]
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.7  ]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]] [[32.702]
 [29.624]
 [32.702]
 [32.702]
 [32.702]
 [32.702]
 [32.702]] [[0.763]
 [0.817]
 [0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.763]]
UNIT TEST: sample policy line 217 mcts : [0.02  0.429 0.224 0.02  0.204 0.061 0.041]
printing an ep nov before normalisation:  33.69337292925049
printing an ep nov before normalisation:  45.90825383880197
Printing some Q and Qe and total Qs values:  [[0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]
 [0.692]] [[34.711]
 [34.711]
 [34.711]
 [34.711]
 [34.711]
 [34.711]
 [34.711]] [[2.252]
 [2.252]
 [2.252]
 [2.252]
 [2.252]
 [2.252]
 [2.252]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.11995333333333322 0.6816666666666666 0.6816666666666666
printing an ep nov before normalisation:  49.47784900665283
actor:  1 policy actor:  1  step number:  57 total reward:  0.4  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.11995333333333322 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.11406065526506
maxi score, test score, baseline:  0.11995333333333322 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11995333333333322 0.6816666666666666 0.6816666666666666
maxi score, test score, baseline:  0.11995333333333322 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.173]
 [0.314]
 [0.135]
 [0.136]
 [0.136]
 [0.202]
 [0.166]] [[33.938]
 [45.582]
 [33.169]
 [33.904]
 [33.714]
 [38.451]
 [35.971]] [[0.273]
 [0.486]
 [0.23 ]
 [0.235]
 [0.234]
 [0.329]
 [0.278]]
maxi score, test score, baseline:  0.11995333333333322 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.11995333333333322 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11995333333333322 0.6816666666666666 0.6816666666666666
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.711]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]] [[31.854]
 [34.147]
 [31.854]
 [31.854]
 [31.854]
 [31.854]
 [31.854]] [[0.666]
 [0.711]
 [0.666]
 [0.666]
 [0.666]
 [0.666]
 [0.666]]
maxi score, test score, baseline:  0.11995333333333322 0.6816666666666666 0.6816666666666666
maxi score, test score, baseline:  0.11995333333333322 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.737]
 [0.801]
 [0.741]
 [0.743]
 [0.744]
 [0.742]
 [0.761]] [[40.788]
 [48.608]
 [41.195]
 [39.827]
 [39.886]
 [40.261]
 [45.669]] [[0.737]
 [0.801]
 [0.741]
 [0.743]
 [0.744]
 [0.742]
 [0.761]]
actor:  1 policy actor:  1  step number:  57 total reward:  0.3466666666666661  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.427]
 [0.427]
 [0.505]
 [0.427]
 [0.569]
 [0.469]
 [0.427]] [[38.675]
 [38.675]
 [46.694]
 [38.675]
 [38.171]
 [40.633]
 [38.675]] [[1.272]
 [1.272]
 [1.666]
 [1.272]
 [1.393]
 [1.391]
 [1.272]]
siam score:  -0.76210916
Printing some Q and Qe and total Qs values:  [[-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]
 [-0.011]]
maxi score, test score, baseline:  0.11995333333333322 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  47 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.195]
 [0.195]] [[55.577]
 [55.577]
 [55.577]
 [55.577]
 [55.577]
 [55.577]
 [55.577]] [[1.365]
 [1.365]
 [1.365]
 [1.365]
 [1.365]
 [1.365]
 [1.365]]
maxi score, test score, baseline:  0.11921999999999988 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.76284087
maxi score, test score, baseline:  0.11921999999999988 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]
 [0.587]] [[42.161]
 [42.161]
 [42.161]
 [42.161]
 [42.161]
 [42.161]
 [42.161]] [[1.415]
 [1.415]
 [1.415]
 [1.415]
 [1.415]
 [1.415]
 [1.415]]
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.642]
 [0.619]
 [0.619]
 [0.619]
 [0.61 ]
 [0.619]] [[37.882]
 [35.164]
 [37.882]
 [37.882]
 [37.882]
 [33.949]
 [37.882]] [[1.408]
 [1.332]
 [1.408]
 [1.408]
 [1.408]
 [1.255]
 [1.408]]
maxi score, test score, baseline:  0.11921999999999988 0.6816666666666666 0.6816666666666666
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.081]
 [0.06 ]
 [0.068]
 [0.06 ]
 [0.068]
 [0.055]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.068]
 [0.081]
 [0.06 ]
 [0.068]
 [0.06 ]
 [0.068]
 [0.055]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.11628666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.082]
 [0.096]
 [0.093]
 [0.088]
 [0.087]
 [0.082]
 [0.083]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.082]
 [0.096]
 [0.093]
 [0.088]
 [0.087]
 [0.082]
 [0.083]]
Printing some Q and Qe and total Qs values:  [[-0.068]
 [-0.039]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.068]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.068]
 [-0.039]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.068]
 [-0.068]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11628666666666654 0.6816666666666666 0.6816666666666666
maxi score, test score, baseline:  0.11628666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.565005134126054
maxi score, test score, baseline:  0.11628666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.568]
 [0.583]
 [0.573]
 [0.573]
 [0.573]
 [0.584]] [[37.172]
 [40.295]
 [34.019]
 [37.172]
 [37.172]
 [37.172]
 [36.003]] [[1.829]
 [2.013]
 [1.649]
 [1.829]
 [1.829]
 [1.829]
 [1.77 ]]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5600000000000003  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11628666666666654 0.6816666666666666 0.6816666666666666
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.11628666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11628666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11628666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11628666666666654 0.6816666666666666 0.6816666666666666
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11628666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.37268201394977
actions average: 
K:  3  action  0 :  tensor([0.2785, 0.0196, 0.0745, 0.1281, 0.2895, 0.0826, 0.1272],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0133, 0.8982, 0.0107, 0.0148, 0.0088, 0.0057, 0.0484],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0719, 0.0148, 0.4888, 0.0760, 0.0948, 0.1532, 0.1005],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0835, 0.1360, 0.0994, 0.2892, 0.1455, 0.1209, 0.1255],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0864, 0.0069, 0.0604, 0.1081, 0.5745, 0.0740, 0.0897],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1242, 0.0029, 0.1232, 0.1213, 0.1371, 0.3715, 0.1198],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1140, 0.0281, 0.1843, 0.1290, 0.1418, 0.1312, 0.2716],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  51 total reward:  0.2933333333333331  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  58.02370548248291
maxi score, test score, baseline:  0.11628666666666652 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.684]
 [0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]] [[42.634]
 [44.685]
 [42.634]
 [42.634]
 [42.634]
 [42.634]
 [42.634]] [[1.68]
 [1.9 ]
 [1.68]
 [1.68]
 [1.68]
 [1.68]
 [1.68]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  56 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  64 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  27.80556974996152
actor:  1 policy actor:  1  step number:  45 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  25.097031593322754
maxi score, test score, baseline:  0.11628666666666652 0.6816666666666666 0.6816666666666666
maxi score, test score, baseline:  0.11628666666666652 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11628666666666652 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11628666666666652 0.6816666666666666 0.6816666666666666
maxi score, test score, baseline:  0.11628666666666652 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.89865367617885
maxi score, test score, baseline:  0.11628666666666652 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.99617493434174
maxi score, test score, baseline:  0.11628666666666652 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  62 total reward:  0.1666666666666663  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.11628666666666652 0.6816666666666666 0.6816666666666666
maxi score, test score, baseline:  0.11628666666666652 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11628666666666652 0.6816666666666666 0.6816666666666666
maxi score, test score, baseline:  0.11628666666666652 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  66 total reward:  0.12666666666666593  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  41.62518972942856
printing an ep nov before normalisation:  52.2362844815042
actions average: 
K:  0  action  0 :  tensor([0.4371, 0.0041, 0.1067, 0.1055, 0.1455, 0.1002, 0.1010],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0055, 0.9612, 0.0068, 0.0047, 0.0011, 0.0014, 0.0194],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0502, 0.0183, 0.5694, 0.1090, 0.0569, 0.0894, 0.1067],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1306, 0.0059, 0.1460, 0.2610, 0.1318, 0.1593, 0.1653],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0606, 0.0025, 0.0608, 0.0811, 0.6531, 0.0823, 0.0596],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1153, 0.0021, 0.1523, 0.1164, 0.1162, 0.3675, 0.1301],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0790, 0.1326, 0.0647, 0.1691, 0.0620, 0.0714, 0.4211],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.11628666666666652 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11628666666666652 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.56587975342642
maxi score, test score, baseline:  0.11628666666666652 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11628666666666652 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.0525528089863
actor:  0 policy actor:  0  step number:  46 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]] [[35.872]
 [35.872]
 [35.872]
 [35.872]
 [35.872]
 [35.872]
 [35.872]] [[0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]
 [0.925]]
actor:  0 policy actor:  0  step number:  55 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  0.333
siam score:  -0.76118904
printing an ep nov before normalisation:  40.9260320119365
printing an ep nov before normalisation:  61.57555862965371
maxi score, test score, baseline:  0.11600666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.98494243341405
printing an ep nov before normalisation:  52.054590831858455
maxi score, test score, baseline:  0.11600666666666654 0.6816666666666666 0.6816666666666666
printing an ep nov before normalisation:  42.03181482970938
actor:  1 policy actor:  1  step number:  67 total reward:  0.19999999999999918  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.11600666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.43936623768958
siam score:  -0.76445884
maxi score, test score, baseline:  0.11600666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11600666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11600666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.28210124319075
actor:  1 policy actor:  1  step number:  50 total reward:  0.4733333333333335  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  42.665311480575745
printing an ep nov before normalisation:  38.42204411824544
maxi score, test score, baseline:  0.11600666666666654 0.6816666666666666 0.6816666666666666
maxi score, test score, baseline:  0.11600666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  66 total reward:  0.2599999999999998  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.11600666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.00120537349484
siam score:  -0.7652191
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  47.98230262122752
actor:  1 policy actor:  1  step number:  65 total reward:  0.11999999999999955  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.11600666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.4190, 0.0557, 0.0656, 0.1061, 0.1621, 0.1033, 0.0883],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0079, 0.9396, 0.0043, 0.0064, 0.0017, 0.0023, 0.0378],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1212, 0.0413, 0.2996, 0.1324, 0.1075, 0.1832, 0.1148],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1137, 0.0233, 0.0940, 0.3725, 0.0994, 0.1508, 0.1463],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1405, 0.0059, 0.0842, 0.1157, 0.3466, 0.1701, 0.1369],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0868, 0.0052, 0.1067, 0.1404, 0.0631, 0.4499, 0.1479],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1030, 0.0400, 0.0764, 0.0858, 0.0903, 0.1570, 0.4476],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.11600666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76508987
maxi score, test score, baseline:  0.11600666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.252230428139974
maxi score, test score, baseline:  0.11600666666666654 0.6816666666666666 0.6816666666666666
printing an ep nov before normalisation:  21.847822665865312
actor:  1 policy actor:  1  step number:  51 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  64 total reward:  0.03333333333333277  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.11600666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.42968008990668
siam score:  -0.7681309
maxi score, test score, baseline:  0.11600666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 59.4540637556897
maxi score, test score, baseline:  0.11600666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.511834144592285
actor:  0 policy actor:  0  step number:  37 total reward:  0.4933333333333332  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.11635333333333323 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.44132229794067
maxi score, test score, baseline:  0.11635333333333323 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.318812604936625
printing an ep nov before normalisation:  34.215198318397476
maxi score, test score, baseline:  0.11635333333333323 0.6816666666666666 0.6816666666666666
maxi score, test score, baseline:  0.11635333333333323 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.09463368530447
actor:  0 policy actor:  0  step number:  41 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7734302
maxi score, test score, baseline:  0.11915333333333321 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  54 total reward:  0.43333333333333357  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.674815820151935
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.616]
 [0.598]
 [0.581]
 [0.585]
 [0.572]
 [0.599]] [[47.32 ]
 [45.527]
 [46.832]
 [49.309]
 [49.274]
 [50.702]
 [46.672]] [[1.119]
 [1.104]
 [1.106]
 [1.129]
 [1.132]
 [1.142]
 [1.104]]
printing an ep nov before normalisation:  43.04687067928853
printing an ep nov before normalisation:  54.010502770767054
maxi score, test score, baseline:  0.11915333333333321 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.257642383653945
printing an ep nov before normalisation:  39.91305505965043
maxi score, test score, baseline:  0.11915333333333321 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 43.78258061350351
maxi score, test score, baseline:  0.11915333333333321 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.116]
 [0.027]
 [0.06 ]
 [0.022]
 [0.056]
 [0.075]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.018]
 [0.116]
 [0.027]
 [0.06 ]
 [0.022]
 [0.056]
 [0.075]]
maxi score, test score, baseline:  0.11915333333333321 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11915333333333321 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  48 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  55.58714835491531
maxi score, test score, baseline:  0.11961999999999987 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.11961999999999987 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.72231439099117
maxi score, test score, baseline:  0.11961999999999987 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.4761020603829
maxi score, test score, baseline:  0.11961999999999987 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.94286394772459
printing an ep nov before normalisation:  35.915416939762125
maxi score, test score, baseline:  0.11961999999999987 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  0.1933333333333327  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4199999999999995  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.11961999999999987 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11961999999999987 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.135024909154254
Printing some Q and Qe and total Qs values:  [[0.194]
 [0.233]
 [0.194]
 [0.194]
 [0.194]
 [0.194]
 [0.194]] [[48.261]
 [62.565]
 [48.261]
 [48.261]
 [48.261]
 [48.261]
 [48.261]] [[1.216]
 [1.813]
 [1.216]
 [1.216]
 [1.216]
 [1.216]
 [1.216]]
maxi score, test score, baseline:  0.11961999999999987 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.11961999999999987 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.2933333333333328  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.1667, 0.0195, 0.1317, 0.1244, 0.3073, 0.1321, 0.1183],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0091, 0.9281, 0.0044, 0.0102, 0.0033, 0.0044, 0.0405],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1080, 0.0526, 0.4519, 0.0742, 0.0801, 0.1055, 0.1276],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1368, 0.0859, 0.1343, 0.2330, 0.1278, 0.1352, 0.1469],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0943, 0.0271, 0.0728, 0.0923, 0.5462, 0.0723, 0.0950],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0947, 0.0544, 0.1517, 0.0836, 0.0846, 0.4456, 0.0854],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1020, 0.3888, 0.1073, 0.0972, 0.0860, 0.0930, 0.1256],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  42 total reward:  0.5133333333333335  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.828]
 [0.828]
 [0.828]
 [0.828]
 [0.828]
 [0.828]
 [0.828]] [[55.177]
 [55.177]
 [55.177]
 [55.177]
 [55.177]
 [55.177]
 [55.177]] [[0.828]
 [0.828]
 [0.828]
 [0.828]
 [0.828]
 [0.828]
 [0.828]]
maxi score, test score, baseline:  0.11964666666666654 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.344]
 [0.344]] [[56.87]
 [56.87]
 [56.87]
 [56.87]
 [56.87]
 [56.87]
 [56.87]] [[1.655]
 [1.655]
 [1.655]
 [1.655]
 [1.655]
 [1.655]
 [1.655]]
printing an ep nov before normalisation:  43.43706396513723
line 256 mcts: sample exp_bonus 18.00279892486331
actor:  0 policy actor:  0  step number:  47 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  31.135758233254343
Printing some Q and Qe and total Qs values:  [[-0.104]
 [-0.075]
 [-0.107]
 [-0.109]
 [-0.076]
 [-0.109]
 [-0.115]] [[27.397]
 [25.244]
 [26.275]
 [26.332]
 [31.493]
 [34.414]
 [26.56 ]] [[0.368]
 [0.36 ]
 [0.345]
 [0.344]
 [0.466]
 [0.484]
 [0.343]]
printing an ep nov before normalisation:  15.77296257019043
maxi score, test score, baseline:  0.1190333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1190333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1190333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.735333714712105
actor:  0 policy actor:  0  step number:  40 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12189999999999987 0.6816666666666666 0.6816666666666666
actor:  1 policy actor:  1  step number:  57 total reward:  0.2533333333333325  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.12189999999999987 0.6816666666666666 0.6816666666666666
printing an ep nov before normalisation:  55.00599838205089
actor:  0 policy actor:  0  step number:  49 total reward:  0.2666666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1222333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.35 ]
 [0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]] [[35.725]
 [40.053]
 [35.725]
 [35.725]
 [35.725]
 [35.725]
 [35.725]] [[0.689]
 [0.774]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]]
maxi score, test score, baseline:  0.1222333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1222333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1222333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1222333333333332 0.6816666666666666 0.6816666666666666
actor:  1 policy actor:  1  step number:  57 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1222333333333332 0.6816666666666666 0.6816666666666666
maxi score, test score, baseline:  0.1222333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]
 [0.532]] [[52.037]
 [52.037]
 [52.037]
 [52.037]
 [52.037]
 [52.037]
 [52.037]] [[2.426]
 [2.426]
 [2.426]
 [2.426]
 [2.426]
 [2.426]
 [2.426]]
printing an ep nov before normalisation:  40.11866513612617
actor:  1 policy actor:  1  step number:  57 total reward:  0.2933333333333328  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  60.199644788212495
UNIT TEST: sample policy line 217 mcts : [0.02  0.388 0.041 0.041 0.367 0.122 0.02 ]
maxi score, test score, baseline:  0.1222333333333332 0.6816666666666666 0.6816666666666666
siam score:  -0.7622139
actions average: 
K:  2  action  0 :  tensor([0.5810, 0.0160, 0.0695, 0.0696, 0.0799, 0.0616, 0.1225],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0181, 0.9159, 0.0053, 0.0059, 0.0019, 0.0020, 0.0510],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1101, 0.0081, 0.4414, 0.1149, 0.0854, 0.1192, 0.1210],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1232, 0.0128, 0.1036, 0.3904, 0.1165, 0.1179, 0.1355],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1101, 0.0065, 0.0788, 0.1801, 0.4045, 0.1306, 0.0895],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0894, 0.0019, 0.1894, 0.1032, 0.0913, 0.4256, 0.0993],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1338, 0.1881, 0.1003, 0.0978, 0.0894, 0.0950, 0.2956],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1222333333333332 0.6816666666666666 0.6816666666666666
maxi score, test score, baseline:  0.1222333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.2533333333333331  reward:  1.0 rdn_beta:  2.0
siam score:  -0.76359946
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.35 ]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]] [[35.726]
 [34.882]
 [35.726]
 [35.726]
 [35.726]
 [35.726]
 [35.726]] [[1.603]
 [1.57 ]
 [1.603]
 [1.603]
 [1.603]
 [1.603]
 [1.603]]
siam score:  -0.7620356
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1222333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1222333333333332 0.6816666666666666 0.6816666666666666
line 256 mcts: sample exp_bonus 38.88128216092447
printing an ep nov before normalisation:  40.48544757868519
maxi score, test score, baseline:  0.1222333333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.30510158612012
actor:  1 policy actor:  1  step number:  51 total reward:  0.44000000000000006  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1222333333333332 0.6816666666666666 0.6816666666666666
using explorer policy with actor:  1
maxi score, test score, baseline:  0.12223333333333322 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.56 ]
 [0.643]
 [0.546]
 [0.534]
 [0.542]
 [0.55 ]
 [0.514]] [[32.176]
 [34.988]
 [32.868]
 [33.137]
 [34.437]
 [33.772]
 [33.203]] [[0.997]
 [1.155]
 [1.002]
 [0.996]
 [1.039]
 [1.029]
 [0.979]]
maxi score, test score, baseline:  0.12223333333333322 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.46107195585359
actor:  1 policy actor:  1  step number:  47 total reward:  0.5466666666666667  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.764]
 [0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]] [[42.231]
 [43.124]
 [42.231]
 [42.231]
 [42.231]
 [42.231]
 [42.231]] [[1.241]
 [1.314]
 [1.241]
 [1.241]
 [1.241]
 [1.241]
 [1.241]]
printing an ep nov before normalisation:  45.52736128698453
maxi score, test score, baseline:  0.12223333333333322 0.6816666666666666 0.6816666666666666
maxi score, test score, baseline:  0.12223333333333322 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.79401686145559
maxi score, test score, baseline:  0.12223333333333322 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.12223333333333322 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.20278656863654
maxi score, test score, baseline:  0.12223333333333322 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.071553230285645
actor:  1 policy actor:  1  step number:  66 total reward:  0.16666666666666596  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  45.972694157796475
maxi score, test score, baseline:  0.12223333333333322 0.6816666666666666 0.6816666666666666
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.361]
 [0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.355]] [[60.399]
 [63.96 ]
 [60.399]
 [60.399]
 [60.399]
 [60.399]
 [60.399]] [[1.879]
 [2.028]
 [1.879]
 [1.879]
 [1.879]
 [1.879]
 [1.879]]
maxi score, test score, baseline:  0.12223333333333322 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  44 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.12223333333333322 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.714]
 [0.714]
 [0.714]
 [0.714]
 [0.714]
 [0.613]
 [0.714]] [[48.645]
 [48.645]
 [48.645]
 [48.645]
 [48.645]
 [50.402]
 [48.645]] [[2.283]
 [2.283]
 [2.283]
 [2.283]
 [2.283]
 [2.28 ]
 [2.283]]
Printing some Q and Qe and total Qs values:  [[0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]
 [0.312]] [[42.633]
 [42.633]
 [42.633]
 [42.633]
 [42.633]
 [42.633]
 [42.633]] [[1.418]
 [1.418]
 [1.418]
 [1.418]
 [1.418]
 [1.418]
 [1.418]]
maxi score, test score, baseline:  0.12223333333333322 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.3866666666666667  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  37.86723754487535
maxi score, test score, baseline:  0.12223333333333322 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.04859531881368
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.809]
 [0.91 ]
 [0.797]
 [0.803]
 [0.803]
 [0.801]
 [0.809]] [[57.237]
 [46.073]
 [55.434]
 [54.871]
 [55.042]
 [56.111]
 [48.266]] [[0.809]
 [0.91 ]
 [0.797]
 [0.803]
 [0.803]
 [0.801]
 [0.809]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]
 [0.485]] [[43.813]
 [43.813]
 [43.813]
 [43.813]
 [43.813]
 [43.813]
 [43.813]] [[1.479]
 [1.479]
 [1.479]
 [1.479]
 [1.479]
 [1.479]
 [1.479]]
printing an ep nov before normalisation:  10.613757973459101
actor:  1 policy actor:  1  step number:  59 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  55 total reward:  0.4133333333333329  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.12223333333333322 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.12223333333333322 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  40 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.12517999999999985 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.659]
 [0.738]
 [0.674]
 [0.657]
 [0.656]
 [0.704]
 [0.659]] [[42.936]
 [36.913]
 [38.564]
 [42.859]
 [42.893]
 [38.195]
 [42.46 ]] [[0.659]
 [0.738]
 [0.674]
 [0.657]
 [0.656]
 [0.704]
 [0.659]]
maxi score, test score, baseline:  0.12517999999999985 0.6816666666666666 0.6816666666666666
maxi score, test score, baseline:  0.12517999999999985 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.12517999999999985 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  35 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1282466666666665 0.6816666666666666 0.6816666666666666
siam score:  -0.77038836
line 256 mcts: sample exp_bonus 26.912041130244184
maxi score, test score, baseline:  0.1282466666666665 0.6816666666666666 0.6816666666666666
printing an ep nov before normalisation:  44.757652282714844
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1282466666666665 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76978636
maxi score, test score, baseline:  0.1282466666666665 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0
actor:  0 policy actor:  0  step number:  39 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  0  action  0 :  tensor([0.2868, 0.0250, 0.1349, 0.1362, 0.1627, 0.1299, 0.1244],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0040,     0.9594,     0.0023,     0.0020,     0.0007,     0.0005,
            0.0311], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0841, 0.0021, 0.4629, 0.1105, 0.1005, 0.1641, 0.0757],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1148, 0.0047, 0.1324, 0.3348, 0.1644, 0.1455, 0.1034],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0942, 0.0061, 0.1054, 0.1451, 0.4327, 0.1343, 0.0823],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1183, 0.0073, 0.1253, 0.1536, 0.1225, 0.3850, 0.0879],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1479, 0.0715, 0.1321, 0.1352, 0.1172, 0.1008, 0.2954],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1288733333333332 0.6816666666666666 0.6816666666666666
maxi score, test score, baseline:  0.1288733333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.20703787876036
printing an ep nov before normalisation:  46.6090294679367
line 256 mcts: sample exp_bonus 41.9372858162265
printing an ep nov before normalisation:  32.355390400789254
maxi score, test score, baseline:  0.1288733333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1288733333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.4363064864478
printing an ep nov before normalisation:  50.730395231643335
printing an ep nov before normalisation:  49.84851173217761
actor:  0 policy actor:  0  step number:  44 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  43.07012223940045
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.059]
 [0.02 ]
 [0.02 ]
 [0.02 ]
 [0.02 ]
 [0.016]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.02 ]
 [0.059]
 [0.02 ]
 [0.02 ]
 [0.02 ]
 [0.02 ]
 [0.016]]
maxi score, test score, baseline:  0.1291133333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.1291133333333332 0.6816666666666666 0.6816666666666666
printing an ep nov before normalisation:  36.99320952097575
printing an ep nov before normalisation:  36.74098605208499
actor:  1 policy actor:  1  step number:  50 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.776]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.691]] [[47.894]
 [47.068]
 [36.096]
 [36.096]
 [36.096]
 [36.096]
 [47.165]] [[0.635]
 [0.776]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.691]]
maxi score, test score, baseline:  0.1291133333333332 0.6816666666666666 0.6816666666666666
printing an ep nov before normalisation:  54.74706241813471
actor:  1 policy actor:  1  step number:  58 total reward:  0.19333333333333302  reward:  1.0 rdn_beta:  1.667
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.1291133333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]
 [0.051]]
printing an ep nov before normalisation:  50.78585756465167
printing an ep nov before normalisation:  42.81594965528685
printing an ep nov before normalisation:  43.54246939700341
printing an ep nov before normalisation:  43.24872888728425
printing an ep nov before normalisation:  35.158931150861605
maxi score, test score, baseline:  0.1291133333333332 0.6816666666666666 0.6816666666666666
maxi score, test score, baseline:  0.1291133333333332 0.6816666666666666 0.6816666666666666
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.97963570034618
printing an ep nov before normalisation:  40.956687927246094
printing an ep nov before normalisation:  36.84842348098755
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  0.04696027138777481
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.486]
 [0.486]
 [0.486]
 [0.655]
 [0.486]
 [0.486]] [[39.139]
 [39.139]
 [39.139]
 [39.139]
 [56.299]
 [39.139]
 [39.139]] [[0.776]
 [0.776]
 [0.776]
 [0.776]
 [1.193]
 [0.776]
 [0.776]]
Printing some Q and Qe and total Qs values:  [[0.404]
 [0.541]
 [0.614]
 [0.524]
 [0.542]
 [0.602]
 [0.541]] [[45.608]
 [44.463]
 [43.553]
 [42.044]
 [45.771]
 [39.465]
 [46.135]] [[1.303]
 [1.395]
 [1.433]
 [1.284]
 [1.447]
 [1.263]
 [1.459]]
maxi score, test score, baseline:  0.16227333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.25333333333333274  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  57.75533882249245
printing an ep nov before normalisation:  70.13877103667154
printing an ep nov before normalisation:  51.165186842931845
actor:  1 policy actor:  1  step number:  49 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.468]
 [0.447]
 [0.447]
 [0.447]
 [0.447]
 [0.447]] [[39.899]
 [35.426]
 [39.899]
 [39.899]
 [39.899]
 [39.899]
 [39.899]] [[1.436]
 [1.26 ]
 [1.436]
 [1.436]
 [1.436]
 [1.436]
 [1.436]]
printing an ep nov before normalisation:  51.10023461149862
printing an ep nov before normalisation:  48.4214973449707
actor:  1 policy actor:  1  step number:  51 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.16227333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16227333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  48.59587457498897
maxi score, test score, baseline:  0.16227333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  66 total reward:  0.11333333333333273  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  32.94283002091576
siam score:  -0.765915
printing an ep nov before normalisation:  52.106725691524026
printing an ep nov before normalisation:  43.545144257496744
printing an ep nov before normalisation:  32.28413428068794
maxi score, test score, baseline:  0.16227333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  66 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.16192666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.92680172756351
maxi score, test score, baseline:  0.16192666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16192666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16192666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16192666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16192666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  43 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  69 total reward:  0.013333333333332975  reward:  1.0 rdn_beta:  1.667
siam score:  -0.7609438
maxi score, test score, baseline:  0.16199333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.7285737991333
siam score:  -0.7633249
printing an ep nov before normalisation:  30.571622848510742
maxi score, test score, baseline:  0.16199333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  35 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  59 total reward:  0.14666666666666595  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  0  action  0 :  tensor([0.3075, 0.0210, 0.1179, 0.1187, 0.1696, 0.1156, 0.1497],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0078,     0.9675,     0.0058,     0.0009,     0.0003,     0.0004,
            0.0174], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1183, 0.0199, 0.2998, 0.1064, 0.1129, 0.2103, 0.1325],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1183, 0.0075, 0.1285, 0.3637, 0.1321, 0.1316, 0.1181],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1118, 0.0090, 0.0883, 0.0735, 0.4826, 0.1047, 0.1302],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0673, 0.0555, 0.1310, 0.0564, 0.0655, 0.5494, 0.0750],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1022, 0.2368, 0.1105, 0.1622, 0.0979, 0.0963, 0.1941],
       grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 52.36386244664153
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.066]
 [0.027]
 [0.01 ]
 [0.011]
 [0.009]
 [0.012]] [[25.524]
 [32.702]
 [26.692]
 [25.513]
 [25.629]
 [25.316]
 [25.359]] [[0.01 ]
 [0.066]
 [0.027]
 [0.01 ]
 [0.011]
 [0.009]
 [0.012]]
maxi score, test score, baseline:  0.16253999999999988 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16253999999999988 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16253999999999988 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  0.04666666666666597  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.16253999999999988 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.46084499359131
Printing some Q and Qe and total Qs values:  [[0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]
 [0.374]] [[37.186]
 [37.186]
 [37.186]
 [37.186]
 [37.186]
 [37.186]
 [37.186]] [[0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]
 [0.56]]
printing an ep nov before normalisation:  49.080934523116994
printing an ep nov before normalisation:  33.512122631073
maxi score, test score, baseline:  0.16253999999999988 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.031536502333914
maxi score, test score, baseline:  0.16253999999999988 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.32845991298944
printing an ep nov before normalisation:  39.499582492731996
printing an ep nov before normalisation:  50.12501101466183
actor:  1 policy actor:  1  step number:  37 total reward:  0.5466666666666667  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  48 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  44 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.9078254699707
actions average: 
K:  4  action  0 :  tensor([0.3853, 0.0637, 0.0776, 0.1533, 0.1615, 0.0703, 0.0883],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0342, 0.8538, 0.0087, 0.0270, 0.0113, 0.0068, 0.0583],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0721, 0.0031, 0.5588, 0.0718, 0.0849, 0.1158, 0.0936],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1811, 0.0102, 0.1460, 0.1535, 0.1587, 0.1560, 0.1946],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0971, 0.0210, 0.0444, 0.1146, 0.5616, 0.0705, 0.0908],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1421, 0.1076, 0.1434, 0.1005, 0.0983, 0.2842, 0.1238],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1082, 0.2535, 0.1132, 0.1083, 0.1031, 0.1517, 0.1620],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.16252666666666654 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  44 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  26.156244473026295
maxi score, test score, baseline:  0.1618733333333332 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.1618733333333332 0.6913333333333336 0.6913333333333336
actor:  1 policy actor:  1  step number:  50 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  36.771159931356
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  43.24451899705667
printing an ep nov before normalisation:  35.2186983689129
maxi score, test score, baseline:  0.1618733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1618733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.308571814425235
actor:  0 policy actor:  0  step number:  43 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]
 [0.755]] [[38.809]
 [38.809]
 [38.809]
 [38.809]
 [38.809]
 [38.809]
 [38.809]] [[1.615]
 [1.615]
 [1.615]
 [1.615]
 [1.615]
 [1.615]
 [1.615]]
printing an ep nov before normalisation:  40.58555491454748
using explorer policy with actor:  1
maxi score, test score, baseline:  0.16131333333333323 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.16131333333333323 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.597]
 [0.597]
 [0.597]
 [0.58 ]
 [0.597]
 [0.597]] [[13.733]
 [13.733]
 [13.733]
 [13.733]
 [14.228]
 [13.733]
 [13.733]] [[2.223]
 [2.223]
 [2.223]
 [2.223]
 [2.333]
 [2.223]
 [2.223]]
siam score:  -0.76520354
actor:  1 policy actor:  1  step number:  53 total reward:  0.2799999999999999  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.16131333333333323 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  40.93859259144689
Printing some Q and Qe and total Qs values:  [[0.162]
 [0.294]
 [0.158]
 [0.207]
 [0.158]
 [0.204]
 [0.158]] [[47.774]
 [38.835]
 [43.654]
 [41.961]
 [43.654]
 [41.778]
 [43.654]] [[2.162]
 [1.624]
 [1.849]
 [1.772]
 [1.849]
 [1.755]
 [1.849]]
maxi score, test score, baseline:  0.16131333333333323 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16131333333333323 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  0  action  0 :  tensor([0.3875, 0.0237, 0.1260, 0.1108, 0.1279, 0.1117, 0.1123],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0019,     0.9770,     0.0014,     0.0030,     0.0005,     0.0001,
            0.0161], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1001, 0.0110, 0.3867, 0.1181, 0.1217, 0.1461, 0.1163],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1182, 0.0249, 0.1539, 0.2583, 0.1550, 0.1383, 0.1515],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1391, 0.0208, 0.1381, 0.1038, 0.3682, 0.1221, 0.1078],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0653, 0.0100, 0.1422, 0.0697, 0.0814, 0.5645, 0.0669],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1055, 0.0742, 0.1031, 0.0883, 0.0921, 0.0947, 0.4421],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.16131333333333323 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.073820991568304
maxi score, test score, baseline:  0.16131333333333323 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16131333333333323 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16131333333333323 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16131333333333323 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  53 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.338]
 [0.286]
 [0.286]
 [0.286]
 [0.286]
 [0.286]] [[40.673]
 [50.839]
 [40.673]
 [40.673]
 [40.673]
 [40.673]
 [40.673]] [[0.776]
 [1.005]
 [0.776]
 [0.776]
 [0.776]
 [0.776]
 [0.776]]
maxi score, test score, baseline:  0.16032666666666656 0.6913333333333336 0.6913333333333336
Printing some Q and Qe and total Qs values:  [[0.296]
 [0.329]
 [0.296]
 [0.296]
 [0.275]
 [0.296]
 [0.296]] [[43.321]
 [49.687]
 [43.321]
 [43.321]
 [44.814]
 [43.321]
 [43.321]] [[0.637]
 [0.767]
 [0.637]
 [0.637]
 [0.638]
 [0.637]
 [0.637]]
printing an ep nov before normalisation:  52.49388492569844
maxi score, test score, baseline:  0.16032666666666656 0.6913333333333336 0.6913333333333336
actor:  1 policy actor:  1  step number:  70 total reward:  0.07333333333333303  reward:  1.0 rdn_beta:  2.0
siam score:  -0.7781566
Printing some Q and Qe and total Qs values:  [[0.114]
 [0.196]
 [0.094]
 [0.171]
 [0.084]
 [0.078]
 [0.138]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.114]
 [0.196]
 [0.094]
 [0.171]
 [0.084]
 [0.078]
 [0.138]]
maxi score, test score, baseline:  0.16032666666666656 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.16032666666666656 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.4232, 0.0059, 0.1221, 0.1247, 0.1284, 0.0980, 0.0977],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0065,     0.9459,     0.0041,     0.0036,     0.0009,     0.0008,
            0.0382], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1079, 0.0053, 0.4440, 0.0893, 0.0866, 0.1710, 0.0959],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1431, 0.0032, 0.1506, 0.2649, 0.1448, 0.1454, 0.1480],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1413, 0.0019, 0.1294, 0.1235, 0.3767, 0.0993, 0.1278],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1086, 0.0017, 0.1285, 0.1340, 0.1290, 0.3825, 0.1158],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0709, 0.0647, 0.1363, 0.0952, 0.0771, 0.0698, 0.4860],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.16032666666666656 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.27 ]
 [0.413]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]] [[59.187]
 [43.985]
 [59.187]
 [59.187]
 [59.187]
 [59.187]
 [59.187]] [[0.27 ]
 [0.413]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]]
line 256 mcts: sample exp_bonus 42.49792043244723
maxi score, test score, baseline:  0.16032666666666656 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.16032666666666656 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  40.69609851547564
printing an ep nov before normalisation:  44.609893411877145
Printing some Q and Qe and total Qs values:  [[0.855]
 [0.878]
 [0.872]
 [0.872]
 [0.855]
 [0.847]
 [0.855]] [[33.789]
 [33.479]
 [33.937]
 [34.565]
 [33.789]
 [34.983]
 [33.789]] [[1.45 ]
 [1.463]
 [1.472]
 [1.492]
 [1.45 ]
 [1.481]
 [1.45 ]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.16032666666666656 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  42 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1596733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1596733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1596733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1596733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1596733333333332 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  49.95075866422715
printing an ep nov before normalisation:  55.34193560820032
maxi score, test score, baseline:  0.1596733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1596733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.48  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  2  action  0 :  tensor([0.1570, 0.0260, 0.0935, 0.1342, 0.2860, 0.0986, 0.2047],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0033,     0.9672,     0.0019,     0.0014,     0.0005,     0.0004,
            0.0253], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1218, 0.0095, 0.3363, 0.1199, 0.1020, 0.1324, 0.1781],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1206, 0.0581, 0.0907, 0.3554, 0.1433, 0.1170, 0.1149],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1131, 0.0060, 0.0552, 0.0622, 0.6393, 0.0709, 0.0533],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0557, 0.0070, 0.0775, 0.0854, 0.0689, 0.6293, 0.0762],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1222, 0.2114, 0.1055, 0.1073, 0.1170, 0.1229, 0.2137],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1596733333333332 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.1596733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.3624, 0.0726, 0.1017, 0.1212, 0.1050, 0.1081, 0.1291],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0134, 0.9035, 0.0118, 0.0113, 0.0091, 0.0094, 0.0416],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0889, 0.0139, 0.5549, 0.0663, 0.0613, 0.1052, 0.1095],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1322, 0.0554, 0.1845, 0.1711, 0.1237, 0.1514, 0.1818],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1354, 0.0030, 0.0522, 0.0869, 0.5821, 0.0688, 0.0717],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1219, 0.0132, 0.1532, 0.0561, 0.0699, 0.5067, 0.0791],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0875, 0.1823, 0.0988, 0.1298, 0.0984, 0.0948, 0.3085],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  70 total reward:  0.01999999999999902  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  43 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.505]
 [0.616]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.505]] [[33.782]
 [51.174]
 [33.782]
 [33.782]
 [33.782]
 [33.782]
 [33.782]] [[0.61 ]
 [0.829]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]
 [0.61 ]]
siam score:  -0.7727147
maxi score, test score, baseline:  0.1596733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.11881536260186
printing an ep nov before normalisation:  36.11477028062289
maxi score, test score, baseline:  0.1596733333333332 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.1596733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.36450246307051
siam score:  -0.7740547
maxi score, test score, baseline:  0.1596733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.657329082489014
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.26666666666666594  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1596733333333332 0.6913333333333336 0.6913333333333336
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.174]
 [0.201]
 [0.157]
 [0.169]
 [0.167]
 [0.153]
 [0.163]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.174]
 [0.201]
 [0.157]
 [0.169]
 [0.167]
 [0.153]
 [0.163]]
Printing some Q and Qe and total Qs values:  [[0.267]
 [0.279]
 [0.27 ]
 [0.258]
 [0.27 ]
 [0.271]
 [0.263]] [[61.621]
 [57.796]
 [59.734]
 [60.595]
 [59.719]
 [60.253]
 [62.114]] [[1.606]
 [1.474]
 [1.538]
 [1.558]
 [1.538]
 [1.558]
 [1.621]]
printing an ep nov before normalisation:  59.582879913742254
printing an ep nov before normalisation:  46.19910717010498
actions average: 
K:  4  action  0 :  tensor([0.4431, 0.1012, 0.0887, 0.0628, 0.1231, 0.0917, 0.0893],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0057, 0.9291, 0.0073, 0.0073, 0.0045, 0.0042, 0.0419],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1351, 0.0163, 0.3925, 0.0915, 0.1131, 0.1191, 0.1324],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1400, 0.0066, 0.1560, 0.2209, 0.1766, 0.1372, 0.1627],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0636, 0.0868, 0.0552, 0.0575, 0.6317, 0.0484, 0.0568],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0930, 0.0087, 0.1476, 0.1015, 0.1355, 0.3939, 0.1197],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1501, 0.1271, 0.1484, 0.1147, 0.1230, 0.0933, 0.2434],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1596733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7740294
maxi score, test score, baseline:  0.1596733333333332 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  41.82564495840801
maxi score, test score, baseline:  0.1596733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1596733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.4866666666666669  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  24.2830228805542
maxi score, test score, baseline:  0.1596733333333332 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.1596733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1596733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 34.42979779121915
printing an ep nov before normalisation:  16.161482334136963
actor:  1 policy actor:  1  step number:  65 total reward:  0.0666666666666661  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  67 total reward:  0.119999999999999  reward:  1.0 rdn_beta:  0.333
siam score:  -0.773414
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.126]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.126]
 [0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
printing an ep nov before normalisation:  46.24728592112341
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.162]
 [0.182]
 [0.191]
 [0.196]
 [0.224]
 [0.199]] [[28.01 ]
 [31.139]
 [28.24 ]
 [27.817]
 [27.989]
 [27.464]
 [27.712]] [[0.137]
 [0.162]
 [0.182]
 [0.191]
 [0.196]
 [0.224]
 [0.199]]
maxi score, test score, baseline:  0.1596733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1596733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1596733333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.30328540337935
line 256 mcts: sample exp_bonus 45.448491944095906
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]] [[42.602]
 [42.602]
 [42.602]
 [42.602]
 [42.602]
 [42.602]
 [42.602]] [[1.592]
 [1.592]
 [1.592]
 [1.592]
 [1.592]
 [1.592]
 [1.592]]
printing an ep nov before normalisation:  11.276210123993478
printing an ep nov before normalisation:  34.25872576728104
actor:  0 policy actor:  0  step number:  53 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  45 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  0.001588659586104768
maxi score, test score, baseline:  0.15548666666666655 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15548666666666655 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.78806310726179
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.58 ]
 [0.535]
 [0.388]
 [0.303]
 [0.256]
 [0.467]] [[60.66 ]
 [53.299]
 [60.28 ]
 [67.344]
 [68.534]
 [69.038]
 [63.54 ]] [[1.161]
 [1.183]
 [1.27 ]
 [1.257]
 [1.194]
 [1.157]
 [1.264]]
printing an ep nov before normalisation:  49.34841282830083
printing an ep nov before normalisation:  36.173998482174476
maxi score, test score, baseline:  0.15548666666666655 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.651]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]] [[46.28 ]
 [48.981]
 [46.28 ]
 [46.28 ]
 [46.28 ]
 [46.28 ]
 [46.28 ]] [[1.063]
 [1.229]
 [1.063]
 [1.063]
 [1.063]
 [1.063]
 [1.063]]
maxi score, test score, baseline:  0.15548666666666655 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15548666666666655 0.6913333333333336 0.6913333333333336
actor:  0 policy actor:  0  step number:  41 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.68 ]
 [0.493]
 [0.493]
 [0.493]
 [0.493]
 [0.493]] [[45.907]
 [53.694]
 [45.907]
 [45.907]
 [45.907]
 [45.907]
 [45.907]] [[0.638]
 [0.873]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]]
maxi score, test score, baseline:  0.15511333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.06879425048828
printing an ep nov before normalisation:  35.32716989517212
maxi score, test score, baseline:  0.15511333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  0.5466666666666667  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.15511333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15511333333333321 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.15511333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15511333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7692419
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15511333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.67399224947495
printing an ep nov before normalisation:  41.89205652627821
Printing some Q and Qe and total Qs values:  [[-0.043]
 [ 0.089]
 [ 0.195]
 [ 0.109]
 [ 0.089]
 [ 0.167]
 [ 0.089]] [[45.214]
 [41.97 ]
 [40.176]
 [48.251]
 [41.97 ]
 [43.448]
 [41.97 ]] [[1.717]
 [1.591]
 [1.556]
 [2.109]
 [1.591]
 [1.786]
 [1.591]]
printing an ep nov before normalisation:  38.14718371720237
actions average: 
K:  2  action  0 :  tensor([0.3139, 0.0146, 0.1581, 0.1302, 0.1377, 0.1273, 0.1182],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0102, 0.9359, 0.0116, 0.0057, 0.0026, 0.0025, 0.0315],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1121, 0.0022, 0.5153, 0.0950, 0.0832, 0.1137, 0.0785],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1610, 0.0275, 0.1371, 0.2357, 0.1424, 0.1552, 0.1412],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1007, 0.0286, 0.0781, 0.0758, 0.5666, 0.0831, 0.0671],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1005, 0.0046, 0.1122, 0.0620, 0.0612, 0.5744, 0.0852],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1315, 0.1879, 0.1049, 0.0662, 0.0583, 0.0659, 0.3853],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  39.02876341201068
maxi score, test score, baseline:  0.15511333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15511333333333321 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.15511333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  41.48514547426229
maxi score, test score, baseline:  0.15511333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15511333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.79898738861084
printing an ep nov before normalisation:  42.96236998407478
actor:  1 policy actor:  1  step number:  52 total reward:  0.4600000000000001  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]]
actions average: 
K:  3  action  0 :  tensor([0.1950, 0.0272, 0.0971, 0.0761, 0.3404, 0.1252, 0.1389],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0146, 0.9047, 0.0094, 0.0192, 0.0078, 0.0084, 0.0358],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1478, 0.0021, 0.2959, 0.1276, 0.1417, 0.1550, 0.1299],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1715, 0.0045, 0.1722, 0.1551, 0.1591, 0.1871, 0.1506],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1133, 0.0027, 0.0692, 0.0870, 0.5570, 0.0796, 0.0912],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1114, 0.0024, 0.1241, 0.1236, 0.1167, 0.4180, 0.1038],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1039, 0.1130, 0.0780, 0.1256, 0.1412, 0.0834, 0.3549],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.15511333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  73 total reward:  0.26666666666666583  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.15511333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.854519651156494
actor:  0 policy actor:  0  step number:  48 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.59 ]
 [0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]] [[31.092]
 [51.269]
 [31.092]
 [31.092]
 [31.092]
 [31.092]
 [31.092]] [[0.62 ]
 [0.807]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]]
maxi score, test score, baseline:  0.15451333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.5916494117814
maxi score, test score, baseline:  0.15451333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15451333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]] [[40.98]
 [40.98]
 [40.98]
 [40.98]
 [40.98]
 [40.98]
 [40.98]] [[1.069]
 [1.069]
 [1.069]
 [1.069]
 [1.069]
 [1.069]
 [1.069]]
printing an ep nov before normalisation:  39.9331187314413
maxi score, test score, baseline:  0.15451333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.3016082691844
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.625]
 [0.383]
 [0.558]
 [0.383]
 [0.383]
 [0.383]] [[66.898]
 [59.681]
 [66.898]
 [58.063]
 [66.898]
 [66.898]
 [66.898]] [[2.014]
 [2.019]
 [2.014]
 [1.899]
 [2.014]
 [2.014]
 [2.014]]
Printing some Q and Qe and total Qs values:  [[0.113]
 [0.297]
 [0.113]
 [0.113]
 [0.113]
 [0.113]
 [0.113]] [[37.031]
 [50.131]
 [37.031]
 [37.031]
 [37.031]
 [37.031]
 [37.031]] [[0.68 ]
 [1.237]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]
 [0.68 ]]
maxi score, test score, baseline:  0.15451333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15451333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15451333333333322 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.15451333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15451333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3866666666666667  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  41 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  42.75145043843647
printing an ep nov before normalisation:  55.59726239025873
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.45238354797325
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.827014066047596
printing an ep nov before normalisation:  43.10407309401607
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15411333333333319 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.42872219657621
siam score:  -0.76754653
Printing some Q and Qe and total Qs values:  [[0.728]
 [0.731]
 [0.726]
 [0.679]
 [0.728]
 [0.655]
 [0.741]] [[41.007]
 [45.177]
 [30.327]
 [42.592]
 [41.007]
 [43.455]
 [43.489]] [[1.875]
 [1.995]
 [1.574]
 [1.871]
 [1.875]
 [1.871]
 [1.958]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  23.187519764879507
actor:  1 policy actor:  1  step number:  63 total reward:  0.30666666666666664  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  67 total reward:  0.22666666666666568  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  44.88258587840636
Printing some Q and Qe and total Qs values:  [[ 0.013]
 [ 0.162]
 [ 0.102]
 [ 0.1  ]
 [-0.034]
 [ 0.135]
 [-0.04 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.013]
 [ 0.162]
 [ 0.102]
 [ 0.1  ]
 [-0.034]
 [ 0.135]
 [-0.04 ]]
maxi score, test score, baseline:  0.15411333333333319 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.11641228243796
maxi score, test score, baseline:  0.15411333333333319 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15411333333333319 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.36146354675293
actor:  1 policy actor:  1  step number:  45 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.15411333333333319 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.37333333333333263  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.09475796136188
printing an ep nov before normalisation:  57.57222842742146
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.70569030909925
maxi score, test score, baseline:  0.15411333333333319 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.448]
 [0.27 ]
 [0.282]
 [0.291]
 [0.263]
 [0.334]] [[36.163]
 [38.796]
 [37.138]
 [39.075]
 [38.115]
 [37.537]
 [36.349]] [[0.895]
 [1.134]
 [0.903]
 [0.977]
 [0.956]
 [0.909]
 [0.941]]
printing an ep nov before normalisation:  42.74658092143826
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.461]
 [0.706]
 [0.53 ]
 [0.461]
 [0.461]
 [0.519]
 [0.461]] [[43.543]
 [37.988]
 [43.936]
 [43.543]
 [43.543]
 [41.415]
 [43.543]] [[1.747]
 [1.672]
 [1.839]
 [1.747]
 [1.747]
 [1.682]
 [1.747]]
printing an ep nov before normalisation:  36.809838263759374
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15411333333333319 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  30.353035993611336
actor:  1 policy actor:  1  step number:  59 total reward:  0.23999999999999966  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  36.56076431274414
printing an ep nov before normalisation:  27.804017066955566
actor:  1 policy actor:  1  step number:  61 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15411333333333319 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15411333333333319 0.6913333333333336 0.6913333333333336
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.649]
 [0.539]
 [0.537]
 [0.549]
 [0.522]
 [0.642]] [[39.19 ]
 [45.107]
 [37.929]
 [38.537]
 [41.265]
 [38.31 ]
 [40.094]] [[0.559]
 [0.649]
 [0.539]
 [0.537]
 [0.549]
 [0.522]
 [0.642]]
printing an ep nov before normalisation:  56.14341794921894
printing an ep nov before normalisation:  44.25733658771496
maxi score, test score, baseline:  0.15411333333333319 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.76111324332222
Printing some Q and Qe and total Qs values:  [[ 0.434]
 [ 0.638]
 [-0.062]
 [ 0.434]
 [ 0.434]
 [ 0.434]
 [ 0.434]] [[33.769]
 [41.949]
 [41.066]
 [33.769]
 [33.769]
 [33.769]
 [33.769]] [[1.262]
 [2.004]
 [1.246]
 [1.262]
 [1.262]
 [1.262]
 [1.262]]
maxi score, test score, baseline:  0.15411333333333319 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7731367
actor:  0 policy actor:  0  step number:  35 total reward:  0.4933333333333332  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.15371333333333323 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.834]
 [0.739]
 [0.726]
 [0.739]
 [0.735]
 [0.731]] [[45.582]
 [39.816]
 [45.407]
 [45.698]
 [45.407]
 [45.743]
 [45.854]] [[2.543]
 [2.264]
 [2.574]
 [2.582]
 [2.574]
 [2.594]
 [2.598]]
maxi score, test score, baseline:  0.15032666666666653 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.11 ]
 [0.119]
 [0.125]
 [0.11 ]
 [0.11 ]
 [0.115]
 [0.11 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.11 ]
 [0.119]
 [0.125]
 [0.11 ]
 [0.11 ]
 [0.115]
 [0.11 ]]
printing an ep nov before normalisation:  20.44473555770466
printing an ep nov before normalisation:  55.587131507990364
actor:  1 policy actor:  1  step number:  53 total reward:  0.5333333333333339  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  33.55778694152832
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.225075346968296
printing an ep nov before normalisation:  55.45108236199894
actor:  1 policy actor:  1  step number:  61 total reward:  0.1733333333333329  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  52 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.15032666666666653 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.15032666666666653 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.29511403566437
printing an ep nov before normalisation:  49.720776081085205
maxi score, test score, baseline:  0.15032666666666653 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15032666666666653 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.15032666666666653 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]] [[57.426]
 [57.426]
 [57.426]
 [57.426]
 [57.426]
 [57.426]
 [57.426]] [[1.261]
 [1.261]
 [1.261]
 [1.261]
 [1.261]
 [1.261]
 [1.261]]
printing an ep nov before normalisation:  42.47745424480149
actor:  1 policy actor:  1  step number:  60 total reward:  0.13999999999999946  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.49510839042527
printing an ep nov before normalisation:  37.543900518771885
printing an ep nov before normalisation:  29.967382465045244
maxi score, test score, baseline:  0.15032666666666653 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7732107
printing an ep nov before normalisation:  31.63013458251953
siam score:  -0.77263784
actor:  1 policy actor:  1  step number:  47 total reward:  0.5333333333333335  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  50.24699917920485
maxi score, test score, baseline:  0.15032666666666653 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.062465495527512
actor:  1 policy actor:  1  step number:  69 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.15032666666666653 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.44847951343222
maxi score, test score, baseline:  0.15032666666666653 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15032666666666653 0.6913333333333336 0.6913333333333336
actor:  1 policy actor:  1  step number:  57 total reward:  0.30666666666666653  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  31.291929909353293
Printing some Q and Qe and total Qs values:  [[0.868]
 [0.891]
 [0.825]
 [0.803]
 [0.805]
 [0.806]
 [0.867]] [[42.159]
 [44.118]
 [38.247]
 [26.233]
 [26.189]
 [26.35 ]
 [42.678]] [[0.868]
 [0.891]
 [0.825]
 [0.803]
 [0.805]
 [0.806]
 [0.867]]
printing an ep nov before normalisation:  41.1827589531261
printing an ep nov before normalisation:  28.73860796402831
actor:  0 policy actor:  0  step number:  52 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.14948666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.74444865989256
Printing some Q and Qe and total Qs values:  [[ 0.095]
 [-0.   ]
 [ 0.095]
 [ 0.095]
 [ 0.095]
 [ 0.095]
 [ 0.095]] [[36.176]
 [54.513]
 [36.176]
 [36.176]
 [36.176]
 [36.176]
 [36.176]] [[0.702]
 [1.207]
 [0.702]
 [0.702]
 [0.702]
 [0.702]
 [0.702]]
Printing some Q and Qe and total Qs values:  [[ 0.412]
 [ 0.367]
 [-0.042]
 [ 0.343]
 [ 0.386]
 [ 0.237]
 [ 0.352]] [[41.754]
 [43.472]
 [40.591]
 [42.5  ]
 [43.705]
 [36.556]
 [44.951]] [[0.625]
 [0.596]
 [0.161]
 [0.563]
 [0.617]
 [0.402]
 [0.595]]
maxi score, test score, baseline:  0.14948666666666652 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  55 total reward:  0.29333333333333267  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.14868666666666655 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14868666666666655 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14868666666666655 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.148]
 [0.182]
 [0.148]
 [0.148]
 [0.127]
 [0.148]
 [0.148]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.148]
 [0.182]
 [0.148]
 [0.148]
 [0.127]
 [0.148]
 [0.148]]
actions average: 
K:  4  action  0 :  tensor([0.1757, 0.0160, 0.1725, 0.1367, 0.1545, 0.1461, 0.1985],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0133, 0.9047, 0.0093, 0.0180, 0.0142, 0.0088, 0.0316],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0992, 0.1406, 0.2784, 0.0847, 0.0594, 0.1244, 0.2133],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0551, 0.1534, 0.0783, 0.3228, 0.0435, 0.1251, 0.2218],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1698, 0.0617, 0.0776, 0.0748, 0.4512, 0.0849, 0.0799],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0617, 0.0037, 0.1140, 0.0638, 0.0563, 0.6299, 0.0706],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1521, 0.0218, 0.1462, 0.1801, 0.1367, 0.1596, 0.2034],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.14868666666666655 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14868666666666655 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.14868666666666655 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.924280169281225
UNIT TEST: sample policy line 217 mcts : [0.184 0.204 0.102 0.224 0.122 0.082 0.082]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actor:  0 policy actor:  0  step number:  34 total reward:  0.54  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.316]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]] [[47.181]
 [54.338]
 [47.181]
 [47.181]
 [47.181]
 [47.181]
 [47.181]] [[1.101]
 [1.316]
 [1.101]
 [1.101]
 [1.101]
 [1.101]
 [1.101]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.23697022141857
actor:  1 policy actor:  1  step number:  44 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]
 [0.21]] [[53.999]
 [53.999]
 [53.999]
 [53.999]
 [53.999]
 [53.999]
 [53.999]] [[1.874]
 [1.874]
 [1.874]
 [1.874]
 [1.874]
 [1.874]
 [1.874]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1449933333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1449933333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1449933333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.631]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]] [[46.108]
 [45.19 ]
 [46.108]
 [46.108]
 [46.108]
 [46.108]
 [46.108]] [[1.273]
 [1.353]
 [1.273]
 [1.273]
 [1.273]
 [1.273]
 [1.273]]
maxi score, test score, baseline:  0.1449933333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.7334917790554
actor:  1 policy actor:  1  step number:  65 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.68609815898306
Printing some Q and Qe and total Qs values:  [[0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.487]
 [0.544]
 [0.487]] [[47.648]
 [47.648]
 [47.648]
 [47.648]
 [47.648]
 [55.493]
 [47.648]] [[1.579]
 [1.579]
 [1.579]
 [1.579]
 [1.579]
 [1.955]
 [1.579]]
maxi score, test score, baseline:  0.1449933333333332 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.1449933333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.42000000000000004  reward:  1.0 rdn_beta:  1.667
siam score:  -0.77974135
maxi score, test score, baseline:  0.1449933333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1449933333333332 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  22.70308330373343
actor:  1 policy actor:  1  step number:  59 total reward:  0.3733333333333333  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  44.2169189453125
maxi score, test score, baseline:  0.1449933333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.314]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]] [[45.85]
 [56.19]
 [45.85]
 [45.85]
 [45.85]
 [45.85]
 [45.85]] [[1.167]
 [1.492]
 [1.167]
 [1.167]
 [1.167]
 [1.167]
 [1.167]]
Printing some Q and Qe and total Qs values:  [[0.189]
 [0.189]
 [0.189]
 [0.189]
 [0.189]
 [0.189]
 [0.189]] [[41.245]
 [41.245]
 [41.245]
 [41.245]
 [41.245]
 [41.245]
 [41.245]] [[1.334]
 [1.334]
 [1.334]
 [1.334]
 [1.334]
 [1.334]
 [1.334]]
Printing some Q and Qe and total Qs values:  [[0.192]
 [0.217]
 [0.188]
 [0.188]
 [0.187]
 [0.188]
 [0.188]] [[41.34 ]
 [35.928]
 [42.443]
 [42.443]
 [41.152]
 [42.443]
 [42.443]] [[1.342]
 [1.065]
 [1.399]
 [1.399]
 [1.326]
 [1.399]
 [1.399]]
maxi score, test score, baseline:  0.1449933333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.186128750143325
maxi score, test score, baseline:  0.1449933333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.107936862942786
Printing some Q and Qe and total Qs values:  [[0.852]
 [0.873]
 [0.84 ]
 [0.475]
 [0.683]
 [0.817]
 [0.841]] [[28.599]
 [30.28 ]
 [28.686]
 [28.708]
 [28.539]
 [28.967]
 [29.314]] [[1.849]
 [1.929]
 [1.84 ]
 [1.476]
 [1.678]
 [1.827]
 [1.863]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1449933333333332 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.1449933333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1449933333333332 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  49.33705192746692
printing an ep nov before normalisation:  41.2850284576416
printing an ep nov before normalisation:  37.54764942342264
maxi score, test score, baseline:  0.1449933333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1449933333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.356049697533575
printing an ep nov before normalisation:  27.99033169596473
actor:  1 policy actor:  1  step number:  54 total reward:  0.40666666666666673  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[-0.028]
 [-0.028]
 [-0.011]
 [-0.005]
 [-0.028]
 [-0.024]
 [-0.028]] [[45.175]
 [45.175]
 [56.107]
 [57.569]
 [45.175]
 [58.138]
 [45.175]] [[0.41 ]
 [0.41 ]
 [0.591]
 [0.619]
 [0.41 ]
 [0.608]
 [0.41 ]]
actor:  0 policy actor:  0  step number:  41 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.14459333333333318 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.14459333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.74 ]
 [0.775]
 [0.671]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.672]] [[41.824]
 [40.367]
 [33.625]
 [41.824]
 [41.824]
 [41.824]
 [38.189]] [[0.74 ]
 [0.775]
 [0.671]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.672]]
printing an ep nov before normalisation:  33.80763873227011
actor:  0 policy actor:  0  step number:  65 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.74 ]
 [0.854]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.73 ]] [[44.277]
 [41.264]
 [44.277]
 [44.277]
 [44.277]
 [44.277]
 [53.289]] [[0.74 ]
 [0.854]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.73 ]]
maxi score, test score, baseline:  0.14365999999999984 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14365999999999984 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.811]
 [0.866]
 [0.811]
 [0.811]
 [0.74 ]
 [0.811]
 [0.858]] [[38.477]
 [44.867]
 [38.477]
 [38.477]
 [33.963]
 [38.477]
 [36.613]] [[0.811]
 [0.866]
 [0.811]
 [0.811]
 [0.74 ]
 [0.811]
 [0.858]]
printing an ep nov before normalisation:  58.68481028499642
printing an ep nov before normalisation:  41.14110469818115
actions average: 
K:  2  action  0 :  tensor([0.3201, 0.0201, 0.1122, 0.1284, 0.1603, 0.1371, 0.1218],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0049,     0.9756,     0.0012,     0.0016,     0.0011,     0.0003,
            0.0153], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1044, 0.0229, 0.2944, 0.1414, 0.1640, 0.1438, 0.1291],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1229, 0.0581, 0.1718, 0.1766, 0.1771, 0.1615, 0.1321],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1032, 0.0074, 0.1105, 0.1218, 0.4310, 0.1035, 0.1226],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0734, 0.0034, 0.1787, 0.0699, 0.0865, 0.5104, 0.0777],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0951, 0.1650, 0.1180, 0.1267, 0.1412, 0.1139, 0.2401],
       grad_fn=<DivBackward0>)
siam score:  -0.7747342
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.564]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]] [[47.606]
 [37.266]
 [47.606]
 [47.606]
 [47.606]
 [47.606]
 [47.606]] [[1.021]
 [0.953]
 [1.021]
 [1.021]
 [1.021]
 [1.021]
 [1.021]]
maxi score, test score, baseline:  0.14028666666666653 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.14028666666666653 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.619]
 [0.579]
 [0.574]
 [0.481]
 [0.574]
 [0.48 ]] [[37.914]
 [44.488]
 [42.017]
 [40.011]
 [38.13 ]
 [40.011]
 [39.357]] [[0.891]
 [1.169]
 [1.076]
 [1.028]
 [0.894]
 [1.028]
 [0.92 ]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4066666666666662  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  38 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.062]
 [0.224]
 [0.089]
 [0.089]
 [0.004]
 [0.089]
 [0.069]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.062]
 [0.224]
 [0.089]
 [0.089]
 [0.004]
 [0.089]
 [0.069]]
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.123]
 [0.105]
 [0.107]
 [0.109]
 [0.113]
 [0.107]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.107]
 [0.123]
 [0.105]
 [0.107]
 [0.109]
 [0.113]
 [0.107]]
printing an ep nov before normalisation:  43.58965873718262
Printing some Q and Qe and total Qs values:  [[0.27 ]
 [0.371]
 [0.323]
 [0.323]
 [0.281]
 [0.265]
 [0.323]] [[19.291]
 [31.354]
 [26.253]
 [28.552]
 [20.476]
 [24.174]
 [28.552]] [[0.318]
 [0.536]
 [0.439]
 [0.461]
 [0.34 ]
 [0.361]
 [0.461]]
Printing some Q and Qe and total Qs values:  [[0.343]
 [0.423]
 [0.343]
 [0.343]
 [0.343]
 [0.343]
 [0.343]] [[31.165]
 [35.26 ]
 [31.165]
 [31.165]
 [31.165]
 [31.165]
 [31.165]] [[0.974]
 [1.215]
 [0.974]
 [0.974]
 [0.974]
 [0.974]
 [0.974]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.23004865646362
line 256 mcts: sample exp_bonus 44.0368398949245
maxi score, test score, baseline:  0.13977999999999985 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  32.80816078186035
Printing some Q and Qe and total Qs values:  [[0.773]
 [0.761]
 [0.763]
 [0.763]
 [0.773]
 [0.762]
 [0.769]] [[20.173]
 [18.901]
 [19.502]
 [13.852]
 [14.513]
 [19.652]
 [19.648]] [[1.259]
 [1.217]
 [1.233]
 [1.097]
 [1.123]
 [1.236]
 [1.243]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  43.71685981750488
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.281]
 [0.225]
 [0.225]
 [0.225]
 [0.225]
 [0.225]] [[33.233]
 [42.383]
 [33.233]
 [33.233]
 [33.233]
 [33.233]
 [33.233]] [[0.449]
 [0.663]
 [0.449]
 [0.449]
 [0.449]
 [0.449]
 [0.449]]
Printing some Q and Qe and total Qs values:  [[-0.005]
 [ 0.441]
 [ 0.271]
 [ 0.339]
 [ 0.339]
 [ 0.319]
 [ 0.311]] [[33.284]
 [41.726]
 [31.513]
 [41.49 ]
 [41.49 ]
 [29.87 ]
 [32.048]] [[0.549]
 [1.259]
 [0.769]
 [1.149]
 [1.149]
 [0.766]
 [0.825]]
maxi score, test score, baseline:  0.13977999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  61 total reward:  0.13333333333333264  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.13869999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13869999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.99148956866071
siam score:  -0.7705277
actor:  1 policy actor:  1  step number:  54 total reward:  0.36666666666666603  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.13869999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13869999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13869999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 43.23079711336285
printing an ep nov before normalisation:  41.119047048148246
printing an ep nov before normalisation:  40.81870819463597
siam score:  -0.7770343
maxi score, test score, baseline:  0.13869999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.9195350822299
siam score:  -0.77781236
maxi score, test score, baseline:  0.13869999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.53115928572511
maxi score, test score, baseline:  0.13869999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.60928620734056
printing an ep nov before normalisation:  37.238978180861665
actor:  1 policy actor:  1  step number:  37 total reward:  0.4933333333333332  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.151]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.117]
 [0.151]
 [0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]]
printing an ep nov before normalisation:  53.54936632328323
actor:  1 policy actor:  1  step number:  69 total reward:  0.02666666666666584  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  50.14753336662842
maxi score, test score, baseline:  0.13869999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.096]
 [0.241]
 [0.096]
 [0.096]
 [0.096]
 [0.096]
 [0.096]] [[43.236]
 [48.938]
 [43.236]
 [43.236]
 [43.236]
 [43.236]
 [43.236]] [[0.513]
 [0.766]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]]
maxi score, test score, baseline:  0.13869999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13869999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.004925734021981043
maxi score, test score, baseline:  0.13869999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13869999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13869999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3733333333333334  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.397]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]] [[28.337]
 [36.966]
 [28.337]
 [28.337]
 [28.337]
 [28.337]
 [28.337]] [[0.744]
 [1.096]
 [0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]]
maxi score, test score, baseline:  0.13869999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  65 total reward:  0.21333333333333238  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.095]
 [0.073]
 [0.076]
 [0.075]
 [0.084]
 [0.078]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.076]
 [0.095]
 [0.073]
 [0.076]
 [0.075]
 [0.084]
 [0.078]]
printing an ep nov before normalisation:  55.789923109001805
maxi score, test score, baseline:  0.13849999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  23.07259421130842
printing an ep nov before normalisation:  55.532477983033615
maxi score, test score, baseline:  0.13849999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13849999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.483]
 [0.467]
 [0.561]
 [0.483]
 [0.535]
 [0.483]
 [0.483]] [[35.407]
 [40.32 ]
 [41.042]
 [35.407]
 [39.594]
 [35.407]
 [35.407]] [[1.492]
 [1.785]
 [1.924]
 [1.492]
 [1.808]
 [1.492]
 [1.492]]
maxi score, test score, baseline:  0.13849999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.79428854816679
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13849999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.4800000000000002  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  45 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  72.16079893452516
maxi score, test score, baseline:  0.13849999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 39.41475659516717
maxi score, test score, baseline:  0.13849999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13849999999999985 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.13849999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.07769613266715
printing an ep nov before normalisation:  33.958072662353516
actor:  0 policy actor:  0  step number:  50 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  45 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  32.1990442276001
printing an ep nov before normalisation:  53.15109519033931
maxi score, test score, baseline:  0.1411666666666665 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  39.03448076250113
printing an ep nov before normalisation:  43.41849747814359
maxi score, test score, baseline:  0.1411666666666665 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.556]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]] [[53.747]
 [59.101]
 [53.747]
 [53.747]
 [53.747]
 [53.747]
 [53.747]] [[1.593]
 [1.874]
 [1.593]
 [1.593]
 [1.593]
 [1.593]
 [1.593]]
maxi score, test score, baseline:  0.1411666666666665 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.1411666666666665 0.6913333333333336 0.6913333333333336
actor:  0 policy actor:  0  step number:  51 total reward:  0.33333333333333304  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  3  action  0 :  tensor([0.3648, 0.0019, 0.0928, 0.1197, 0.2277, 0.0986, 0.0947],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0204, 0.8304, 0.0182, 0.0253, 0.0162, 0.0182, 0.0713],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0890, 0.0015, 0.5731, 0.0768, 0.0688, 0.1203, 0.0705],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1291, 0.0110, 0.1294, 0.3247, 0.1268, 0.1549, 0.1241],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1512, 0.0050, 0.1315, 0.1636, 0.2780, 0.1363, 0.1344],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([    0.0002,     0.0000,     0.0322,     0.0009,     0.0016,     0.9650,
            0.0001], grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1004, 0.1886, 0.0931, 0.1643, 0.0758, 0.0669, 0.3108],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  47.919481646161216
printing an ep nov before normalisation:  47.10568229860337
using explorer policy with actor:  1
maxi score, test score, baseline:  0.13831333333333318 0.6913333333333336 0.6913333333333336
Printing some Q and Qe and total Qs values:  [[0.869]
 [0.724]
 [0.724]
 [0.724]
 [0.819]
 [0.724]
 [0.17 ]] [[48.377]
 [29.411]
 [29.411]
 [29.411]
 [48.314]
 [29.411]
 [48.684]] [[0.869]
 [0.724]
 [0.724]
 [0.724]
 [0.819]
 [0.724]
 [0.17 ]]
line 256 mcts: sample exp_bonus 54.87967899952539
printing an ep nov before normalisation:  37.62197494506836
actor:  1 policy actor:  1  step number:  71 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.13831333333333318 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.13831333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  42 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  49.92305733204644
maxi score, test score, baseline:  0.13815333333333318 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.13815333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.39847011848346
printing an ep nov before normalisation:  46.23154358337479
maxi score, test score, baseline:  0.13815333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13815333333333318 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  47.72719161183364
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.547]
 [0.476]
 [0.476]
 [0.476]
 [0.523]
 [0.476]] [[41.917]
 [43.006]
 [41.917]
 [41.917]
 [41.917]
 [34.9  ]
 [41.917]] [[0.747]
 [0.83 ]
 [0.747]
 [0.747]
 [0.747]
 [0.724]
 [0.747]]
maxi score, test score, baseline:  0.13815333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.264]
 [0.264]
 [0.261]
 [0.264]
 [0.264]
 [0.264]] [[71.725]
 [71.725]
 [71.725]
 [71.003]
 [71.725]
 [71.725]
 [71.725]] [[1.622]
 [1.622]
 [1.622]
 [1.6  ]
 [1.622]
 [1.622]
 [1.622]]
printing an ep nov before normalisation:  51.9602632522583
actor:  1 policy actor:  1  step number:  63 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  61.39798527580761
maxi score, test score, baseline:  0.13815333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13815333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13815333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.959411069343375
maxi score, test score, baseline:  0.13815333333333318 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  53 total reward:  0.31999999999999973  reward:  1.0 rdn_beta:  0.667
siam score:  -0.783921
actor:  1 policy actor:  1  step number:  62 total reward:  0.23333333333333328  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.14079333333333321 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.14079333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14079333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.48742961883545
maxi score, test score, baseline:  0.14079333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.23999999999999932  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.14079333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.4266666666666663  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  77.03814393326205
maxi score, test score, baseline:  0.14079333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14079333333333321 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  40.99208193579936
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.21720989068202
printing an ep nov before normalisation:  41.585353537698715
maxi score, test score, baseline:  0.14079333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14079333333333321 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  46.53452807504393
printing an ep nov before normalisation:  34.95462561019775
actor:  1 policy actor:  1  step number:  48 total reward:  0.4066666666666666  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  60 total reward:  0.17999999999999938  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  37.8910359135821
maxi score, test score, baseline:  0.14079333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.090714961700833
maxi score, test score, baseline:  0.14079333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14079333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.20538791813294
printing an ep nov before normalisation:  44.01310580728678
maxi score, test score, baseline:  0.14079333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14079333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14079333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14079333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14079333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14079333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14079333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14079333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  70 total reward:  0.17999999999999938  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  56 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  47 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  1.667
siam score:  -0.7733252
maxi score, test score, baseline:  0.14079333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14079333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.3593539479416
maxi score, test score, baseline:  0.14079333333333321 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.14079333333333321 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.09252555138212
actor:  0 policy actor:  0  step number:  46 total reward:  0.33999999999999986  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7699591
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14101999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14101999999999987 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  48.59655390191831
maxi score, test score, baseline:  0.14101999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14101999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76484346
maxi score, test score, baseline:  0.14101999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14101999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.11999999999999922  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  58 total reward:  0.1799999999999996  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  43.07551637027272
maxi score, test score, baseline:  0.14101999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14101999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.884265870071445
maxi score, test score, baseline:  0.14101999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.8642251206667
printing an ep nov before normalisation:  38.795413970947266
printing an ep nov before normalisation:  56.22229573524762
maxi score, test score, baseline:  0.13793999999999984 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13793999999999984 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.578960109836164
printing an ep nov before normalisation:  52.4615684150009
actor:  0 policy actor:  0  step number:  47 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.13805999999999985 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.13805999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13805999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13805999999999985 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.13805999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.326666666666666  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  46.94248015231251
printing an ep nov before normalisation:  48.887200812593484
maxi score, test score, baseline:  0.13805999999999985 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.4533333333333337  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  37.56490249939135
actor:  0 policy actor:  0  step number:  37 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1384333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1384333333333332 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.1384333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1384333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.54969882152839
maxi score, test score, baseline:  0.1384333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.61411559115069
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1384333333333332 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.1384333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.92924988905022
actions average: 
K:  1  action  0 :  tensor([0.3642, 0.0117, 0.1161, 0.1193, 0.1372, 0.1264, 0.1252],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0131,     0.9523,     0.0027,     0.0023,     0.0013,     0.0007,
            0.0276], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0919, 0.0227, 0.4499, 0.0898, 0.1141, 0.1183, 0.1133],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0721, 0.0217, 0.1090, 0.4654, 0.1027, 0.1241, 0.1050],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1380, 0.0045, 0.0462, 0.0625, 0.6323, 0.0521, 0.0644],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0646, 0.0024, 0.1285, 0.0765, 0.0767, 0.5734, 0.0780],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0523, 0.2750, 0.0669, 0.0930, 0.0752, 0.0640, 0.3736],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  59 total reward:  0.3333333333333325  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1384333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.571917991983156
actor:  1 policy actor:  1  step number:  49 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  52.83311252823678
printing an ep nov before normalisation:  39.424226871956805
maxi score, test score, baseline:  0.1384333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.071]
 [0.129]
 [0.108]] [[58.09 ]
 [58.09 ]
 [58.09 ]
 [58.09 ]
 [68.24 ]
 [66.211]
 [58.09 ]] [[0.536]
 [0.536]
 [0.536]
 [0.536]
 [0.627]
 [0.66 ]
 [0.536]]
maxi score, test score, baseline:  0.1384333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.61437306854269
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.361]
 [0.252]
 [0.252]
 [0.252]
 [0.252]
 [0.252]] [[38.538]
 [53.341]
 [38.538]
 [38.538]
 [38.538]
 [38.538]
 [38.538]] [[0.511]
 [0.791]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.511]]
printing an ep nov before normalisation:  67.86201197588744
maxi score, test score, baseline:  0.1384333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.513749454938015
printing an ep nov before normalisation:  65.13347688423154
printing an ep nov before normalisation:  17.926812171936035
actions average: 
K:  4  action  0 :  tensor([0.1393, 0.0137, 0.1267, 0.1581, 0.2548, 0.1501, 0.1572],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0126, 0.8523, 0.0231, 0.0286, 0.0068, 0.0265, 0.0501],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1200, 0.0228, 0.3221, 0.1402, 0.1163, 0.1403, 0.1384],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0903, 0.0090, 0.1064, 0.2772, 0.1649, 0.2377, 0.1145],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0959, 0.0812, 0.1009, 0.1261, 0.3896, 0.1072, 0.0991],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1034, 0.0467, 0.1741, 0.1517, 0.1182, 0.2789, 0.1270],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1042, 0.0475, 0.1820, 0.1272, 0.1075, 0.1378, 0.2939],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.97044198293069
printing an ep nov before normalisation:  46.30666255950928
printing an ep nov before normalisation:  27.408764712929187
actor:  1 policy actor:  1  step number:  46 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.504]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]] [[35.013]
 [45.656]
 [35.013]
 [35.013]
 [35.013]
 [35.013]
 [35.013]] [[0.773]
 [0.973]
 [0.773]
 [0.773]
 [0.773]
 [0.773]
 [0.773]]
printing an ep nov before normalisation:  33.44005346298218
maxi score, test score, baseline:  0.1384333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.82205069381646
actor:  1 policy actor:  1  step number:  67 total reward:  0.06666666666666565  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1362866666666665 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  59 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1362866666666665 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1362866666666665 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1362866666666665 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1362866666666665 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1362866666666665 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3866666666666667  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.584]
 [0.584]
 [0.584]
 [0.331]
 [0.641]
 [0.584]] [[37.631]
 [38.326]
 [38.326]
 [38.326]
 [40.694]
 [41.974]
 [38.326]] [[1.859]
 [1.906]
 [1.906]
 [1.906]
 [1.809]
 [2.204]
 [1.906]]
maxi score, test score, baseline:  0.1362866666666665 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.741]
 [0.683]
 [0.659]
 [0.637]
 [0.679]
 [0.721]] [[40.153]
 [40.664]
 [39.144]
 [39.265]
 [32.819]
 [39.829]
 [41.508]] [[1.201]
 [1.298]
 [1.2  ]
 [1.18 ]
 [0.99 ]
 [1.214]
 [1.3  ]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.3199999999999994  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1362866666666665 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.39333333333333265  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[ 0.122]
 [ 0.143]
 [-0.019]
 [ 0.127]
 [ 0.066]
 [-0.025]
 [ 0.14 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.122]
 [ 0.143]
 [-0.019]
 [ 0.127]
 [ 0.066]
 [-0.025]
 [ 0.14 ]]
line 256 mcts: sample exp_bonus 35.4962333212421
printing an ep nov before normalisation:  36.04325018026571
maxi score, test score, baseline:  0.1362866666666665 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1362866666666665 0.6913333333333336 0.6913333333333336
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.81954700600981
maxi score, test score, baseline:  0.1362866666666665 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.807]
 [0.925]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]] [[35.274]
 [44.073]
 [35.274]
 [35.274]
 [35.274]
 [35.274]
 [35.274]] [[0.807]
 [0.925]
 [0.807]
 [0.807]
 [0.807]
 [0.807]
 [0.807]]
line 256 mcts: sample exp_bonus 38.870267833268386
using explorer policy with actor:  1
siam score:  -0.7683133
actor:  0 policy actor:  0  step number:  47 total reward:  0.4666666666666669  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.98290837286846
printing an ep nov before normalisation:  37.36763371433992
maxi score, test score, baseline:  0.13921999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.4687452391292
printing an ep nov before normalisation:  41.34888172149658
maxi score, test score, baseline:  0.13921999999999987 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  35.383596051727466
printing an ep nov before normalisation:  27.503807266381173
printing an ep nov before normalisation:  18.60395674275935
maxi score, test score, baseline:  0.13921999999999987 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  33 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  57 total reward:  0.09333333333333271  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  48.04591437718125
maxi score, test score, baseline:  0.13997999999999988 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13997999999999988 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.35]
 [0.35]
 [0.35]
 [0.35]
 [0.35]
 [0.35]
 [0.35]] [[48.351]
 [48.351]
 [48.351]
 [48.351]
 [48.351]
 [48.351]
 [48.351]] [[1.35]
 [1.35]
 [1.35]
 [1.35]
 [1.35]
 [1.35]
 [1.35]]
maxi score, test score, baseline:  0.13997999999999988 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.667]
 [0.759]
 [0.653]
 [0.667]
 [0.657]
 [0.661]
 [0.667]] [[40.558]
 [35.39 ]
 [34.429]
 [40.558]
 [34.626]
 [35.285]
 [40.558]] [[3.237]
 [2.759]
 [2.547]
 [3.237]
 [2.573]
 [2.649]
 [3.237]]
maxi score, test score, baseline:  0.13997999999999988 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  44.97567217685315
actor:  1 policy actor:  1  step number:  50 total reward:  0.3266666666666661  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.13997999999999988 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.769]
 [0.613]
 [0.618]
 [0.703]
 [0.663]
 [0.622]] [[38.322]
 [37.494]
 [37.874]
 [37.747]
 [37.897]
 [38.303]
 [38.673]] [[0.952]
 [1.044]
 [0.893]
 [0.897]
 [0.984]
 [0.95 ]
 [0.914]]
maxi score, test score, baseline:  0.13997999999999988 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  0.17999999999999938  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.13997999999999988 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  44 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.13997999999999988 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  47.48539815806484
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.49 ]
 [0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]] [[34.598]
 [40.583]
 [34.598]
 [34.598]
 [34.598]
 [34.598]
 [34.598]] [[2.014]
 [2.422]
 [2.014]
 [2.014]
 [2.014]
 [2.014]
 [2.014]]
maxi score, test score, baseline:  0.13997999999999988 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.13997999999999988 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  47 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  57 total reward:  0.2799999999999999  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[ 0.107]
 [ 0.107]
 [ 0.107]
 [-0.001]
 [-0.002]
 [ 0.124]
 [ 0.107]] [[50.905]
 [50.905]
 [50.905]
 [54.818]
 [55.456]
 [49.41 ]
 [50.905]] [[1.607]
 [1.607]
 [1.607]
 [1.711]
 [1.744]
 [1.543]
 [1.607]]
maxi score, test score, baseline:  0.14007333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.44235272861834
maxi score, test score, baseline:  0.14007333333333322 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  24.50915084664517
actor:  1 policy actor:  1  step number:  72 total reward:  0.08666666666666589  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.14007333333333322 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7642033
Printing some Q and Qe and total Qs values:  [[0.966]
 [0.993]
 [0.95 ]
 [0.943]
 [0.907]
 [0.908]
 [0.913]] [[41.939]
 [36.927]
 [36.901]
 [32.671]
 [36.694]
 [31.026]
 [33.019]] [[0.966]
 [0.993]
 [0.95 ]
 [0.943]
 [0.907]
 [0.908]
 [0.913]]
printing an ep nov before normalisation:  52.888286681070554
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.087]
 [0.072]
 [0.071]
 [0.072]
 [0.081]
 [0.067]] [[23.179]
 [42.733]
 [24.16 ]
 [24.653]
 [25.022]
 [39.506]
 [25.396]] [[0.236]
 [0.553]
 [0.254]
 [0.261]
 [0.267]
 [0.497]
 [0.268]]
printing an ep nov before normalisation:  52.14747409846303
actor:  0 policy actor:  0  step number:  50 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 60.33372297800568
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.772]
 [0.777]
 [0.775]
 [0.811]
 [0.814]
 [0.677]] [[64.607]
 [66.463]
 [68.125]
 [67.669]
 [64.524]
 [64.412]
 [64.964]] [[1.232]
 [1.323]
 [1.347]
 [1.34 ]
 [1.339]
 [1.341]
 [1.21 ]]
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.746]
 [0.731]
 [0.702]
 [0.737]
 [0.702]
 [0.701]
 [0.701]] [[35.742]
 [36.845]
 [33.668]
 [34.956]
 [33.463]
 [33.595]
 [33.51 ]] [[1.311]
 [1.328]
 [1.207]
 [1.279]
 [1.2  ]
 [1.204]
 [1.201]]
printing an ep nov before normalisation:  44.02517318725586
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.292]
 [0.262]
 [0.262]
 [0.221]
 [0.262]
 [0.27 ]] [[60.04 ]
 [49.023]
 [60.04 ]
 [60.04 ]
 [55.566]
 [60.04 ]
 [65.002]] [[1.151]
 [0.935]
 [1.151]
 [1.151]
 [1.01 ]
 [1.151]
 [1.27 ]]
printing an ep nov before normalisation:  48.46220225659365
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
actor:  1 policy actor:  1  step number:  59 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  38.34269910910759
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
actor:  1 policy actor:  1  step number:  52 total reward:  0.24666666666666648  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.4683593708432
Printing some Q and Qe and total Qs values:  [[0.599]
 [0.613]
 [0.603]
 [0.365]
 [0.576]
 [0.545]
 [0.611]] [[31.039]
 [27.627]
 [27.114]
 [41.521]
 [35.606]
 [43.416]
 [27.048]] [[1.245]
 [1.188]
 [1.168]
 [1.229]
 [1.317]
 [1.449]
 [1.174]]
actor:  1 policy actor:  1  step number:  64 total reward:  0.2199999999999993  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  45.10735063874265
printing an ep nov before normalisation:  46.1446761664153
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7665285
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.168639810653026
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
actor:  1 policy actor:  1  step number:  56 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.21192619854233
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.698047983353085
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.66 ]
 [0.689]
 [0.616]
 [0.616]
 [0.616]
 [0.616]
 [0.616]] [[36.565]
 [45.029]
 [35.1  ]
 [35.1  ]
 [35.1  ]
 [35.1  ]
 [35.1  ]] [[1.511]
 [1.955]
 [1.395]
 [1.395]
 [1.395]
 [1.395]
 [1.395]]
printing an ep nov before normalisation:  52.7628655699121
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.2133333333333327  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
printing an ep nov before normalisation:  39.160715098720004
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.6466630059737
actor:  1 policy actor:  1  step number:  61 total reward:  0.32000000000000006  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.3399999999999993  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.163]
 [0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.076]
 [0.163]
 [0.076]
 [0.076]
 [0.076]
 [0.076]
 [0.076]]
printing an ep nov before normalisation:  59.55671595899419
printing an ep nov before normalisation:  67.82912397200964
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.769]
 [0.769]
 [0.769]
 [0.769]
 [0.793]
 [0.769]
 [0.769]] [[47.871]
 [47.871]
 [47.871]
 [47.871]
 [58.807]
 [47.871]
 [47.871]] [[1.393]
 [1.393]
 [1.393]
 [1.393]
 [1.629]
 [1.393]
 [1.393]]
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.850427151599476
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.2666666666666665  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.18539971800508
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.29333333333333256  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]] [[61.148]
 [61.148]
 [61.148]
 [61.148]
 [61.148]
 [61.148]
 [61.148]] [[1.696]
 [1.696]
 [1.696]
 [1.696]
 [1.696]
 [1.696]
 [1.696]]
maxi score, test score, baseline:  0.1398333333333332 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.96106013627202
actor:  1 policy actor:  1  step number:  49 total reward:  0.36  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.295]
 [0.33 ]
 [0.295]
 [0.295]
 [0.295]
 [0.295]
 [0.295]] [[40.828]
 [58.794]
 [40.828]
 [40.828]
 [40.828]
 [40.828]
 [40.828]] [[0.604]
 [0.858]
 [0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]]
siam score:  -0.7706549
maxi score, test score, baseline:  0.13745999999999986 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.75175952911377
Starting evaluation
printing an ep nov before normalisation:  37.48155580537781
printing an ep nov before normalisation:  56.894969155208926
line 256 mcts: sample exp_bonus 52.0175499991428
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.5  ]
 [0.409]
 [0.409]] [[48.477]
 [48.477]
 [48.477]
 [48.477]
 [57.122]
 [48.477]
 [48.477]] [[0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.5  ]
 [0.409]
 [0.409]]
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 44.13073881276831
printing an ep nov before normalisation:  38.88164043426514
printing an ep nov before normalisation:  44.59594566357815
printing an ep nov before normalisation:  45.43123839220189
printing an ep nov before normalisation:  40.18096446990967
Printing some Q and Qe and total Qs values:  [[0.969]
 [1.006]
 [0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]] [[42.681]
 [40.15 ]
 [42.681]
 [42.681]
 [42.681]
 [42.681]
 [42.681]] [[0.969]
 [1.006]
 [0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]]
printing an ep nov before normalisation:  39.2955680492455
actor:  1 policy actor:  1  step number:  55 total reward:  0.4266666666666663  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.13745999999999986 0.6913333333333336 0.6913333333333336
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.89542432041441
printing an ep nov before normalisation:  38.591802959199654
Printing some Q and Qe and total Qs values:  [[0.647]
 [1.007]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]] [[35.33 ]
 [28.256]
 [33.615]
 [33.615]
 [33.615]
 [33.615]
 [33.615]] [[0.647]
 [1.007]
 [0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]]
printing an ep nov before normalisation:  36.52201004601424
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  42.52212517277235
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  28 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  28 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.083]
 [0.091]
 [0.073]
 [0.079]
 [0.079]
 [0.077]
 [0.079]] [[16.775]
 [39.945]
 [12.237]
 [12.168]
 [12.054]
 [12.198]
 [11.931]] [[0.411]
 [1.109]
 [0.266]
 [0.27 ]
 [0.267]
 [0.27 ]
 [0.264]]
Printing some Q and Qe and total Qs values:  [[ 0.053]
 [ 0.187]
 [ 0.094]
 [ 0.134]
 [-0.025]
 [ 0.117]
 [ 0.186]] [[55.578]
 [50.491]
 [55.582]
 [53.027]
 [57.508]
 [47.761]
 [49.631]] [[1.244]
 [1.23 ]
 [1.284]
 [1.251]
 [1.222]
 [1.08 ]
 [1.204]]
printing an ep nov before normalisation:  37.21876382827759
actor:  1 policy actor:  1  step number:  59 total reward:  0.2133333333333325  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1744733333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.31966249148051
maxi score, test score, baseline:  0.1744733333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.97192262548983
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  45.49748281310196
maxi score, test score, baseline:  0.1744733333333332 0.684 0.684
maxi score, test score, baseline:  0.1744733333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.23679453045431
printing an ep nov before normalisation:  44.75108944835721
actor:  1 policy actor:  1  step number:  61 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  67 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1744733333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.20061849909493
siam score:  -0.77651834
printing an ep nov before normalisation:  41.037077374467216
maxi score, test score, baseline:  0.1744733333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.524848939844084
printing an ep nov before normalisation:  46.42673962634987
printing an ep nov before normalisation:  48.18932107489425
printing an ep nov before normalisation:  48.30875643095378
maxi score, test score, baseline:  0.1744733333333332 0.684 0.684
maxi score, test score, baseline:  0.1744733333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1744733333333332 0.684 0.684
maxi score, test score, baseline:  0.1744733333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  55 total reward:  0.3066666666666661  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.366]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.358]
 [0.366]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]]
maxi score, test score, baseline:  0.1744733333333332 0.684 0.684
printing an ep nov before normalisation:  70.17871328574414
actor:  1 policy actor:  1  step number:  70 total reward:  0.16666666666666596  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1744733333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  41 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  44 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17465999999999984 0.684 0.684
actions average: 
K:  1  action  0 :  tensor([0.5086, 0.0191, 0.0787, 0.0910, 0.1204, 0.0844, 0.0978],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0112, 0.9077, 0.0174, 0.0095, 0.0022, 0.0026, 0.0493],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0765, 0.0030, 0.4699, 0.1304, 0.1217, 0.1031, 0.0953],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1169, 0.0208, 0.1054, 0.3824, 0.1133, 0.1051, 0.1561],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0937, 0.0041, 0.0980, 0.1110, 0.5032, 0.0979, 0.0921],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0588, 0.0161, 0.1186, 0.1016, 0.0800, 0.5448, 0.0801],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1164, 0.1116, 0.1214, 0.1320, 0.1107, 0.1154, 0.2925],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.17465999999999984 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.60545732210684
printing an ep nov before normalisation:  49.627163099979995
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17465999999999984 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  63.72788812293781
line 256 mcts: sample exp_bonus 68.30101116937192
maxi score, test score, baseline:  0.17465999999999984 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.81950808932541
actor:  1 policy actor:  1  step number:  48 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  60.111663589125435
printing an ep nov before normalisation:  50.61054418449609
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.48959123239041
actor:  1 policy actor:  1  step number:  60 total reward:  0.29999999999999993  reward:  1.0 rdn_beta:  2.0
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.292]
 [0.37 ]
 [0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]] [[51.387]
 [49.363]
 [51.387]
 [51.387]
 [51.387]
 [51.387]
 [51.387]] [[1.625]
 [1.604]
 [1.625]
 [1.625]
 [1.625]
 [1.625]
 [1.625]]
printing an ep nov before normalisation:  48.51059732817235
printing an ep nov before normalisation:  53.121269941123465
actor:  1 policy actor:  1  step number:  67 total reward:  0.2133333333333327  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actor:  0 policy actor:  0  step number:  66 total reward:  0.0466666666666663  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  34.75353240966797
actor:  1 policy actor:  1  step number:  56 total reward:  0.21999999999999975  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.154]
 [0.311]
 [0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]] [[45.297]
 [46.453]
 [45.297]
 [45.297]
 [45.297]
 [45.297]
 [45.297]] [[0.663]
 [0.846]
 [0.663]
 [0.663]
 [0.663]
 [0.663]
 [0.663]]
maxi score, test score, baseline:  0.17431333333333318 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.763]
 [0.843]
 [0.763]
 [0.669]
 [0.666]
 [0.661]
 [0.654]] [[62.482]
 [55.526]
 [62.482]
 [77.25 ]
 [75.397]
 [74.774]
 [70.52 ]] [[1.143]
 [1.164]
 [1.143]
 [1.174]
 [1.155]
 [1.145]
 [1.102]]
printing an ep nov before normalisation:  46.284016835903486
printing an ep nov before normalisation:  43.01877975463867
Printing some Q and Qe and total Qs values:  [[0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.686]
 [0.712]
 [0.712]] [[39.829]
 [39.829]
 [39.829]
 [39.829]
 [56.252]
 [39.829]
 [39.829]] [[1.003]
 [1.003]
 [1.003]
 [1.003]
 [1.181]
 [1.003]
 [1.003]]
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.585]
 [0.585]
 [0.464]
 [0.698]
 [0.585]
 [0.585]] [[46.292]
 [46.292]
 [46.292]
 [36.021]
 [46.653]
 [46.292]
 [46.292]] [[1.675]
 [1.675]
 [1.675]
 [1.077]
 [1.804]
 [1.675]
 [1.675]]
printing an ep nov before normalisation:  45.049732240579395
printing an ep nov before normalisation:  46.81048217210817
printing an ep nov before normalisation:  37.25559853253026
maxi score, test score, baseline:  0.17431333333333318 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.48918157002138
printing an ep nov before normalisation:  46.146332377874785
printing an ep nov before normalisation:  33.584499597700265
actor:  1 policy actor:  1  step number:  48 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  72 total reward:  0.04666666666666586  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  51 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17431333333333318 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.17431333333333318 0.684 0.684
maxi score, test score, baseline:  0.17431333333333318 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.67853824084766
maxi score, test score, baseline:  0.17431333333333318 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.42408060896307
actor:  1 policy actor:  1  step number:  55 total reward:  0.34666666666666646  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  48.414551222751065
maxi score, test score, baseline:  0.17431333333333318 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.78287884816082
maxi score, test score, baseline:  0.17431333333333318 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.153]
 [0.074]
 [0.2  ]
 [0.204]
 [0.205]
 [0.205]
 [0.222]] [[23.016]
 [40.804]
 [23.064]
 [23.106]
 [23.141]
 [23.346]
 [32.622]] [[0.755]
 [1.607]
 [0.805]
 [0.812]
 [0.814]
 [0.825]
 [1.327]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.17431333333333318 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.4626, 0.0245, 0.1398, 0.0617, 0.0528, 0.1098, 0.1488],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0138, 0.9285, 0.0080, 0.0111, 0.0038, 0.0039, 0.0309],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1377, 0.0095, 0.3873, 0.1092, 0.0878, 0.1355, 0.1330],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1238, 0.0320, 0.1331, 0.2802, 0.0826, 0.2009, 0.1474],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1239, 0.0377, 0.0796, 0.0738, 0.5171, 0.0912, 0.0768],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0626, 0.0246, 0.1564, 0.0691, 0.0464, 0.5749, 0.0660],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0365, 0.3021, 0.0336, 0.0550, 0.0260, 0.0335, 0.5132],
       grad_fn=<DivBackward0>)
actions average: 
K:  3  action  0 :  tensor([0.4249, 0.0388, 0.0645, 0.0672, 0.1113, 0.2018, 0.0914],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0096, 0.9278, 0.0080, 0.0092, 0.0033, 0.0046, 0.0375],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1354, 0.0773, 0.3122, 0.0889, 0.0854, 0.1905, 0.1103],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1572, 0.0614, 0.1281, 0.2168, 0.1082, 0.1433, 0.1850],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1186, 0.0271, 0.0836, 0.0789, 0.4983, 0.1061, 0.0873],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0580, 0.0023, 0.0875, 0.0556, 0.0563, 0.6988, 0.0415],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1417, 0.0683, 0.1278, 0.1317, 0.0969, 0.1549, 0.2787],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.17431333333333318 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17431333333333318 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.84984589260333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  49 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17431333333333318 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7772329
maxi score, test score, baseline:  0.17431333333333318 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  13.725028038024902
maxi score, test score, baseline:  0.17431333333333318 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  59 total reward:  0.22666666666666646  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.755]
 [0.813]
 [0.755]
 [0.789]
 [0.755]
 [0.755]
 [0.855]] [[36.861]
 [41.56 ]
 [36.861]
 [44.595]
 [36.861]
 [36.861]
 [41.511]] [[0.755]
 [0.813]
 [0.755]
 [0.789]
 [0.755]
 [0.755]
 [0.855]]
printing an ep nov before normalisation:  35.55544716107354
printing an ep nov before normalisation:  48.943290875888906
maxi score, test score, baseline:  0.17431333333333318 0.684 0.684
actor:  0 policy actor:  0  step number:  52 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]] [[38.868]
 [38.868]
 [38.868]
 [38.868]
 [38.868]
 [38.868]
 [38.868]] [[0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]]
Printing some Q and Qe and total Qs values:  [[ 0.229]
 [ 0.23 ]
 [ 0.262]
 [ 0.233]
 [-0.059]
 [ 0.323]
 [ 0.247]] [[41.288]
 [42.109]
 [43.29 ]
 [42.775]
 [43.9  ]
 [45.073]
 [42.602]] [[0.704]
 [0.725]
 [0.786]
 [0.744]
 [0.48 ]
 [0.89 ]
 [0.754]]
Printing some Q and Qe and total Qs values:  [[0.722]
 [0.743]
 [0.729]
 [0.73 ]
 [0.737]
 [0.739]
 [0.728]] [[35.576]
 [39.19 ]
 [30.883]
 [31.958]
 [33.607]
 [33.544]
 [31.286]] [[0.722]
 [0.743]
 [0.729]
 [0.73 ]
 [0.737]
 [0.739]
 [0.728]]
printing an ep nov before normalisation:  57.521997676631294
maxi score, test score, baseline:  0.17464666666666653 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17464666666666653 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17464666666666653 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17464666666666653 0.684 0.684
maxi score, test score, baseline:  0.17464666666666653 0.684 0.684
maxi score, test score, baseline:  0.17464666666666653 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.79637946696632
maxi score, test score, baseline:  0.17464666666666653 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.5180, 0.0164, 0.0855, 0.1047, 0.1242, 0.0764, 0.0749],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0025,     0.9661,     0.0069,     0.0022,     0.0003,     0.0005,
            0.0215], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0920, 0.0170, 0.3759, 0.1983, 0.1312, 0.1033, 0.0822],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1373, 0.0317, 0.1506, 0.1840, 0.1806, 0.2056, 0.1102],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1578, 0.0067, 0.1342, 0.1334, 0.3546, 0.1253, 0.0880],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0843, 0.0082, 0.1883, 0.0914, 0.1100, 0.4340, 0.0837],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0906, 0.1589, 0.1085, 0.1550, 0.0984, 0.0878, 0.3008],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.17464666666666653 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.745070474063223
actor:  0 policy actor:  0  step number:  36 total reward:  0.48666666666666647  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17448666666666654 0.684 0.684
printing an ep nov before normalisation:  46.38766133063463
printing an ep nov before normalisation:  54.97300326009421
maxi score, test score, baseline:  0.17448666666666654 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  74 total reward:  0.04666666666666586  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  44.17219686018275
printing an ep nov before normalisation:  40.55234247252998
maxi score, test score, baseline:  0.1765799999999999 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1765799999999999 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1765799999999999 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  52 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  44.189026126973985
actor:  1 policy actor:  1  step number:  32 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.64791531507076
printing an ep nov before normalisation:  37.23388769823837
maxi score, test score, baseline:  0.17687333333333322 0.684 0.684
printing an ep nov before normalisation:  34.945961347860354
printing an ep nov before normalisation:  31.045398712158203
actor:  1 policy actor:  1  step number:  65 total reward:  0.173333333333333  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  0.17687333333333322 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17687333333333322 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.98509216308594
maxi score, test score, baseline:  0.17687333333333322 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.92077922821045
maxi score, test score, baseline:  0.17687333333333322 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.256866269574015
printing an ep nov before normalisation:  53.06599674701265
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  55.53345253359503
printing an ep nov before normalisation:  40.227952003479004
actor:  0 policy actor:  0  step number:  50 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.482]
 [0.552]
 [0.482]
 [0.482]
 [0.482]
 [0.482]
 [0.482]] [[30.906]
 [40.8  ]
 [30.906]
 [30.906]
 [30.906]
 [30.906]
 [30.906]] [[0.81 ]
 [1.057]
 [0.81 ]
 [0.81 ]
 [0.81 ]
 [0.81 ]
 [0.81 ]]
actor:  1 policy actor:  1  step number:  84 total reward:  0.03333333333333266  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17941999999999986 0.684 0.684
maxi score, test score, baseline:  0.17941999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.67576599121094
printing an ep nov before normalisation:  53.36983680725098
maxi score, test score, baseline:  0.17941999999999986 0.684 0.684
printing an ep nov before normalisation:  51.12684354789774
printing an ep nov before normalisation:  58.147364868379796
maxi score, test score, baseline:  0.17941999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17941999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.308]
 [0.308]
 [0.313]
 [0.402]
 [0.011]
 [0.165]] [[44.69 ]
 [43.549]
 [43.549]
 [44.695]
 [45.189]
 [45.135]
 [42.913]] [[1.242]
 [1.074]
 [1.074]
 [1.121]
 [1.229]
 [0.836]
 [0.908]]
printing an ep nov before normalisation:  51.57288897841816
maxi score, test score, baseline:  0.17941999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17941999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.24240900986297
printing an ep nov before normalisation:  44.400223501192244
maxi score, test score, baseline:  0.17941999999999986 0.684 0.684
actor:  1 policy actor:  1  step number:  66 total reward:  0.17999999999999927  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17941999999999986 0.684 0.684
actor:  1 policy actor:  1  step number:  61 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  48.22353372789908
Printing some Q and Qe and total Qs values:  [[0.42]
 [0.42]
 [0.42]
 [0.42]
 [0.42]
 [0.42]
 [0.42]] [[46.556]
 [46.556]
 [46.556]
 [46.556]
 [46.556]
 [46.556]
 [46.556]] [[1.905]
 [1.905]
 [1.905]
 [1.905]
 [1.905]
 [1.905]
 [1.905]]
printing an ep nov before normalisation:  50.20741868076903
printing an ep nov before normalisation:  41.476903400098216
maxi score, test score, baseline:  0.17941999999999986 0.684 0.684
maxi score, test score, baseline:  0.17941999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.9931806373188
printing an ep nov before normalisation:  49.32785208785192
line 256 mcts: sample exp_bonus 32.662825628463494
printing an ep nov before normalisation:  71.07969935833377
maxi score, test score, baseline:  0.17941999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.97950035078286
maxi score, test score, baseline:  0.17941999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
Printing some Q and Qe and total Qs values:  [[0.714]
 [0.801]
 [0.718]
 [0.706]
 [0.697]
 [0.72 ]
 [0.7  ]] [[46.144]
 [43.622]
 [47.45 ]
 [49.2  ]
 [51.59 ]
 [47.099]
 [49.882]] [[0.714]
 [0.801]
 [0.718]
 [0.706]
 [0.697]
 [0.72 ]
 [0.7  ]]
printing an ep nov before normalisation:  64.69551343951234
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]
 [0.632]] [[31.109]
 [31.109]
 [31.109]
 [31.109]
 [31.109]
 [31.109]
 [31.109]] [[1.524]
 [1.524]
 [1.524]
 [1.524]
 [1.524]
 [1.524]
 [1.524]]
printing an ep nov before normalisation:  26.43426211171734
actor:  1 policy actor:  1  step number:  69 total reward:  0.06666666666666599  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17941999999999986 0.684 0.684
actor:  0 policy actor:  0  step number:  41 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.18224666666666653 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.23613347790418
maxi score, test score, baseline:  0.18224666666666653 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  70 total reward:  0.16666666666666552  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18224666666666653 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17977999999999988 0.684 0.684
printing an ep nov before normalisation:  51.293992163501876
actor:  1 policy actor:  1  step number:  43 total reward:  0.4399999999999995  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17977999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.07078116288262
maxi score, test score, baseline:  0.17977999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17977999999999988 0.684 0.684
printing an ep nov before normalisation:  54.21715734335115
printing an ep nov before normalisation:  42.27203044564093
printing an ep nov before normalisation:  47.87678498272296
maxi score, test score, baseline:  0.17977999999999988 0.684 0.684
actor:  0 policy actor:  0  step number:  46 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  61 total reward:  0.34666666666666657  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  66.2810360755038
maxi score, test score, baseline:  0.18229999999999985 0.684 0.684
printing an ep nov before normalisation:  41.000267807850854
maxi score, test score, baseline:  0.18229999999999985 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.833077338591515
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.79 ]
 [0.652]
 [0.728]
 [0.427]
 [0.672]
 [0.769]] [[38.309]
 [36.1  ]
 [32.212]
 [37.466]
 [33.263]
 [32.79 ]
 [36.049]] [[0.592]
 [0.79 ]
 [0.652]
 [0.728]
 [0.427]
 [0.672]
 [0.769]]
maxi score, test score, baseline:  0.18229999999999985 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.883861167067465
printing an ep nov before normalisation:  45.38533383291067
maxi score, test score, baseline:  0.18229999999999985 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.317575629523006
actor:  0 policy actor:  0  step number:  38 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.24]
 [0.24]
 [0.24]
 [0.24]
 [0.24]
 [0.24]
 [0.24]] [[58.392]
 [58.392]
 [58.392]
 [58.392]
 [58.392]
 [58.392]
 [58.392]] [[2.008]
 [2.008]
 [2.008]
 [2.008]
 [2.008]
 [2.008]
 [2.008]]
maxi score, test score, baseline:  0.18540666666666655 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.574]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]] [[44.746]
 [48.303]
 [44.746]
 [44.746]
 [44.746]
 [44.746]
 [44.746]] [[2.203]
 [2.499]
 [2.203]
 [2.203]
 [2.203]
 [2.203]
 [2.203]]
Printing some Q and Qe and total Qs values:  [[0.953]
 [0.972]
 [0.968]
 [0.864]
 [0.864]
 [0.864]
 [0.904]] [[31.105]
 [32.062]
 [34.644]
 [26.808]
 [26.808]
 [26.808]
 [29.607]] [[0.953]
 [0.972]
 [0.968]
 [0.864]
 [0.864]
 [0.864]
 [0.904]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.972]
 [0.427]
 [0.972]
 [0.972]
 [0.972]
 [0.972]
 [0.972]] [[27.554]
 [ 0.022]
 [27.554]
 [27.554]
 [27.554]
 [27.554]
 [27.554]] [[0.972]
 [0.427]
 [0.972]
 [0.972]
 [0.972]
 [0.972]
 [0.972]]
actor:  0 policy actor:  0  step number:  58 total reward:  0.0733333333333327  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1850733333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.49 ]
 [0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]] [[37.603]
 [52.065]
 [37.603]
 [37.603]
 [37.603]
 [37.603]
 [37.603]] [[0.584]
 [0.743]
 [0.584]
 [0.584]
 [0.584]
 [0.584]
 [0.584]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  63.537610060997956
printing an ep nov before normalisation:  39.77874279022217
Printing some Q and Qe and total Qs values:  [[0.267]
 [0.57 ]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.267]
 [0.57 ]
 [0.267]
 [0.267]
 [0.267]
 [0.267]
 [0.267]]
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.655]
 [0.499]
 [0.499]
 [0.373]
 [0.499]
 [0.478]] [[38.638]
 [45.474]
 [38.638]
 [38.638]
 [46.523]
 [38.638]
 [42.814]] [[0.648]
 [0.856]
 [0.648]
 [0.648]
 [0.581]
 [0.648]
 [0.659]]
printing an ep nov before normalisation:  44.8451449791514
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.47517395019531
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.698]
 [0.524]
 [0.524]
 [0.532]
 [0.57 ]
 [0.524]] [[33.029]
 [39.779]
 [33.029]
 [33.029]
 [31.761]
 [37.418]
 [33.029]] [[0.724]
 [0.974]
 [0.724]
 [0.724]
 [0.718]
 [0.82 ]
 [0.724]]
maxi score, test score, baseline:  0.1850733333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  41 total reward:  0.47999999999999987  reward:  1.0 rdn_beta:  0.333
siam score:  -0.78544885
maxi score, test score, baseline:  0.1850733333333332 0.684 0.684
Printing some Q and Qe and total Qs values:  [[0.376]
 [0.463]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.272]] [[31.673]
 [47.044]
 [31.673]
 [31.673]
 [31.673]
 [31.673]
 [31.052]] [[0.376]
 [0.463]
 [0.376]
 [0.376]
 [0.376]
 [0.376]
 [0.272]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1850733333333332 0.684 0.684
maxi score, test score, baseline:  0.1850733333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.1733333333333329  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.791]
 [0.901]
 [0.801]
 [0.803]
 [0.803]
 [0.813]
 [0.799]] [[49.382]
 [34.739]
 [52.332]
 [52.445]
 [52.722]
 [47.23 ]
 [55.472]] [[0.791]
 [0.901]
 [0.801]
 [0.803]
 [0.803]
 [0.813]
 [0.799]]
printing an ep nov before normalisation:  54.46694216814706
maxi score, test score, baseline:  0.1850733333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  39 total reward:  0.45333333333333303  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.43699267574798
printing an ep nov before normalisation:  57.18146634699247
printing an ep nov before normalisation:  46.877905544975754
printing an ep nov before normalisation:  38.787849324808235
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.4066666666666666  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  66 total reward:  0.31333333333333246  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.201]
 [0.273]
 [0.139]
 [0.201]
 [0.132]
 [0.127]
 [0.189]] [[56.767]
 [44.419]
 [46.025]
 [56.767]
 [44.919]
 [48.407]
 [39.115]] [[0.868]
 [0.78 ]
 [0.667]
 [0.868]
 [0.646]
 [0.686]
 [0.629]]
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.14179328449607
printing an ep nov before normalisation:  43.88630444763354
printing an ep nov before normalisation:  51.91766819449338
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.70759597358317
siam score:  -0.77109635
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.64621464775215
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  66 total reward:  0.19333333333333302  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.444]
 [0.386]
 [0.357]
 [0.357]
 [0.398]
 [0.357]] [[42.469]
 [36.029]
 [40.943]
 [42.469]
 [42.469]
 [44.985]
 [42.469]] [[1.549]
 [1.327]
 [1.505]
 [1.549]
 [1.549]
 [1.711]
 [1.549]]
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.054]
 [-0.05 ]
 [ 0.569]
 [ 0.569]
 [ 0.569]
 [-0.071]
 [-0.054]] [[39.686]
 [40.934]
 [41.003]
 [41.003]
 [41.003]
 [43.418]
 [41.949]] [[0.501]
 [0.543]
 [1.163]
 [1.163]
 [1.163]
 [0.596]
 [0.569]]
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.218]
 [0.107]
 [0.107]
 [0.107]
 [0.107]
 [0.107]] [[43.471]
 [53.983]
 [43.471]
 [43.471]
 [43.471]
 [43.471]
 [43.471]] [[0.728]
 [1.14 ]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.728]]
printing an ep nov before normalisation:  56.43092808613793
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.57980251312256
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  0.26666666666666605  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.494]
 [0.402]
 [0.401]
 [0.405]
 [0.406]
 [0.399]] [[38.914]
 [42.726]
 [42.692]
 [42.866]
 [43.01 ]
 [44.331]
 [38.914]] [[1.382]
 [1.643]
 [1.55 ]
 [1.556]
 [1.567]
 [1.625]
 [1.382]]
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.522]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]] [[38.122]
 [40.961]
 [38.122]
 [38.122]
 [38.122]
 [38.122]
 [38.122]] [[1.461]
 [1.719]
 [1.461]
 [1.461]
 [1.461]
 [1.461]
 [1.461]]
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77131915
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
siam score:  -0.77185273
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.26666666666666594  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18520666666666655 0.684 0.684
printing an ep nov before normalisation:  13.232302212767832
actor:  1 policy actor:  1  step number:  53 total reward:  0.2666666666666665  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  67 total reward:  0.1466666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1818199999999999 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.53460279184538
maxi score, test score, baseline:  0.1818199999999999 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1818199999999999 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1818199999999999 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.61347123272942
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]
 [0.358]] [[59.257]
 [59.257]
 [59.257]
 [59.257]
 [59.257]
 [59.257]
 [59.257]] [[1.186]
 [1.186]
 [1.186]
 [1.186]
 [1.186]
 [1.186]
 [1.186]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.570629088047774
printing an ep nov before normalisation:  33.189159648291586
maxi score, test score, baseline:  0.1818199999999999 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1818199999999999 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1818199999999999 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.1799999999999996  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  58 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.295]
 [0.217]
 [0.295]
 [0.295]
 [0.294]
 [0.291]
 [0.295]] [[54.659]
 [65.203]
 [54.659]
 [54.659]
 [66.15 ]
 [66.115]
 [54.659]] [[1.423]
 [1.713]
 [1.423]
 [1.423]
 [1.823]
 [1.819]
 [1.423]]
maxi score, test score, baseline:  0.1818199999999999 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1818199999999999 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.561]
 [0.391]
 [0.391]
 [0.391]
 [0.391]
 [0.391]] [[36.053]
 [42.046]
 [36.053]
 [36.053]
 [36.053]
 [36.053]
 [36.053]] [[1.429]
 [1.935]
 [1.429]
 [1.429]
 [1.429]
 [1.429]
 [1.429]]
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.272]
 [0.273]
 [0.234]
 [0.207]
 [0.255]
 [0.255]] [[36.74 ]
 [33.609]
 [34.987]
 [37.125]
 [36.802]
 [35.92 ]
 [36.193]] [[1.603]
 [1.515]
 [1.624]
 [1.752]
 [1.7  ]
 [1.678]
 [1.7  ]]
siam score:  -0.7653694
maxi score, test score, baseline:  0.1818199999999999 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1818199999999999 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  54 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1808999999999999 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.687]
 [0.889]
 [0.69 ]
 [0.843]
 [0.84 ]
 [0.816]
 [0.823]] [[33.733]
 [22.384]
 [33.017]
 [31.317]
 [31.386]
 [30.872]
 [30.287]] [[2.487]
 [2.08 ]
 [2.453]
 [2.514]
 [2.515]
 [2.462]
 [2.438]]
printing an ep nov before normalisation:  46.22113090466536
maxi score, test score, baseline:  0.1808999999999999 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  49 total reward:  0.41333333333333333  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.094]
 [0.084]
 [0.092]
 [1.5  ]
 [0.093]
 [0.091]
 [0.093]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.094]
 [0.084]
 [0.092]
 [1.5  ]
 [0.093]
 [0.091]
 [0.093]]
maxi score, test score, baseline:  0.1808999999999999 0.684 0.684
Printing some Q and Qe and total Qs values:  [[-0.004]
 [-0.011]
 [-0.001]
 [-0.007]
 [-0.004]
 [-0.019]
 [-0.004]] [[32.16 ]
 [37.71 ]
 [34.25 ]
 [35.238]
 [35.097]
 [45.096]
 [32.16 ]] [[0.873]
 [1.017]
 [0.934]
 [0.954]
 [0.953]
 [1.211]
 [0.873]]
maxi score, test score, baseline:  0.1808999999999999 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.71615846016195
printing an ep nov before normalisation:  0.00020017162682961498
printing an ep nov before normalisation:  63.68845201542908
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  64.53513369243636
maxi score, test score, baseline:  0.1808999999999999 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1808999999999999 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1808999999999999 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.63258300722893
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.310510357968262
maxi score, test score, baseline:  0.1808999999999999 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.3782, 0.0318, 0.1521, 0.0956, 0.1111, 0.1286, 0.1027],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0073, 0.9431, 0.0061, 0.0064, 0.0022, 0.0039, 0.0309],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1282, 0.0225, 0.2701, 0.0850, 0.1176, 0.2616, 0.1149],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0989, 0.0312, 0.1578, 0.3652, 0.1201, 0.1430, 0.0838],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0903, 0.0383, 0.0633, 0.1458, 0.5009, 0.0873, 0.0742],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0854, 0.0398, 0.1396, 0.1005, 0.0937, 0.4553, 0.0856],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1059, 0.0988, 0.1055, 0.1090, 0.1135, 0.1273, 0.3399],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  44.925418808331294
actor:  1 policy actor:  1  step number:  63 total reward:  0.09333333333333294  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1808999999999999 0.684 0.684
printing an ep nov before normalisation:  41.78711265828391
maxi score, test score, baseline:  0.1808999999999999 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1808999999999999 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1808999999999999 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.3999, 0.0083, 0.1047, 0.1196, 0.1356, 0.1120, 0.1199],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0029, 0.9618, 0.0022, 0.0026, 0.0018, 0.0019, 0.0268],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0467, 0.0099, 0.6656, 0.0476, 0.0655, 0.0850, 0.0796],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1603, 0.0044, 0.1753, 0.1775, 0.1602, 0.1821, 0.1402],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1345, 0.0063, 0.0777, 0.0864, 0.5009, 0.0989, 0.0953],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0854, 0.0201, 0.1763, 0.0821, 0.0818, 0.4819, 0.0724],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0810, 0.1100, 0.1067, 0.1916, 0.1037, 0.1294, 0.2775],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  32.17992114533732
actor:  0 policy actor:  0  step number:  48 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1800333333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1800333333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.29363386588511
actor:  1 policy actor:  1  step number:  47 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  39.67841386795044
actor:  1 policy actor:  1  step number:  60 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  53.54985217345775
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1800333333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.21091455199369
Printing some Q and Qe and total Qs values:  [[0.771]
 [0.814]
 [0.771]
 [0.771]
 [0.771]
 [0.771]
 [0.771]] [[49.66 ]
 [50.786]
 [49.66 ]
 [49.66 ]
 [49.66 ]
 [49.66 ]
 [49.66 ]] [[1.687]
 [1.762]
 [1.687]
 [1.687]
 [1.687]
 [1.687]
 [1.687]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  46.74766422571839
line 256 mcts: sample exp_bonus 44.95594620655606
printing an ep nov before normalisation:  60.60918183180416
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1800333333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1800333333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4066666666666666  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1800333333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.1800333333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1800333333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.829]
 [0.   ]
 [0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]] [[19.968]
 [ 0.   ]
 [19.968]
 [19.968]
 [19.968]
 [19.968]
 [19.968]] [[1.387]
 [0.   ]
 [1.387]
 [1.387]
 [1.387]
 [1.387]
 [1.387]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actor:  1 policy actor:  1  step number:  63 total reward:  0.2133333333333327  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.813]
 [0.944]
 [0.897]
 [0.909]
 [0.917]
 [0.886]
 [0.843]] [[44.66 ]
 [43.909]
 [43.955]
 [42.666]
 [43.269]
 [44.709]
 [40.793]] [[0.813]
 [0.944]
 [0.897]
 [0.909]
 [0.917]
 [0.886]
 [0.843]]
actor:  0 policy actor:  0  step number:  46 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  47.62693692295612
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.2894, 0.0249, 0.1216, 0.1209, 0.1186, 0.1337, 0.1909],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0045,     0.9723,     0.0015,     0.0007,     0.0003,     0.0003,
            0.0203], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1118, 0.0037, 0.3655, 0.1150, 0.1048, 0.1920, 0.1073],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1035, 0.0242, 0.0933, 0.3869, 0.1296, 0.1312, 0.1313],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1412, 0.0033, 0.1276, 0.1420, 0.3040, 0.1516, 0.1304],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0892, 0.0058, 0.1109, 0.0931, 0.0892, 0.5322, 0.0794],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1701, 0.0129, 0.1126, 0.1051, 0.1194, 0.1263, 0.3537],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.150865220026176
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.311]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]] [[36.685]
 [42.4  ]
 [36.685]
 [36.685]
 [36.685]
 [36.685]
 [36.685]] [[0.957]
 [1.241]
 [0.957]
 [0.957]
 [0.957]
 [0.957]
 [0.957]]
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.113]
 [0.156]
 [0.127]
 [0.112]
 [0.127]
 [0.113]
 [0.112]] [[45.115]
 [52.009]
 [46.507]
 [43.801]
 [46.507]
 [44.006]
 [45.806]] [[0.612]
 [0.803]
 [0.655]
 [0.583]
 [0.655]
 [0.588]
 [0.626]]
printing an ep nov before normalisation:  59.96947168974513
siam score:  -0.7698658
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.2933333333333328  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.83376916526316
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.596]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]] [[45.773]
 [39.27 ]
 [45.773]
 [45.773]
 [45.773]
 [45.773]
 [45.773]] [[1.495]
 [1.477]
 [1.495]
 [1.495]
 [1.495]
 [1.495]
 [1.495]]
printing an ep nov before normalisation:  38.92931091655606
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.40999437115504
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.696]] [[47.244]
 [47.244]
 [47.244]
 [47.244]
 [47.244]
 [47.244]
 [47.244]] [[2.086]
 [2.086]
 [2.086]
 [2.086]
 [2.086]
 [2.086]
 [2.086]]
actor:  1 policy actor:  1  step number:  59 total reward:  0.2533333333333331  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  54 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
printing an ep nov before normalisation:  24.850597381591797
actor:  1 policy actor:  1  step number:  60 total reward:  0.273333333333333  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  62.178552586855254
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76568705
printing an ep nov before normalisation:  42.03010378996336
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.26509928226003
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
printing an ep nov before normalisation:  32.39596465160157
printing an ep nov before normalisation:  40.701680183410645
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.48652628100059
printing an ep nov before normalisation:  62.34033652227357
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.29999999999999993  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.556]
 [0.473]
 [0.475]
 [0.475]
 [0.472]
 [0.52 ]] [[45.145]
 [52.811]
 [50.087]
 [45.145]
 [45.145]
 [55.947]
 [47.903]] [[1.058]
 [1.316]
 [1.17 ]
 [1.058]
 [1.058]
 [1.304]
 [1.166]]
actor:  1 policy actor:  1  step number:  62 total reward:  0.2599999999999991  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  47.0347298044338
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.697179361962824
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17929999999999988 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.4666666666666668  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  31 total reward:  0.6000000000000001  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.1791133333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.72132041118492
Printing some Q and Qe and total Qs values:  [[ 0.347]
 [-0.089]
 [ 0.347]
 [ 0.347]
 [ 0.347]
 [ 0.457]
 [ 0.347]] [[36.933]
 [43.919]
 [36.933]
 [36.933]
 [36.933]
 [44.763]
 [36.933]] [[0.513]
 [0.14 ]
 [0.513]
 [0.513]
 [0.513]
 [0.693]
 [0.513]]
maxi score, test score, baseline:  0.1791133333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.701]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]] [[47.187]
 [44.784]
 [47.187]
 [47.187]
 [47.187]
 [47.187]
 [47.187]] [[0.911]
 [1.01 ]
 [0.911]
 [0.911]
 [0.911]
 [0.911]
 [0.911]]
maxi score, test score, baseline:  0.1791133333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1791133333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.042]
 [-0.14 ]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]
 [-0.042]] [[36.768]
 [47.784]
 [36.768]
 [36.768]
 [36.768]
 [36.768]
 [36.768]] [[0.168]
 [0.193]
 [0.168]
 [0.168]
 [0.168]
 [0.168]
 [0.168]]
siam score:  -0.77119964
printing an ep nov before normalisation:  34.85628353734027
siam score:  -0.77255607
maxi score, test score, baseline:  0.1791133333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1791133333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1791133333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1791133333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.44234659203979
maxi score, test score, baseline:  0.1791133333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1791133333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1791133333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1791133333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1791133333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.465839390175965
maxi score, test score, baseline:  0.1791133333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  41 total reward:  0.5600000000000002  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  61.649367289182905
maxi score, test score, baseline:  0.1791133333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.231]
 [0.231]
 [0.238]
 [0.231]
 [0.231]
 [0.231]] [[69.17]
 [69.17]
 [69.17]
 [69.79]
 [69.17]
 [69.17]
 [69.17]] [[1.189]
 [1.189]
 [1.189]
 [1.211]
 [1.189]
 [1.189]
 [1.189]]
actor:  0 policy actor:  0  step number:  50 total reward:  0.2999999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  36.66983238101521
actor:  1 policy actor:  1  step number:  56 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17832666666666652 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17832666666666652 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.56055396180816
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.417]
 [0.277]
 [0.305]
 [0.262]
 [0.246]
 [0.274]] [[27.876]
 [38.807]
 [26.528]
 [30.06 ]
 [32.279]
 [29.348]
 [27.723]] [[0.459]
 [0.794]
 [0.469]
 [0.55 ]
 [0.54 ]
 [0.48 ]
 [0.484]]
actions average: 
K:  2  action  0 :  tensor([0.5135, 0.0136, 0.0908, 0.0927, 0.1236, 0.0853, 0.0804],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0037,     0.9737,     0.0028,     0.0044,     0.0008,     0.0007,
            0.0140], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1182, 0.0205, 0.3951, 0.1166, 0.1152, 0.1120, 0.1225],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1142, 0.0103, 0.0849, 0.5036, 0.0869, 0.0960, 0.1041],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1049, 0.0029, 0.0768, 0.1217, 0.5042, 0.0906, 0.0990],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0965, 0.0057, 0.0930, 0.0682, 0.1005, 0.5776, 0.0585],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0545, 0.2885, 0.0761, 0.1221, 0.0552, 0.0557, 0.3479],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.17832666666666652 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.91122375296642
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.531]
 [0.601]
 [0.535]
 [0.534]
 [0.534]
 [0.554]
 [0.55 ]] [[15.502]
 [17.57 ]
 [15.719]
 [13.648]
 [13.648]
 [15.358]
 [12.807]] [[1.159]
 [1.381]
 [1.179]
 [1.026]
 [1.026]
 [1.172]
 [0.982]]
printing an ep nov before normalisation:  37.882270515273575
actor:  1 policy actor:  1  step number:  71 total reward:  0.239999999999999  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.637]
 [0.661]
 [0.589]
 [0.58 ]
 [0.501]
 [0.59 ]
 [0.623]] [[41.533]
 [36.571]
 [44.182]
 [37.015]
 [39.742]
 [42.183]
 [35.804]] [[1.156]
 [1.118]
 [1.142]
 [1.042]
 [0.998]
 [1.117]
 [1.07 ]]
maxi score, test score, baseline:  0.17832666666666652 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]] [[42.076]
 [42.076]
 [42.076]
 [42.076]
 [42.076]
 [42.076]
 [42.076]] [[28.63]
 [28.63]
 [28.63]
 [28.63]
 [28.63]
 [28.63]
 [28.63]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  34.98345546175155
printing an ep nov before normalisation:  43.57462765495268
maxi score, test score, baseline:  0.17832666666666652 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.11855166261554
maxi score, test score, baseline:  0.17832666666666652 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.72790394017612
actor:  1 policy actor:  1  step number:  45 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  58 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  46.38813347688169
maxi score, test score, baseline:  0.17832666666666652 0.684 0.684
maxi score, test score, baseline:  0.17832666666666652 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actor:  0 policy actor:  0  step number:  57 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1774733333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.21 ]
 [0.079]
 [0.335]
 [0.21 ]
 [0.21 ]
 [0.345]
 [0.21 ]] [[49.333]
 [46.375]
 [60.103]
 [49.333]
 [49.333]
 [44.306]
 [49.333]] [[1.533]
 [1.292]
 [2.058]
 [1.533]
 [1.533]
 [1.482]
 [1.533]]
printing an ep nov before normalisation:  45.149405533875196
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17408666666666656 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.307185401743205
printing an ep nov before normalisation:  44.35032550910195
printing an ep nov before normalisation:  37.65750168315698
actor:  0 policy actor:  0  step number:  56 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  39.596316991817474
actor:  0 policy actor:  0  step number:  55 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1722333333333332 0.684 0.684
maxi score, test score, baseline:  0.1722333333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.0979620054596
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
Printing some Q and Qe and total Qs values:  [[0.179]
 [0.2  ]
 [0.179]
 [0.179]
 [0.179]
 [0.179]
 [0.179]] [[39.868]
 [47.516]
 [39.868]
 [39.868]
 [39.868]
 [39.868]
 [39.868]] [[0.906]
 [1.199]
 [0.906]
 [0.906]
 [0.906]
 [0.906]
 [0.906]]
maxi score, test score, baseline:  0.1722333333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.00010919570923
maxi score, test score, baseline:  0.1722333333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.95730085087286
maxi score, test score, baseline:  0.1722333333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.06712222996997
actor:  0 policy actor:  0  step number:  41 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1717533333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.878525723784538
maxi score, test score, baseline:  0.1717533333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1717533333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1717533333333332 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.604523655968386
printing an ep nov before normalisation:  31.425066228331783
actor:  1 policy actor:  1  step number:  60 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  64.94825234962815
actor:  0 policy actor:  0  step number:  56 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 39.21929597854614
maxi score, test score, baseline:  0.17096666666666652 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.816021497869485
maxi score, test score, baseline:  0.17096666666666652 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.62 ]
 [0.736]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.588]] [[32.948]
 [51.641]
 [32.948]
 [32.948]
 [32.948]
 [32.948]
 [44.97 ]] [[0.62 ]
 [0.736]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.62 ]
 [0.588]]
maxi score, test score, baseline:  0.17096666666666652 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  40 total reward:  0.4066666666666663  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  29.662024974822998
maxi score, test score, baseline:  0.16700666666666653 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.789983163541976
Printing some Q and Qe and total Qs values:  [[0.754]
 [0.879]
 [0.818]
 [0.777]
 [0.773]
 [0.818]
 [0.867]] [[44.041]
 [57.724]
 [35.974]
 [45.064]
 [50.794]
 [35.974]
 [56.4  ]] [[0.754]
 [0.879]
 [0.818]
 [0.777]
 [0.773]
 [0.818]
 [0.867]]
printing an ep nov before normalisation:  69.5268571806242
siam score:  -0.76615417
maxi score, test score, baseline:  0.16700666666666653 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.769]
 [0.838]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]] [[33.525]
 [40.451]
 [37.675]
 [37.675]
 [37.675]
 [37.675]
 [37.675]] [[0.769]
 [0.838]
 [0.759]
 [0.759]
 [0.759]
 [0.759]
 [0.759]]
printing an ep nov before normalisation:  43.8365423182841
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.577]
 [0.603]] [[44.317]
 [44.317]
 [44.317]
 [44.317]
 [44.317]
 [34.028]
 [44.317]] [[0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.603]
 [0.577]
 [0.603]]
maxi score, test score, baseline:  0.16700666666666653 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3733333333333334  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.16361999999999985 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  48 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  64.88258099188748
printing an ep nov before normalisation:  12.734454870223999
using explorer policy with actor:  1
maxi score, test score, baseline:  0.16299333333333318 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.12559457839969
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16299333333333318 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.773]
 [0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]] [[39.353]
 [49.021]
 [39.353]
 [39.353]
 [39.353]
 [39.353]
 [39.353]] [[0.991]
 [1.306]
 [0.991]
 [0.991]
 [0.991]
 [0.991]
 [0.991]]
maxi score, test score, baseline:  0.16299333333333318 0.684 0.684
maxi score, test score, baseline:  0.16299333333333318 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16299333333333318 0.684 0.684
line 256 mcts: sample exp_bonus 38.269994097799845
actor:  1 policy actor:  1  step number:  46 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  74.77269121648214
Printing some Q and Qe and total Qs values:  [[0.251]
 [0.229]
 [0.251]
 [0.251]
 [0.251]
 [0.251]
 [0.251]] [[64.101]
 [66.474]
 [64.101]
 [64.101]
 [64.101]
 [64.101]
 [64.101]] [[1.922]
 [2.011]
 [1.922]
 [1.922]
 [1.922]
 [1.922]
 [1.922]]
actions average: 
K:  4  action  0 :  tensor([0.3884, 0.1775, 0.1132, 0.0635, 0.0681, 0.0719, 0.1175],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0275, 0.8327, 0.0248, 0.0300, 0.0063, 0.0316, 0.0471],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1111, 0.0040, 0.2217, 0.2041, 0.1190, 0.1818, 0.1583],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1085, 0.0468, 0.1614, 0.1517, 0.1387, 0.2508, 0.1420],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1493, 0.0440, 0.1188, 0.1130, 0.3614, 0.1104, 0.1032],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0821, 0.0012, 0.1068, 0.0693, 0.0686, 0.5516, 0.1203],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1241, 0.1065, 0.1361, 0.1257, 0.1235, 0.1693, 0.2147],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  47 total reward:  0.34666666666666657  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  44.77572438201416
maxi score, test score, baseline:  0.16229999999999986 0.684 0.684
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.45117960006186
actor:  1 policy actor:  1  step number:  40 total reward:  0.5  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.16229999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.16229999999999986 0.684 0.684
printing an ep nov before normalisation:  43.69205778376503
maxi score, test score, baseline:  0.16229999999999986 0.684 0.684
maxi score, test score, baseline:  0.16229999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.41 ]
 [0.452]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]
 [0.41 ]] [[40.203]
 [55.416]
 [40.203]
 [40.203]
 [40.203]
 [40.203]
 [40.203]] [[0.853]
 [1.222]
 [0.853]
 [0.853]
 [0.853]
 [0.853]
 [0.853]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.2199999999999993  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  40.66918749076724
maxi score, test score, baseline:  0.16229999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16229999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16229999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  36.8929262263831
actor:  1 policy actor:  1  step number:  54 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  35.76473420493912
printing an ep nov before normalisation:  34.78587996900457
maxi score, test score, baseline:  0.16229999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  47 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  49.41738010984449
Printing some Q and Qe and total Qs values:  [[0.457]
 [0.536]
 [0.457]
 [0.457]
 [0.447]
 [0.457]
 [0.457]] [[43.129]
 [38.279]
 [43.129]
 [43.129]
 [37.239]
 [43.129]
 [43.129]] [[0.871]
 [0.869]
 [0.871]
 [0.871]
 [0.762]
 [0.871]
 [0.871]]
maxi score, test score, baseline:  0.1617666666666665 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.79533325683365
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]
 [0.087]] [[69.578]
 [69.578]
 [69.578]
 [69.578]
 [69.578]
 [69.578]
 [69.578]] [[0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.926]]
UNIT TEST: sample policy line 217 mcts : [0.163 0.204 0.143 0.122 0.143 0.102 0.122]
printing an ep nov before normalisation:  49.063308454529505
maxi score, test score, baseline:  0.1617666666666665 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1617666666666665 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.942781985125766
printing an ep nov before normalisation:  43.840253306344806
actor:  1 policy actor:  1  step number:  65 total reward:  0.14666666666666595  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1617666666666665 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.3831, 0.0354, 0.1336, 0.1082, 0.0971, 0.1188, 0.1239],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0082, 0.9192, 0.0148, 0.0116, 0.0056, 0.0074, 0.0333],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1559, 0.0070, 0.3022, 0.1206, 0.1172, 0.1545, 0.1427],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1573, 0.0184, 0.1643, 0.1682, 0.1492, 0.1561, 0.1865],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1743, 0.0080, 0.1256, 0.1183, 0.3303, 0.1274, 0.1161],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0506, 0.1301, 0.1503, 0.0822, 0.0515, 0.4571, 0.0781],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1665, 0.1005, 0.1006, 0.0838, 0.0908, 0.0910, 0.3668],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1617666666666665 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.29239417267168
maxi score, test score, baseline:  0.1617666666666665 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4133333333333329  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.248618125915527
maxi score, test score, baseline:  0.1617666666666665 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.529082260830336
printing an ep nov before normalisation:  42.99778897675794
printing an ep nov before normalisation:  54.7018779050078
printing an ep nov before normalisation:  59.05858596250053
Printing some Q and Qe and total Qs values:  [[0.16 ]
 [0.069]
 [0.069]
 [0.069]
 [0.323]
 [0.069]
 [0.069]] [[42.91 ]
 [43.128]
 [43.128]
 [43.128]
 [37.994]
 [43.128]
 [43.128]] [[1.027]
 [0.944]
 [0.944]
 [0.944]
 [1.005]
 [0.944]
 [0.944]]
printing an ep nov before normalisation:  30.823571807673446
printing an ep nov before normalisation:  40.54887857652086
printing an ep nov before normalisation:  48.8514544920692
actor:  0 policy actor:  0  step number:  38 total reward:  0.43333333333333324  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.385213073803016
actor:  1 policy actor:  1  step number:  45 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.728]
 [0.789]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.731]] [[32.957]
 [46.327]
 [32.957]
 [32.957]
 [32.957]
 [32.957]
 [37.237]] [[0.728]
 [0.789]
 [0.728]
 [0.728]
 [0.728]
 [0.728]
 [0.731]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1612466666666665 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.313]
 [0.255]
 [0.285]
 [0.254]
 [0.285]
 [0.285]] [[32.677]
 [46.326]
 [32.039]
 [43.402]
 [32.426]
 [43.402]
 [43.402]] [[0.767]
 [1.689]
 [0.74 ]
 [1.479]
 [0.763]
 [1.479]
 [1.479]]
printing an ep nov before normalisation:  47.211385762476894
maxi score, test score, baseline:  0.1612466666666665 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.268715717543756
siam score:  -0.7702276
actor:  0 policy actor:  0  step number:  47 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  11.715061664581299
actor:  1 policy actor:  1  step number:  50 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.16069999999999984 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.30666666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.16069999999999984 0.684 0.684
maxi score, test score, baseline:  0.16069999999999984 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.0156574543078
maxi score, test score, baseline:  0.16069999999999984 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.4515, 0.0017, 0.1356, 0.1128, 0.0912, 0.0891, 0.1181],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0045,     0.9467,     0.0072,     0.0010,     0.0004,     0.0005,
            0.0398], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0702, 0.0054, 0.4741, 0.0930, 0.0771, 0.1877, 0.0925],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0889, 0.0144, 0.1159, 0.4974, 0.0768, 0.0820, 0.1245],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1208, 0.0020, 0.1076, 0.0911, 0.4896, 0.0803, 0.1086],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([    0.0419,     0.0001,     0.1301,     0.0447,     0.0401,     0.6830,
            0.0600], grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1442, 0.0420, 0.1519, 0.1609, 0.1234, 0.1197, 0.2578],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.16069999999999984 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  37 total reward:  0.52  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.04628270441105
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.53 ]
 [0.51 ]
 [0.505]
 [0.507]
 [0.505]
 [0.497]] [[55.112]
 [51.141]
 [54.291]
 [53.868]
 [53.921]
 [53.431]
 [52.868]] [[1.716]
 [1.633]
 [1.713]
 [1.695]
 [1.699]
 [1.681]
 [1.655]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.4733333333333335  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
printing an ep nov before normalisation:  46.19514244293529
Printing some Q and Qe and total Qs values:  [[0.31]
 [0.31]
 [0.31]
 [0.31]
 [0.31]
 [0.31]
 [0.31]] [[47.082]
 [47.082]
 [47.082]
 [47.082]
 [47.082]
 [47.082]
 [47.082]] [[1.949]
 [1.949]
 [1.949]
 [1.949]
 [1.949]
 [1.949]
 [1.949]]
printing an ep nov before normalisation:  64.44368737635556
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.399]
 [0.33 ]
 [0.33 ]
 [0.346]
 [0.33 ]
 [0.342]] [[37.551]
 [46.752]
 [39.613]
 [39.613]
 [35.866]
 [39.613]
 [42.846]] [[0.779]
 [1.031]
 [0.799]
 [0.799]
 [0.73 ]
 [0.799]
 [0.885]]
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.9187301644979
actor:  1 policy actor:  1  step number:  55 total reward:  0.3733333333333333  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.093]
 [0.142]
 [0.092]
 [0.093]
 [0.088]
 [0.135]
 [0.088]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.093]
 [0.142]
 [0.092]
 [0.093]
 [0.088]
 [0.135]
 [0.088]]
Printing some Q and Qe and total Qs values:  [[ 0.059]
 [ 0.172]
 [ 0.059]
 [ 0.129]
 [ 0.141]
 [-0.149]
 [ 0.059]] [[45.864]
 [43.152]
 [45.864]
 [49.532]
 [48.993]
 [51.812]
 [45.864]] [[1.573]
 [1.527]
 [1.573]
 [1.857]
 [1.838]
 [1.714]
 [1.573]]
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]] [[45.322]
 [45.322]
 [45.322]
 [45.322]
 [45.322]
 [45.322]
 [45.322]] [[2.073]
 [2.073]
 [2.073]
 [2.073]
 [2.073]
 [2.073]
 [2.073]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.34666666666666623  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.57516855690466
printing an ep nov before normalisation:  43.872591084824705
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.63744663959416
actor:  1 policy actor:  1  step number:  39 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.25999999999999923  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.490312396051
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.18086051687974
line 256 mcts: sample exp_bonus 41.361299568798074
printing an ep nov before normalisation:  45.40891863533383
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.628]
 [0.523]
 [0.499]
 [0.499]
 [0.579]
 [0.499]] [[32.038]
 [40.696]
 [34.468]
 [32.038]
 [32.038]
 [38.624]
 [32.038]] [[0.87 ]
 [1.208]
 [0.953]
 [0.87 ]
 [0.87 ]
 [1.109]
 [0.87 ]]
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([0.3486, 0.0062, 0.1167, 0.1177, 0.1585, 0.1189, 0.1336],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0110, 0.8996, 0.0062, 0.0055, 0.0031, 0.0017, 0.0729],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1266, 0.0059, 0.2980, 0.1396, 0.1216, 0.1872, 0.1211],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0972, 0.0185, 0.1024, 0.4295, 0.0921, 0.1293, 0.1310],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1199, 0.0350, 0.1454, 0.1285, 0.2957, 0.1555, 0.1201],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1104, 0.0183, 0.1630, 0.1025, 0.1122, 0.3641, 0.1295],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1226, 0.0059, 0.1968, 0.1558, 0.1103, 0.1729, 0.2358],
       grad_fn=<DivBackward0>)
actions average: 
K:  3  action  0 :  tensor([0.2597, 0.0152, 0.1347, 0.1330, 0.2050, 0.1409, 0.1115],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0055, 0.9196, 0.0027, 0.0074, 0.0048, 0.0009, 0.0589],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0484, 0.0070, 0.6985, 0.0641, 0.0628, 0.0703, 0.0488],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1359, 0.0439, 0.1437, 0.2304, 0.1299, 0.1634, 0.1528],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1115, 0.0237, 0.0642, 0.0825, 0.5653, 0.0829, 0.0699],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0412, 0.0074, 0.1237, 0.0562, 0.0510, 0.6713, 0.0493],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1812, 0.0766, 0.1353, 0.1445, 0.1160, 0.1433, 0.2031],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.3068425438166
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7719107
printing an ep nov before normalisation:  42.88516770765223
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
printing an ep nov before normalisation:  44.128217697143555
actor:  1 policy actor:  1  step number:  64 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  62 total reward:  0.01999999999999913  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]]
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 56.316653307071284
using explorer policy with actor:  1
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
printing an ep nov before normalisation:  46.17680549621582
actor:  1 policy actor:  1  step number:  65 total reward:  0.14666666666666617  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.644]
 [0.615]
 [0.609]
 [0.611]
 [0.613]
 [0.612]] [[35.006]
 [39.369]
 [34.196]
 [34.562]
 [35.145]
 [34.783]
 [35.501]] [[1.487]
 [1.814]
 [1.449]
 [1.467]
 [1.507]
 [1.485]
 [1.531]]
printing an ep nov before normalisation:  42.3671604222978
actor:  1 policy actor:  1  step number:  62 total reward:  0.40666666666666673  reward:  1.0 rdn_beta:  1.333
siam score:  -0.77237487
actor:  1 policy actor:  1  step number:  70 total reward:  0.11333333333333251  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.75859868547904
printing an ep nov before normalisation:  41.20835236488777
printing an ep nov before normalisation:  37.56701156082476
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.514]
 [0.409]
 [0.409]
 [0.409]
 [0.409]
 [0.409]] [[35.11 ]
 [42.324]
 [35.11 ]
 [35.11 ]
 [35.11 ]
 [35.11 ]
 [35.11 ]] [[0.957]
 [1.309]
 [0.957]
 [0.957]
 [0.957]
 [0.957]
 [0.957]]
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.47 ]
 [0.554]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]
 [0.47 ]] [[46.811]
 [47.797]
 [46.811]
 [46.811]
 [46.811]
 [46.811]
 [46.811]] [[1.358]
 [1.477]
 [1.358]
 [1.358]
 [1.358]
 [1.358]
 [1.358]]
actions average: 
K:  3  action  0 :  tensor([0.5006, 0.0252, 0.0843, 0.0907, 0.0897, 0.0908, 0.1185],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0031, 0.9539, 0.0053, 0.0063, 0.0027, 0.0036, 0.0251],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0985, 0.0737, 0.2502, 0.1350, 0.1038, 0.2303, 0.1085],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0524, 0.1276, 0.0968, 0.4619, 0.0615, 0.1074, 0.0923],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1282, 0.0781, 0.0569, 0.1110, 0.5184, 0.0507, 0.0566],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0872, 0.0438, 0.1713, 0.1138, 0.0932, 0.3932, 0.0976],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1120, 0.1563, 0.1109, 0.1095, 0.0773, 0.1142, 0.3199],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  43.479658035729734
using explorer policy with actor:  1
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.853600190469784
printing an ep nov before normalisation:  40.485333490513845
actor:  1 policy actor:  1  step number:  62 total reward:  0.3533333333333326  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 39.60178369251111
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.42666666666666675  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.544070202965568
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.4800000000000002  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.451]
 [0.337]
 [0.337]
 [0.339]
 [0.336]
 [0.334]] [[52.852]
 [49.702]
 [54.898]
 [53.643]
 [54.085]
 [53.78 ]
 [53.321]] [[1.884]
 [1.871]
 [1.977]
 [1.924]
 [1.945]
 [1.928]
 [1.907]]
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
printing an ep nov before normalisation:  67.63917731022495
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.551]
 [0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]] [[33.362]
 [48.584]
 [33.362]
 [33.362]
 [33.362]
 [33.362]
 [33.362]] [[1.293]
 [2.054]
 [1.293]
 [1.293]
 [1.293]
 [1.293]
 [1.293]]
Printing some Q and Qe and total Qs values:  [[0.711]
 [0.779]
 [0.714]
 [0.716]
 [0.717]
 [0.717]
 [0.716]] [[24.49 ]
 [49.776]
 [25.437]
 [33.57 ]
 [33.801]
 [33.633]
 [23.895]] [[0.711]
 [0.779]
 [0.714]
 [0.716]
 [0.717]
 [0.717]
 [0.716]]
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.618]
 [0.687]
 [0.146]
 [0.618]
 [0.642]
 [0.618]] [[39.81 ]
 [39.81 ]
 [43.743]
 [37.789]
 [39.81 ]
 [37.714]
 [39.81 ]] [[1.823]
 [1.823]
 [2.109]
 [1.24 ]
 [1.823]
 [1.732]
 [1.823]]
printing an ep nov before normalisation:  44.42024886885308
actor:  1 policy actor:  1  step number:  55 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  38.25465202331543
siam score:  -0.7661615
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.4933827059111
printing an ep nov before normalisation:  29.62828805745703
actor:  1 policy actor:  1  step number:  57 total reward:  0.42666666666666675  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16373999999999986 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.29733378491192
printing an ep nov before normalisation:  20.771713256835938
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.789]
 [0.857]
 [0.756]
 [0.775]
 [0.789]
 [0.789]
 [0.78 ]] [[28.248]
 [34.308]
 [29.132]
 [29.635]
 [28.248]
 [28.248]
 [32.989]] [[0.789]
 [0.857]
 [0.756]
 [0.775]
 [0.789]
 [0.789]
 [0.78 ]]
actor:  0 policy actor:  0  step number:  74 total reward:  0.08666666666666545  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.16312666666666653 0.684 0.684
printing an ep nov before normalisation:  43.60507067192053
using explorer policy with actor:  1
maxi score, test score, baseline:  0.16312666666666653 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  60 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1632466666666665 0.684 0.684
maxi score, test score, baseline:  0.1632466666666665 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1632466666666665 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1632466666666665 0.684 0.684
maxi score, test score, baseline:  0.1632466666666665 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1632466666666665 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.17257290342196
maxi score, test score, baseline:  0.1632466666666665 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.23759078979492
printing an ep nov before normalisation:  37.820000648498535
maxi score, test score, baseline:  0.1632466666666665 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.2670, 0.0080, 0.1152, 0.1081, 0.1176, 0.1396, 0.2446],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0118, 0.9144, 0.0098, 0.0084, 0.0028, 0.0028, 0.0499],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1022, 0.0077, 0.2611, 0.1516, 0.1550, 0.2080, 0.1144],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0888, 0.1570, 0.1045, 0.3188, 0.0944, 0.1093, 0.1273],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0513, 0.0260, 0.0261, 0.0524, 0.7586, 0.0469, 0.0387],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1002, 0.0021, 0.1074, 0.1083, 0.1163, 0.4476, 0.1183],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0591, 0.1877, 0.0903, 0.1782, 0.0653, 0.0907, 0.3286],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  25.91844254527526
maxi score, test score, baseline:  0.1632466666666665 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1632466666666665 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.701]
 [0.558]
 [0.701]
 [0.701]
 [0.701]
 [0.675]
 [0.701]] [[11.914]
 [30.07 ]
 [11.914]
 [11.914]
 [11.914]
 [12.342]
 [11.914]] [[0.701]
 [0.558]
 [0.701]
 [0.701]
 [0.701]
 [0.675]
 [0.701]]
printing an ep nov before normalisation:  13.417791499221371
actor:  1 policy actor:  1  step number:  61 total reward:  0.22666666666666657  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  40.73354923407884
maxi score, test score, baseline:  0.1632466666666665 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1632466666666665 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1632466666666665 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1632466666666665 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.10666666666666613  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  60.007836792953505
maxi score, test score, baseline:  0.1632466666666665 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.615]
 [0.506]
 [0.582]
 [0.463]
 [0.582]
 [0.582]] [[38.793]
 [38.325]
 [37.039]
 [41.552]
 [40.56 ]
 [41.552]
 [41.552]] [[1.192]
 [1.208]
 [1.069]
 [1.249]
 [1.108]
 [1.249]
 [1.249]]
actor:  0 policy actor:  0  step number:  39 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  50 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.733]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]] [[34.838]
 [64.575]
 [34.838]
 [34.838]
 [34.838]
 [34.838]
 [34.838]] [[0.594]
 [0.96 ]
 [0.594]
 [0.594]
 [0.594]
 [0.594]
 [0.594]]
actor:  0 policy actor:  0  step number:  37 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.113]
 [0.158]
 [0.113]
 [0.113]
 [0.113]
 [0.113]
 [0.113]] [[22.293]
 [39.974]
 [22.293]
 [22.293]
 [22.293]
 [22.293]
 [22.293]] [[0.307]
 [0.951]
 [0.307]
 [0.307]
 [0.307]
 [0.307]
 [0.307]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  46.28733104389029
maxi score, test score, baseline:  0.16615333333333315 0.684 0.684
Printing some Q and Qe and total Qs values:  [[0.104]
 [0.104]
 [0.053]
 [0.104]
 [0.104]
 [0.104]
 [0.104]] [[ 0.   ]
 [ 0.   ]
 [50.327]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.116]
 [-0.116]
 [ 1.266]
 [-0.116]
 [-0.116]
 [-0.116]
 [-0.116]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.22666666666666624  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.16615333333333315 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16615333333333315 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16615333333333315 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.4611663893751
maxi score, test score, baseline:  0.16615333333333315 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.618961334228516
Starting evaluation
maxi score, test score, baseline:  0.16615333333333315 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.27 ]
 [0.378]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.27 ]
 [0.378]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]
 [0.27 ]]
maxi score, test score, baseline:  0.16615333333333315 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]] [[39.628]
 [39.628]
 [39.628]
 [39.628]
 [39.628]
 [39.628]
 [39.628]] [[0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]
 [0.37]]
printing an ep nov before normalisation:  37.60467131229771
printing an ep nov before normalisation:  40.73826675711917
printing an ep nov before normalisation:  41.92833060235268
printing an ep nov before normalisation:  55.28570271891433
maxi score, test score, baseline:  0.16615333333333315 0.684 0.684
printing an ep nov before normalisation:  49.64989438140141
printing an ep nov before normalisation:  50.50081757976017
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  46.21714155601824
printing an ep nov before normalisation:  39.82440265154466
maxi score, test score, baseline:  0.16615333333333315 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16615333333333315 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.88762644162898
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.801]
 [0.951]
 [0.9  ]
 [0.895]
 [0.87 ]
 [0.9  ]] [[32.979]
 [25.415]
 [30.681]
 [28.259]
 [33.261]
 [32.027]
 [27.703]] [[0.713]
 [0.801]
 [0.951]
 [0.9  ]
 [0.895]
 [0.87 ]
 [0.9  ]]
printing an ep nov before normalisation:  0.0019366310505120055
printing an ep nov before normalisation:  5.5924827564886925
maxi score, test score, baseline:  0.16615333333333315 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  0.19172666666666655 0.684 0.684
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  1 policy actor:  1  step number:  63 total reward:  0.18666666666666598  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  52 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1949399999999999 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.42983150482178
printing an ep nov before normalisation:  40.04646779566414
maxi score, test score, baseline:  0.1949399999999999 0.6930000000000002 0.6930000000000002
actor:  1 policy actor:  1  step number:  62 total reward:  0.3533333333333335  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1949399999999999 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0017183072850457393
actor:  0 policy actor:  0  step number:  43 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  44 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  53.33646046460205
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19773999999999986 0.6930000000000002 0.6930000000000002
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.799963804104415
actor:  0 policy actor:  0  step number:  34 total reward:  0.54  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  48 total reward:  0.37999999999999967  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.19785999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.795219732040884
maxi score, test score, baseline:  0.19785999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19785999999999987 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.19785999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19785999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.78939042772566
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  69 total reward:  0.14666666666666606  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  2  action  0 :  tensor([0.3817, 0.0510, 0.1137, 0.1045, 0.1268, 0.0982, 0.1241],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0041, 0.9021, 0.0063, 0.0042, 0.0015, 0.0021, 0.0796],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0736, 0.0422, 0.4238, 0.1046, 0.0821, 0.1498, 0.1239],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0857, 0.0499, 0.1195, 0.3959, 0.1011, 0.1245, 0.1234],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1090, 0.0035, 0.0944, 0.1078, 0.4755, 0.1011, 0.1088],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0545, 0.0017, 0.1723, 0.0728, 0.0706, 0.5670, 0.0611],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1176, 0.0045, 0.1706, 0.1610, 0.1178, 0.1525, 0.2759],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  41.23880223964589
maxi score, test score, baseline:  0.19785999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  59 total reward:  0.19999999999999962  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  62.18583927799976
maxi score, test score, baseline:  0.20025999999999985 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.32474327087402
Printing some Q and Qe and total Qs values:  [[0.247]
 [0.331]
 [0.34 ]
 [0.305]
 [0.029]
 [0.36 ]
 [0.339]] [[50.914]
 [50.113]
 [48.277]
 [51.869]
 [54.453]
 [44.29 ]
 [49.89 ]] [[1.222]
 [1.275]
 [1.212]
 [1.317]
 [1.142]
 [1.077]
 [1.275]]
maxi score, test score, baseline:  0.20025999999999985 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0
Printing some Q and Qe and total Qs values:  [[0.155]
 [0.224]
 [0.143]
 [0.256]
 [0.197]
 [0.143]
 [0.152]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.155]
 [0.224]
 [0.143]
 [0.256]
 [0.197]
 [0.143]
 [0.152]]
actor:  1 policy actor:  1  step number:  57 total reward:  0.2799999999999997  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  47.90694712522415
siam score:  -0.7726283
printing an ep nov before normalisation:  35.693817138671875
maxi score, test score, baseline:  0.20025999999999985 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.20025999999999985 0.6930000000000002 0.6930000000000002
Printing some Q and Qe and total Qs values:  [[0.306]
 [0.321]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.306]
 [0.321]
 [0.306]
 [0.306]
 [0.306]
 [0.306]
 [0.306]]
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.652]
 [0.462]
 [0.466]
 [0.514]
 [0.495]
 [0.474]] [[56.95 ]
 [45.696]
 [53.4  ]
 [57.003]
 [51.976]
 [52.64 ]
 [51.52 ]] [[2.059]
 [1.755]
 [1.89 ]
 [2.047]
 [1.882]
 [1.891]
 [1.823]]
maxi score, test score, baseline:  0.20025999999999985 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.20025999999999985 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20025999999999985 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.871]
 [0.938]
 [0.871]
 [0.805]
 [0.782]
 [0.76 ]
 [0.902]] [[38.085]
 [35.863]
 [38.085]
 [41.664]
 [42.022]
 [45.712]
 [38.783]] [[0.871]
 [0.938]
 [0.871]
 [0.805]
 [0.782]
 [0.76 ]
 [0.902]]
printing an ep nov before normalisation:  45.8633323833518
actor:  0 policy actor:  0  step number:  41 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  53 total reward:  0.41333333333333333  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  44.05283055137923
maxi score, test score, baseline:  0.20308666666666653 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.25 ]
 [0.27 ]
 [0.259]
 [0.256]
 [0.257]
 [0.256]
 [0.269]] [[46.291]
 [47.499]
 [47.568]
 [48.273]
 [50.237]
 [47.393]
 [47.831]] [[0.996]
 [1.06 ]
 [1.052]
 [1.075]
 [1.148]
 [1.042]
 [1.071]]
printing an ep nov before normalisation:  48.63428545840925
printing an ep nov before normalisation:  47.38670138628006
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.464]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]] [[72.882]
 [66.793]
 [72.882]
 [72.882]
 [72.882]
 [72.882]
 [72.882]] [[1.698]
 [1.605]
 [1.698]
 [1.698]
 [1.698]
 [1.698]
 [1.698]]
actor:  1 policy actor:  1  step number:  66 total reward:  0.0066666666666659324  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20308666666666653 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.20308666666666653 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.791969304111184
maxi score, test score, baseline:  0.20308666666666653 0.6930000000000002 0.6930000000000002
actor:  1 policy actor:  1  step number:  51 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20308666666666653 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.35241735234634
maxi score, test score, baseline:  0.20308666666666653 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.20308666666666653 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20308666666666653 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20308666666666653 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20308666666666653 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.631158107109705
maxi score, test score, baseline:  0.20308666666666653 0.6930000000000002 0.6930000000000002
actor:  0 policy actor:  0  step number:  62 total reward:  0.0066666666666659324  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  54.60766477721038
printing an ep nov before normalisation:  0.0161669772978712
actor:  1 policy actor:  1  step number:  55 total reward:  0.30666666666666664  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20293999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.142]
 [0.17 ]
 [0.131]
 [0.152]
 [0.143]
 [0.129]
 [0.156]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.142]
 [0.17 ]
 [0.131]
 [0.152]
 [0.143]
 [0.129]
 [0.156]]
siam score:  -0.7800042
maxi score, test score, baseline:  0.20293999999999987 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.20293999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20293999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20293999999999987 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.20293999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.2683, 0.0432, 0.1143, 0.2321, 0.1164, 0.0966, 0.1291],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0069, 0.8999, 0.0109, 0.0169, 0.0059, 0.0260, 0.0337],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1515, 0.0396, 0.2410, 0.1519, 0.1297, 0.1330, 0.1533],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1430, 0.0634, 0.1604, 0.2544, 0.1086, 0.1210, 0.1492],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1240, 0.0190, 0.1077, 0.0739, 0.4988, 0.0884, 0.0882],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0989, 0.0170, 0.1352, 0.0904, 0.0869, 0.4474, 0.1242],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1080, 0.1558, 0.1165, 0.1061, 0.0968, 0.0978, 0.3191],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.20293999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20020666666666653 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.39552664763985
maxi score, test score, baseline:  0.20020666666666653 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.34576206925418
actor:  1 policy actor:  1  step number:  58 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.20020666666666653 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.451]
 [0.401]
 [0.402]
 [0.431]
 [0.431]
 [0.431]] [[38.913]
 [44.333]
 [39.036]
 [39.288]
 [38.913]
 [38.913]
 [38.913]] [[0.954]
 [1.118]
 [0.928]
 [0.935]
 [0.954]
 [0.954]
 [0.954]]
maxi score, test score, baseline:  0.20020666666666653 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.52 ]
 [0.459]
 [0.459]
 [0.459]
 [0.459]
 [0.509]] [[37.394]
 [47.012]
 [37.394]
 [37.394]
 [37.394]
 [37.394]
 [52.376]] [[0.777]
 [0.983]
 [0.777]
 [0.777]
 [0.777]
 [0.777]
 [1.053]]
printing an ep nov before normalisation:  65.97302984900554
maxi score, test score, baseline:  0.19743333333333318 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.5490, 0.0220, 0.0749, 0.0960, 0.0790, 0.0726, 0.1064],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0114, 0.9175, 0.0076, 0.0136, 0.0084, 0.0122, 0.0294],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0971, 0.0063, 0.5111, 0.0969, 0.0840, 0.1153, 0.0893],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1259, 0.0378, 0.1307, 0.2806, 0.1481, 0.1471, 0.1298],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0976, 0.0255, 0.0865, 0.1197, 0.4995, 0.0868, 0.0844],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1234, 0.0038, 0.1665, 0.1512, 0.1145, 0.3342, 0.1065],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0738, 0.1233, 0.0980, 0.1253, 0.0990, 0.1042, 0.3763],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.19743333333333318 0.6930000000000002 0.6930000000000002
printing an ep nov before normalisation:  43.37888389008511
maxi score, test score, baseline:  0.19743333333333318 0.6930000000000002 0.6930000000000002
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
using explorer policy with actor:  1
siam score:  -0.7857135
actor:  1 policy actor:  1  step number:  54 total reward:  0.3933333333333333  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  59.5878914808499
printing an ep nov before normalisation:  45.99611259287166
actor:  1 policy actor:  1  step number:  50 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.19743333333333318 0.6930000000000002 0.6930000000000002
printing an ep nov before normalisation:  47.172372807038954
maxi score, test score, baseline:  0.19743333333333318 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.19743333333333318 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19743333333333318 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.74210166931152
printing an ep nov before normalisation:  51.096614850449505
actor:  1 policy actor:  1  step number:  59 total reward:  0.19999999999999984  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.156]
 [0.166]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.156]
 [0.166]
 [0.156]
 [0.156]
 [0.156]
 [0.156]
 [0.156]]
Printing some Q and Qe and total Qs values:  [[0.144]
 [0.129]
 [0.113]
 [0.113]
 [0.113]
 [0.113]
 [0.124]] [[43.077]
 [43.826]
 [32.204]
 [32.204]
 [32.204]
 [32.204]
 [49.276]] [[0.776]
 [0.777]
 [0.505]
 [0.505]
 [0.505]
 [0.505]
 [0.892]]
maxi score, test score, baseline:  0.19743333333333318 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19743333333333318 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.32666666666666666  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19743333333333318 0.6930000000000002 0.6930000000000002
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19743333333333318 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  33.12193426390423
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.713]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]] [[43.803]
 [36.743]
 [43.803]
 [43.803]
 [43.803]
 [43.803]
 [43.803]] [[0.688]
 [0.713]
 [0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]]
maxi score, test score, baseline:  0.19743333333333318 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19743333333333318 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19743333333333318 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19743333333333318 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.413299560546875
printing an ep nov before normalisation:  41.21398357421422
printing an ep nov before normalisation:  48.45812621673693
actor:  0 policy actor:  0  step number:  47 total reward:  0.3333333333333328  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1948599999999999 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]
 [0.108]] [[49.584]
 [49.584]
 [49.584]
 [49.584]
 [49.584]
 [49.584]
 [49.584]] [[0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  69 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1948599999999999 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.35306476719928
maxi score, test score, baseline:  0.1948599999999999 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.2461576461792
siam score:  -0.7786829
maxi score, test score, baseline:  0.1948599999999999 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.57065542549067
maxi score, test score, baseline:  0.1948599999999999 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.0179447534493
maxi score, test score, baseline:  0.1948599999999999 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.779626
maxi score, test score, baseline:  0.1948599999999999 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.071]
 [-0.071]
 [-0.071]
 [-0.09 ]
 [-0.071]
 [-0.071]
 [-0.071]] [[29.593]
 [29.593]
 [29.593]
 [22.239]
 [29.593]
 [29.593]
 [29.593]] [[1.027]
 [1.027]
 [1.027]
 [0.577]
 [1.027]
 [1.027]
 [1.027]]
actions average: 
K:  4  action  0 :  tensor([0.4888, 0.0056, 0.0952, 0.0945, 0.1375, 0.0902, 0.0882],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0288, 0.8293, 0.0151, 0.0374, 0.0107, 0.0170, 0.0618],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1165, 0.0195, 0.3761, 0.1247, 0.1061, 0.1531, 0.1041],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0892, 0.0674, 0.1210, 0.2747, 0.1062, 0.1295, 0.2121],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1875, 0.0909, 0.1198, 0.1404, 0.1804, 0.1330, 0.1480],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1102, 0.0223, 0.1631, 0.0904, 0.1061, 0.3840, 0.1239],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1119, 0.0732, 0.1232, 0.1057, 0.1027, 0.0973, 0.3860],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1948599999999999 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1948599999999999 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.1948599999999999 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.1948599999999999 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  42 total reward:  0.4066666666666662  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.661]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]] [[32.384]
 [36.72 ]
 [32.384]
 [32.384]
 [32.384]
 [32.384]
 [32.384]] [[1.573]
 [1.997]
 [1.573]
 [1.573]
 [1.573]
 [1.573]
 [1.573]]
maxi score, test score, baseline:  0.19256666666666655 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.78192990828618
actor:  1 policy actor:  1  step number:  75 total reward:  0.1466666666666656  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.90032806227576
Printing some Q and Qe and total Qs values:  [[0.804]
 [0.851]
 [0.79 ]
 [0.791]
 [0.825]
 [0.791]
 [0.788]] [[42.151]
 [42.776]
 [40.48 ]
 [40.492]
 [42.277]
 [40.609]
 [41.041]] [[0.804]
 [0.851]
 [0.79 ]
 [0.791]
 [0.825]
 [0.791]
 [0.788]]
maxi score, test score, baseline:  0.19256666666666655 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.35882853030396
actor:  0 policy actor:  0  step number:  45 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  65.1045540131747
maxi score, test score, baseline:  0.19528666666666655 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.19528666666666655 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.11855034377053
printing an ep nov before normalisation:  46.4100391188257
actor:  1 policy actor:  1  step number:  64 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  38 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.19576666666666656 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7810615
maxi score, test score, baseline:  0.19576666666666656 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.682236785872625
siam score:  -0.77808076
maxi score, test score, baseline:  0.19576666666666656 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19576666666666656 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.19576666666666656 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19576666666666656 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.3497, 0.0079, 0.1069, 0.1263, 0.2119, 0.0952, 0.1020],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0085, 0.9576, 0.0052, 0.0030, 0.0014, 0.0011, 0.0232],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0928, 0.0192, 0.4654, 0.1060, 0.0853, 0.1397, 0.0917],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1172, 0.0089, 0.1289, 0.3448, 0.1437, 0.1336, 0.1229],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1249, 0.0089, 0.0757, 0.0869, 0.5568, 0.0755, 0.0714],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0865, 0.0040, 0.2182, 0.0976, 0.0815, 0.4298, 0.0823],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1025, 0.0999, 0.1218, 0.1226, 0.1013, 0.1110, 0.3408],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.57 ]
 [0.354]
 [0.478]
 [0.429]
 [0.478]
 [0.43 ]] [[38.918]
 [44.54 ]
 [45.174]
 [38.918]
 [47.205]
 [38.918]
 [47.544]] [[0.66 ]
 [0.815]
 [0.605]
 [0.66 ]
 [0.702]
 [0.66 ]
 [0.707]]
actions average: 
K:  1  action  0 :  tensor([0.3479, 0.0172, 0.1289, 0.1427, 0.1445, 0.1037, 0.1151],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0134, 0.9135, 0.0070, 0.0179, 0.0028, 0.0020, 0.0434],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0837, 0.0025, 0.5509, 0.0827, 0.0702, 0.1227, 0.0873],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1478, 0.0084, 0.1855, 0.1859, 0.1297, 0.1695, 0.1733],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1072, 0.0207, 0.0799, 0.0910, 0.5412, 0.0666, 0.0933],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0658, 0.0018, 0.1368, 0.0855, 0.0693, 0.5618, 0.0790],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1422, 0.0735, 0.0915, 0.0971, 0.0885, 0.0706, 0.4366],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  49 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  43.8608964284261
maxi score, test score, baseline:  0.19576666666666656 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19576666666666656 0.6930000000000002 0.6930000000000002
actor:  0 policy actor:  0  step number:  33 total reward:  0.5333333333333333  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  73 total reward:  0.09333333333333249  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.214]
 [0.351]
 [0.265]
 [0.141]
 [0.172]
 [0.193]
 [0.141]] [[59.322]
 [55.393]
 [56.952]
 [53.123]
 [60.388]
 [60.319]
 [53.123]] [[1.05 ]
 [1.076]
 [1.034]
 [0.801]
 [1.039]
 [1.058]
 [0.801]]
printing an ep nov before normalisation:  41.05356520315485
maxi score, test score, baseline:  0.1962733333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.62638566464689
actor:  1 policy actor:  1  step number:  38 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1962733333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1962733333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1962733333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1962733333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1933533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1933533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.62959383774113
printing an ep nov before normalisation:  49.47425639292601
actor:  1 policy actor:  1  step number:  68 total reward:  0.1933333333333329  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1933533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1933533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.272]
 [0.373]
 [0.272]
 [0.272]
 [0.287]
 [0.272]
 [0.303]] [[43.874]
 [40.751]
 [43.874]
 [43.874]
 [50.647]
 [43.874]
 [47.665]] [[1.242]
 [1.188]
 [1.242]
 [1.242]
 [1.593]
 [1.242]
 [1.462]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.45333333333333325  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1933533333333332 0.6930000000000002 0.6930000000000002
printing an ep nov before normalisation:  60.17018030085521
printing an ep nov before normalisation:  46.918197840843725
maxi score, test score, baseline:  0.1933533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  39.133567215318635
maxi score, test score, baseline:  0.1933533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  0.48666666666666647  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  60 total reward:  0.23333333333333273  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1933533333333332 0.6930000000000002 0.6930000000000002
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1933533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.36480140686035
printing an ep nov before normalisation:  67.37018430286832
maxi score, test score, baseline:  0.1933533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.1933533333333332 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.1933533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1933533333333332 0.6930000000000002 0.6930000000000002
Printing some Q and Qe and total Qs values:  [[0.704]
 [0.845]
 [0.705]
 [0.788]
 [0.708]
 [0.804]
 [0.712]] [[37.005]
 [46.397]
 [39.72 ]
 [46.417]
 [40.025]
 [44.463]
 [39.346]] [[0.861]
 [1.081]
 [0.884]
 [1.025]
 [0.89 ]
 [1.024]
 [0.888]]
actor:  0 policy actor:  0  step number:  39 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.82483410079324
maxi score, test score, baseline:  0.1932733333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1932733333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3199999999999993  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.5  ]
 [1.5  ]
 [0.109]
 [1.5  ]
 [1.5  ]
 [1.5  ]
 [1.5  ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.5  ]
 [1.5  ]
 [0.109]
 [1.5  ]
 [1.5  ]
 [1.5  ]
 [1.5  ]]
maxi score, test score, baseline:  0.1932733333333332 0.6930000000000002 0.6930000000000002
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.406]
 [0.406]
 [0.259]
 [0.256]
 [0.232]
 [0.293]] [[47.942]
 [49.869]
 [49.869]
 [52.736]
 [51.613]
 [54.724]
 [49.908]] [[1.503]
 [1.697]
 [1.697]
 [1.689]
 [1.632]
 [1.76 ]
 [1.585]]
printing an ep nov before normalisation:  26.2950362339516
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.255]
 [0.382]
 [0.382]] [[49.893]
 [49.893]
 [49.893]
 [49.893]
 [54.316]
 [49.893]
 [49.893]] [[1.751]
 [1.751]
 [1.751]
 [1.751]
 [1.816]
 [1.751]
 [1.751]]
maxi score, test score, baseline:  0.1932733333333332 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.1932733333333332 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.1932733333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1932733333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  70 total reward:  0.28666666666666574  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1932733333333332 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.1932733333333332 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.1932733333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1932733333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.416546138907734
maxi score, test score, baseline:  0.1932733333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.061 0.286 0.204 0.102 0.122 0.163 0.061]
printing an ep nov before normalisation:  46.73338183807165
maxi score, test score, baseline:  0.19060666666666654 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.86478338445504
actions average: 
K:  0  action  0 :  tensor([0.3841, 0.0125, 0.1299, 0.0987, 0.1067, 0.1233, 0.1447],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0048,     0.9401,     0.0071,     0.0049,     0.0005,     0.0018,
            0.0408], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0921, 0.0080, 0.5086, 0.0877, 0.0718, 0.1250, 0.1067],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1565, 0.0017, 0.1536, 0.2646, 0.1272, 0.1439, 0.1526],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1218, 0.0011, 0.1100, 0.0861, 0.4372, 0.0966, 0.1472],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1053, 0.0018, 0.1421, 0.0900, 0.0730, 0.4702, 0.1176],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1012, 0.0968, 0.1340, 0.1141, 0.0636, 0.0856, 0.4047],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.494]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]] [[34.338]
 [52.76 ]
 [34.338]
 [34.338]
 [34.338]
 [34.338]
 [34.338]] [[0.437]
 [0.494]
 [0.437]
 [0.437]
 [0.437]
 [0.437]
 [0.437]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.5000000000000002  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  47.77420117002104
maxi score, test score, baseline:  0.19060666666666654 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19060666666666654 0.6930000000000002 0.6930000000000002
siam score:  -0.775553
maxi score, test score, baseline:  0.19060666666666654 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.19060666666666654 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.272422313690186
siam score:  -0.77469176
actor:  0 policy actor:  0  step number:  33 total reward:  0.5466666666666666  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  43.20215559784561
maxi score, test score, baseline:  0.19369999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.41938350726072
maxi score, test score, baseline:  0.19369999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.923155766266476
maxi score, test score, baseline:  0.19369999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0
printing an ep nov before normalisation:  0.0
Printing some Q and Qe and total Qs values:  [[0.521]
 [0.572]
 [0.561]
 [0.521]
 [0.52 ]
 [0.531]
 [0.52 ]] [[32.239]
 [47.216]
 [47.174]
 [32.583]
 [32.713]
 [36.979]
 [32.016]] [[0.751]
 [1.089]
 [1.077]
 [0.758]
 [0.759]
 [0.852]
 [0.746]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  57.702875710766484
maxi score, test score, baseline:  0.19369999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  45.15919147840087
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19369999999999987 0.6930000000000002 0.6930000000000002
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.659]
 [0.566]
 [0.566]
 [0.566]
 [0.566]
 [0.566]] [[40.284]
 [43.934]
 [40.284]
 [40.284]
 [40.284]
 [40.284]
 [40.284]] [[1.621]
 [1.914]
 [1.621]
 [1.621]
 [1.621]
 [1.621]
 [1.621]]
line 256 mcts: sample exp_bonus 46.04157174827222
printing an ep nov before normalisation:  43.43271751915928
maxi score, test score, baseline:  0.19369999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.72308141590606
printing an ep nov before normalisation:  46.728745543109184
maxi score, test score, baseline:  0.19369999999999987 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.19369999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.32666666666666655  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  39.594815067775535
maxi score, test score, baseline:  0.19369999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19369999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.4266666666666663  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.19369999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19369999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7639237
line 256 mcts: sample exp_bonus 0.028927985608157385
Printing some Q and Qe and total Qs values:  [[0.095]
 [0.103]
 [0.098]
 [0.096]
 [0.098]
 [0.1  ]
 [0.084]] [[54.249]
 [48.184]
 [53.421]
 [53.12 ]
 [53.096]
 [53.189]
 [52.728]] [[1.736]
 [1.447]
 [1.699]
 [1.682]
 [1.682]
 [1.69 ]
 [1.65 ]]
printing an ep nov before normalisation:  59.85350011333379
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  0.52  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  60.59521929739695
printing an ep nov before normalisation:  49.521379836924844
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]] [[37.535]
 [37.535]
 [37.535]
 [37.535]
 [37.535]
 [37.535]
 [37.535]] [[1.394]
 [1.394]
 [1.394]
 [1.394]
 [1.394]
 [1.394]
 [1.394]]
actor:  0 policy actor:  0  step number:  39 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  34.007625579833984
printing an ep nov before normalisation:  32.308497799271855
actor:  1 policy actor:  1  step number:  51 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  40.99957859879394
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19404666666666653 0.6930000000000002 0.6930000000000002
actor:  1 policy actor:  1  step number:  59 total reward:  0.22666666666666657  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.19404666666666653 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  46 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  12.800292144694925
printing an ep nov before normalisation:  18.841941715953013
maxi score, test score, baseline:  0.19328666666666655 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19328666666666655 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
siam score:  -0.7677277
maxi score, test score, baseline:  0.19328666666666655 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.19328666666666655 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19328666666666655 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.649]
 [0.604]
 [0.603]
 [0.715]
 [0.605]
 [0.634]] [[42.809]
 [43.714]
 [41.928]
 [41.821]
 [41.332]
 [41.935]
 [42.017]] [[1.42 ]
 [1.502]
 [1.39 ]
 [1.385]
 [1.479]
 [1.391]
 [1.423]]
maxi score, test score, baseline:  0.19328666666666655 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19328666666666655 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19328666666666655 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.19328666666666655 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.02075615655049
maxi score, test score, baseline:  0.18989999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.29133098315089
maxi score, test score, baseline:  0.18989999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  0.10666666666666613  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  61.646929212002135
printing an ep nov before normalisation:  35.79026142489265
Printing some Q and Qe and total Qs values:  [[0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]
 [0.264]] [[56.763]
 [56.763]
 [56.763]
 [56.763]
 [56.763]
 [56.763]
 [56.763]] [[1.646]
 [1.646]
 [1.646]
 [1.646]
 [1.646]
 [1.646]
 [1.646]]
line 256 mcts: sample exp_bonus 34.52812188863754
Printing some Q and Qe and total Qs values:  [[0.821]
 [0.77 ]
 [0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]] [[39.237]
 [45.99 ]
 [39.237]
 [39.237]
 [39.237]
 [39.237]
 [39.237]] [[1.419]
 [1.561]
 [1.419]
 [1.419]
 [1.419]
 [1.419]
 [1.419]]
printing an ep nov before normalisation:  51.25582548384518
printing an ep nov before normalisation:  49.89418290805531
maxi score, test score, baseline:  0.18989999999999987 0.6930000000000002 0.6930000000000002
actor:  1 policy actor:  1  step number:  42 total reward:  0.5133333333333335  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  42 total reward:  0.646666666666667  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.833]
 [0.846]
 [0.846]
 [0.846]
 [0.906]
 [0.846]
 [0.846]] [[62.517]
 [56.832]
 [56.832]
 [56.832]
 [65.476]
 [56.832]
 [56.832]] [[0.833]
 [0.846]
 [0.846]
 [0.846]
 [0.906]
 [0.846]
 [0.846]]
maxi score, test score, baseline:  0.18989999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18989999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.3266666666666659  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18989999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18989999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18989999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18989999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18989999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.769727566361084
siam score:  -0.7727392
maxi score, test score, baseline:  0.18989999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.914330590790364
actor:  0 policy actor:  0  step number:  50 total reward:  0.2599999999999997  reward:  1.0 rdn_beta:  0.667
siam score:  -0.77079946
printing an ep nov before normalisation:  38.95489493815319
siam score:  -0.77080643
maxi score, test score, baseline:  0.18903333333333322 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18903333333333322 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18903333333333322 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18903333333333322 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18903333333333322 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.33333333333333326  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.18903333333333322 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.18903333333333322 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18903333333333322 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.219390670380506
actor:  0 policy actor:  0  step number:  54 total reward:  0.1799999999999996  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  48 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  36.67899247079774
maxi score, test score, baseline:  0.18800666666666652 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18800666666666652 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  36 total reward:  0.5  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  38 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  0.667
siam score:  -0.76520187
Printing some Q and Qe and total Qs values:  [[0.193]
 [0.098]
 [0.098]
 [0.138]
 [0.111]
 [0.126]
 [0.173]] [[50.755]
 [48.171]
 [48.171]
 [53.825]
 [56.333]
 [52.011]
 [49.915]] [[1.493]
 [1.272]
 [1.272]
 [1.588]
 [1.684]
 [1.487]
 [1.432]]
printing an ep nov before normalisation:  62.32913753981597
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  64.84169397896328
maxi score, test score, baseline:  0.18761999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.81394251283852
printing an ep nov before normalisation:  25.360307693481445
maxi score, test score, baseline:  0.18761999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18761999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.867286682128906
printing an ep nov before normalisation:  53.89127932223882
maxi score, test score, baseline:  0.18761999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18761999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18761999999999987 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.18761999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.39031088185867
printing an ep nov before normalisation:  42.730327362157254
printing an ep nov before normalisation:  41.72041893005371
maxi score, test score, baseline:  0.18761999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  68 total reward:  0.11333333333333284  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18761999999999987 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.18761999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actor:  1 policy actor:  1  step number:  64 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  2  action  0 :  tensor([0.4170, 0.0571, 0.0783, 0.0951, 0.1549, 0.0476, 0.1501],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0136, 0.9245, 0.0069, 0.0095, 0.0043, 0.0027, 0.0386],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0802, 0.0032, 0.4459, 0.1002, 0.0907, 0.1710, 0.1089],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1493, 0.0237, 0.1428, 0.1923, 0.1623, 0.1426, 0.1868],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.1392,     0.0006,     0.0540,     0.0668,     0.6228,     0.0521,
            0.0645], grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0831, 0.0018, 0.1118, 0.0883, 0.0886, 0.5268, 0.0997],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1208, 0.0778, 0.1597, 0.1636, 0.1419, 0.1694, 0.1668],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.18761999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.317]
 [0.434]
 [0.317]
 [0.317]
 [0.317]
 [0.317]
 [0.317]] [[35.041]
 [47.037]
 [35.041]
 [35.041]
 [35.041]
 [35.041]
 [35.041]] [[0.908]
 [1.44 ]
 [0.908]
 [0.908]
 [0.908]
 [0.908]
 [0.908]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  44.96897973673831
maxi score, test score, baseline:  0.18761999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.04188302716472
printing an ep nov before normalisation:  36.58391832025194
actions average: 
K:  1  action  0 :  tensor([0.3569, 0.0498, 0.1383, 0.0932, 0.0819, 0.0766, 0.2034],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0042,     0.9646,     0.0024,     0.0018,     0.0008,     0.0008,
            0.0254], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0694, 0.0154, 0.5127, 0.0845, 0.0754, 0.1286, 0.1140],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1153, 0.0239, 0.1552, 0.2530, 0.1483, 0.1178, 0.1865],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1102, 0.0319, 0.0982, 0.1326, 0.4337, 0.0718, 0.1215],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0602, 0.0050, 0.1864, 0.0625, 0.0553, 0.5388, 0.0918],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1423, 0.0321, 0.1322, 0.1454, 0.1254, 0.1172, 0.3054],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.18761999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.242]
 [-0.24 ]
 [-0.229]
 [-0.232]
 [-0.232]
 [-0.23 ]
 [-0.236]] [[6.172]
 [3.797]
 [8.146]
 [7.884]
 [5.263]
 [6.206]
 [9.986]] [[-0.152]
 [-0.185]
 [-0.11 ]
 [-0.117]
 [-0.155]
 [-0.139]
 [-0.09 ]]
printing an ep nov before normalisation:  49.92366313934326
actor:  0 policy actor:  0  step number:  39 total reward:  0.4399999999999997  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  42 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1871133333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.04812833877731
printing an ep nov before normalisation:  53.45484242398452
Printing some Q and Qe and total Qs values:  [[0.943]
 [0.99 ]
 [0.943]
 [0.943]
 [0.943]
 [0.943]
 [0.943]] [[62.021]
 [65.89 ]
 [62.021]
 [62.021]
 [62.021]
 [62.021]
 [62.021]] [[0.943]
 [0.99 ]
 [0.943]
 [0.943]
 [0.943]
 [0.943]
 [0.943]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.1871133333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.004551343408820685
actor:  1 policy actor:  1  step number:  44 total reward:  0.4600000000000001  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  28.79044153561913
printing an ep nov before normalisation:  45.1319617600379
actor:  0 policy actor:  0  step number:  52 total reward:  0.4866666666666668  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18669999999999987 0.6930000000000002 0.6930000000000002
actor:  1 policy actor:  1  step number:  53 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18669999999999987 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.18669999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18669999999999987 0.6930000000000002 0.6930000000000002
Printing some Q and Qe and total Qs values:  [[0.532]
 [0.584]
 [0.585]
 [0.601]
 [0.422]
 [0.496]
 [0.591]] [[37.835]
 [52.896]
 [53.585]
 [50.304]
 [47.621]
 [48.578]
 [52.578]] [[0.769]
 [1.091]
 [1.105]
 [1.062]
 [0.835]
 [0.926]
 [1.093]]
actions average: 
K:  3  action  0 :  tensor([0.1867, 0.0033, 0.1446, 0.1194, 0.2879, 0.1194, 0.1387],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0057, 0.9631, 0.0101, 0.0040, 0.0026, 0.0026, 0.0119],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0555, 0.0284, 0.5959, 0.0825, 0.0515, 0.1092, 0.0770],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1214, 0.0714, 0.1404, 0.2765, 0.1091, 0.1326, 0.1486],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1220, 0.0877, 0.0796, 0.1113, 0.4455, 0.0655, 0.0885],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0674, 0.2024, 0.1462, 0.0818, 0.0647, 0.3646, 0.0730],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1223, 0.0471, 0.1261, 0.1220, 0.1644, 0.1074, 0.3108],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  60.79033888655693
printing an ep nov before normalisation:  56.62467249941022
maxi score, test score, baseline:  0.18669999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18669999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  2  action  0 :  tensor([0.3362, 0.0923, 0.1299, 0.0966, 0.1395, 0.1020, 0.1035],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0103, 0.9292, 0.0105, 0.0072, 0.0071, 0.0057, 0.0300],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0697, 0.0034, 0.5585, 0.0985, 0.0736, 0.1119, 0.0843],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1300, 0.0417, 0.1561, 0.1879, 0.1390, 0.1786, 0.1668],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0797, 0.0935, 0.0956, 0.1250, 0.3955, 0.1073, 0.1035],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.1086, 0.0093, 0.1424, 0.1378, 0.1057, 0.3566, 0.1396],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1033, 0.0069, 0.1248, 0.2342, 0.1116, 0.1315, 0.2878],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.82541890167798
Printing some Q and Qe and total Qs values:  [[0.356]
 [0.4  ]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.356]
 [0.4  ]
 [0.356]
 [0.356]
 [0.356]
 [0.356]
 [0.356]]
maxi score, test score, baseline:  0.18669999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.573]
 [0.657]
 [0.613]
 [0.611]
 [0.573]
 [0.587]
 [0.573]] [[45.331]
 [45.788]
 [51.162]
 [50.335]
 [45.331]
 [51.476]
 [45.331]] [[1.821]
 [1.929]
 [2.172]
 [2.126]
 [1.821]
 [2.163]
 [1.821]]
printing an ep nov before normalisation:  3.352441956013763
Printing some Q and Qe and total Qs values:  [[ 0.056]
 [ 0.039]
 [ 0.059]
 [ 0.061]
 [ 0.05 ]
 [ 0.041]
 [-0.008]] [[52.098]
 [50.732]
 [52.663]
 [52.884]
 [52.277]
 [51.412]
 [48.013]] [[0.321]
 [0.291]
 [0.329]
 [0.334]
 [0.317]
 [0.3  ]
 [0.218]]
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.7  ]
 [0.627]
 [0.623]
 [0.62 ]
 [0.622]
 [0.699]] [[45.014]
 [42.329]
 [41.355]
 [42.198]
 [43.014]
 [43.166]
 [45.014]] [[0.699]
 [0.7  ]
 [0.627]
 [0.623]
 [0.62 ]
 [0.622]
 [0.699]]
maxi score, test score, baseline:  0.18669999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.55732543459783
maxi score, test score, baseline:  0.18669999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18669999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  51 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.0
siam score:  -0.775813
siam score:  -0.7758192
maxi score, test score, baseline:  0.18603333333333322 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.605184026136044
maxi score, test score, baseline:  0.18603333333333322 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  0.13999999999999924  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  73 total reward:  0.03999999999999948  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18603333333333322 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18603333333333322 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.5461720241747
printing an ep nov before normalisation:  41.45878498478349
maxi score, test score, baseline:  0.18603333333333322 0.6930000000000002 0.6930000000000002
printing an ep nov before normalisation:  53.250239076333
printing an ep nov before normalisation:  19.689487733356163
printing an ep nov before normalisation:  40.15130125061042
line 256 mcts: sample exp_bonus 41.324907243841245
Printing some Q and Qe and total Qs values:  [[-0.094]
 [-0.119]
 [-0.094]
 [-0.094]
 [-0.094]
 [-0.094]
 [-0.094]] [[11.326]
 [27.445]
 [11.326]
 [11.326]
 [11.326]
 [11.326]
 [11.326]] [[0.254]
 [0.725]
 [0.254]
 [0.254]
 [0.254]
 [0.254]
 [0.254]]
actions average: 
K:  3  action  0 :  tensor([0.2080, 0.0011, 0.1233, 0.1049, 0.3207, 0.1162, 0.1258],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0102, 0.9283, 0.0065, 0.0158, 0.0125, 0.0051, 0.0217],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0736, 0.0025, 0.4466, 0.1745, 0.0741, 0.0929, 0.1358],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1005, 0.1011, 0.1326, 0.1594, 0.1116, 0.2352, 0.1596],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1450, 0.0522, 0.0743, 0.0918, 0.4002, 0.0796, 0.1569],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0793, 0.0049, 0.1486, 0.0976, 0.0989, 0.4672, 0.1036],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0677, 0.2686, 0.0670, 0.1197, 0.0695, 0.0662, 0.3413],
       grad_fn=<DivBackward0>)
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.5554],
        [ 0.1193],
        [ 0.0918],
        [ 0.0000],
        [ 0.7291],
        [-0.3674],
        [ 0.0930],
        [ 0.0000],
        [ 0.0000],
        [ 0.7291]], dtype=torch.float64)
-0.09703970119800001 0.45840570195071706
-0.045026434398 0.07429603343455318
-0.032346567066 0.0595010188029585
-0.5543999999999998 -0.5543999999999998
-0.032346567066 0.6967838853651782
-0.032346567066 -0.39972743961696455
-0.032346567066 0.0606480342637575
0.9734999999999999 0.9734999999999999
0.9119154164999999 0.9119154164999999
-0.045546567066 0.6835838853651782
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.122 0.224 0.224 0.061 0.041 0.184 0.143]
printing an ep nov before normalisation:  11.362568727225886
printing an ep nov before normalisation:  70.19425975001222
maxi score, test score, baseline:  0.18264666666666657 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.18264666666666657 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.27892115573956
actor:  1 policy actor:  1  step number:  56 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  30.836052894592285
siam score:  -0.7750337
Printing some Q and Qe and total Qs values:  [[0.83 ]
 [0.874]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]] [[46.056]
 [43.333]
 [46.056]
 [46.056]
 [46.056]
 [46.056]
 [46.056]] [[0.83 ]
 [0.874]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]
 [0.83 ]]
line 256 mcts: sample exp_bonus 35.91241286047199
printing an ep nov before normalisation:  57.53006035152183
printing an ep nov before normalisation:  37.11890775658193
line 256 mcts: sample exp_bonus 35.34179567782788
maxi score, test score, baseline:  0.18264666666666657 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.60423421859741
printing an ep nov before normalisation:  33.83678197860718
printing an ep nov before normalisation:  44.382101083368674
actor:  0 policy actor:  0  step number:  43 total reward:  0.3599999999999999  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  52.90486109273372
maxi score, test score, baseline:  0.18197999999999986 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.9165590540307
printing an ep nov before normalisation:  0.00409054019883115
maxi score, test score, baseline:  0.18197999999999986 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  0.2133333333333326  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18197999999999986 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.97653007507324
maxi score, test score, baseline:  0.18197999999999986 0.6930000000000002 0.6930000000000002
actor:  0 policy actor:  0  step number:  50 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  53.08205222087257
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.2895],
        [0.0000],
        [0.0960],
        [0.5219],
        [0.1640],
        [0.0000],
        [0.2274],
        [0.0889],
        [0.0920],
        [0.2137]], dtype=torch.float64)
-0.09703970119800001 0.19248340006879686
-0.8605284599999999 -0.8605284599999999
-0.032346567066 0.0636689207103732
-0.070771701198 0.4511669577138868
-0.045026434398 0.11896392101467354
-0.38939999999999964 -0.38939999999999964
-0.032346567066 0.19509735348619225
-0.032346567066 0.056601343742602124
-0.032346567066 0.05969468924151016
-0.032346567066 0.1813992302141159
Printing some Q and Qe and total Qs values:  [[0.96]
 [0.96]
 [0.96]
 [0.96]
 [0.96]
 [0.96]
 [0.96]] [[38.417]
 [38.417]
 [38.417]
 [38.417]
 [38.417]
 [38.417]
 [38.417]] [[0.96]
 [0.96]
 [0.96]
 [0.96]
 [0.96]
 [0.96]
 [0.96]]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  68 total reward:  0.11333333333333262  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1801133333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.03505470476563
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  34.53874141062813
maxi score, test score, baseline:  0.1801133333333332 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.1801133333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  41 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  50.324830270430645
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.299795181600565
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.614036997805755
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.74978256028297
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.664]
 [0.619]
 [0.619]
 [0.623]
 [0.619]
 [0.619]] [[53.631]
 [53.356]
 [53.631]
 [53.631]
 [49.449]
 [53.631]
 [53.631]] [[2.619]
 [2.648]
 [2.619]
 [2.619]
 [2.376]
 [2.619]
 [2.619]]
printing an ep nov before normalisation:  34.29798354958737
Printing some Q and Qe and total Qs values:  [[ 0.249]
 [-0.028]
 [ 0.219]
 [ 0.208]
 [ 0.194]
 [ 0.205]
 [ 0.226]] [[30.053]
 [35.533]
 [33.444]
 [32.843]
 [32.671]
 [32.912]
 [31.429]] [[1.248]
 [1.154]
 [1.332]
 [1.3  ]
 [1.28 ]
 [1.299]
 [1.271]]
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
actor:  1 policy actor:  1  step number:  61 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.719]
 [0.698]
 [0.664]
 [0.687]
 [0.638]
 [0.694]] [[40.29 ]
 [37.803]
 [37.659]
 [38.615]
 [38.032]
 [39.321]
 [37.9  ]] [[1.537]
 [1.511]
 [1.483]
 [1.492]
 [1.489]
 [1.497]
 [1.49 ]]
printing an ep nov before normalisation:  42.256501726553466
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.297584213846385
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  1.0
siam score:  -0.7685667
printing an ep nov before normalisation:  45.792066020019654
printing an ep nov before normalisation:  46.16866997196723
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.224]
 [0.229]
 [0.182]
 [0.203]
 [0.167]
 [0.176]
 [0.2  ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.224]
 [0.229]
 [0.182]
 [0.203]
 [0.167]
 [0.176]
 [0.2  ]]
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
printing an ep nov before normalisation:  27.27315902709961
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.491196430327825
actions average: 
K:  0  action  0 :  tensor([0.4087, 0.0013, 0.1054, 0.1392, 0.0849, 0.1322, 0.1283],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0043,     0.9697,     0.0043,     0.0026,     0.0009,     0.0011,
            0.0170], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0971, 0.0010, 0.2762, 0.1232, 0.0942, 0.3052, 0.1031],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1136, 0.0005, 0.1341, 0.3466, 0.1005, 0.1619, 0.1427],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0577,     0.0006,     0.0609,     0.0980,     0.6480,     0.0669,
            0.0679], grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0815, 0.0009, 0.1277, 0.1038, 0.0651, 0.5214, 0.0996],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1149, 0.0621, 0.1312, 0.1507, 0.0983, 0.1342, 0.3086],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.078]
 [-0.078]
 [-0.072]
 [-0.078]
 [-0.078]
 [-0.073]
 [-0.078]] [[24.926]
 [24.926]
 [24.938]
 [24.926]
 [24.474]
 [24.907]
 [24.926]] [[0.366]
 [0.366]
 [0.372]
 [0.366]
 [0.358]
 [0.37 ]
 [0.366]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.30666666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3066666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.18827492850168
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.4873, 0.0442, 0.0831, 0.0641, 0.1414, 0.0720, 0.1080],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0050, 0.9552, 0.0042, 0.0037, 0.0019, 0.0024, 0.0277],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1264, 0.0137, 0.3263, 0.0956, 0.1053, 0.2262, 0.1065],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1142, 0.0489, 0.0900, 0.3330, 0.1422, 0.1278, 0.1440],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1421, 0.0061, 0.1254, 0.0871, 0.4062, 0.1148, 0.1183],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0365, 0.0019, 0.1494, 0.0295, 0.0405, 0.7042, 0.0381],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1052, 0.1097, 0.1489, 0.1500, 0.1002, 0.1489, 0.2371],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.12563437598326
Printing some Q and Qe and total Qs values:  [[0.728]
 [0.848]
 [0.737]
 [0.74 ]
 [0.712]
 [0.721]
 [0.752]] [[33.574]
 [35.716]
 [34.021]
 [33.779]
 [33.621]
 [36.607]
 [33.5  ]] [[2.235]
 [2.451]
 [2.264]
 [2.256]
 [2.221]
 [2.364]
 [2.256]]
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.387]
 [0.799]
 [0.58 ]
 [0.66 ]
 [0.691]
 [0.755]
 [0.601]] [[28.931]
 [24.774]
 [29.314]
 [27.551]
 [28.035]
 [27.474]
 [29.936]] [[1.695]
 [1.919]
 [1.906]
 [1.906]
 [1.958]
 [1.997]
 [1.954]]
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.499]
 [0.458]
 [0.458]
 [0.458]
 [0.458]
 [0.458]] [[39.756]
 [51.009]
 [39.756]
 [39.756]
 [39.756]
 [39.756]
 [39.756]] [[0.777]
 [0.976]
 [0.777]
 [0.777]
 [0.777]
 [0.777]
 [0.777]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.21999999999999975  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  30.960664872470517
printing an ep nov before normalisation:  20.679161163665714
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.16999924072877
Printing some Q and Qe and total Qs values:  [[-0.115]
 [-0.111]
 [-0.113]
 [-0.116]
 [-0.114]
 [-0.113]
 [-0.112]] [[13.768]
 [16.099]
 [12.915]
 [12.65 ]
 [12.838]
 [13.04 ]
 [13.909]] [[0.182]
 [0.237]
 [0.165]
 [0.157]
 [0.163]
 [0.168]
 [0.188]]
printing an ep nov before normalisation:  51.92576069088552
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.35183048248291
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.54725766859932
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.921512059077827
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  56 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  75.21753274306039
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.332]
 [0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]] [[42.816]
 [54.802]
 [42.816]
 [42.816]
 [42.816]
 [42.816]
 [42.816]] [[1.352]
 [1.919]
 [1.352]
 [1.352]
 [1.352]
 [1.352]
 [1.352]]
printing an ep nov before normalisation:  59.12264893005632
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.443]
 [0.443]
 [0.443]
 [0.437]
 [0.443]
 [0.443]] [[50.209]
 [50.576]
 [50.576]
 [50.576]
 [57.762]
 [50.576]
 [50.576]] [[0.871]
 [0.887]
 [0.887]
 [0.887]
 [0.987]
 [0.887]
 [0.887]]
printing an ep nov before normalisation:  31.496737003326416
printing an ep nov before normalisation:  58.4896701349222
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 32.24703362012616
actor:  1 policy actor:  1  step number:  48 total reward:  0.4866666666666668  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.819807338961105
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
actions average: 
K:  4  action  0 :  tensor([0.4167, 0.0022, 0.1126, 0.1394, 0.1204, 0.1116, 0.0972],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0079, 0.9471, 0.0101, 0.0059, 0.0038, 0.0038, 0.0215],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1179, 0.0171, 0.2889, 0.1273, 0.1516, 0.1295, 0.1677],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1541, 0.0501, 0.1070, 0.3130, 0.1513, 0.0901, 0.1343],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1629, 0.0037, 0.0381, 0.0630, 0.6341, 0.0271, 0.0710],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0362, 0.0297, 0.1504, 0.0656, 0.0502, 0.6220, 0.0459],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0590, 0.2533, 0.0897, 0.0754, 0.0596, 0.0571, 0.4059],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1795533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.13578406253887
printing an ep nov before normalisation:  41.39786391598296
printing an ep nov before normalisation:  51.44652410904834
printing an ep nov before normalisation:  41.91941804850444
actor:  0 policy actor:  0  step number:  62 total reward:  0.07333333333333247  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  41.832959440358074
maxi score, test score, baseline:  0.17831333333333318 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17496666666666652 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.716363290506564
maxi score, test score, baseline:  0.17496666666666652 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.71549552709581
actor:  1 policy actor:  1  step number:  56 total reward:  0.17999999999999938  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.96583127697754
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.348]
 [0.348]
 [0.348]
 [0.389]
 [0.348]
 [0.348]] [[45.086]
 [46.245]
 [46.245]
 [46.245]
 [51.918]
 [46.245]
 [46.245]] [[0.593]
 [0.566]
 [0.566]
 [0.566]
 [0.649]
 [0.566]
 [0.566]]
printing an ep nov before normalisation:  60.580981922681175
maxi score, test score, baseline:  0.17161999999999986 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17161999999999986 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.84802952358702
printing an ep nov before normalisation:  49.54198122193668
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  16.03019599581144
actor:  1 policy actor:  1  step number:  57 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  53.49334361401131
printing an ep nov before normalisation:  36.868639228710634
printing an ep nov before normalisation:  43.16289637965002
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actor:  1 policy actor:  1  step number:  60 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17161999999999986 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.6126, 0.0268, 0.0668, 0.0634, 0.0880, 0.0496, 0.0929],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0040, 0.9678, 0.0026, 0.0031, 0.0010, 0.0012, 0.0202],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1071, 0.0225, 0.4286, 0.1098, 0.1094, 0.1105, 0.1123],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0969, 0.0212, 0.0927, 0.5074, 0.1035, 0.0893, 0.0890],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1294, 0.0021, 0.0546, 0.0604, 0.6412, 0.0540, 0.0584],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([    0.0888,     0.0004,     0.1155,     0.0781,     0.0692,     0.5583,
            0.0897], grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1352, 0.1697, 0.1041, 0.1300, 0.1165, 0.0737, 0.2708],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  44.22976452658703
printing an ep nov before normalisation:  61.988962276646625
maxi score, test score, baseline:  0.17161999999999986 0.6930000000000002 0.6930000000000002
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17161999999999986 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.405343303100224
maxi score, test score, baseline:  0.17161999999999986 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.991]
 [1.009]
 [0.986]
 [0.998]
 [0.964]
 [0.979]
 [1.007]] [[44.809]
 [35.991]
 [42.885]
 [40.692]
 [44.93 ]
 [45.361]
 [40.998]] [[0.991]
 [1.009]
 [0.986]
 [0.998]
 [0.964]
 [0.979]
 [1.007]]
actor:  0 policy actor:  0  step number:  38 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17121999999999984 0.6930000000000002 0.6930000000000002
siam score:  -0.7793326
printing an ep nov before normalisation:  24.286198358717627
maxi score, test score, baseline:  0.17121999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.645545764594154
maxi score, test score, baseline:  0.17121999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17121999999999984 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  43 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  57 total reward:  0.2533333333333333  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.549]
 [0.48 ]
 [0.496]
 [0.461]
 [0.48 ]
 [0.572]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.48 ]
 [0.549]
 [0.48 ]
 [0.496]
 [0.461]
 [0.48 ]
 [0.572]]
printing an ep nov before normalisation:  39.57428087008482
maxi score, test score, baseline:  0.1706733333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1706733333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.852]
 [0.762]
 [0.762]
 [0.762]
 [0.762]
 [0.762]] [[45.313]
 [44.606]
 [45.313]
 [45.313]
 [45.313]
 [45.313]
 [45.313]] [[0.762]
 [0.852]
 [0.762]
 [0.762]
 [0.762]
 [0.762]
 [0.762]]
maxi score, test score, baseline:  0.1706733333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  47 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17001999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]
 [0.591]] [[41.105]
 [41.105]
 [41.105]
 [41.105]
 [41.105]
 [41.105]
 [41.105]] [[55.384]
 [55.384]
 [55.384]
 [55.384]
 [55.384]
 [55.384]
 [55.384]]
printing an ep nov before normalisation:  32.30603110477819
actor:  0 policy actor:  0  step number:  37 total reward:  0.47999999999999976  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.16964666666666656 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16964666666666656 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  55.706030598358154
maxi score, test score, baseline:  0.16964666666666656 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.16964666666666656 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.405347922572076
printing an ep nov before normalisation:  41.24473036891928
printing an ep nov before normalisation:  56.72693941404652
printing an ep nov before normalisation:  39.64692259471595
Printing some Q and Qe and total Qs values:  [[0.493]
 [0.543]
 [0.522]
 [0.514]
 [0.51 ]
 [0.518]
 [0.522]] [[36.518]
 [34.687]
 [36.656]
 [37.004]
 [36.905]
 [36.999]
 [37.067]] [[1.661]
 [1.599]
 [1.699]
 [1.712]
 [1.703]
 [1.716]
 [1.724]]
maxi score, test score, baseline:  0.16964666666666656 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.16964666666666656 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.9385710675987
maxi score, test score, baseline:  0.16964666666666656 0.6930000000000002 0.6930000000000002
actor:  1 policy actor:  1  step number:  52 total reward:  0.43333333333333335  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  56 total reward:  0.4600000000000001  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
Printing some Q and Qe and total Qs values:  [[0.193]
 [0.362]
 [0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]] [[32.532]
 [37.502]
 [32.532]
 [32.532]
 [32.532]
 [32.532]
 [32.532]] [[0.851]
 [1.216]
 [0.851]
 [0.851]
 [0.851]
 [0.851]
 [0.851]]
Printing some Q and Qe and total Qs values:  [[0.292]
 [0.295]
 [0.292]
 [0.292]
 [0.292]
 [0.292]
 [0.292]] [[48.33 ]
 [51.736]
 [48.33 ]
 [48.33 ]
 [48.33 ]
 [48.33 ]
 [48.33 ]] [[1.428]
 [1.576]
 [1.428]
 [1.428]
 [1.428]
 [1.428]
 [1.428]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.56720240437757
using explorer policy with actor:  1
maxi score, test score, baseline:  0.16964666666666656 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.05091959872974
printing an ep nov before normalisation:  61.0295994246769
printing an ep nov before normalisation:  49.87029326300056
actor:  1 policy actor:  1  step number:  50 total reward:  0.3533333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.16964666666666656 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.220957540596615
maxi score, test score, baseline:  0.16964666666666656 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  67 total reward:  0.11999999999999933  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.16964666666666656 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.16964666666666656 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76859593
actor:  0 policy actor:  0  step number:  52 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  35.08505446219252
maxi score, test score, baseline:  0.1723533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.6576452255249
printing an ep nov before normalisation:  41.805620193481445
maxi score, test score, baseline:  0.1723533333333332 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.1723533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1723533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  5.155383973942662e-05
Printing some Q and Qe and total Qs values:  [[0.804]
 [0.804]
 [0.804]
 [0.837]
 [0.804]
 [0.804]
 [0.804]] [[26.329]
 [26.329]
 [26.329]
 [25.819]
 [26.329]
 [26.329]
 [26.329]] [[2.188]
 [2.188]
 [2.188]
 [2.17 ]
 [2.188]
 [2.188]
 [2.188]]
maxi score, test score, baseline:  0.1723533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1723533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]] [[57.508]
 [57.508]
 [57.508]
 [57.508]
 [57.508]
 [57.508]
 [57.508]] [[1.812]
 [1.812]
 [1.812]
 [1.812]
 [1.812]
 [1.812]
 [1.812]]
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.612]
 [0.535]
 [0.553]
 [0.554]
 [0.555]
 [0.556]] [[35.041]
 [36.969]
 [38.499]
 [34.922]
 [35.002]
 [35.256]
 [36.446]] [[1.502]
 [1.661]
 [1.667]
 [1.492]
 [1.498]
 [1.513]
 [1.577]]
maxi score, test score, baseline:  0.1723533333333332 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  50 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17471333333333322 0.6930000000000002 0.6930000000000002
printing an ep nov before normalisation:  44.75489281848989
line 256 mcts: sample exp_bonus 44.941657753565586
actor:  1 policy actor:  1  step number:  44 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17471333333333322 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17471333333333322 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.31827657510629
actor:  1 policy actor:  1  step number:  45 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  61 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.76217487129006
maxi score, test score, baseline:  0.17471333333333322 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17471333333333322 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.51428981922376
printing an ep nov before normalisation:  40.089577284869875
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.233]
 [0.358]
 [0.233]
 [0.151]
 [0.164]
 [0.233]
 [0.233]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.233]
 [0.358]
 [0.233]
 [0.151]
 [0.164]
 [0.233]
 [0.233]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.2799999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17471333333333322 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.37700080457752
siam score:  -0.7701793
printing an ep nov before normalisation:  66.4205448155822
Printing some Q and Qe and total Qs values:  [[0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]] [[46.183]
 [46.183]
 [46.183]
 [46.183]
 [46.183]
 [46.183]
 [46.183]] [[0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]]
actions average: 
K:  0  action  0 :  tensor([0.4441, 0.0106, 0.0939, 0.1043, 0.1514, 0.0837, 0.1120],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0091,     0.9515,     0.0077,     0.0031,     0.0008,     0.0007,
            0.0272], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1223, 0.0089, 0.3748, 0.1118, 0.1163, 0.1236, 0.1423],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0895, 0.0097, 0.0922, 0.4635, 0.0978, 0.1149, 0.1325],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1265, 0.0013, 0.0661, 0.0893, 0.5667, 0.0680, 0.0821],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0558, 0.0011, 0.1193, 0.0890, 0.0732, 0.6052, 0.0563],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0935, 0.0336, 0.1128, 0.1257, 0.0878, 0.0809, 0.4656],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  32.977986335754395
printing an ep nov before normalisation:  52.3233607869217
maxi score, test score, baseline:  0.17471333333333322 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.17471333333333322 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17471333333333322 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  56 total reward:  0.13999999999999935  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  67 total reward:  0.07999999999999952  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17493999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.007143962448736829
actor:  1 policy actor:  1  step number:  74 total reward:  0.05999999999999894  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  2  action  0 :  tensor([0.5409, 0.0017, 0.0927, 0.0929, 0.1217, 0.0712, 0.0790],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0105, 0.9299, 0.0085, 0.0055, 0.0026, 0.0030, 0.0400],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1143, 0.0065, 0.3749, 0.1626, 0.1146, 0.1124, 0.1148],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1220, 0.0054, 0.1394, 0.3089, 0.1242, 0.1235, 0.1767],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0972, 0.0069, 0.0799, 0.1106, 0.5367, 0.0842, 0.0844],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0866, 0.0206, 0.1037, 0.1081, 0.0888, 0.4851, 0.1072],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1137, 0.0125, 0.1121, 0.1183, 0.1075, 0.1080, 0.4278],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  36.365430372972945
actor:  0 policy actor:  0  step number:  43 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  43.640780448913574
maxi score, test score, baseline:  0.17504666666666654 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.196033143968044
maxi score, test score, baseline:  0.17504666666666654 0.6930000000000002 0.6930000000000002
maxi score, test score, baseline:  0.17504666666666654 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]
 [0.304]] [[73.161]
 [73.161]
 [73.161]
 [73.161]
 [73.161]
 [73.161]
 [73.161]] [[2.247]
 [2.247]
 [2.247]
 [2.247]
 [2.247]
 [2.247]
 [2.247]]
actions average: 
K:  2  action  0 :  tensor([0.4170, 0.0138, 0.0973, 0.1217, 0.0896, 0.1100, 0.1505],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0016,     0.9762,     0.0015,     0.0031,     0.0008,     0.0013,
            0.0156], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0830, 0.0088, 0.4898, 0.1123, 0.0772, 0.1185, 0.1104],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1290, 0.0211, 0.1341, 0.2959, 0.1214, 0.1485, 0.1499],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0887, 0.0089, 0.0776, 0.1095, 0.4846, 0.0919, 0.1388],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0921, 0.0210, 0.1237, 0.1089, 0.1318, 0.4146, 0.1079],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1275, 0.1889, 0.1009, 0.1351, 0.0836, 0.1077, 0.2563],
       grad_fn=<DivBackward0>)
actions average: 
K:  4  action  0 :  tensor([0.2529, 0.0035, 0.1373, 0.1653, 0.1397, 0.1572, 0.1441],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0096, 0.9329, 0.0080, 0.0099, 0.0048, 0.0071, 0.0276],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0958, 0.0465, 0.3582, 0.1066, 0.0873, 0.1547, 0.1507],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0917, 0.0044, 0.0915, 0.4079, 0.1172, 0.1356, 0.1517],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.1399,     0.0005,     0.0739,     0.1010,     0.5103,     0.0894,
            0.0850], grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1192, 0.0004, 0.1683, 0.1994, 0.1677, 0.1895, 0.1554],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0782, 0.4029, 0.0570, 0.0645, 0.0535, 0.0632, 0.2807],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.17504666666666654 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.06351056743675
maxi score, test score, baseline:  0.17504666666666654 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17504666666666654 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.16713255059762
printing an ep nov before normalisation:  60.93501944979861
actor:  0 policy actor:  0  step number:  62 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7740146
maxi score, test score, baseline:  0.17761999999999986 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17761999999999986 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.08656138387999
maxi score, test score, baseline:  0.17761999999999986 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17761999999999986 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17761999999999986 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17761999999999986 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.884433659746215
actor:  1 policy actor:  1  step number:  50 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17761999999999986 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  33 total reward:  0.5733333333333334  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  59.959115260930794
maxi score, test score, baseline:  0.17765999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.17765999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17765999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.47842119050648
printing an ep nov before normalisation:  42.42875576019287
printing an ep nov before normalisation:  46.88079884696222
maxi score, test score, baseline:  0.17765999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17765999999999987 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  35 total reward:  0.56  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  53 total reward:  0.3733333333333334  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18077999999999989 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18077999999999989 0.6930000000000002 0.6930000000000002
actor:  1 policy actor:  1  step number:  57 total reward:  0.23999999999999944  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  30.551944838629826
maxi score, test score, baseline:  0.18077999999999989 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.035]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.035]]
printing an ep nov before normalisation:  46.38438081435964
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.595]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.525]] [[37.895]
 [56.217]
 [37.795]
 [37.795]
 [37.795]
 [37.795]
 [47.385]] [[0.456]
 [0.595]
 [0.511]
 [0.511]
 [0.511]
 [0.511]
 [0.525]]
Printing some Q and Qe and total Qs values:  [[0.755]
 [0.864]
 [0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.804]] [[51.417]
 [60.74 ]
 [39.588]
 [39.588]
 [39.588]
 [39.588]
 [59.802]] [[0.755]
 [0.864]
 [0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.804]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.18077999999999989 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.78 ]
 [0.816]
 [0.772]
 [0.783]
 [0.782]
 [0.776]
 [0.799]] [[48.799]
 [46.9  ]
 [48.483]
 [48.696]
 [48.626]
 [48.353]
 [48.831]] [[0.78 ]
 [0.816]
 [0.772]
 [0.783]
 [0.782]
 [0.776]
 [0.799]]
printing an ep nov before normalisation:  55.03775251131614
maxi score, test score, baseline:  0.18077999999999989 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.05823970300895
printing an ep nov before normalisation:  51.12413733163149
printing an ep nov before normalisation:  43.958425521850586
printing an ep nov before normalisation:  48.19796654914929
Printing some Q and Qe and total Qs values:  [[1.029]
 [1.029]
 [1.029]
 [1.029]
 [1.029]
 [1.029]
 [1.029]] [[40.453]
 [40.453]
 [40.453]
 [40.453]
 [40.453]
 [40.453]
 [40.453]] [[1.029]
 [1.029]
 [1.029]
 [1.029]
 [1.029]
 [1.029]
 [1.029]]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  37.78188005557838
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  46.921338833653174
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  0.18077999999999989 0.6930000000000002 0.6930000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  29 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  2
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.61466163698702
printing an ep nov before normalisation:  42.46060384854215
maxi score, test score, baseline:  0.2048733333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2048733333333332 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.2048733333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  53 total reward:  0.2133333333333326  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.066]
 [0.078]
 [0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.066]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.066]
 [0.078]
 [0.066]
 [0.066]
 [0.066]
 [0.066]
 [0.066]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.3599999999999999  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.693]
 [0.701]
 [0.742]
 [0.688]
 [0.709]
 [0.71 ]
 [0.689]] [[31.227]
 [18.848]
 [30.851]
 [28.767]
 [29.263]
 [30.162]
 [32.863]] [[1.855]
 [1.366]
 [1.889]
 [1.752]
 [1.793]
 [1.829]
 [1.917]]
maxi score, test score, baseline:  0.20729999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  73 total reward:  0.1466666666666656  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.41756308685541
printing an ep nov before normalisation:  63.07256869190621
printing an ep nov before normalisation:  56.592826851973115
actor:  0 policy actor:  0  step number:  44 total reward:  0.4866666666666668  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  26.114416122436523
printing an ep nov before normalisation:  44.64616775512695
maxi score, test score, baseline:  0.20557999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.88700354694376
actor:  1 policy actor:  1  step number:  45 total reward:  0.3866666666666665  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.20557999999999987 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.20557999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20557999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20557999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.168]
 [0.752]
 [0.752]
 [0.618]
 [0.608]
 [0.542]
 [0.452]] [[35.281]
 [26.821]
 [26.751]
 [33.11 ]
 [31.598]
 [37.11 ]
 [35.778]] [[0.965]
 [1.358]
 [1.356]
 [1.367]
 [1.323]
 [1.381]
 [1.261]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20557999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7693546
actor:  0 policy actor:  0  step number:  38 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  55.641469018929286
line 256 mcts: sample exp_bonus 29.86106915316751
maxi score, test score, baseline:  0.2055933333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7697664
actions average: 
K:  0  action  0 :  tensor([0.3906, 0.0129, 0.1030, 0.0897, 0.1444, 0.1035, 0.1559],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0073, 0.9352, 0.0090, 0.0017, 0.0020, 0.0025, 0.0425],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0974, 0.0027, 0.3028, 0.1475, 0.1194, 0.1774, 0.1527],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1084, 0.0247, 0.1126, 0.3442, 0.0841, 0.1163, 0.2097],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0661,     0.0004,     0.0679,     0.0797,     0.6116,     0.0937,
            0.0806], grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0547, 0.0099, 0.0881, 0.0893, 0.0708, 0.6091, 0.0781],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0995, 0.0828, 0.1391, 0.1265, 0.1023, 0.1304, 0.3194],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.351]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]] [[47.18 ]
 [57.444]
 [47.18 ]
 [47.18 ]
 [47.18 ]
 [47.18 ]
 [47.18 ]] [[1.231]
 [1.586]
 [1.231]
 [1.231]
 [1.231]
 [1.231]
 [1.231]]
line 256 mcts: sample exp_bonus 44.20540000423611
maxi score, test score, baseline:  0.2055933333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.785826683044434
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.38 ]
 [0.433]
 [0.433]
 [0.433]
 [0.417]
 [0.433]] [[59.852]
 [63.082]
 [59.852]
 [59.852]
 [59.852]
 [62.555]
 [59.852]] [[1.87 ]
 [1.927]
 [1.87 ]
 [1.87 ]
 [1.87 ]
 [1.946]
 [1.87 ]]
maxi score, test score, baseline:  0.2032599999999999 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.301781771407335
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.05932612537701
actor:  1 policy actor:  1  step number:  53 total reward:  0.5333333333333338  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  44 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  49 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.802]
 [0.809]
 [0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.802]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.802]
 [0.809]
 [0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.802]]
actions average: 
K:  4  action  0 :  tensor([0.5458, 0.0194, 0.1017, 0.0818, 0.0805, 0.0888, 0.0820],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0137, 0.9322, 0.0114, 0.0109, 0.0070, 0.0086, 0.0161],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1096, 0.0657, 0.3390, 0.1022, 0.0994, 0.2041, 0.0801],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1283, 0.0406, 0.1596, 0.2440, 0.1573, 0.1732, 0.0970],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1194, 0.0122, 0.0731, 0.0576, 0.6395, 0.0534, 0.0449],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0980, 0.0015, 0.1651, 0.1093, 0.0948, 0.4296, 0.1017],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1300, 0.1443, 0.1268, 0.1159, 0.1042, 0.1328, 0.2461],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.20617999999999986 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20617999999999986 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.10928042581476
line 256 mcts: sample exp_bonus 46.54009541017278
Printing some Q and Qe and total Qs values:  [[0.874]
 [0.898]
 [0.874]
 [0.874]
 [0.874]
 [0.874]
 [0.864]] [[37.55 ]
 [56.137]
 [37.55 ]
 [37.55 ]
 [37.55 ]
 [37.55 ]
 [47.422]] [[0.874]
 [0.898]
 [0.874]
 [0.874]
 [0.874]
 [0.874]
 [0.864]]
maxi score, test score, baseline:  0.20617999999999986 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.475]
 [0.413]
 [0.413]
 [0.413]
 [0.413]
 [0.413]] [[41.303]
 [41.163]
 [41.303]
 [41.303]
 [41.303]
 [41.303]
 [41.303]] [[1.744]
 [1.797]
 [1.744]
 [1.744]
 [1.744]
 [1.744]
 [1.744]]
maxi score, test score, baseline:  0.20617999999999986 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.906999518202895
printing an ep nov before normalisation:  21.183042526245117
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20617999999999986 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  58 total reward:  0.0733333333333327  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
siam score:  -0.7846794
printing an ep nov before normalisation:  63.99928124028785
maxi score, test score, baseline:  0.2035933333333332 0.6866666666666668 0.6866666666666668
actor:  1 policy actor:  1  step number:  58 total reward:  0.2866666666666665  reward:  1.0 rdn_beta:  1.667
siam score:  -0.78211564
maxi score, test score, baseline:  0.2035933333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.789]
 [0.795]
 [0.75 ]
 [0.75 ]
 [0.749]
 [0.792]
 [0.75 ]] [[36.552]
 [37.49 ]
 [31.526]
 [31.481]
 [31.512]
 [37.295]
 [31.422]] [[0.789]
 [0.795]
 [0.75 ]
 [0.75 ]
 [0.749]
 [0.792]
 [0.75 ]]
siam score:  -0.77968323
actor:  0 policy actor:  0  step number:  36 total reward:  0.5133333333333334  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.491]
 [0.556]
 [0.501]
 [0.641]
 [0.52 ]
 [0.568]] [[33.932]
 [33.9  ]
 [30.334]
 [28.148]
 [39.663]
 [36.855]
 [32.243]] [[1.709]
 [1.529]
 [1.305]
 [1.074]
 [2.145]
 [1.797]
 [1.472]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.20397999999999988 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.99479779123243
actor:  1 policy actor:  1  step number:  53 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  43.628108858221395
printing an ep nov before normalisation:  56.31856588416468
maxi score, test score, baseline:  0.20397999999999988 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  40 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  72.81161287036802
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.013]
 [1.026]
 [1.013]
 [1.013]
 [1.013]
 [1.013]
 [1.013]] [[34.107]
 [38.947]
 [34.107]
 [34.107]
 [34.107]
 [34.107]
 [34.107]] [[1.013]
 [1.026]
 [1.013]
 [1.013]
 [1.013]
 [1.013]
 [1.013]]
maxi score, test score, baseline:  0.20416666666666655 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  0.6133333333333335  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.071]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.071]]
maxi score, test score, baseline:  0.20416666666666655 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  48 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  43.14603413450989
printing an ep nov before normalisation:  49.470743171126976
maxi score, test score, baseline:  0.2069533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.60681663651247
maxi score, test score, baseline:  0.2069533333333332 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.2069533333333332 0.6866666666666668 0.6866666666666668
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.355]
 [0.355]
 [0.355]
 [0.293]
 [0.355]
 [0.32 ]] [[44.6  ]
 [44.6  ]
 [44.6  ]
 [44.6  ]
 [55.406]
 [44.6  ]
 [51.056]] [[0.93 ]
 [0.93 ]
 [0.93 ]
 [0.93 ]
 [1.139]
 [0.93 ]
 [1.057]]
maxi score, test score, baseline:  0.2069533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.326666666666666  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.20473999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20473999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20473999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20473999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20473999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.67830620720422
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]
 [0.309]] [[26.76]
 [26.76]
 [26.76]
 [26.76]
 [26.76]
 [26.76]
 [26.76]] [[0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]
 [0.855]]
maxi score, test score, baseline:  0.20473999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.04464966794315
siam score:  -0.7766365
maxi score, test score, baseline:  0.20473999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.969222324922455
printing an ep nov before normalisation:  52.20919315862097
maxi score, test score, baseline:  0.20473999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.53422610447149
maxi score, test score, baseline:  0.20473999999999987 0.6866666666666668 0.6866666666666668
printing an ep nov before normalisation:  42.922744748901735
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.784]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]] [[29.214]
 [41.428]
 [29.214]
 [29.214]
 [29.214]
 [29.214]
 [29.214]] [[0.638]
 [0.784]
 [0.638]
 [0.638]
 [0.638]
 [0.638]
 [0.638]]
actor:  1 policy actor:  1  step number:  50 total reward:  0.38  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20473999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20473999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.3435, 0.0238, 0.1256, 0.1031, 0.1608, 0.1170, 0.1262],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0062,     0.9312,     0.0108,     0.0122,     0.0008,     0.0007,
            0.0381], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0840, 0.0213, 0.4713, 0.0964, 0.1003, 0.1193, 0.1074],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0855, 0.0627, 0.1306, 0.3076, 0.1242, 0.1261, 0.1632],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1034, 0.0051, 0.1302, 0.0890, 0.4476, 0.1046, 0.1201],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0442, 0.0217, 0.0896, 0.0698, 0.0656, 0.6591, 0.0499],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0976, 0.2953, 0.1394, 0.0852, 0.0970, 0.0939, 0.1917],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.20473999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20473999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.61920188123547
maxi score, test score, baseline:  0.20473999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20473999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20473999999999987 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.20473999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  52 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20725999999999983 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20725999999999983 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.076]
 [0.076]
 [0.357]
 [0.076]
 [0.076]
 [0.076]] [[52.939]
 [52.939]
 [52.939]
 [53.028]
 [52.939]
 [52.939]
 [52.939]] [[1.25 ]
 [1.25 ]
 [1.25 ]
 [1.535]
 [1.25 ]
 [1.25 ]
 [1.25 ]]
printing an ep nov before normalisation:  58.48169647999639
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]] [[53.033]
 [53.033]
 [53.033]
 [53.033]
 [53.033]
 [53.033]
 [53.033]] [[1.797]
 [1.797]
 [1.797]
 [1.797]
 [1.797]
 [1.797]
 [1.797]]
printing an ep nov before normalisation:  39.66916541021655
maxi score, test score, baseline:  0.20725999999999983 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76496583
line 256 mcts: sample exp_bonus 44.61365117868372
actor:  1 policy actor:  1  step number:  62 total reward:  0.2599999999999991  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20725999999999983 0.6866666666666668 0.6866666666666668
siam score:  -0.7665746
Printing some Q and Qe and total Qs values:  [[0.673]
 [0.682]
 [0.673]
 [0.682]
 [0.673]
 [0.673]
 [0.682]] [[39.377]
 [43.68 ]
 [39.377]
 [37.519]
 [39.377]
 [39.377]
 [38.643]] [[0.974]
 [1.015]
 [0.974]
 [0.968]
 [0.974]
 [0.974]
 [0.976]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3999999999999999  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  50.43642415774246
siam score:  -0.7656369
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.558]
 [0.484]
 [0.484]
 [0.484]
 [0.484]
 [0.484]] [[52.888]
 [59.581]
 [52.888]
 [52.888]
 [52.888]
 [52.888]
 [52.888]] [[0.948]
 [1.117]
 [0.948]
 [0.948]
 [0.948]
 [0.948]
 [0.948]]
maxi score, test score, baseline:  0.20725999999999983 0.6866666666666668 0.6866666666666668
printing an ep nov before normalisation:  57.46132907366389
maxi score, test score, baseline:  0.20725999999999983 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.3927, 0.0159, 0.1272, 0.1171, 0.1273, 0.1217, 0.0982],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0163, 0.9227, 0.0115, 0.0079, 0.0049, 0.0048, 0.0318],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1034, 0.0047, 0.3472, 0.1435, 0.1239, 0.1552, 0.1220],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0725, 0.1908, 0.0712, 0.3307, 0.0577, 0.0611, 0.2160],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0727, 0.0979, 0.0755, 0.1260, 0.4741, 0.0682, 0.0856],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0435, 0.0005, 0.0705, 0.1298, 0.1475, 0.4943, 0.1139],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1626, 0.0975, 0.1230, 0.1430, 0.1169, 0.1276, 0.2295],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.20725999999999983 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  51 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  45.8209298977715
printing an ep nov before normalisation:  50.45665725090237
printing an ep nov before normalisation:  38.37739238339425
printing an ep nov before normalisation:  44.588179357906114
printing an ep nov before normalisation:  36.450371980400874
printing an ep nov before normalisation:  22.848578261468518
maxi score, test score, baseline:  0.20973999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3399999999999993  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.20973999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20973999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.64143410025853
maxi score, test score, baseline:  0.20973999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  61 total reward:  0.1733333333333329  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  43.47759211755313
UNIT TEST: sample policy line 217 mcts : [0.204 0.102 0.102 0.143 0.102 0.102 0.245]
printing an ep nov before normalisation:  31.909323075020833
Printing some Q and Qe and total Qs values:  [[-0.075]
 [ 0.36 ]
 [ 0.49 ]
 [ 0.352]
 [-0.09 ]
 [ 0.506]
 [ 0.332]] [[41.562]
 [39.538]
 [47.232]
 [38.098]
 [41.5  ]
 [46.405]
 [34.404]] [[0.094]
 [0.514]
 [0.7  ]
 [0.496]
 [0.079]
 [0.71 ]
 [0.449]]
printing an ep nov before normalisation:  36.20047212144933
printing an ep nov before normalisation:  63.20965158341276
maxi score, test score, baseline:  0.20957999999999988 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.07538936631512
actor:  1 policy actor:  1  step number:  60 total reward:  0.04666666666666586  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20957999999999988 0.6866666666666668 0.6866666666666668
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.20957999999999988 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20957999999999988 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.14666666666666617  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  20.287123291088264
printing an ep nov before normalisation:  53.11375985399144
printing an ep nov before normalisation:  41.951221900444665
maxi score, test score, baseline:  0.20957999999999988 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.74136697825227
actor:  1 policy actor:  1  step number:  64 total reward:  0.2866666666666656  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  41 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2101133333333332 0.6866666666666668 0.6866666666666668
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.401]
 [0.436]
 [0.379]
 [0.401]
 [0.376]
 [0.374]
 [0.407]] [[41.884]
 [42.506]
 [37.629]
 [41.884]
 [37.475]
 [38.083]
 [41.968]] [[1.354]
 [1.42 ]
 [1.113]
 [1.354]
 [1.102]
 [1.131]
 [1.364]]
printing an ep nov before normalisation:  60.32278406874074
Printing some Q and Qe and total Qs values:  [[0.518]
 [0.54 ]
 [0.524]
 [0.521]
 [0.523]
 [0.51 ]
 [0.505]] [[39.45 ]
 [46.523]
 [48.338]
 [39.914]
 [43.015]
 [39.939]
 [40.763]] [[1.311]
 [1.642]
 [1.706]
 [1.334]
 [1.472]
 [1.325]
 [1.356]]
actor:  1 policy actor:  1  step number:  44 total reward:  0.45999999999999974  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2101133333333332 0.6866666666666668 0.6866666666666668
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2101133333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actor:  1 policy actor:  1  step number:  53 total reward:  0.42666666666666675  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2101133333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.82219910922821
maxi score, test score, baseline:  0.2101133333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  47.648923010557226
printing an ep nov before normalisation:  57.615706549204646
maxi score, test score, baseline:  0.2101133333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.005627659304536792
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  13.94541621208191
maxi score, test score, baseline:  0.2101133333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.016576198752375
actor:  1 policy actor:  1  step number:  76 total reward:  0.07333333333333203  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  54.383738459363435
printing an ep nov before normalisation:  45.82864157126113
siam score:  -0.76348937
printing an ep nov before normalisation:  40.087223052978516
maxi score, test score, baseline:  0.2101133333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.029]
 [ 0.029]
 [ 0.195]
 [ 0.029]
 [-0.029]
 [ 0.211]
 [ 0.029]] [[44.038]
 [44.038]
 [50.182]
 [44.038]
 [57.867]
 [48.088]
 [44.038]] [[0.858]
 [0.858]
 [1.272]
 [0.858]
 [1.357]
 [1.203]
 [0.858]]
printing an ep nov before normalisation:  45.97198622021578
maxi score, test score, baseline:  0.2101133333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.7614711
maxi score, test score, baseline:  0.2101133333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2101133333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  0.3399999999999995  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2101133333333332 0.6866666666666668 0.6866666666666668
actor:  1 policy actor:  1  step number:  60 total reward:  0.086666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2101133333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2101133333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.10918683663816
actor:  1 policy actor:  1  step number:  70 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  50.50557972739881
printing an ep nov before normalisation:  28.678371906280518
printing an ep nov before normalisation:  26.46321770770745
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.074339866638184
printing an ep nov before normalisation:  49.16484832763672
actor:  0 policy actor:  0  step number:  44 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.445]
 [0.507]
 [0.445]
 [0.445]
 [0.445]
 [0.445]
 [0.445]] [[46.249]
 [48.851]
 [46.249]
 [46.249]
 [46.249]
 [46.249]
 [46.249]] [[2.049]
 [2.263]
 [2.049]
 [2.049]
 [2.049]
 [2.049]
 [2.049]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.16666666666666596  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  12.366714477539062
printing an ep nov before normalisation:  13.556661605834961
printing an ep nov before normalisation:  34.97242614970839
printing an ep nov before normalisation:  15.301141723342424
actor:  0 policy actor:  0  step number:  48 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  46.6660005741302
actor:  1 policy actor:  1  step number:  67 total reward:  0.0399999999999997  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  54.6181118681442
maxi score, test score, baseline:  0.2067533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  0.333
siam score:  -0.76977456
maxi score, test score, baseline:  0.2067533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2067533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.855934601553265
line 256 mcts: sample exp_bonus 60.3981338249128
maxi score, test score, baseline:  0.2067533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.3399999999999994  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2067533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.60263144678898
printing an ep nov before normalisation:  44.93025779724121
maxi score, test score, baseline:  0.2067533333333332 0.6866666666666668 0.6866666666666668
actor:  1 policy actor:  1  step number:  48 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2067533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.26899520153103
printing an ep nov before normalisation:  68.91301830834925
printing an ep nov before normalisation:  58.902815681075204
Printing some Q and Qe and total Qs values:  [[0.292]
 [0.324]
 [0.263]
 [0.292]
 [0.264]
 [0.264]
 [0.292]] [[48.608]
 [47.   ]
 [40.454]
 [48.608]
 [40.064]
 [40.269]
 [48.608]] [[1.26 ]
 [1.239]
 [0.96 ]
 [1.26 ]
 [0.948]
 [0.955]
 [1.26 ]]
maxi score, test score, baseline:  0.2067533333333332 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.2067533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2067533333333332 0.6866666666666668 0.6866666666666668
Printing some Q and Qe and total Qs values:  [[0.584]
 [0.626]
 [0.574]
 [0.524]
 [0.609]
 [0.574]
 [0.574]] [[54.139]
 [53.204]
 [50.419]
 [53.257]
 [51.475]
 [50.419]
 [50.419]] [[1.064]
 [1.091]
 [0.994]
 [0.99 ]
 [1.046]
 [0.994]
 [0.994]]
printing an ep nov before normalisation:  58.18715230499767
maxi score, test score, baseline:  0.2067533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.66160901922877
maxi score, test score, baseline:  0.2067533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.2762, 0.0064, 0.1629, 0.1449, 0.1088, 0.1340, 0.1668],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0151, 0.9573, 0.0070, 0.0028, 0.0017, 0.0013, 0.0148],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0806, 0.0669, 0.5250, 0.0839, 0.0471, 0.1086, 0.0878],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0828, 0.1033, 0.1051, 0.3015, 0.0760, 0.1996, 0.1317],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1405, 0.0360, 0.0447, 0.0560, 0.6078, 0.0464, 0.0686],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1281, 0.0028, 0.1608, 0.1542, 0.1199, 0.2838, 0.1505],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0976, 0.1529, 0.0925, 0.1316, 0.0899, 0.0983, 0.3372],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  53 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.461]
 [0.518]
 [0.465]
 [0.515]
 [0.464]
 [0.464]
 [0.462]] [[28.434]
 [41.635]
 [28.552]
 [39.425]
 [28.829]
 [28.793]
 [28.771]] [[1.062]
 [1.665]
 [1.07 ]
 [1.571]
 [1.081]
 [1.079]
 [1.077]]
maxi score, test score, baseline:  0.2067533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7677267
actor:  1 policy actor:  1  step number:  68 total reward:  0.3399999999999992  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  58 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.308]
 [0.31 ]
 [0.308]
 [0.308]
 [0.308]
 [0.308]
 [0.308]] [[43.3  ]
 [50.752]
 [43.3  ]
 [43.3  ]
 [43.3  ]
 [43.3  ]
 [43.3  ]] [[1.088]
 [1.352]
 [1.088]
 [1.088]
 [1.088]
 [1.088]
 [1.088]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2067533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2067533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.889272689819336
Printing some Q and Qe and total Qs values:  [[0.732]
 [0.719]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]] [[47.051]
 [51.036]
 [47.051]
 [47.051]
 [47.051]
 [47.051]
 [47.051]] [[1.871]
 [1.958]
 [1.871]
 [1.871]
 [1.871]
 [1.871]
 [1.871]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.2067533333333332 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.2067533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  60 total reward:  0.3133333333333328  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2067533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.728]
 [0.789]
 [0.725]
 [0.784]
 [0.804]
 [0.855]
 [0.806]] [[64.687]
 [61.272]
 [65.448]
 [63.317]
 [63.227]
 [60.479]
 [63.601]] [[1.352]
 [1.367]
 [1.359]
 [1.389]
 [1.409]
 [1.422]
 [1.416]]
maxi score, test score, baseline:  0.2067533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2067533333333332 0.6866666666666668 0.6866666666666668
printing an ep nov before normalisation:  29.116510414506482
maxi score, test score, baseline:  0.2067533333333332 0.6866666666666668 0.6866666666666668
actions average: 
K:  1  action  0 :  tensor([0.4172, 0.0045, 0.1128, 0.1146, 0.1636, 0.0910, 0.0962],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0029, 0.9525, 0.0028, 0.0194, 0.0011, 0.0023, 0.0189],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0890, 0.0125, 0.3521, 0.1337, 0.1169, 0.1722, 0.1236],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1137, 0.0087, 0.1225, 0.3180, 0.1793, 0.1012, 0.1565],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0758, 0.0041, 0.0764, 0.0834, 0.5860, 0.0832, 0.0910],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1102, 0.0066, 0.1331, 0.1307, 0.1530, 0.3298, 0.1367],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1382, 0.0577, 0.1356, 0.1337, 0.1413, 0.1175, 0.2759],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2067533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7650004
printing an ep nov before normalisation:  34.69222867169327
actor:  1 policy actor:  1  step number:  50 total reward:  0.24666666666666637  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2067533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2067533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  58 total reward:  0.2066666666666661  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.20577999999999988 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20577999999999988 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  46.28329563070054
printing an ep nov before normalisation:  65.90303233787863
printing an ep nov before normalisation:  52.55311210529659
actor:  0 policy actor:  0  step number:  35 total reward:  0.5333333333333333  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  34.873718191043366
printing an ep nov before normalisation:  28.71113419532776
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.312]
 [0.292]
 [0.293]
 [0.295]
 [0.299]
 [0.299]] [[66.393]
 [59.306]
 [60.811]
 [61.955]
 [61.63 ]
 [61.68 ]
 [63.004]] [[2.316]
 [1.942]
 [2.001]
 [2.061]
 [2.047]
 [2.053]
 [2.122]]
siam score:  -0.7665386
actor:  1 policy actor:  1  step number:  46 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20207333333333324 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.54352420912261
Printing some Q and Qe and total Qs values:  [[0.435]
 [0.548]
 [0.435]
 [0.435]
 [0.435]
 [0.435]
 [0.435]] [[28.495]
 [44.023]
 [28.495]
 [28.495]
 [28.495]
 [28.495]
 [28.495]] [[0.576]
 [0.836]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]]
actions average: 
K:  4  action  0 :  tensor([0.3238, 0.0505, 0.0770, 0.1365, 0.1882, 0.1014, 0.1224],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0157, 0.8917, 0.0116, 0.0142, 0.0079, 0.0071, 0.0518],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1223, 0.0840, 0.2468, 0.1464, 0.1286, 0.1772, 0.0947],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1384, 0.0127, 0.1019, 0.3883, 0.1352, 0.1137, 0.1098],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1159, 0.0197, 0.0587, 0.1443, 0.5158, 0.0696, 0.0759],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([    0.0267,     0.0002,     0.1367,     0.0520,     0.0413,     0.7070,
            0.0361], grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1807, 0.1156, 0.1228, 0.1836, 0.1037, 0.1032, 0.1904],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  51.34448337905462
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.568]
 [0.51 ]
 [0.525]
 [0.463]
 [0.525]
 [0.574]] [[49.476]
 [54.934]
 [57.479]
 [49.476]
 [49.728]
 [49.476]
 [46.129]] [[1.603]
 [1.877]
 [1.926]
 [1.603]
 [1.552]
 [1.603]
 [1.51 ]]
maxi score, test score, baseline:  0.20207333333333324 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 57.352144179121744
maxi score, test score, baseline:  0.20207333333333324 0.6866666666666668 0.6866666666666668
Printing some Q and Qe and total Qs values:  [[0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]
 [0.788]] [[69.251]
 [69.251]
 [69.251]
 [69.251]
 [69.251]
 [69.251]
 [69.251]] [[2.788]
 [2.788]
 [2.788]
 [2.788]
 [2.788]
 [2.788]
 [2.788]]
maxi score, test score, baseline:  0.20207333333333324 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.3533333333333334  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20207333333333324 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20207333333333324 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.429531436003195
printing an ep nov before normalisation:  40.57336403847544
maxi score, test score, baseline:  0.20207333333333324 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20207333333333324 0.6866666666666668 0.6866666666666668
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.20207333333333324 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.36197403182196
printing an ep nov before normalisation:  28.75347255359391
siam score:  -0.7738362
maxi score, test score, baseline:  0.20207333333333324 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20207333333333324 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.70862716712353
maxi score, test score, baseline:  0.20207333333333324 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20207333333333324 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  0.23999999999999955  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.20116666666666655 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20116666666666655 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.101]
 [0.383]
 [0.403]
 [0.315]
 [0.25 ]
 [0.351]
 [0.342]] [[47.061]
 [42.043]
 [40.499]
 [38.064]
 [40.432]
 [40.576]
 [42.179]] [[1.909]
 [1.869]
 [1.789]
 [1.544]
 [1.631]
 [1.742]
 [1.836]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20116666666666655 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.099]
 [-0.099]
 [-0.099]
 [-0.102]
 [-0.099]
 [-0.09 ]
 [-0.099]] [[58.057]
 [58.057]
 [58.057]
 [62.16 ]
 [58.057]
 [63.033]
 [58.057]] [[1.53 ]
 [1.53 ]
 [1.53 ]
 [1.712]
 [1.53 ]
 [1.764]
 [1.53 ]]
maxi score, test score, baseline:  0.20116666666666655 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  42 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2006999999999999 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.8391262131412
maxi score, test score, baseline:  0.2006999999999999 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2006999999999999 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 46.401589609415424
actions average: 
K:  3  action  0 :  tensor([0.1863, 0.0311, 0.1306, 0.1402, 0.2143, 0.1290, 0.1684],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0058, 0.9559, 0.0060, 0.0056, 0.0040, 0.0048, 0.0180],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1072, 0.0229, 0.4142, 0.1201, 0.0880, 0.0838, 0.1638],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0640, 0.0041, 0.1539, 0.3807, 0.0607, 0.1923, 0.1443],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1254, 0.0259, 0.0858, 0.0971, 0.4655, 0.1122, 0.0881],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0750, 0.0251, 0.2203, 0.0634, 0.0618, 0.4652, 0.0893],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0917, 0.3371, 0.0656, 0.0837, 0.0676, 0.0585, 0.2960],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  50.32601188960279
Printing some Q and Qe and total Qs values:  [[0.242]
 [0.26 ]
 [0.242]
 [0.242]
 [0.242]
 [0.242]
 [0.242]] [[43.532]
 [44.389]
 [43.532]
 [43.532]
 [43.532]
 [43.532]
 [43.532]] [[1.815]
 [1.894]
 [1.815]
 [1.815]
 [1.815]
 [1.815]
 [1.815]]
actor:  0 policy actor:  0  step number:  41 total reward:  0.4133333333333329  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2001399999999999 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.80968369859327
printing an ep nov before normalisation:  47.70588183687502
printing an ep nov before normalisation:  41.0262145812319
maxi score, test score, baseline:  0.2001399999999999 0.6866666666666668 0.6866666666666668
printing an ep nov before normalisation:  53.96792328297602
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.616]
 [0.556]
 [0.556]
 [0.556]
 [0.556]
 [0.556]] [[36.062]
 [36.901]
 [36.062]
 [36.062]
 [36.062]
 [36.062]
 [36.062]] [[1.228]
 [1.322]
 [1.228]
 [1.228]
 [1.228]
 [1.228]
 [1.228]]
Printing some Q and Qe and total Qs values:  [[0.65 ]
 [0.65 ]
 [0.65 ]
 [0.65 ]
 [0.736]
 [0.65 ]
 [0.65 ]] [[46.616]
 [46.616]
 [46.616]
 [46.616]
 [57.069]
 [46.616]
 [46.616]] [[1.061]
 [1.061]
 [1.061]
 [1.061]
 [1.32 ]
 [1.061]
 [1.061]]
maxi score, test score, baseline:  0.2001399999999999 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.184 0.306 0.429 0.02  0.02  0.02  0.02 ]
actor:  1 policy actor:  1  step number:  56 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2001399999999999 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.12078874238841
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.29664633335408
Printing some Q and Qe and total Qs values:  [[0.246]
 [0.349]
 [0.246]
 [0.246]
 [0.246]
 [0.246]
 [0.246]] [[44.36 ]
 [45.136]
 [44.36 ]
 [44.36 ]
 [44.36 ]
 [44.36 ]
 [44.36 ]] [[1.861]
 [2.016]
 [1.861]
 [1.861]
 [1.861]
 [1.861]
 [1.861]]
maxi score, test score, baseline:  0.2001399999999999 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.2001399999999999 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.05899569123356
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.622]
 [0.752]
 [0.599]
 [0.73 ]
 [0.7  ]
 [0.591]] [[38.492]
 [40.599]
 [37.367]
 [38.352]
 [29.322]
 [35.189]
 [38.09 ]] [[0.956]
 [0.955]
 [1.055]
 [0.911]
 [0.957]
 [0.982]
 [0.9  ]]
actor:  1 policy actor:  1  step number:  64 total reward:  0.20666666666666578  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2001399999999999 0.6866666666666668 0.6866666666666668
printing an ep nov before normalisation:  44.41159248352051
maxi score, test score, baseline:  0.2001399999999999 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2001399999999999 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7698272
maxi score, test score, baseline:  0.2001399999999999 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  56.399056573474425
maxi score, test score, baseline:  0.2001399999999999 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 45.486367563190555
actor:  1 policy actor:  1  step number:  56 total reward:  0.3266666666666659  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  67.52347054627172
maxi score, test score, baseline:  0.2001399999999999 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.2280189086327
actor:  1 policy actor:  1  step number:  54 total reward:  0.4466666666666669  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  67.69900698478834
actor:  1 policy actor:  1  step number:  63 total reward:  0.3333333333333325  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.71826888268936
maxi score, test score, baseline:  0.2001399999999999 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.25217628479004
maxi score, test score, baseline:  0.2001399999999999 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.2001399999999999 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.112]
 [0.126]
 [0.114]
 [0.112]
 [0.112]
 [0.114]
 [0.112]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.112]
 [0.126]
 [0.114]
 [0.112]
 [0.112]
 [0.114]
 [0.112]]
printing an ep nov before normalisation:  59.03113901883363
maxi score, test score, baseline:  0.2001399999999999 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.2001399999999999 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.25065106410754
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.679439067840576
actor:  0 policy actor:  0  step number:  45 total reward:  0.3199999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1993933333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.676908671243154
maxi score, test score, baseline:  0.1993933333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1993933333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1993933333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.66053319797386
actor:  1 policy actor:  1  step number:  49 total reward:  0.3599999999999993  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.1993933333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1993933333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  39 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19888666666666655 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.41023752864328
maxi score, test score, baseline:  0.19888666666666655 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.19888666666666655 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]
 [0.689]] [[59.813]
 [59.813]
 [59.813]
 [59.813]
 [59.813]
 [59.813]
 [59.813]] [[1.91]
 [1.91]
 [1.91]
 [1.91]
 [1.91]
 [1.91]
 [1.91]]
maxi score, test score, baseline:  0.19888666666666655 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.0324233641561
printing an ep nov before normalisation:  52.32981242251361
actor:  0 policy actor:  0  step number:  52 total reward:  0.2999999999999996  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.614]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]
 [0.52 ]] [[35.801]
 [39.922]
 [35.801]
 [35.801]
 [35.801]
 [35.801]
 [35.801]] [[1.48 ]
 [1.789]
 [1.48 ]
 [1.48 ]
 [1.48 ]
 [1.48 ]
 [1.48 ]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.2533333333333332  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.223]
 [0.254]
 [0.223]
 [0.223]
 [0.223]
 [0.223]
 [0.223]] [[48.507]
 [61.871]
 [48.507]
 [48.507]
 [48.507]
 [48.507]
 [48.507]] [[0.703]
 [0.992]
 [0.703]
 [0.703]
 [0.703]
 [0.703]
 [0.703]]
actor:  1 policy actor:  1  step number:  56 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1980999999999999 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.97278114929703
printing an ep nov before normalisation:  49.56253452652084
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1980999999999999 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.50670173545129
printing an ep nov before normalisation:  46.28897730264376
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actor:  0 policy actor:  0  step number:  34 total reward:  0.5533333333333335  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.19781999999999988 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.528582237932277
printing an ep nov before normalisation:  25.112745545173855
actor:  1 policy actor:  1  step number:  46 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  55 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.19781999999999988 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19781999999999988 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.19781999999999988 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.19781999999999988 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.19781999999999988 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19781999999999988 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19781999999999988 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.19781999999999988 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19781999999999988 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  43 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.667
siam score:  -0.7661073
maxi score, test score, baseline:  0.19728666666666655 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  49.546060225580824
printing an ep nov before normalisation:  46.992845500806375
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.466]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]] [[45.382]
 [51.679]
 [45.382]
 [45.382]
 [45.382]
 [45.382]
 [45.382]] [[1.549]
 [1.807]
 [1.549]
 [1.549]
 [1.549]
 [1.549]
 [1.549]]
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]] [[21.263]
 [21.263]
 [21.263]
 [21.263]
 [21.263]
 [21.263]
 [21.263]] [[0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]]
printing an ep nov before normalisation:  42.61077301422386
maxi score, test score, baseline:  0.19728666666666655 0.6866666666666668 0.6866666666666668
printing an ep nov before normalisation:  43.777084665408324
printing an ep nov before normalisation:  36.029716628005964
printing an ep nov before normalisation:  50.373215163596726
maxi score, test score, baseline:  0.19728666666666655 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.014334090657825982
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  60 total reward:  0.29999999999999916  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  53 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]] [[47.604]
 [47.604]
 [47.604]
 [47.604]
 [47.604]
 [47.604]
 [47.604]] [[0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]
 [0.559]]
actor:  0 policy actor:  0  step number:  66 total reward:  0.16666666666666574  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.19528666666666655 0.6866666666666668 0.6866666666666668
printing an ep nov before normalisation:  52.24474669761148
maxi score, test score, baseline:  0.19528666666666655 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.19528666666666655 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.19943235806164
maxi score, test score, baseline:  0.19528666666666655 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.19528666666666655 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.10772636606712
maxi score, test score, baseline:  0.19528666666666655 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.19528666666666655 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76979846
actor:  0 policy actor:  0  step number:  40 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.19480666666666652 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 42.75979896315833
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.091]
 [0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]] [[41.238]
 [55.282]
 [41.238]
 [41.238]
 [41.238]
 [41.238]
 [41.238]] [[1.131]
 [1.624]
 [1.131]
 [1.131]
 [1.131]
 [1.131]
 [1.131]]
rdn beta is 0 so we're just using the maxi policy
actor:  1 policy actor:  1  step number:  45 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  48 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  44.81354489337202
printing an ep nov before normalisation:  46.47392797910704
maxi score, test score, baseline:  0.19400666666666655 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]] [[44.559]
 [44.559]
 [44.559]
 [44.559]
 [44.559]
 [44.559]
 [44.559]] [[1.235]
 [1.235]
 [1.235]
 [1.235]
 [1.235]
 [1.235]
 [1.235]]
printing an ep nov before normalisation:  50.72775424444361
maxi score, test score, baseline:  0.19400666666666655 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.451075321509315
actor:  1 policy actor:  1  step number:  39 total reward:  0.4933333333333332  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.19400666666666655 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.3066666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.19400666666666655 0.6866666666666668 0.6866666666666668
actor:  1 policy actor:  1  step number:  42 total reward:  0.5400000000000003  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]
 [0.069]] [[44.709]
 [44.709]
 [44.709]
 [44.709]
 [44.709]
 [44.709]
 [44.709]] [[89.486]
 [89.486]
 [89.486]
 [89.486]
 [89.486]
 [89.486]
 [89.486]]
maxi score, test score, baseline:  0.19400666666666655 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.19400666666666655 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19400666666666655 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19400666666666655 0.6866666666666668 0.6866666666666668
line 256 mcts: sample exp_bonus 50.318017028308965
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  49.32119308875058
printing an ep nov before normalisation:  53.10922717082247
maxi score, test score, baseline:  0.19400666666666655 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19400666666666655 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  39 total reward:  0.43999999999999984  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1935133333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.735]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]] [[45.002]
 [46.486]
 [45.002]
 [45.002]
 [45.002]
 [45.002]
 [45.002]] [[1.873]
 [2.124]
 [1.873]
 [1.873]
 [1.873]
 [1.873]
 [1.873]]
maxi score, test score, baseline:  0.1935133333333332 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.1935133333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.498]
 [0.5  ]
 [0.491]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]] [[13.52 ]
 [12.526]
 [14.576]
 [12.526]
 [12.526]
 [12.526]
 [12.526]] [[1.642]
 [1.561]
 [1.725]
 [1.561]
 [1.561]
 [1.561]
 [1.561]]
printing an ep nov before normalisation:  42.28817172025216
printing an ep nov before normalisation:  23.026709269920648
actor:  1 policy actor:  1  step number:  66 total reward:  0.27333333333333265  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1935133333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.30666666666666664  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  39.474424138794184
printing an ep nov before normalisation:  49.0711144713431
maxi score, test score, baseline:  0.1935133333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.30810269120707
actions average: 
K:  0  action  0 :  tensor([0.2471, 0.0034, 0.1658, 0.1506, 0.1553, 0.1119, 0.1659],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0080, 0.9430, 0.0058, 0.0027, 0.0019, 0.0019, 0.0367],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1145, 0.0440, 0.3544, 0.1205, 0.1212, 0.1074, 0.1379],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1385, 0.0025, 0.1446, 0.3338, 0.1381, 0.1114, 0.1311],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0768, 0.0012, 0.0516, 0.0388, 0.7488, 0.0413, 0.0414],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0791, 0.0007, 0.1096, 0.0674, 0.0734, 0.6035, 0.0662],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1514, 0.0063, 0.1796, 0.1686, 0.1591, 0.1317, 0.2033],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  39.86436367034912
maxi score, test score, baseline:  0.1935133333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1935133333333332 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.1935133333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.1935133333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1935133333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1935133333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  0.5400000000000001  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1935133333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1935133333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1935133333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.67110030886953
printing an ep nov before normalisation:  51.802199686418504
maxi score, test score, baseline:  0.1935133333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.600697807189256
printing an ep nov before normalisation:  43.98580115747835
siam score:  -0.7691724
printing an ep nov before normalisation:  46.581276116959984
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  39 total reward:  0.5733333333333335  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  44 total reward:  0.3799999999999999  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.19291333333333321 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19291333333333321 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.666]
 [0.514]
 [0.514]
 [0.514]
 [0.514]
 [0.514]] [[31.147]
 [46.111]
 [31.147]
 [31.147]
 [31.147]
 [31.147]
 [31.147]] [[0.725]
 [1.162]
 [0.725]
 [0.725]
 [0.725]
 [0.725]
 [0.725]]
siam score:  -0.77430326
printing an ep nov before normalisation:  43.06974617938608
maxi score, test score, baseline:  0.19291333333333321 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.53321845812166
Printing some Q and Qe and total Qs values:  [[0.634]
 [0.678]
 [0.646]
 [0.637]
 [0.642]
 [0.633]
 [0.641]] [[35.115]
 [34.316]
 [34.812]
 [34.238]
 [35.036]
 [34.579]
 [35.535]] [[0.634]
 [0.678]
 [0.646]
 [0.637]
 [0.642]
 [0.633]
 [0.641]]
maxi score, test score, baseline:  0.19291333333333321 0.6866666666666668 0.6866666666666668
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.417]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]] [[27.413]
 [31.512]
 [27.413]
 [27.413]
 [27.413]
 [27.413]
 [27.413]] [[1.091]
 [1.294]
 [1.091]
 [1.091]
 [1.091]
 [1.091]
 [1.091]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1895799999999999 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.1895799999999999 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.254678335469386
printing an ep nov before normalisation:  42.29499502092312
maxi score, test score, baseline:  0.1895799999999999 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.42945347049772
maxi score, test score, baseline:  0.1895799999999999 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.2866666666666662  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1895799999999999 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76726425
actions average: 
K:  3  action  0 :  tensor([0.2761, 0.0644, 0.0988, 0.1393, 0.1295, 0.0927, 0.1993],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0061, 0.9141, 0.0062, 0.0282, 0.0042, 0.0045, 0.0367],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1224, 0.0179, 0.3676, 0.1382, 0.1113, 0.1168, 0.1259],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1335, 0.0269, 0.0702, 0.3547, 0.1074, 0.0641, 0.2432],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1863, 0.0279, 0.1064, 0.1172, 0.3430, 0.0993, 0.1200],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0856, 0.0669, 0.1204, 0.1183, 0.0895, 0.3831, 0.1363],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1704, 0.0095, 0.1481, 0.2031, 0.1718, 0.1613, 0.1358],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1895799999999999 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  45 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18893999999999986 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.18893999999999986 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3999999999999996  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.782]
 [0.918]
 [0.766]
 [0.776]
 [0.774]
 [0.767]
 [0.769]] [[37.704]
 [37.458]
 [38.1  ]
 [38.352]
 [38.085]
 [38.055]
 [39.236]] [[0.782]
 [0.918]
 [0.766]
 [0.776]
 [0.774]
 [0.767]
 [0.769]]
maxi score, test score, baseline:  0.18893999999999986 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18893999999999986 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18893999999999986 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.149]
 [0.182]
 [0.151]
 [0.122]
 [0.172]
 [0.125]
 [0.163]] [[36.061]
 [36.981]
 [37.039]
 [28.355]
 [37.848]
 [28.212]
 [35.637]] [[0.382]
 [0.425]
 [0.395]
 [0.275]
 [0.424]
 [0.277]
 [0.392]]
printing an ep nov before normalisation:  34.57571744918823
maxi score, test score, baseline:  0.18893999999999986 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  51 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18905999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.236239011466665
actor:  1 policy actor:  1  step number:  59 total reward:  0.13333333333333264  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  40.15040600706691
Printing some Q and Qe and total Qs values:  [[0.895]
 [0.898]
 [0.897]
 [0.897]
 [0.894]
 [0.896]
 [0.917]] [[39.005]
 [43.633]
 [38.835]
 [39.096]
 [39.458]
 [38.968]
 [40.072]] [[0.895]
 [0.898]
 [0.897]
 [0.897]
 [0.894]
 [0.896]
 [0.917]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  44 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1898333333333332 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.1898333333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1898333333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  0.1898333333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1898333333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.7545, 0.0039, 0.0416, 0.0404, 0.0770, 0.0303, 0.0524],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0035,     0.9796,     0.0024,     0.0015,     0.0004,     0.0002,
            0.0123], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0949, 0.0219, 0.3727, 0.1006, 0.1124, 0.1789, 0.1187],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1230, 0.0266, 0.1162, 0.3118, 0.1379, 0.1258, 0.1587],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0687, 0.0014, 0.0540, 0.0571, 0.6949, 0.0485, 0.0754],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1153, 0.0297, 0.1542, 0.1213, 0.1388, 0.3202, 0.1203],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0939, 0.2105, 0.0945, 0.0854, 0.0973, 0.0835, 0.3349],
       grad_fn=<DivBackward0>)
actions average: 
K:  3  action  0 :  tensor([nan, nan, nan, nan, nan, nan, nan], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0140, 0.9188, 0.0098, 0.0086, 0.0043, 0.0041, 0.0404],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1217, 0.0993, 0.2769, 0.1136, 0.1146, 0.1379, 0.1360],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1391, 0.0058, 0.1335, 0.2959, 0.1368, 0.1025, 0.1865],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1187, 0.0102, 0.0963, 0.1324, 0.4314, 0.0981, 0.1129],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0910, 0.0050, 0.1937, 0.0947, 0.1192, 0.3958, 0.1005],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1052, 0.1505, 0.1188, 0.1038, 0.1180, 0.1032, 0.3004],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  59 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  39 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.19016666666666654 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18732666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18732666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.1103105522639
actor:  1 policy actor:  1  step number:  50 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18732666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.969]
 [1.005]
 [0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]] [[51.938]
 [51.244]
 [51.938]
 [51.938]
 [51.938]
 [51.938]
 [51.938]] [[0.969]
 [1.005]
 [0.969]
 [0.969]
 [0.969]
 [0.969]
 [0.969]]
maxi score, test score, baseline:  0.18732666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.085]
 [-0.069]
 [ 0.224]
 [ 0.064]
 [-0.014]
 [ 0.159]
 [-0.003]] [[59.033]
 [51.892]
 [52.848]
 [62.782]
 [58.64 ]
 [58.029]
 [55.653]] [[1.092]
 [0.68 ]
 [1.007]
 [1.207]
 [0.979]
 [1.13 ]
 [0.882]]
maxi score, test score, baseline:  0.18732666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.595798200265726
maxi score, test score, baseline:  0.18732666666666656 0.6866666666666668 0.6866666666666668
actor:  0 policy actor:  0  step number:  44 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  0.026407080584931464
printing an ep nov before normalisation:  32.09206376756941
Printing some Q and Qe and total Qs values:  [[0.16 ]
 [0.235]
 [0.166]
 [0.161]
 [0.16 ]
 [0.159]
 [0.164]] [[33.346]
 [44.605]
 [33.359]
 [33.682]
 [33.632]
 [33.632]
 [38.477]] [[0.43 ]
 [0.714]
 [0.436]
 [0.437]
 [0.435]
 [0.434]
 [0.529]]
maxi score, test score, baseline:  0.18715333333333323 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.47324905505655
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.512]
 [0.462]
 [0.462]
 [0.474]
 [0.454]
 [0.468]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.451]
 [0.512]
 [0.462]
 [0.462]
 [0.474]
 [0.454]
 [0.468]]
maxi score, test score, baseline:  0.18715333333333323 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18715333333333323 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18715333333333323 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18496666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.175845092385934
actor:  1 policy actor:  1  step number:  57 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  58 total reward:  0.23333333333333295  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  52.89642207015835
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.644]
 [0.572]
 [0.572]
 [0.572]
 [0.572]
 [0.572]] [[50.17 ]
 [51.723]
 [50.17 ]
 [50.17 ]
 [50.17 ]
 [50.17 ]
 [50.17 ]] [[2.13 ]
 [2.282]
 [2.13 ]
 [2.13 ]
 [2.13 ]
 [2.13 ]
 [2.13 ]]
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.716]
 [0.626]
 [0.626]
 [0.626]
 [0.626]
 [0.646]] [[55.763]
 [48.326]
 [55.763]
 [55.763]
 [55.763]
 [55.763]
 [55.701]] [[2.47 ]
 [2.179]
 [2.47 ]
 [2.47 ]
 [2.47 ]
 [2.47 ]
 [2.487]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18496666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76826745
maxi score, test score, baseline:  0.18496666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.18496666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  56.41701290290147
printing an ep nov before normalisation:  51.60940450855633
printing an ep nov before normalisation:  34.9929043288301
Printing some Q and Qe and total Qs values:  [[0.346]
 [0.319]
 [0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]] [[41.038]
 [42.245]
 [41.038]
 [41.038]
 [41.038]
 [41.038]
 [41.038]] [[1.694]
 [1.736]
 [1.694]
 [1.694]
 [1.694]
 [1.694]
 [1.694]]
maxi score, test score, baseline:  0.18496666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18496666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7718858
actions average: 
K:  1  action  0 :  tensor([0.2963, 0.0068, 0.1242, 0.1483, 0.1298, 0.1552, 0.1395],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0073, 0.9180, 0.0059, 0.0230, 0.0051, 0.0060, 0.0348],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1258, 0.0040, 0.3579, 0.1313, 0.1039, 0.1431, 0.1341],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0872, 0.0054, 0.0935, 0.4475, 0.1119, 0.0950, 0.1595],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1265, 0.0043, 0.1031, 0.1370, 0.3791, 0.1242, 0.1258],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0908, 0.0011, 0.1200, 0.1043, 0.0778, 0.5080, 0.0981],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1288, 0.0675, 0.1052, 0.1118, 0.1016, 0.1048, 0.3804],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  30.174789428710938
siam score:  -0.7713591
siam score:  -0.77395785
maxi score, test score, baseline:  0.1823533333333332 0.6866666666666668 0.6866666666666668
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.19040680808767
actor:  1 policy actor:  1  step number:  49 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  53.16745834625652
printing an ep nov before normalisation:  50.01068357091173
Printing some Q and Qe and total Qs values:  [[0.173]
 [0.153]
 [0.175]
 [0.175]
 [0.182]
 [0.181]
 [0.181]] [[45.065]
 [44.066]
 [45.846]
 [45.997]
 [44.509]
 [44.968]
 [44.378]] [[1.707]
 [1.624]
 [1.757]
 [1.766]
 [1.681]
 [1.709]
 [1.672]]
maxi score, test score, baseline:  0.1823533333333332 0.6866666666666668 0.6866666666666668
actor:  1 policy actor:  1  step number:  56 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1823533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.52556552506973
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]] [[52.457]
 [52.457]
 [52.457]
 [52.457]
 [52.457]
 [52.457]
 [52.457]] [[2.302]
 [2.302]
 [2.302]
 [2.302]
 [2.302]
 [2.302]
 [2.302]]
siam score:  -0.76695234
maxi score, test score, baseline:  0.1823533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.946796344635594
maxi score, test score, baseline:  0.1823533333333332 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.1823533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  64 total reward:  0.29999999999999927  reward:  1.0 rdn_beta:  1.667
siam score:  -0.76163816
maxi score, test score, baseline:  0.1823533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.767757415771484
maxi score, test score, baseline:  0.1823533333333332 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.1823533333333332 0.6866666666666668 0.6866666666666668
printing an ep nov before normalisation:  48.19457977249137
actor:  1 policy actor:  1  step number:  69 total reward:  0.06666666666666576  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1823533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.44666666666666643  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.461]
 [0.442]
 [0.443]
 [0.442]
 [0.444]
 [0.441]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.441]
 [0.461]
 [0.442]
 [0.443]
 [0.442]
 [0.444]
 [0.441]]
maxi score, test score, baseline:  0.1823533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.68502980650268
maxi score, test score, baseline:  0.1823533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1823533333333332 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.1823533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  73 total reward:  0.13333333333333242  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1823533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 46.306931398714454
printing an ep nov before normalisation:  54.440871638774894
maxi score, test score, baseline:  0.1823533333333332 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  61 total reward:  0.2666666666666667  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.161]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.161]
 [0.161]]
actor:  0 policy actor:  0  step number:  54 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18236666666666654 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.7714958190918
Printing some Q and Qe and total Qs values:  [[0.856]
 [0.883]
 [0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.856]
 [0.883]
 [0.856]
 [0.856]
 [0.856]
 [0.856]
 [0.856]]
maxi score, test score, baseline:  0.18236666666666654 0.6866666666666668 0.6866666666666668
printing an ep nov before normalisation:  39.7765587941656
printing an ep nov before normalisation:  34.808472224644255
maxi score, test score, baseline:  0.18236666666666654 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.18236666666666654 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18236666666666654 0.6866666666666668 0.6866666666666668
actor:  0 policy actor:  0  step number:  45 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  67 total reward:  0.21333333333333293  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18217999999999987 0.6866666666666668 0.6866666666666668
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  69.7511475343424
maxi score, test score, baseline:  0.18217999999999987 0.6866666666666668 0.6866666666666668
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.248]
 [0.177]
 [0.173]
 [0.172]
 [0.175]
 [0.102]] [[49.884]
 [50.561]
 [51.582]
 [51.717]
 [51.538]
 [51.537]
 [52.178]] [[1.427]
 [1.595]
 [1.576]
 [1.579]
 [1.569]
 [1.571]
 [1.531]]
printing an ep nov before normalisation:  57.349705589097326
maxi score, test score, baseline:  0.18217999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.626]
 [0.585]
 [0.615]
 [0.615]
 [0.615]
 [0.615]] [[44.246]
 [42.629]
 [43.7  ]
 [48.327]
 [48.327]
 [48.327]
 [48.327]] [[2.006]
 [1.97 ]
 [1.989]
 [2.278]
 [2.278]
 [2.278]
 [2.278]]
maxi score, test score, baseline:  0.18217999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.1599999999999996  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  40 total reward:  0.4733333333333333  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  45 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18512666666666652 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.18512666666666652 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18512666666666652 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.555568695789724
maxi score, test score, baseline:  0.18512666666666652 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18512666666666652 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.7611866953211
Printing some Q and Qe and total Qs values:  [[0.46]
 [0.55]
 [0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]] [[27.984]
 [59.953]
 [27.984]
 [27.984]
 [27.984]
 [27.984]
 [27.984]] [[0.576]
 [0.835]
 [0.576]
 [0.576]
 [0.576]
 [0.576]
 [0.576]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.3133333333333326  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18512666666666652 0.6866666666666668 0.6866666666666668
actor:  0 policy actor:  0  step number:  40 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 69.92490279922157
printing an ep nov before normalisation:  46.47136688232422
Printing some Q and Qe and total Qs values:  [[0.405]
 [0.546]
 [0.405]
 [0.405]
 [0.405]
 [0.405]
 [0.405]] [[39.925]
 [49.164]
 [39.925]
 [39.925]
 [39.925]
 [39.925]
 [39.925]] [[0.742]
 [1.051]
 [0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.742]]
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.435]
 [0.435]
 [0.435]
 [0.412]
 [0.435]
 [0.435]] [[54.951]
 [50.387]
 [50.387]
 [50.387]
 [49.259]
 [50.387]
 [50.387]] [[0.745]
 [0.693]
 [0.693]
 [0.693]
 [0.662]
 [0.693]
 [0.693]]
maxi score, test score, baseline:  0.18817999999999985 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.64270647546157
maxi score, test score, baseline:  0.18817999999999985 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.422]
 [ 0.43 ]
 [-0.088]
 [ 0.278]
 [ 0.351]
 [-0.025]
 [ 0.413]] [[38.453]
 [38.731]
 [34.702]
 [35.025]
 [37.772]
 [35.249]
 [36.425]] [[0.672]
 [0.683]
 [0.117]
 [0.487]
 [0.592]
 [0.186]
 [0.638]]
printing an ep nov before normalisation:  58.446799859298125
printing an ep nov before normalisation:  52.12759741223803
printing an ep nov before normalisation:  51.20054228287351
Printing some Q and Qe and total Qs values:  [[0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]] [[45.153]
 [45.153]
 [45.153]
 [45.153]
 [45.153]
 [45.153]
 [45.153]] [[1.703]
 [1.703]
 [1.703]
 [1.703]
 [1.703]
 [1.703]
 [1.703]]
maxi score, test score, baseline:  0.18817999999999985 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.03130379806601
printing an ep nov before normalisation:  0.00015952914054651046
line 256 mcts: sample exp_bonus 40.74509508081316
actor:  1 policy actor:  1  step number:  46 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  51 total reward:  0.2666666666666665  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18817999999999985 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18817999999999985 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18817999999999985 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18817999999999985 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18817999999999985 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.04511668761209
maxi score, test score, baseline:  0.18817999999999985 0.6866666666666668 0.6866666666666668
printing an ep nov before normalisation:  62.293867676357046
Printing some Q and Qe and total Qs values:  [[0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.609]
 [0.652]
 [0.609]] [[45.881]
 [45.881]
 [45.881]
 [45.881]
 [45.881]
 [42.512]
 [45.881]] [[1.942]
 [1.942]
 [1.942]
 [1.942]
 [1.942]
 [1.836]
 [1.942]]
actor:  1 policy actor:  1  step number:  43 total reward:  0.46666666666666656  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18817999999999985 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.627]
 [0.549]
 [0.549]
 [0.549]
 [0.549]
 [0.549]] [[33.157]
 [46.25 ]
 [33.157]
 [33.157]
 [33.157]
 [33.157]
 [33.157]] [[1.067]
 [1.62 ]
 [1.067]
 [1.067]
 [1.067]
 [1.067]
 [1.067]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.097]
 [0.215]
 [0.097]
 [0.097]
 [0.097]
 [0.097]
 [0.097]] [[33.726]
 [43.402]
 [33.726]
 [33.726]
 [33.726]
 [33.726]
 [33.726]] [[0.529]
 [0.986]
 [0.529]
 [0.529]
 [0.529]
 [0.529]
 [0.529]]
maxi score, test score, baseline:  0.18817999999999985 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  41 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18817999999999985 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18553999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.38 ]
 [ 0.464]
 [-0.005]
 [ 0.351]
 [ 0.396]
 [ 0.408]
 [ 0.398]] [[34.985]
 [48.771]
 [32.39 ]
 [34.188]
 [37.416]
 [39.774]
 [39.303]] [[0.676]
 [0.975]
 [0.25 ]
 [0.634]
 [0.729]
 [0.778]
 [0.761]]
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]
 [0.106]] [[45.522]
 [45.522]
 [45.522]
 [45.522]
 [45.522]
 [45.522]
 [45.522]] [[0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.436]
 [0.436]]
actor:  1 policy actor:  1  step number:  63 total reward:  0.13333333333333275  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18553999999999987 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.18553999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.135]
 [0.254]
 [0.131]
 [0.135]
 [0.137]
 [0.135]
 [0.145]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.135]
 [0.254]
 [0.131]
 [0.135]
 [0.137]
 [0.135]
 [0.145]]
maxi score, test score, baseline:  0.18553999999999987 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.18553999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  0.0799999999999993  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  47 total reward:  0.3999999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18553999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18553999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.74 ]
 [0.884]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]
 [0.74 ]] [[59.643]
 [62.19 ]
 [59.643]
 [59.643]
 [59.643]
 [59.643]
 [59.643]] [[1.023]
 [1.184]
 [1.023]
 [1.023]
 [1.023]
 [1.023]
 [1.023]]
maxi score, test score, baseline:  0.18553999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.61795551982604
maxi score, test score, baseline:  0.18553999999999987 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  37 total reward:  0.4933333333333332  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18852666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18852666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.2236, 0.0458, 0.1534, 0.1376, 0.1280, 0.1465, 0.1651],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0060, 0.9581, 0.0052, 0.0042, 0.0018, 0.0019, 0.0228],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0574, 0.0034, 0.5833, 0.0996, 0.0652, 0.1082, 0.0830],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1278, 0.0059, 0.1409, 0.2552, 0.1694, 0.1654, 0.1354],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0777, 0.0010, 0.0689, 0.0631, 0.6576, 0.0671, 0.0647],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1037, 0.0026, 0.1322, 0.1311, 0.1025, 0.4020, 0.1259],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0950, 0.0653, 0.1065, 0.1295, 0.1038, 0.1213, 0.3785],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.18852666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.35382318496704
printing an ep nov before normalisation:  32.423575565544255
actor:  1 policy actor:  1  step number:  53 total reward:  0.41333333333333333  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.18852666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.95681727301149
printing an ep nov before normalisation:  31.86492084644444
maxi score, test score, baseline:  0.18852666666666656 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.18852666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.368]
 [0.393]
 [0.368]
 [0.368]
 [0.368]
 [0.385]
 [0.368]] [[41.375]
 [44.823]
 [41.375]
 [41.375]
 [41.375]
 [41.112]
 [41.375]] [[1.12 ]
 [1.251]
 [1.12 ]
 [1.12 ]
 [1.12 ]
 [1.128]
 [1.12 ]]
printing an ep nov before normalisation:  54.107191038190706
siam score:  -0.7598787
maxi score, test score, baseline:  0.18852666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.19141985328104
actor:  1 policy actor:  1  step number:  51 total reward:  0.2666666666666664  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18852666666666656 0.6866666666666668 0.6866666666666668
maxi score, test score, baseline:  0.18852666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18852666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.34716876893224
maxi score, test score, baseline:  0.18852666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.745582335260984
maxi score, test score, baseline:  0.18852666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.14517212531795
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]] [[10.464]
 [10.464]
 [10.464]
 [10.464]
 [10.464]
 [10.464]
 [10.464]] [[11.147]
 [11.147]
 [11.147]
 [11.147]
 [11.147]
 [11.147]
 [11.147]]
actor:  1 policy actor:  1  step number:  48 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.18852666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.347717899993306
Starting evaluation
maxi score, test score, baseline:  0.18852666666666656 0.6866666666666668 0.6866666666666668
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.478]
 [0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]] [[44.565]
 [50.206]
 [40.831]
 [40.831]
 [40.831]
 [40.831]
 [40.831]] [[0.859]
 [0.996]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]
 [0.732]] [[38.479]
 [38.479]
 [38.479]
 [38.479]
 [38.479]
 [38.479]
 [38.479]] [[1.299]
 [1.299]
 [1.299]
 [1.299]
 [1.299]
 [1.299]
 [1.299]]
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]] [[33.655]
 [33.655]
 [33.655]
 [33.655]
 [33.655]
 [33.655]
 [33.655]] [[0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]]
line 256 mcts: sample exp_bonus 35.97993831271351
printing an ep nov before normalisation:  41.40481396447202
printing an ep nov before normalisation:  38.08337328393957
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.18852666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18852666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]] [[47.062]
 [47.062]
 [47.062]
 [47.062]
 [47.062]
 [47.062]
 [47.062]] [[0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]]
printing an ep nov before normalisation:  40.496006167464145
printing an ep nov before normalisation:  36.14648133011108
printing an ep nov before normalisation:  35.100813967974034
printing an ep nov before normalisation:  39.30174630636926
Printing some Q and Qe and total Qs values:  [[0.966]
 [0.966]
 [0.966]
 [0.966]
 [0.966]
 [0.966]
 [0.966]] [[38.795]
 [38.795]
 [38.795]
 [38.795]
 [38.795]
 [38.795]
 [38.795]] [[0.966]
 [0.966]
 [0.966]
 [0.966]
 [0.966]
 [0.966]
 [0.966]]
printing an ep nov before normalisation:  45.11130514229769
Printing some Q and Qe and total Qs values:  [[0.977]
 [1.001]
 [0.977]
 [0.977]
 [0.977]
 [0.977]
 [0.977]] [[35.318]
 [37.47 ]
 [35.318]
 [35.318]
 [35.318]
 [35.318]
 [35.318]] [[0.977]
 [1.001]
 [0.977]
 [0.977]
 [0.977]
 [0.977]
 [0.977]]
maxi score, test score, baseline:  0.18852666666666656 0.6866666666666668 0.6866666666666668
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.718]
 [0.848]
 [0.722]
 [0.725]
 [0.722]
 [0.729]
 [0.796]] [[36.773]
 [34.344]
 [35.82 ]
 [36.741]
 [36.3  ]
 [35.609]
 [35.988]] [[0.718]
 [0.848]
 [0.722]
 [0.725]
 [0.722]
 [0.729]
 [0.796]]
printing an ep nov before normalisation:  35.514914247445304
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  2.9753264055580075
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6733333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.658]
 [0.656]
 [0.662]
 [0.659]
 [0.659]
 [0.658]] [[44.283]
 [41.87 ]
 [23.713]
 [41.71 ]
 [39.316]
 [40.389]
 [45.349]] [[2.532]
 [2.43 ]
 [1.659]
 [2.427]
 [2.322]
 [2.368]
 [2.577]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  0  action  0 :  tensor([0.4690, 0.0024, 0.1171, 0.0896, 0.1181, 0.0962, 0.1075],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0072, 0.9367, 0.0068, 0.0055, 0.0018, 0.0018, 0.0402],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0936, 0.0017, 0.4183, 0.0926, 0.0919, 0.1888, 0.1132],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1025, 0.0258, 0.1282, 0.3700, 0.1103, 0.1170, 0.1463],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1265, 0.0011, 0.1535, 0.1290, 0.3091, 0.1422, 0.1387],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0996, 0.0128, 0.1386, 0.1263, 0.1112, 0.3899, 0.1217],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1424, 0.0129, 0.1326, 0.1353, 0.1217, 0.1192, 0.3359],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.21672666666666654 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.14666666666666606  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  64 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  40.16852041590522
maxi score, test score, baseline:  0.21672666666666654 0.6870000000000002 0.6870000000000002
printing an ep nov before normalisation:  46.05909516310129
maxi score, test score, baseline:  0.21672666666666654 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.21672666666666654 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21672666666666654 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.059]
 [ 0.344]
 [ 0.319]
 [ 0.242]
 [-0.092]
 [ 0.364]
 [ 0.282]] [[37.849]
 [34.324]
 [35.423]
 [35.059]
 [36.773]
 [34.265]
 [34.616]] [[0.957]
 [1.184]
 [1.214]
 [1.119]
 [0.871]
 [1.201]
 [1.136]]
printing an ep nov before normalisation:  53.16201351566626
maxi score, test score, baseline:  0.21672666666666654 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  0.43999999999999995  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21672666666666654 0.6870000000000002 0.6870000000000002
printing an ep nov before normalisation:  42.278700732356114
maxi score, test score, baseline:  0.21672666666666654 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.337]
 [0.387]
 [0.337]
 [0.337]
 [0.337]
 [0.337]
 [0.337]] [[37.945]
 [42.269]
 [37.945]
 [37.945]
 [37.945]
 [37.945]
 [37.945]] [[0.674]
 [0.802]
 [0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]]
maxi score, test score, baseline:  0.21672666666666654 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21672666666666654 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21672666666666654 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21672666666666654 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  70 total reward:  0.20666666666666633  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  49.90567257922766
actor:  1 policy actor:  1  step number:  47 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  44.61752837778501
printing an ep nov before normalisation:  52.86745585354931
actor:  1 policy actor:  1  step number:  57 total reward:  0.173333333333333  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.436]
 [0.402]
 [0.402]
 [0.402]
 [0.402]
 [0.402]] [[51.93 ]
 [53.723]
 [51.93 ]
 [51.93 ]
 [51.93 ]
 [51.93 ]
 [51.93 ]] [[0.684]
 [0.736]
 [0.684]
 [0.684]
 [0.684]
 [0.684]
 [0.684]]
printing an ep nov before normalisation:  36.951567288334005
Printing some Q and Qe and total Qs values:  [[0.3  ]
 [0.356]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]] [[37.355]
 [39.464]
 [40.129]
 [40.129]
 [40.129]
 [40.129]
 [40.129]] [[0.3  ]
 [0.356]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]]
maxi score, test score, baseline:  0.21672666666666654 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.2129415479235
printing an ep nov before normalisation:  54.21191674384879
maxi score, test score, baseline:  0.21672666666666654 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.635571799762026
maxi score, test score, baseline:  0.21672666666666654 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.14666666666666628  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  41.511651181447114
maxi score, test score, baseline:  0.2140199999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2140199999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.77802645659463
printing an ep nov before normalisation:  37.66868052379559
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  77 total reward:  0.11999999999999911  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2140199999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2140199999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2140199999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2140199999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.321]
 [0.302]
 [0.307]
 [0.302]
 [0.302]
 [0.317]] [[41.532]
 [45.925]
 [43.147]
 [41.299]
 [43.147]
 [43.147]
 [44.806]] [[0.91 ]
 [1.031]
 [0.936]
 [0.89 ]
 [0.936]
 [0.936]
 [0.997]]
maxi score, test score, baseline:  0.2140199999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 56.499035956589594
maxi score, test score, baseline:  0.2140199999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2140199999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.19999999999999973  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  56 total reward:  0.32666666666666655  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2140199999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.51108980178833
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2140199999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2140199999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77993715
maxi score, test score, baseline:  0.2140199999999999 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.2140199999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.84995870626878
actions average: 
K:  0  action  0 :  tensor([0.3301, 0.0263, 0.1097, 0.1146, 0.1719, 0.1081, 0.1393],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0018,     0.9707,     0.0016,     0.0018,     0.0002,     0.0002,
            0.0238], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1218, 0.0042, 0.3638, 0.1228, 0.1321, 0.1232, 0.1321],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1078, 0.0651, 0.1069, 0.2884, 0.1291, 0.1055, 0.1971],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1045, 0.0067, 0.0148, 0.0306, 0.8013, 0.0202, 0.0220],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1010, 0.0012, 0.1119, 0.1139, 0.1169, 0.4540, 0.1011],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1660, 0.2128, 0.0694, 0.0377, 0.0549, 0.0281, 0.4311],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2140199999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.4387, 0.0267, 0.1033, 0.0958, 0.1268, 0.0938, 0.1148],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0135, 0.8845, 0.0160, 0.0159, 0.0105, 0.0099, 0.0498],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1428, 0.0353, 0.1667, 0.1573, 0.1532, 0.2103, 0.1345],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1446, 0.0219, 0.1518, 0.1805, 0.1322, 0.2079, 0.1611],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1146, 0.0641, 0.0728, 0.1665, 0.4050, 0.0711, 0.1059],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1024, 0.0122, 0.1368, 0.1683, 0.1191, 0.3464, 0.1147],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1350, 0.1739, 0.1090, 0.1106, 0.1114, 0.1084, 0.2518],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2140199999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.07923220495906
actor:  0 policy actor:  0  step number:  51 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  47.54675890331245
printing an ep nov before normalisation:  41.158288825407276
maxi score, test score, baseline:  0.21397999999999986 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.21397999999999986 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.455]
 [0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]] [[34.857]
 [37.302]
 [34.857]
 [34.857]
 [34.857]
 [34.857]
 [34.857]] [[1.037]
 [1.215]
 [1.037]
 [1.037]
 [1.037]
 [1.037]
 [1.037]]
maxi score, test score, baseline:  0.21397999999999986 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  62 total reward:  0.2866666666666665  reward:  1.0 rdn_beta:  1.0
siam score:  -0.7810693
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.771]
 [0.522]
 [0.522]
 [0.522]
 [0.522]
 [0.522]] [[44.249]
 [46.154]
 [44.249]
 [44.249]
 [44.249]
 [44.249]
 [44.249]] [[2.21 ]
 [2.582]
 [2.21 ]
 [2.21 ]
 [2.21 ]
 [2.21 ]
 [2.21 ]]
maxi score, test score, baseline:  0.2137933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 50.18446451605107
actor:  1 policy actor:  1  step number:  53 total reward:  0.3466666666666661  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2137933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.092]
 [0.213]
 [0.092]
 [0.094]
 [0.093]
 [0.092]
 [0.092]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.092]
 [0.213]
 [0.092]
 [0.094]
 [0.093]
 [0.092]
 [0.092]]
maxi score, test score, baseline:  0.2137933333333332 0.6870000000000002 0.6870000000000002
actor:  1 policy actor:  1  step number:  56 total reward:  0.31333333333333313  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2137933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.838685294757944
printing an ep nov before normalisation:  37.337939739227295
printing an ep nov before normalisation:  44.554861467777066
maxi score, test score, baseline:  0.2137933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2137933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.355]
 [0.379]
 [0.355]
 [0.262]
 [0.265]
 [0.268]
 [0.267]] [[35.522]
 [35.843]
 [35.522]
 [26.924]
 [26.958]
 [27.053]
 [27.349]] [[1.519]
 [1.561]
 [1.519]
 [0.936]
 [0.941]
 [0.949]
 [0.966]]
printing an ep nov before normalisation:  44.443950812057935
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.611]
 [0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]] [[34.993]
 [44.64 ]
 [34.993]
 [34.993]
 [34.993]
 [34.993]
 [34.993]] [[1.573]
 [2.034]
 [1.573]
 [1.573]
 [1.573]
 [1.573]
 [1.573]]
printing an ep nov before normalisation:  34.91120868259006
maxi score, test score, baseline:  0.2137933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2137933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2137933333333332 0.6870000000000002 0.6870000000000002
actor:  1 policy actor:  1  step number:  62 total reward:  0.2866666666666663  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2137933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2137933333333332 0.6870000000000002 0.6870000000000002
actor:  1 policy actor:  1  step number:  52 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21115333333333322 0.6870000000000002 0.6870000000000002
Printing some Q and Qe and total Qs values:  [[0.316]
 [0.403]
 [0.316]
 [0.316]
 [0.316]
 [0.316]
 [0.316]] [[37.452]
 [38.17 ]
 [37.452]
 [37.452]
 [37.452]
 [37.452]
 [37.452]] [[0.965]
 [1.076]
 [0.965]
 [0.965]
 [0.965]
 [0.965]
 [0.965]]
maxi score, test score, baseline:  0.21115333333333322 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.68102008739448
maxi score, test score, baseline:  0.21115333333333322 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21115333333333322 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.21115333333333322 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.15226706634846
printing an ep nov before normalisation:  46.577215494150934
maxi score, test score, baseline:  0.21115333333333322 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21115333333333322 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.21115333333333322 0.6870000000000002 0.6870000000000002
actor:  0 policy actor:  0  step number:  40 total reward:  0.4199999999999998  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  1.0
siam score:  -0.7763855
maxi score, test score, baseline:  0.21097999999999986 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.629]
 [0.513]
 [0.513]
 [0.513]
 [0.513]
 [0.513]] [[48.974]
 [45.783]
 [48.974]
 [48.974]
 [48.974]
 [48.974]
 [48.974]] [[0.924]
 [0.996]
 [0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]]
maxi score, test score, baseline:  0.21097999999999986 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]] [[35.51]
 [35.51]
 [35.51]
 [35.51]
 [35.51]
 [35.51]
 [35.51]] [[0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]]
printing an ep nov before normalisation:  53.478991329624336
maxi score, test score, baseline:  0.21097999999999986 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  34 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  34.487066316521556
maxi score, test score, baseline:  0.21117999999999992 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.21117999999999992 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.24975037574768
maxi score, test score, baseline:  0.21117999999999992 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.21117999999999992 0.6870000000000002 0.6870000000000002
actor:  1 policy actor:  1  step number:  43 total reward:  0.4800000000000001  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.21117999999999992 0.6870000000000002 0.6870000000000002
using explorer policy with actor:  1
maxi score, test score, baseline:  0.21117999999999992 0.6870000000000002 0.6870000000000002
actor:  1 policy actor:  1  step number:  50 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21117999999999992 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.35409594802599
actor:  1 policy actor:  1  step number:  56 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  1.0
siam score:  -0.76655656
maxi score, test score, baseline:  0.21117999999999992 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  0.333
siam score:  -0.76526916
Printing some Q and Qe and total Qs values:  [[0.259]
 [0.363]
 [0.262]
 [0.26 ]
 [0.193]
 [0.278]
 [0.278]] [[45.196]
 [45.105]
 [44.262]
 [43.186]
 [39.239]
 [38.155]
 [38.155]] [[0.648]
 [0.751]
 [0.637]
 [0.618]
 [0.491]
 [0.559]
 [0.559]]
maxi score, test score, baseline:  0.21117999999999992 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  43.442459457651836
maxi score, test score, baseline:  0.21117999999999992 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7593484
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actor:  1 policy actor:  1  step number:  46 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  61 total reward:  0.2133333333333327  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.028]
 [0.028]
 [0.028]
 [0.054]
 [0.18 ]
 [0.028]] [[47.423]
 [47.423]
 [47.423]
 [47.423]
 [60.245]
 [52.682]
 [47.423]] [[0.501]
 [0.501]
 [0.501]
 [0.501]
 [0.913]
 [0.812]
 [0.501]]
printing an ep nov before normalisation:  60.94363060757211
printing an ep nov before normalisation:  33.655970096588135
printing an ep nov before normalisation:  49.478394933973725
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.43 ]
 [0.43 ]
 [0.43 ]
 [0.457]
 [0.43 ]
 [0.43 ]] [[44.179]
 [44.179]
 [44.179]
 [44.179]
 [48.842]
 [44.179]
 [44.179]] [[1.099]
 [1.099]
 [1.099]
 [1.099]
 [1.265]
 [1.099]
 [1.099]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.21999999999999942  reward:  1.0 rdn_beta:  1.667
siam score:  -0.7539325
printing an ep nov before normalisation:  54.513679845206774
maxi score, test score, baseline:  0.21117999999999992 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21117999999999992 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  50.49865429334987
actor:  1 policy actor:  1  step number:  56 total reward:  0.12666666666666604  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  58 total reward:  0.35333333333333317  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21117999999999992 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.45062999234664
maxi score, test score, baseline:  0.21117999999999992 0.6870000000000002 0.6870000000000002
Printing some Q and Qe and total Qs values:  [[0.966]
 [0.966]
 [0.966]
 [0.966]
 [0.966]
 [0.966]
 [0.966]] [[45.892]
 [45.892]
 [45.892]
 [45.892]
 [45.892]
 [45.892]
 [45.892]] [[0.966]
 [0.966]
 [0.966]
 [0.966]
 [0.966]
 [0.966]
 [0.966]]
actor:  0 policy actor:  0  step number:  45 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.21392666666666654 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.34454728953027
Printing some Q and Qe and total Qs values:  [[0.259]
 [0.259]
 [0.259]
 [0.259]
 [0.259]
 [0.259]
 [0.259]] [[61.425]
 [61.425]
 [61.425]
 [61.425]
 [61.425]
 [61.425]
 [61.425]] [[2.259]
 [2.259]
 [2.259]
 [2.259]
 [2.259]
 [2.259]
 [2.259]]
maxi score, test score, baseline:  0.21392666666666654 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.54817669530097
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.43456025006967
maxi score, test score, baseline:  0.21392666666666654 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21392666666666654 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.21392666666666654 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21392666666666654 0.6870000000000002 0.6870000000000002
printing an ep nov before normalisation:  38.3413910683613
maxi score, test score, baseline:  0.21392666666666657 0.6870000000000002 0.6870000000000002
printing an ep nov before normalisation:  31.49328132898657
maxi score, test score, baseline:  0.21392666666666657 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.10164697020673
maxi score, test score, baseline:  0.21392666666666657 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.814340171827645
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  48 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2133933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2133933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.85196157758507
maxi score, test score, baseline:  0.2133933333333332 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.2133933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.805295314081505
maxi score, test score, baseline:  0.2133933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.102 0.184 0.082 0.102 0.306 0.061 0.163]
actor:  1 policy actor:  1  step number:  72 total reward:  0.12666666666666582  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2133933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.55190335880942
maxi score, test score, baseline:  0.2133933333333332 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.2133933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3199999999999994  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  4  action  0 :  tensor([0.3240, 0.1452, 0.1134, 0.1066, 0.0838, 0.1089, 0.1182],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0404, 0.8064, 0.0322, 0.0582, 0.0112, 0.0033, 0.0482],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0677, 0.0091, 0.6296, 0.1156, 0.0690, 0.0574, 0.0516],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1205, 0.1057, 0.1264, 0.3056, 0.1131, 0.1066, 0.1221],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1102, 0.0045, 0.0984, 0.0965, 0.5102, 0.1130, 0.0672],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1444, 0.0253, 0.1830, 0.1141, 0.1303, 0.2953, 0.1077],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1283, 0.0879, 0.1636, 0.1485, 0.1364, 0.1504, 0.1849],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2133933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[ 0.2521],
        [-0.2807],
        [ 0.0000],
        [ 0.0957],
        [ 0.2659],
        [ 0.1004],
        [ 0.0943],
        [ 0.6811],
        [ 0.2998],
        [ 0.0000]], dtype=torch.float64)
-0.032346567066 0.21977646464998998
-0.084359833866 -0.36510166080187806
0.92779335 0.92779335
-0.032346567066 0.06332108258639904
-0.083839701198 0.18201563912584273
-0.032346567066 0.06803014082175729
-0.032346567066 0.06190807482106262
-0.083839701198 0.5972288898539746
-0.071422513866 0.22833078463134032
-0.0065999999999994926 -0.0065999999999994926
printing an ep nov before normalisation:  41.9757579641113
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2133933333333332 0.6870000000000002 0.6870000000000002
printing an ep nov before normalisation:  54.23606191674782
actor:  1 policy actor:  1  step number:  58 total reward:  0.33999999999999997  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  50.233365621982394
actor:  1 policy actor:  1  step number:  42 total reward:  0.5666666666666669  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2133933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2133933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.692]
 [0.584]
 [0.591]
 [0.588]
 [0.602]
 [0.612]] [[35.553]
 [46.544]
 [43.4  ]
 [36.401]
 [43.684]
 [44.295]
 [42.4  ]] [[0.586]
 [0.692]
 [0.584]
 [0.591]
 [0.588]
 [0.602]
 [0.612]]
maxi score, test score, baseline:  0.2133933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.674]
 [0.59 ]
 [0.59 ]
 [0.59 ]
 [0.606]
 [0.615]] [[37.06 ]
 [47.473]
 [37.06 ]
 [37.06 ]
 [37.06 ]
 [34.855]
 [34.726]] [[1.354]
 [1.929]
 [1.354]
 [1.354]
 [1.354]
 [1.266]
 [1.268]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.32666666666666655  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2133933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2133933333333332 0.6870000000000002 0.6870000000000002
printing an ep nov before normalisation:  48.12441928456136
actor:  1 policy actor:  1  step number:  52 total reward:  0.2999999999999995  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  52 total reward:  0.1933333333333328  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
siam score:  -0.7560405
actor:  1 policy actor:  1  step number:  52 total reward:  0.35333333333333283  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2136599999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2136599999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.7572735
Printing some Q and Qe and total Qs values:  [[-0.077]
 [-0.069]
 [-0.04 ]
 [-0.044]
 [-0.099]
 [-0.051]
 [-0.038]] [[47.23 ]
 [41.93 ]
 [44.855]
 [48.268]
 [47.021]
 [46.506]
 [52.032]] [[1.375]
 [1.039]
 [1.258]
 [1.475]
 [1.339]
 [1.354]
 [1.726]]
printing an ep nov before normalisation:  49.991520317259265
printing an ep nov before normalisation:  56.51740660343032
maxi score, test score, baseline:  0.2136599999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.51630448081047
maxi score, test score, baseline:  0.2136599999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]
 [0.456]] [[42.27]
 [42.27]
 [42.27]
 [42.27]
 [42.27]
 [42.27]
 [42.27]] [[0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]
 [0.866]]
maxi score, test score, baseline:  0.2136599999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2136599999999999 0.6870000000000002 0.6870000000000002
printing an ep nov before normalisation:  44.94129943851469
actor:  1 policy actor:  1  step number:  60 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2136599999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]
 [0.586]] [[46.111]
 [46.111]
 [46.111]
 [46.111]
 [46.111]
 [46.111]
 [46.111]] [[1.126]
 [1.126]
 [1.126]
 [1.126]
 [1.126]
 [1.126]
 [1.126]]
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]] [[32.829]
 [32.829]
 [32.829]
 [32.829]
 [32.829]
 [32.829]
 [32.829]] [[0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]
 [0.463]]
actor:  1 policy actor:  1  step number:  67 total reward:  0.2399999999999991  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.2136599999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.22662016417042
Printing some Q and Qe and total Qs values:  [[0.128]
 [0.163]
 [0.126]
 [0.127]
 [0.124]
 [0.125]
 [0.122]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.128]
 [0.163]
 [0.126]
 [0.127]
 [0.124]
 [0.125]
 [0.122]]
actor:  1 policy actor:  1  step number:  57 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  25.29355525970459
Printing some Q and Qe and total Qs values:  [[0.362]
 [0.326]
 [0.302]
 [0.31 ]
 [0.362]
 [0.325]
 [0.362]] [[64.181]
 [50.289]
 [56.6  ]
 [57.661]
 [64.181]
 [56.778]
 [64.181]] [[2.362]
 [1.749]
 [1.988]
 [2.039]
 [2.362]
 [2.018]
 [2.362]]
maxi score, test score, baseline:  0.21027333333333323 0.6870000000000002 0.6870000000000002
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.817]
 [0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.809]] [[31.952]
 [34.786]
 [34.456]
 [34.456]
 [34.456]
 [34.456]
 [34.456]] [[0.816]
 [0.817]
 [0.809]
 [0.809]
 [0.809]
 [0.809]
 [0.809]]
maxi score, test score, baseline:  0.21027333333333323 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.21027333333333323 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21027333333333323 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.3666666666666665  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  0.0
actor:  0 policy actor:  0  step number:  46 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20964666666666656 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.20964666666666656 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20964666666666656 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.20964666666666656 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.015265464782715
maxi score, test score, baseline:  0.20964666666666656 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.6318700815803
printing an ep nov before normalisation:  41.040844440788405
printing an ep nov before normalisation:  10.173349938581728
printing an ep nov before normalisation:  39.03759524667506
actor:  1 policy actor:  1  step number:  57 total reward:  0.25333333333333286  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20964666666666656 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.25333333333333274  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20964666666666656 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.20964666666666656 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.503]
 [0.438]
 [0.438]
 [0.438]
 [0.438]
 [0.438]] [[40.088]
 [46.993]
 [40.088]
 [40.088]
 [40.088]
 [40.088]
 [40.088]] [[1.499]
 [1.878]
 [1.499]
 [1.499]
 [1.499]
 [1.499]
 [1.499]]
actions average: 
K:  2  action  0 :  tensor([0.4004, 0.0015, 0.1007, 0.1241, 0.1651, 0.1054, 0.1028],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0050, 0.9296, 0.0042, 0.0038, 0.0014, 0.0014, 0.0546],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0803, 0.0086, 0.4593, 0.0880, 0.0944, 0.1541, 0.1154],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0910, 0.0095, 0.0878, 0.3758, 0.1273, 0.1292, 0.1793],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1082, 0.0069, 0.0867, 0.0937, 0.4935, 0.1075, 0.1036],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0635, 0.0419, 0.1491, 0.1037, 0.0748, 0.4547, 0.1123],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1208, 0.1243, 0.0727, 0.0801, 0.1171, 0.0840, 0.4010],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  44.05194424514528
maxi score, test score, baseline:  0.20964666666666656 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.46900770194764
printing an ep nov before normalisation:  45.34500599541493
actor:  0 policy actor:  0  step number:  46 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20888666666666653 0.6870000000000002 0.6870000000000002
actor:  1 policy actor:  1  step number:  48 total reward:  0.5933333333333337  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7654763
maxi score, test score, baseline:  0.20888666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20888666666666653 0.6870000000000002 0.6870000000000002
printing an ep nov before normalisation:  49.23374557702088
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  40.02074704704429
printing an ep nov before normalisation:  50.50818149839691
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.49809853266211
printing an ep nov before normalisation:  45.825612105826636
printing an ep nov before normalisation:  0.0028177416788821574
line 256 mcts: sample exp_bonus 53.83928840641145
actor:  0 policy actor:  0  step number:  41 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.296]
 [0.281]
 [0.281]
 [0.281]
 [0.281]
 [0.281]] [[28.097]
 [44.77 ]
 [28.097]
 [28.097]
 [28.097]
 [28.097]
 [28.097]] [[0.595]
 [1.205]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]]
maxi score, test score, baseline:  0.20832666666666658 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.261]
 [0.39 ]
 [0.261]
 [0.261]
 [0.261]
 [0.261]
 [0.261]] [[36.464]
 [38.913]
 [36.464]
 [36.464]
 [36.464]
 [36.464]
 [36.464]] [[1.283]
 [1.546]
 [1.283]
 [1.283]
 [1.283]
 [1.283]
 [1.283]]
printing an ep nov before normalisation:  29.382372644545814
siam score:  -0.76941925
maxi score, test score, baseline:  0.20832666666666658 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20832666666666658 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20832666666666658 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.20832666666666658 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20832666666666658 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.015552752373650947
printing an ep nov before normalisation:  44.01338953380563
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  61 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20832666666666658 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.939346493188474
Printing some Q and Qe and total Qs values:  [[0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.58 ]] [[12.421]
 [12.421]
 [12.421]
 [12.421]
 [12.421]
 [12.421]
 [20.774]] [[1.045]
 [1.045]
 [1.045]
 [1.045]
 [1.045]
 [1.045]
 [1.553]]
actor:  1 policy actor:  1  step number:  42 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  59 total reward:  0.32000000000000006  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20832666666666658 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.3787, 0.0031, 0.1152, 0.1087, 0.1251, 0.1253, 0.1439],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0093, 0.9573, 0.0056, 0.0050, 0.0027, 0.0045, 0.0156],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1528, 0.0043, 0.5438, 0.0545, 0.0548, 0.1094, 0.0803],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1630, 0.0329, 0.1022, 0.3005, 0.1201, 0.1137, 0.1676],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1076, 0.0161, 0.0988, 0.1160, 0.4035, 0.1145, 0.1435],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0867, 0.0173, 0.1403, 0.0922, 0.0971, 0.4788, 0.0876],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1135, 0.1043, 0.1052, 0.1309, 0.1133, 0.1221, 0.3108],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.20832666666666658 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20832666666666658 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.37115805866252
maxi score, test score, baseline:  0.20832666666666658 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.20832666666666658 0.6870000000000002 0.6870000000000002
printing an ep nov before normalisation:  44.94117155840379
maxi score, test score, baseline:  0.20832666666666658 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.61489120833391
actor:  0 policy actor:  0  step number:  41 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20773999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  50 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.20708666666666656 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20708666666666656 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.24666666666666626  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20708666666666656 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20708666666666656 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20708666666666656 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20708666666666656 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20708666666666656 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.21195517351721
maxi score, test score, baseline:  0.20708666666666656 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.44535132945691
maxi score, test score, baseline:  0.20708666666666656 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.070122955902086
printing an ep nov before normalisation:  40.653885679544416
maxi score, test score, baseline:  0.20708666666666656 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.2811, 0.0279, 0.1171, 0.1459, 0.1598, 0.1287, 0.1396],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0016,     0.9807,     0.0015,     0.0039,     0.0005,     0.0007,
            0.0111], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0991, 0.0272, 0.4285, 0.1096, 0.1024, 0.1122, 0.1210],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1410, 0.0058, 0.1555, 0.2166, 0.1606, 0.1473, 0.1731],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1117, 0.0366, 0.1167, 0.1136, 0.3703, 0.0940, 0.1570],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0661, 0.0084, 0.1121, 0.1072, 0.0967, 0.4905, 0.1190],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0990, 0.1900, 0.1674, 0.1280, 0.0951, 0.1009, 0.2196],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.512]
 [0.559]
 [0.49 ]
 [0.49 ]
 [0.556]
 [0.506]
 [0.49 ]] [[38.205]
 [37.619]
 [35.547]
 [35.547]
 [37.606]
 [37.34 ]
 [35.547]] [[1.654]
 [1.666]
 [1.474]
 [1.474]
 [1.662]
 [1.597]
 [1.474]]
maxi score, test score, baseline:  0.20708666666666656 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.089376832453084
actor:  1 policy actor:  1  step number:  46 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20708666666666656 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  51.992677165045286
maxi score, test score, baseline:  0.20708666666666656 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.69321807972658
maxi score, test score, baseline:  0.20708666666666656 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  52 total reward:  0.20666666666666633  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  49.58001273337455
maxi score, test score, baseline:  0.20611333333333323 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.20611333333333323 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20611333333333323 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.584953991188776
printing an ep nov before normalisation:  46.0255981248771
printing an ep nov before normalisation:  54.110418920722616
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20611333333333323 0.6870000000000002 0.6870000000000002
actor:  1 policy actor:  1  step number:  49 total reward:  0.4  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.20611333333333323 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20611333333333323 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.86310668719962
printing an ep nov before normalisation:  57.0438814476213
actor:  1 policy actor:  1  step number:  45 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20611333333333323 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20611333333333323 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.20611333333333323 0.6870000000000002 0.6870000000000002
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]
 [0.535]] [[54.671]
 [54.671]
 [54.671]
 [54.671]
 [54.671]
 [54.671]
 [54.671]] [[1.805]
 [1.805]
 [1.805]
 [1.805]
 [1.805]
 [1.805]
 [1.805]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20611333333333323 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20611333333333323 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20611333333333323 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  56 total reward:  0.27333333333333265  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  50 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.20611333333333323 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  34.89263223189516
maxi score, test score, baseline:  0.20611333333333323 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.456]
 [0.402]
 [0.402]
 [0.382]
 [0.381]
 [0.376]] [[50.989]
 [43.433]
 [50.989]
 [50.989]
 [49.584]
 [49.331]
 [49.501]] [[1.05 ]
 [0.938]
 [1.05 ]
 [1.05 ]
 [0.998]
 [0.992]
 [0.991]]
maxi score, test score, baseline:  0.20611333333333323 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20611333333333323 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  59 total reward:  0.26666666666666616  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  3  action  0 :  tensor([0.5093, 0.0352, 0.0680, 0.0919, 0.1087, 0.0662, 0.1207],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0076, 0.9366, 0.0086, 0.0084, 0.0055, 0.0061, 0.0273],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0972, 0.0556, 0.4518, 0.0977, 0.0613, 0.0896, 0.1468],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1161, 0.0168, 0.1291, 0.2620, 0.1292, 0.1631, 0.1837],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1012, 0.0103, 0.0733, 0.0542, 0.6405, 0.0638, 0.0567],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0703, 0.0080, 0.1622, 0.0940, 0.0815, 0.4992, 0.0847],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1472, 0.0057, 0.1523, 0.1692, 0.1458, 0.1704, 0.2093],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.20611333333333323 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.282]
 [0.288]
 [0.288]
 [0.287]
 [0.289]
 [0.294]] [[35.205]
 [45.231]
 [35.709]
 [35.336]
 [35.154]
 [35.005]
 [35.367]] [[0.962]
 [1.435]
 [0.991]
 [0.972]
 [0.963]
 [0.958]
 [0.981]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.695205211639404
line 256 mcts: sample exp_bonus 55.40299532087797
printing an ep nov before normalisation:  46.62716865539551
Printing some Q and Qe and total Qs values:  [[0.82 ]
 [0.874]
 [0.791]
 [0.8  ]
 [0.791]
 [0.791]
 [0.858]] [[41.619]
 [44.965]
 [37.22 ]
 [31.877]
 [37.22 ]
 [37.22 ]
 [39.408]] [[0.82 ]
 [0.874]
 [0.791]
 [0.8  ]
 [0.791]
 [0.791]
 [0.858]]
printing an ep nov before normalisation:  46.98815635067393
maxi score, test score, baseline:  0.20611333333333323 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.20611333333333323 0.6870000000000002 0.6870000000000002
printing an ep nov before normalisation:  46.80276393890381
maxi score, test score, baseline:  0.20611333333333323 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]
 [0.705]] [[46.496]
 [46.496]
 [46.496]
 [46.496]
 [46.496]
 [46.496]
 [46.496]] [[2.314]
 [2.314]
 [2.314]
 [2.314]
 [2.314]
 [2.314]
 [2.314]]
siam score:  -0.7712865
maxi score, test score, baseline:  0.20611333333333323 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  44 total reward:  0.4199999999999997  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  57 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  58 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  1  action  0 :  tensor([0.2340, 0.0208, 0.1159, 0.1295, 0.2349, 0.1255, 0.1394],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0011,     0.9796,     0.0020,     0.0022,     0.0002,     0.0015,
            0.0134], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1131, 0.1218, 0.3901, 0.0798, 0.0947, 0.0865, 0.1140],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1586, 0.0587, 0.1123, 0.2868, 0.1428, 0.1075, 0.1333],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1972, 0.0759, 0.0927, 0.1120, 0.3109, 0.1059, 0.1053],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1127, 0.0467, 0.1733, 0.1236, 0.1013, 0.3097, 0.1327],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1285, 0.1520, 0.1034, 0.1319, 0.1352, 0.1119, 0.2370],
       grad_fn=<DivBackward0>)
siam score:  -0.7709143
maxi score, test score, baseline:  0.20556666666666654 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20556666666666654 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.20556666666666654 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.656]
 [0.656]
 [0.656]
 [0.656]
 [0.719]
 [0.656]
 [0.656]] [[64.075]
 [64.075]
 [64.075]
 [64.075]
 [67.219]
 [64.075]
 [64.075]] [[1.19]
 [1.19]
 [1.19]
 [1.19]
 [1.29]
 [1.19]
 [1.19]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.20556666666666654 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20556666666666654 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.46917581640154
printing an ep nov before normalisation:  37.17380708167151
printing an ep nov before normalisation:  47.40733360595832
maxi score, test score, baseline:  0.20556666666666654 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  43 total reward:  0.48  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.658346128450674
actions average: 
K:  1  action  0 :  tensor([0.4381, 0.0086, 0.0916, 0.1005, 0.1528, 0.1053, 0.1030],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0084, 0.9537, 0.0083, 0.0063, 0.0036, 0.0035, 0.0161],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1273, 0.0285, 0.5764, 0.0558, 0.0627, 0.0753, 0.0739],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1097, 0.0511, 0.1034, 0.3294, 0.1128, 0.1120, 0.1816],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1294, 0.0021, 0.0837, 0.1105, 0.4762, 0.0996, 0.0985],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1534, 0.0033, 0.1276, 0.1593, 0.1393, 0.2763, 0.1408],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1024, 0.1750, 0.0771, 0.0955, 0.0809, 0.0674, 0.4017],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2051399999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  68 total reward:  0.08666666666666578  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  58 total reward:  0.1799999999999995  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2051399999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2051399999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2051399999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.1538, 0.0381, 0.1429, 0.1725, 0.1551, 0.1700, 0.1677],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0214, 0.8827, 0.0247, 0.0047, 0.0023, 0.0030, 0.0613],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1363, 0.0291, 0.2459, 0.1475, 0.1237, 0.1714, 0.1462],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0692, 0.2200, 0.0787, 0.3104, 0.0877, 0.0706, 0.1636],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1021, 0.0063, 0.0769, 0.0846, 0.5529, 0.0814, 0.0958],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0721, 0.0082, 0.1474, 0.1119, 0.0886, 0.4706, 0.1012],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0626, 0.3087, 0.0568, 0.1185, 0.0601, 0.0623, 0.3310],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2051399999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.85579010406766
actor:  0 policy actor:  0  step number:  47 total reward:  0.2666666666666664  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  45.25870653715882
maxi score, test score, baseline:  0.20428666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20428666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20428666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  51 total reward:  0.33333333333333315  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.91467173965062
maxi score, test score, baseline:  0.20356666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20356666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.89]
 [0.89]
 [0.89]
 [0.89]
 [0.89]
 [0.89]
 [0.89]] [[43.334]
 [43.334]
 [43.334]
 [43.334]
 [43.334]
 [43.334]
 [43.334]] [[1.89]
 [1.89]
 [1.89]
 [1.89]
 [1.89]
 [1.89]
 [1.89]]
maxi score, test score, baseline:  0.20356666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  0.13333333333333253  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20356666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 37.6270693597618
printing an ep nov before normalisation:  32.92542405374318
Printing some Q and Qe and total Qs values:  [[0.689]
 [0.752]
 [0.525]
 [0.712]
 [0.586]
 [0.585]
 [0.749]] [[33.304]
 [35.829]
 [28.501]
 [27.519]
 [28.752]
 [25.868]
 [32.003]] [[0.904]
 [0.995]
 [0.685]
 [0.86 ]
 [0.748]
 [0.714]
 [0.949]]
printing an ep nov before normalisation:  33.59265503614413
actor:  1 policy actor:  1  step number:  62 total reward:  0.2599999999999989  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  0.4611876027439621
Printing some Q and Qe and total Qs values:  [[0.331]
 [0.347]
 [0.331]
 [0.331]
 [0.331]
 [0.329]
 [0.331]] [[64.207]
 [62.3  ]
 [64.207]
 [64.207]
 [64.207]
 [73.448]
 [64.207]] [[1.976]
 [1.919]
 [1.976]
 [1.976]
 [1.976]
 [2.329]
 [1.976]]
maxi score, test score, baseline:  0.20356666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.1996338591313
actor:  1 policy actor:  1  step number:  55 total reward:  0.37333333333333285  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.20356666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.20356666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  48 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  47.61670694765813
maxi score, test score, baseline:  0.2029933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.3933333333333333  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2029933333333332 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.2029933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2029933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.013957765696943625
maxi score, test score, baseline:  0.2029933333333332 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.2029933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.156]
 [0.19 ]
 [0.16 ]
 [0.161]
 [0.175]
 [0.159]
 [0.163]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.156]
 [0.19 ]
 [0.16 ]
 [0.161]
 [0.175]
 [0.159]
 [0.163]]
maxi score, test score, baseline:  0.2029933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2029933333333332 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.2029933333333332 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.2029933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2029933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.3839, 0.0038, 0.0855, 0.0987, 0.1977, 0.1029, 0.1274],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0025,     0.9832,     0.0022,     0.0012,     0.0011,     0.0008,
            0.0090], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0576, 0.0052, 0.4533, 0.1161, 0.0893, 0.1779, 0.1005],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1142, 0.0011, 0.0986, 0.1843, 0.2912, 0.1486, 0.1620],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1091, 0.0011, 0.0663, 0.0775, 0.5649, 0.0779, 0.1031],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0702, 0.0017, 0.0803, 0.0806, 0.0821, 0.6052, 0.0798],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1628, 0.0648, 0.1021, 0.1264, 0.1466, 0.1101, 0.2873],
       grad_fn=<DivBackward0>)
siam score:  -0.7657278
actor:  1 policy actor:  1  step number:  43 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2029933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.2633, 0.0063, 0.1502, 0.1433, 0.1479, 0.1411, 0.1480],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0038,     0.9347,     0.0091,     0.0110,     0.0007,     0.0016,
            0.0390], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1413, 0.0063, 0.2891, 0.1278, 0.1293, 0.1318, 0.1745],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1356, 0.0114, 0.1528, 0.2770, 0.1254, 0.1374, 0.1605],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1279, 0.0024, 0.1009, 0.1137, 0.4482, 0.1057, 0.1011],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1458, 0.0080, 0.1384, 0.0999, 0.1215, 0.3616, 0.1247],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1608, 0.0389, 0.1488, 0.1153, 0.1008, 0.0982, 0.3372],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  62.21776493547764
maxi score, test score, baseline:  0.2029933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2029933333333332 0.6870000000000002 0.6870000000000002
actor:  1 policy actor:  1  step number:  45 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]] [[70.993]
 [70.993]
 [70.993]
 [70.993]
 [70.993]
 [70.993]
 [70.993]] [[2.557]
 [2.557]
 [2.557]
 [2.557]
 [2.557]
 [2.557]
 [2.557]]
printing an ep nov before normalisation:  19.433723286450544
printing an ep nov before normalisation:  28.82762908935547
maxi score, test score, baseline:  0.2029933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.480933825756324
maxi score, test score, baseline:  0.2029933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.738986749103436
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  45.99066336588805
maxi score, test score, baseline:  0.2029933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2029933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.78594161633515
printing an ep nov before normalisation:  39.63912071122806
printing an ep nov before normalisation:  0.005343697432067529
actor:  1 policy actor:  1  step number:  57 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.814]
 [0.814]
 [0.814]
 [0.814]
 [0.814]
 [0.814]
 [0.814]] [[43.167]
 [43.167]
 [43.167]
 [43.167]
 [43.167]
 [43.167]
 [43.167]] [[0.814]
 [0.814]
 [0.814]
 [0.814]
 [0.814]
 [0.814]
 [0.814]]
maxi score, test score, baseline:  0.2029933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2029933333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19960666666666654 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19960666666666654 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19960666666666654 0.6870000000000002 0.6870000000000002
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  31.051742988879848
maxi score, test score, baseline:  0.19960666666666654 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.19621999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.311]
 [0.26 ]
 [0.25 ]
 [0.251]
 [0.251]
 [0.25 ]] [[46.634]
 [59.399]
 [38.891]
 [38.135]
 [37.923]
 [37.216]
 [36.911]] [[0.943]
 [1.262]
 [0.734]
 [0.707]
 [0.703]
 [0.686]
 [0.678]]
line 256 mcts: sample exp_bonus 38.43968816320614
printing an ep nov before normalisation:  47.523494232895736
Printing some Q and Qe and total Qs values:  [[-0.104]
 [ 0.262]
 [ 0.153]
 [-0.   ]
 [ 0.153]
 [-0.069]
 [ 0.153]] [[38.491]
 [54.71 ]
 [43.085]
 [39.106]
 [43.085]
 [40.841]
 [43.085]] [[0.665]
 [1.683]
 [1.107]
 [0.793]
 [1.107]
 [0.795]
 [1.107]]
maxi score, test score, baseline:  0.19621999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19621999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.02  0.367 0.286 0.041 0.02  0.245 0.02 ]
maxi score, test score, baseline:  0.19621999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19621999999999987 0.6870000000000002 0.6870000000000002
printing an ep nov before normalisation:  48.493590865678186
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.19621999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.29999999999999993  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  54 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  56.03258775490252
maxi score, test score, baseline:  0.19621999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.6231405357056
printing an ep nov before normalisation:  36.26230239868164
printing an ep nov before normalisation:  17.880996804241818
maxi score, test score, baseline:  0.19621999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  69 total reward:  0.1466666666666665  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.19621999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19621999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75898224
actor:  0 policy actor:  0  step number:  50 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]]
printing an ep nov before normalisation:  76.94558365479668
maxi score, test score, baseline:  0.19551333333333323 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.27 ]
 [0.184]
 [0.268]
 [0.23 ]
 [0.18 ]
 [0.259]] [[32.047]
 [28.297]
 [30.709]
 [30.093]
 [35.774]
 [34.507]
 [30.163]] [[0.621]
 [0.565]
 [0.539]
 [0.608]
 [0.714]
 [0.632]
 [0.6  ]]
printing an ep nov before normalisation:  47.090403088774465
actor:  0 policy actor:  0  step number:  43 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.109]
 [0.205]
 [0.165]
 [0.109]
 [0.109]
 [0.109]
 [0.178]] [[ 0.   ]
 [51.447]
 [51.802]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [50.951]] [[-0.178]
 [ 0.483]
 [ 0.448]
 [-0.178]
 [-0.178]
 [-0.178]
 [ 0.451]]
actor:  0 policy actor:  0  step number:  38 total reward:  0.5  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.19448666666666653 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.19448666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.545176932034785
actor:  1 policy actor:  1  step number:  56 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.19448666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19448666666666653 0.6870000000000002 0.6870000000000002
actor:  1 policy actor:  1  step number:  51 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.42138862609863
printing an ep nov before normalisation:  0.0
actor:  1 policy actor:  1  step number:  48 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.19448666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
siam score:  -0.7661896
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19448666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.395]
 [0.491]
 [0.395]
 [0.395]
 [0.395]
 [0.395]
 [0.395]] [[34.667]
 [42.944]
 [34.667]
 [34.667]
 [34.667]
 [34.667]
 [34.667]] [[0.937]
 [1.28 ]
 [0.937]
 [0.937]
 [0.937]
 [0.937]
 [0.937]]
actor:  0 policy actor:  0  step number:  53 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  39.80453662746926
actor:  0 policy actor:  0  step number:  46 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  36.301694517720854
maxi score, test score, baseline:  0.19293999999999986 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]] [[44.346]
 [44.346]
 [44.346]
 [44.346]
 [44.346]
 [44.346]
 [44.346]] [[0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]]
actor:  1 policy actor:  1  step number:  58 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.19293999999999986 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.25909963418154
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19293999999999986 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.421]
 [0.331]
 [0.331]
 [0.331]
 [0.331]
 [0.331]] [[27.7  ]
 [37.713]
 [43.655]
 [43.655]
 [43.655]
 [43.655]
 [43.655]] [[1.294]
 [2.421]
 [2.946]
 [2.946]
 [2.946]
 [2.946]
 [2.946]]
printing an ep nov before normalisation:  47.09419196877187
maxi score, test score, baseline:  0.19293999999999986 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.19293999999999986 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19293999999999986 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19293999999999986 0.6870000000000002 0.6870000000000002
printing an ep nov before normalisation:  52.80573034502047
Printing some Q and Qe and total Qs values:  [[ 0.186]
 [ 0.25 ]
 [-0.14 ]
 [ 0.124]
 [ 0.162]
 [-0.04 ]
 [ 0.202]] [[36.434]
 [33.046]
 [18.026]
 [12.83 ]
 [38.689]
 [14.094]
 [36.819]] [[ 0.351]
 [ 0.392]
 [-0.104]
 [ 0.124]
 [ 0.343]
 [-0.031]
 [ 0.37 ]]
Printing some Q and Qe and total Qs values:  [[0.219]
 [0.341]
 [0.213]
 [0.219]
 [0.218]
 [0.219]
 [0.219]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.219]
 [0.341]
 [0.213]
 [0.219]
 [0.218]
 [0.219]
 [0.219]]
siam score:  -0.767807
maxi score, test score, baseline:  0.19293999999999986 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19293999999999986 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4533333333333335  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  48.23262811508598
maxi score, test score, baseline:  0.19293999999999986 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  70 total reward:  0.04666666666666608  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19176666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19176666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19176666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19176666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76906174
printing an ep nov before normalisation:  39.48157884540425
maxi score, test score, baseline:  0.19176666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.47702980041504
siam score:  -0.7667833
printing an ep nov before normalisation:  38.9029598236084
maxi score, test score, baseline:  0.19176666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19176666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.5200000000000004  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.221]
 [0.221]
 [0.221]
 [0.221]
 [0.221]
 [0.221]
 [0.221]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.221]
 [0.221]
 [0.221]
 [0.221]
 [0.221]
 [0.221]
 [0.221]]
printing an ep nov before normalisation:  49.13909435272217
maxi score, test score, baseline:  0.19176666666666653 0.6870000000000002 0.6870000000000002
actor:  1 policy actor:  1  step number:  69 total reward:  0.02666666666666606  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  2  action  0 :  tensor([0.1247, 0.2770, 0.1117, 0.1263, 0.1200, 0.0969, 0.1434],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0031, 0.9529, 0.0019, 0.0069, 0.0010, 0.0010, 0.0332],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0691, 0.0328, 0.6016, 0.0558, 0.0412, 0.1335, 0.0661],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1417, 0.0288, 0.1402, 0.2736, 0.1448, 0.1411, 0.1298],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1031, 0.0320, 0.0981, 0.1197, 0.4413, 0.0920, 0.1136],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0639, 0.0007, 0.1034, 0.0816, 0.0791, 0.6167, 0.0547],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1285, 0.2439, 0.0987, 0.0981, 0.1653, 0.0661, 0.1993],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  53.03434288057089
maxi score, test score, baseline:  0.19176666666666653 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.19176666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19176666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.383]
 [0.348]
 [0.348]
 [0.348]
 [0.348]
 [0.348]] [[48.804]
 [56.682]
 [48.804]
 [48.804]
 [48.804]
 [48.804]
 [48.804]] [[1.59 ]
 [2.058]
 [1.59 ]
 [1.59 ]
 [1.59 ]
 [1.59 ]
 [1.59 ]]
Printing some Q and Qe and total Qs values:  [[0.192]
 [0.359]
 [0.192]
 [0.192]
 [0.192]
 [0.192]
 [0.192]] [[40.609]
 [43.89 ]
 [40.609]
 [40.609]
 [40.609]
 [40.609]
 [40.609]] [[1.523]
 [1.893]
 [1.523]
 [1.523]
 [1.523]
 [1.523]
 [1.523]]
maxi score, test score, baseline:  0.19176666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.52750013139587
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.42 ]
 [0.396]
 [0.317]
 [0.342]
 [0.31 ]
 [0.31 ]] [[37.877]
 [38.842]
 [37.877]
 [31.429]
 [34.671]
 [31.369]
 [31.467]] [[1.957]
 [2.048]
 [1.957]
 [1.433]
 [1.682]
 [1.423]
 [1.429]]
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.705]
 [0.628]
 [0.628]
 [0.603]
 [0.604]
 [0.628]] [[60.281]
 [54.786]
 [60.281]
 [60.281]
 [59.819]
 [60.231]
 [60.281]] [[1.961]
 [1.857]
 [1.961]
 [1.961]
 [1.92 ]
 [1.935]
 [1.961]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19176666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.2677, 0.0024, 0.1363, 0.1482, 0.1499, 0.1456, 0.1499],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0063, 0.9254, 0.0063, 0.0037, 0.0018, 0.0019, 0.0546],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1299, 0.0406, 0.2131, 0.1571, 0.1445, 0.1481, 0.1667],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1368, 0.0159, 0.1422, 0.2277, 0.1496, 0.1603, 0.1675],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0782,     0.0003,     0.0487,     0.0688,     0.6814,     0.0730,
            0.0497], grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0788, 0.0295, 0.0936, 0.0972, 0.0807, 0.5230, 0.0972],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1126, 0.1303, 0.1201, 0.1810, 0.1197, 0.1282, 0.2080],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.19176666666666653 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.19176666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.16170053691828
actor:  1 policy actor:  1  step number:  64 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.326666666666666  reward:  1.0 rdn_beta:  1.333
siam score:  -0.7551681
printing an ep nov before normalisation:  45.457432416280014
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
siam score:  -0.75294006
printing an ep nov before normalisation:  47.122166203347774
maxi score, test score, baseline:  0.19176666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19176666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  39 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1923533333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.49602265294031
actor:  0 policy actor:  0  step number:  40 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1929399999999999 0.6870000000000002 0.6870000000000002
printing an ep nov before normalisation:  36.24202005620702
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  54.84173317564559
maxi score, test score, baseline:  0.1929399999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.22666666666666602  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1929399999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.63091407054758
printing an ep nov before normalisation:  27.30539321899414
actor:  0 policy actor:  0  step number:  47 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1935799999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.949971199035645
printing an ep nov before normalisation:  41.301393481594204
maxi score, test score, baseline:  0.1935799999999999 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  40.854502200911014
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.37999999999999956  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  44 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  0.667
siam score:  -0.7633997
maxi score, test score, baseline:  0.19369999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.0913813166112
printing an ep nov before normalisation:  58.50442264635996
maxi score, test score, baseline:  0.19369999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.247778061340576
maxi score, test score, baseline:  0.19369999999999987 0.6870000000000002 0.6870000000000002
Printing some Q and Qe and total Qs values:  [[0.621]
 [0.77 ]
 [0.515]
 [0.683]
 [0.701]
 [0.569]
 [0.642]] [[30.728]
 [28.328]
 [29.332]
 [31.716]
 [31.505]
 [31.055]
 [30.533]] [[1.523]
 [1.601]
 [1.376]
 [1.614]
 [1.626]
 [1.48 ]
 [1.538]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.346666666666666  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.19369999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  34 total reward:  0.54  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.781681060791016
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19391333333333322 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.37873138623253
actor:  1 policy actor:  1  step number:  34 total reward:  0.6333333333333335  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.353]
 [0.262]
 [0.293]
 [0.258]
 [0.253]
 [0.28 ]] [[47.178]
 [48.113]
 [47.178]
 [40.644]
 [48.426]
 [48.591]
 [47.242]] [[0.433]
 [0.531]
 [0.433]
 [0.413]
 [0.439]
 [0.435]
 [0.451]]
maxi score, test score, baseline:  0.19391333333333322 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19391333333333322 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19391333333333322 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75472224
maxi score, test score, baseline:  0.19391333333333322 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  52 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  44.08846378326416
printing an ep nov before normalisation:  40.07232926684965
maxi score, test score, baseline:  0.1966733333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  0.20666666666666622  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  58.53837549644434
siam score:  -0.7583214
maxi score, test score, baseline:  0.1966733333333332 0.6870000000000002 0.6870000000000002
actor:  1 policy actor:  1  step number:  67 total reward:  0.14666666666666595  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.777]
 [0.83 ]
 [0.769]
 [0.752]
 [0.77 ]
 [0.76 ]
 [0.756]] [[45.054]
 [46.576]
 [44.106]
 [37.139]
 [45.608]
 [42.943]
 [36.818]] [[0.777]
 [0.83 ]
 [0.769]
 [0.752]
 [0.77 ]
 [0.76 ]
 [0.756]]
Printing some Q and Qe and total Qs values:  [[0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]] [[40.511]
 [40.511]
 [40.511]
 [40.511]
 [40.511]
 [40.511]
 [40.511]] [[0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]]
maxi score, test score, baseline:  0.1966733333333332 0.6870000000000002 0.6870000000000002
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  47.837354877785494
maxi score, test score, baseline:  0.1966733333333332 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.372814523095045
actor:  0 policy actor:  0  step number:  52 total reward:  0.2733333333333329  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  1  action  0 :  tensor([0.3658, 0.0114, 0.1141, 0.1159, 0.1455, 0.1257, 0.1218],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0037,     0.9534,     0.0035,     0.0016,     0.0004,     0.0005,
            0.0370], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0720, 0.0047, 0.5783, 0.0907, 0.0703, 0.0968, 0.0871],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1408, 0.0170, 0.1495, 0.2283, 0.1550, 0.1741, 0.1353],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0906,     0.0002,     0.0012,     0.0187,     0.8840,     0.0025,
            0.0028], grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.0731, 0.0814, 0.1046, 0.0687, 0.0758, 0.5253, 0.0711],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0946, 0.0246, 0.1014, 0.1070, 0.0921, 0.0821, 0.4982],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  54 total reward:  0.38  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.215]
 [0.092]
 [0.215]
 [0.215]
 [0.215]
 [0.215]
 [0.215]] [[45.688]
 [58.57 ]
 [45.688]
 [45.688]
 [45.688]
 [45.688]
 [45.688]] [[0.956]
 [1.249]
 [0.956]
 [0.956]
 [0.956]
 [0.956]
 [0.956]]
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.4269, 0.0033, 0.1130, 0.1213, 0.1057, 0.1119, 0.1179],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0054, 0.9513, 0.0076, 0.0083, 0.0042, 0.0042, 0.0190],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0884, 0.0082, 0.4766, 0.1017, 0.0822, 0.1287, 0.1142],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1065, 0.0439, 0.1224, 0.3211, 0.1152, 0.1329, 0.1579],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0965, 0.0114, 0.0768, 0.0734, 0.5930, 0.0752, 0.0738],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1509, 0.0093, 0.1522, 0.2373, 0.1426, 0.1316, 0.1762],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.2241, 0.0039, 0.1347, 0.1613, 0.1356, 0.1519, 0.1885],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.943955559063596
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
actor:  1 policy actor:  1  step number:  63 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.774]
 [0.774]
 [0.774]
 [0.774]
 [0.774]
 [0.774]
 [0.774]] [[48.606]
 [48.606]
 [48.606]
 [48.606]
 [48.606]
 [48.606]
 [48.606]] [[1.327]
 [1.327]
 [1.327]
 [1.327]
 [1.327]
 [1.327]
 [1.327]]
line 256 mcts: sample exp_bonus 12.374095808414864
Printing some Q and Qe and total Qs values:  [[ 0.24 ]
 [ 0.419]
 [ 0.392]
 [ 0.392]
 [ 0.285]
 [-0.079]
 [ 0.218]] [[36.457]
 [39.72 ]
 [40.359]
 [40.359]
 [35.183]
 [33.979]
 [34.981]] [[0.838]
 [1.117]
 [1.109]
 [1.109]
 [0.845]
 [0.444]
 [0.771]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  63 total reward:  0.13333333333333297  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.38492102779756
Printing some Q and Qe and total Qs values:  [[0.287]
 [0.299]
 [0.247]
 [0.241]
 [0.226]
 [0.247]
 [0.247]] [[37.42 ]
 [45.415]
 [44.52 ]
 [38.377]
 [38.738]
 [44.52 ]
 [44.52 ]] [[0.997]
 [1.344]
 [1.255]
 [0.991]
 [0.991]
 [1.255]
 [1.255]]
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.907417699877584
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.85706494975075
actor:  1 policy actor:  1  step number:  47 total reward:  0.4133333333333331  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.1768, 0.0292, 0.1513, 0.1456, 0.2214, 0.1401, 0.1356],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0137, 0.9230, 0.0106, 0.0072, 0.0060, 0.0061, 0.0334],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0843, 0.0027, 0.5621, 0.0872, 0.0757, 0.1111, 0.0769],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1236, 0.0093, 0.1182, 0.4121, 0.1089, 0.1057, 0.1223],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1311, 0.0047, 0.1035, 0.0873, 0.4644, 0.0977, 0.1113],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([    0.0003,     0.0002,     0.1188,     0.0124,     0.0033,     0.8591,
            0.0060], grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0957, 0.1650, 0.0985, 0.1051, 0.0822, 0.0861, 0.3674],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.88883291755169
printing an ep nov before normalisation:  52.17898805881281
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.15649127960205
printing an ep nov before normalisation:  29.379957964840614
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  35.67460536956787
printing an ep nov before normalisation:  73.91403047024211
actor:  1 policy actor:  1  step number:  59 total reward:  0.2666666666666657  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  48 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.550537109375
using explorer policy with actor:  1
printing an ep nov before normalisation:  63.2257897199883
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.62185478210449
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  0.3533333333333333  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]
 [0.382]] [[31.916]
 [31.916]
 [31.916]
 [31.916]
 [31.916]
 [31.916]
 [31.916]] [[0.797]
 [0.797]
 [0.797]
 [0.797]
 [0.797]
 [0.797]
 [0.797]]
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.881]
 [0.671]
 [0.671]
 [0.671]
 [0.671]
 [0.671]] [[47.792]
 [38.854]
 [47.792]
 [47.792]
 [47.792]
 [47.792]
 [47.792]] [[0.963]
 [1.067]
 [0.963]
 [0.963]
 [0.963]
 [0.963]
 [0.963]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.35999999999999976  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.158]
 [0.158]
 [0.158]
 [0.158]
 [0.169]
 [0.158]
 [0.158]] [[57.766]
 [57.766]
 [57.766]
 [57.766]
 [52.787]
 [57.766]
 [57.766]] [[2.123]
 [2.123]
 [2.123]
 [2.123]
 [1.854]
 [2.123]
 [2.123]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.94600883084241
printing an ep nov before normalisation:  42.90908187975371
actor:  1 policy actor:  1  step number:  67 total reward:  0.013333333333332864  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]
 [0.197]] [[56.052]
 [56.052]
 [56.052]
 [56.052]
 [56.052]
 [56.052]
 [56.052]] [[1.197]
 [1.197]
 [1.197]
 [1.197]
 [1.197]
 [1.197]
 [1.197]]
siam score:  -0.7525441
maxi score, test score, baseline:  0.19921999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.28729064122674
actor:  0 policy actor:  0  step number:  34 total reward:  0.5800000000000001  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.19985999999999987 0.6870000000000002 0.6870000000000002
printing an ep nov before normalisation:  53.58783194213819
maxi score, test score, baseline:  0.19985999999999987 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.19985999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19985999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.432352656558336
printing an ep nov before normalisation:  35.87373849778638
maxi score, test score, baseline:  0.19985999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.27999999999999947  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.642]
 [0.717]
 [0.642]
 [0.642]
 [0.642]
 [0.642]
 [0.656]] [[31.938]
 [44.524]
 [31.938]
 [31.938]
 [31.938]
 [31.938]
 [36.114]] [[0.782]
 [0.965]
 [0.782]
 [0.782]
 [0.782]
 [0.782]
 [0.832]]
printing an ep nov before normalisation:  48.468287340889844
maxi score, test score, baseline:  0.19985999999999987 0.6870000000000002 0.6870000000000002
actor:  1 policy actor:  1  step number:  55 total reward:  0.3199999999999994  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.19985999999999987 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.19985999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.25999999999999956  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.19985999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.9680451064738
printing an ep nov before normalisation:  38.40925536456396
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19985999999999987 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.5413221323377
actor:  1 policy actor:  1  step number:  57 total reward:  0.2799999999999999  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.19692666666666653 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.973489207663015
printing an ep nov before normalisation:  41.359722106832386
actor:  1 policy actor:  1  step number:  66 total reward:  0.12666666666666615  reward:  1.0 rdn_beta:  1.0
siam score:  -0.76424974
maxi score, test score, baseline:  0.19692666666666653 0.6870000000000002 0.6870000000000002
printing an ep nov before normalisation:  41.908789914912525
printing an ep nov before normalisation:  48.97211946020494
actor:  0 policy actor:  0  step number:  49 total reward:  0.3066666666666661  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.852]
 [0.89 ]
 [0.162]
 [0.852]
 [0.852]
 [0.852]
 [0.852]] [[39.491]
 [35.778]
 [34.036]
 [39.491]
 [39.491]
 [39.491]
 [39.491]] [[0.852]
 [0.89 ]
 [0.162]
 [0.852]
 [0.852]
 [0.852]
 [0.852]]
actor:  0 policy actor:  0  step number:  51 total reward:  0.21333333333333282  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.19897999999999988 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.762357
printing an ep nov before normalisation:  25.41386842727661
maxi score, test score, baseline:  0.19897999999999988 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.42788643419271
printing an ep nov before normalisation:  38.98011445991083
actor:  0 policy actor:  0  step number:  50 total reward:  0.35333333333333306  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.20168666666666651 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.843]
 [0.843]
 [0.843]
 [0.843]
 [0.843]
 [0.843]
 [0.843]] [[40.975]
 [40.975]
 [40.975]
 [40.975]
 [40.975]
 [40.975]
 [40.975]] [[2.388]
 [2.388]
 [2.388]
 [2.388]
 [2.388]
 [2.388]
 [2.388]]
printing an ep nov before normalisation:  55.364770736752284
actor:  1 policy actor:  1  step number:  53 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  1.667
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.814]
 [0.874]
 [0.808]
 [0.836]
 [0.815]
 [0.808]
 [0.872]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.814]
 [0.874]
 [0.808]
 [0.836]
 [0.815]
 [0.808]
 [0.872]]
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.334]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.293]
 [0.334]
 [0.293]
 [0.293]
 [0.293]
 [0.293]
 [0.293]]
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.519]
 [0.439]
 [0.463]
 [0.432]
 [0.463]
 [0.451]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.429]
 [0.519]
 [0.439]
 [0.463]
 [0.432]
 [0.463]
 [0.451]]
printing an ep nov before normalisation:  31.68099224841206
actor:  1 policy actor:  1  step number:  58 total reward:  0.4066666666666664  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.778]
 [0.708]
 [0.708]
 [0.708]
 [0.789]
 [0.708]
 [0.708]] [[41.682]
 [40.823]
 [40.823]
 [40.823]
 [43.682]
 [40.823]
 [40.823]] [[0.778]
 [0.708]
 [0.708]
 [0.708]
 [0.789]
 [0.708]
 [0.708]]
printing an ep nov before normalisation:  51.781427249188454
printing an ep nov before normalisation:  48.850841480778534
printing an ep nov before normalisation:  44.492454174320635
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.315]
 [0.329]
 [0.311]
 [0.019]
 [0.292]
 [0.037]] [[41.99 ]
 [39.361]
 [38.05 ]
 [39.326]
 [45.504]
 [41.933]
 [40.696]] [[1.487]
 [1.579]
 [1.505]
 [1.572]
 [1.692]
 [1.727]
 [1.389]]
printing an ep nov before normalisation:  37.55455559759111
printing an ep nov before normalisation:  35.00864303285525
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.844]
 [0.751]
 [0.791]
 [0.717]
 [0.714]
 [0.741]] [[37.852]
 [38.393]
 [36.139]
 [37.312]
 [36.059]
 [35.776]
 [36.263]] [[0.719]
 [0.844]
 [0.751]
 [0.791]
 [0.717]
 [0.714]
 [0.741]]
printing an ep nov before normalisation:  39.10123595923723
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  40.83668439697577
printing an ep nov before normalisation:  30.402095317840576
maxi score, test score, baseline:  0.20168666666666651 0.6870000000000002 0.6870000000000002
maxi score, test score, baseline:  0.20168666666666651 0.6870000000000002 0.6870000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.005220469482196677
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
printing an ep nov before normalisation:  0.0018425712369207758
actor:  0 policy actor:  0  step number:  27 total reward:  0.6800000000000002  reward:  1.0 rdn_beta:  2
maxi score, test score, baseline:  0.23132666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23132666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  59 total reward:  0.0799999999999994  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2304733333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.662]
 [0.625]
 [0.625]
 [0.625]
 [0.625]
 [0.625]] [[44.962]
 [44.132]
 [44.962]
 [44.962]
 [44.962]
 [44.962]
 [44.962]] [[1.398]
 [1.407]
 [1.398]
 [1.398]
 [1.398]
 [1.398]
 [1.398]]
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.801]
 [0.739]
 [0.665]
 [0.739]
 [0.739]
 [0.739]] [[59.128]
 [56.947]
 [59.128]
 [52.883]
 [59.128]
 [59.128]
 [59.128]] [[2.739]
 [2.727]
 [2.739]
 [2.454]
 [2.739]
 [2.739]
 [2.739]]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.27382668186408
maxi score, test score, baseline:  0.2304733333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3933333333333331  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2304733333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2304733333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.91328724230236
printing an ep nov before normalisation:  38.478949514599925
printing an ep nov before normalisation:  25.742585547725703
maxi score, test score, baseline:  0.2304733333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.98903423529842
actor:  0 policy actor:  0  step number:  53 total reward:  0.17333333333333267  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.23035333333333322 0.6920000000000002 0.6920000000000002
actor:  1 policy actor:  1  step number:  66 total reward:  0.1266666666666656  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  42.65557671630098
printing an ep nov before normalisation:  31.62427629743304
printing an ep nov before normalisation:  62.537545520302906
maxi score, test score, baseline:  0.23035333333333322 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.130251884280476
printing an ep nov before normalisation:  31.347262859344482
printing an ep nov before normalisation:  62.18450762173846
maxi score, test score, baseline:  0.23035333333333322 0.6920000000000002 0.6920000000000002
printing an ep nov before normalisation:  35.64299305531565
siam score:  -0.76757765
siam score:  -0.7660437
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.276]
 [0.258]
 [0.258]
 [0.258]
 [0.258]
 [0.258]] [[46.433]
 [49.238]
 [46.433]
 [46.433]
 [46.433]
 [46.433]
 [46.433]] [[1.104]
 [1.228]
 [1.104]
 [1.104]
 [1.104]
 [1.104]
 [1.104]]
maxi score, test score, baseline:  0.23035333333333322 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.23035333333333322 0.6920000000000002 0.6920000000000002
maxi score, test score, baseline:  0.23035333333333322 0.6920000000000002 0.6920000000000002
maxi score, test score, baseline:  0.23035333333333322 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  62 total reward:  0.0733333333333328  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.23249999999999985 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7581316
printing an ep nov before normalisation:  44.215671937897156
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.601]
 [0.559]
 [0.552]
 [0.535]
 [0.59 ]
 [0.59 ]] [[30.945]
 [41.559]
 [30.282]
 [30.211]
 [31.704]
 [37.294]
 [37.294]] [[1.184]
 [1.644]
 [1.164]
 [1.154]
 [1.195]
 [1.468]
 [1.468]]
maxi score, test score, baseline:  0.23249999999999985 0.6920000000000002 0.6920000000000002
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.421]
 [0.416]
 [0.421]
 [0.421]
 [0.409]
 [0.421]] [[52.676]
 [52.676]
 [42.32 ]
 [52.676]
 [52.676]
 [49.893]
 [52.676]] [[1.348]
 [1.348]
 [0.987]
 [1.348]
 [1.348]
 [1.24 ]
 [1.348]]
maxi score, test score, baseline:  0.23249999999999985 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.548]
 [0.476]
 [0.515]
 [0.521]
 [0.486]
 [0.517]] [[42.071]
 [51.823]
 [35.286]
 [37.697]
 [47.862]
 [36.098]
 [37.359]] [[1.092]
 [1.34 ]
 [0.859]
 [0.958]
 [1.216]
 [0.889]
 [0.951]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  44.47524949811069
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  0.3266666666666662  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  45.98183118187488
maxi score, test score, baseline:  0.23249999999999985 0.6920000000000002 0.6920000000000002
printing an ep nov before normalisation:  0.09145239622213808
actor:  1 policy actor:  1  step number:  41 total reward:  0.4933333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.23249999999999985 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.7583659
maxi score, test score, baseline:  0.23249999999999985 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23249999999999985 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.59390378464037
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  0.48  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  53.687826686159426
maxi score, test score, baseline:  0.23249999999999985 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.277520000528476
maxi score, test score, baseline:  0.23249999999999985 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.164]
 [0.208]
 [0.164]
 [0.245]
 [0.245]
 [0.245]
 [0.163]] [[42.714]
 [41.766]
 [41.113]
 [45.206]
 [45.206]
 [45.206]
 [42.765]] [[0.788]
 [0.806]
 [0.744]
 [0.935]
 [0.935]
 [0.935]
 [0.788]]
maxi score, test score, baseline:  0.23249999999999985 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23249999999999985 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23249999999999985 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.18666666666666643  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  50.41007134608884
using explorer policy with actor:  1
siam score:  -0.75688636
maxi score, test score, baseline:  0.23249999999999985 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.23249999999999985 0.6920000000000002 0.6920000000000002
maxi score, test score, baseline:  0.23249999999999985 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23249999999999985 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23249999999999985 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23249999999999985 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  36 total reward:  0.5666666666666667  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[-0.021]
 [ 0.247]
 [ 0.126]
 [ 0.163]
 [ 0.076]
 [ 0.129]
 [ 0.207]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[-0.021]
 [ 0.247]
 [ 0.126]
 [ 0.163]
 [ 0.076]
 [ 0.129]
 [ 0.207]]
maxi score, test score, baseline:  0.23563333333333317 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  39 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  1.0
siam score:  -0.75047654
maxi score, test score, baseline:  0.23571333333333322 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.23571333333333322 0.6920000000000002 0.6920000000000002
maxi score, test score, baseline:  0.23571333333333322 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23571333333333322 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23571333333333322 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.23571333333333322 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.15233268207234
maxi score, test score, baseline:  0.23571333333333322 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.3999999999999997  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.16307512788584
maxi score, test score, baseline:  0.23571333333333322 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.47306477600207
printing an ep nov before normalisation:  51.7973975610589
printing an ep nov before normalisation:  46.4686224090676
printing an ep nov before normalisation:  59.2531072768144
printing an ep nov before normalisation:  37.01502408965846
actor:  1 policy actor:  1  step number:  64 total reward:  0.27333333333333265  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.23571333333333322 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23571333333333322 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.474]
 [0.621]
 [0.474]
 [0.474]
 [0.474]
 [0.474]
 [0.474]] [[44.906]
 [50.808]
 [44.906]
 [44.906]
 [44.906]
 [44.906]
 [44.906]] [[0.799]
 [1.029]
 [0.799]
 [0.799]
 [0.799]
 [0.799]
 [0.799]]
maxi score, test score, baseline:  0.23571333333333322 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23571333333333322 0.6920000000000002 0.6920000000000002
printing an ep nov before normalisation:  35.09788990020752
actions average: 
K:  0  action  0 :  tensor([0.4070, 0.0140, 0.1108, 0.1001, 0.1313, 0.1286, 0.1082],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0019,     0.9509,     0.0036,     0.0010,     0.0002,     0.0003,
            0.0422], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1198, 0.0027, 0.4228, 0.1070, 0.1130, 0.1194, 0.1152],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1596, 0.0086, 0.1363, 0.2071, 0.1873, 0.1608, 0.1403],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1173, 0.0015, 0.0710, 0.0911, 0.5658, 0.0865, 0.0667],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.1433, 0.0030, 0.1291, 0.1416, 0.1453, 0.3037, 0.1339],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1239, 0.1007, 0.1519, 0.1002, 0.0989, 0.1146, 0.3099],
       grad_fn=<DivBackward0>)
siam score:  -0.7597818
actor:  1 policy actor:  1  step number:  45 total reward:  0.48  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  0.5266666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.23571333333333322 0.6920000000000002 0.6920000000000002
printing an ep nov before normalisation:  49.728321340315695
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.646]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]] [[34.726]
 [41.296]
 [34.726]
 [34.726]
 [34.726]
 [34.726]
 [34.726]] [[0.497]
 [0.646]
 [0.497]
 [0.497]
 [0.497]
 [0.497]
 [0.497]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.36934185028076
maxi score, test score, baseline:  0.23571333333333322 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]] [[40.945]
 [40.945]
 [40.945]
 [40.945]
 [40.945]
 [40.945]
 [40.945]] [[0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]]
actor:  0 policy actor:  0  step number:  47 total reward:  0.34666666666666635  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  49.08763855969717
maxi score, test score, baseline:  0.23592666666666653 0.6920000000000002 0.6920000000000002
printing an ep nov before normalisation:  27.596359606984322
actor:  1 policy actor:  1  step number:  58 total reward:  0.40666666666666673  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.23592666666666653 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.18]
 [0.18]
 [0.18]
 [0.18]
 [0.18]
 [0.18]
 [0.18]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.18]
 [0.18]
 [0.18]
 [0.18]
 [0.18]
 [0.18]
 [0.18]]
maxi score, test score, baseline:  0.23592666666666653 0.6920000000000002 0.6920000000000002
maxi score, test score, baseline:  0.23592666666666653 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  70 total reward:  0.21999999999999897  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.23836666666666653 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]
 [0.111]]
printing an ep nov before normalisation:  58.535407648559875
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  68 total reward:  0.24666666666666603  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  30.42231437276195
printing an ep nov before normalisation:  45.130703786332916
maxi score, test score, baseline:  0.23836666666666653 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.95661039742364
printing an ep nov before normalisation:  58.094925337955445
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.674]
 [0.537]
 [0.558]
 [0.564]
 [0.556]
 [0.568]] [[59.015]
 [44.871]
 [58.125]
 [58.458]
 [57.403]
 [58.593]
 [59.979]] [[0.877]
 [0.894]
 [0.856]
 [0.88 ]
 [0.878]
 [0.879]
 [0.901]]
actions average: 
K:  1  action  0 :  tensor([0.5251, 0.0748, 0.0470, 0.0571, 0.1902, 0.0395, 0.0662],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0090, 0.9245, 0.0121, 0.0093, 0.0062, 0.0067, 0.0321],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0963, 0.0096, 0.4351, 0.1111, 0.1066, 0.1206, 0.1207],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0796, 0.0012, 0.0858, 0.5385, 0.1095, 0.1054, 0.0799],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1669, 0.0130, 0.0903, 0.1534, 0.3252, 0.0844, 0.1669],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1026, 0.0012, 0.1113, 0.1019, 0.1133, 0.4610, 0.1086],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1809, 0.1540, 0.0940, 0.1184, 0.1048, 0.0912, 0.2568],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  53.23999263903032
printing an ep nov before normalisation:  43.476188074878614
actor:  1 policy actor:  1  step number:  47 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.23836666666666653 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23836666666666653 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76061004
actor:  1 policy actor:  1  step number:  40 total reward:  0.45999999999999985  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  42.996031193431435
maxi score, test score, baseline:  0.23836666666666653 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23836666666666653 0.6920000000000002 0.6920000000000002
maxi score, test score, baseline:  0.23836666666666653 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.462]
 [0.39 ]
 [0.39 ]
 [0.386]
 [0.382]
 [0.389]] [[57.019]
 [44.026]
 [51.875]
 [53.813]
 [54.84 ]
 [55.856]
 [57.247]] [[1.057]
 [0.917]
 [0.971]
 [1.002]
 [1.015]
 [1.026]
 [1.056]]
printing an ep nov before normalisation:  39.40644025802612
printing an ep nov before normalisation:  58.68785900482717
maxi score, test score, baseline:  0.23836666666666653 0.6920000000000002 0.6920000000000002
printing an ep nov before normalisation:  50.31233148006561
actor:  1 policy actor:  1  step number:  48 total reward:  0.4733333333333334  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  67 total reward:  0.06666666666666599  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.32 ]
 [0.405]
 [0.289]
 [0.297]
 [0.303]
 [0.287]
 [0.318]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.32 ]
 [0.405]
 [0.289]
 [0.297]
 [0.303]
 [0.287]
 [0.318]]
Printing some Q and Qe and total Qs values:  [[0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]] [[33.851]
 [33.851]
 [33.851]
 [33.851]
 [33.851]
 [33.851]
 [33.851]] [[0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]
 [0.813]]
actions average: 
K:  3  action  0 :  tensor([0.2685, 0.0377, 0.1100, 0.1545, 0.1347, 0.1279, 0.1668],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0088, 0.9343, 0.0110, 0.0066, 0.0078, 0.0081, 0.0234],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0986, 0.1103, 0.3316, 0.1145, 0.1221, 0.1043, 0.1187],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0932, 0.0097, 0.1344, 0.4164, 0.1067, 0.1151, 0.1245],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1327, 0.0052, 0.0478, 0.0610, 0.6434, 0.0521, 0.0578],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0799, 0.0075, 0.1560, 0.0780, 0.0844, 0.5140, 0.0801],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1371, 0.1234, 0.1409, 0.1348, 0.1531, 0.1498, 0.1610],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  39 total reward:  0.5066666666666667  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  33 total reward:  0.6533333333333335  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  3  action  0 :  tensor([0.5498, 0.0302, 0.0885, 0.0668, 0.1084, 0.0733, 0.0830],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0084, 0.9250, 0.0112, 0.0082, 0.0072, 0.0072, 0.0327],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1357, 0.1110, 0.1654, 0.1430, 0.1538, 0.1519, 0.1393],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0995, 0.0382, 0.1959, 0.2377, 0.1118, 0.1745, 0.1425],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0991, 0.0228, 0.0729, 0.0904, 0.5486, 0.0862, 0.0801],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1055, 0.0634, 0.1561, 0.1067, 0.1191, 0.3355, 0.1137],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1333, 0.0259, 0.1687, 0.1523, 0.1415, 0.1517, 0.2265],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  50.4778000061633
maxi score, test score, baseline:  0.24468666666666655 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.65173422551482
printing an ep nov before normalisation:  44.48814129384023
maxi score, test score, baseline:  0.24468666666666655 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24468666666666655 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.67618776510317
actor:  1 policy actor:  1  step number:  61 total reward:  0.18666666666666598  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.24468666666666655 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actor:  0 policy actor:  0  step number:  42 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2450066666666665 0.6920000000000002 0.6920000000000002
actor:  1 policy actor:  1  step number:  52 total reward:  0.4333333333333329  reward:  1.0 rdn_beta:  2.0
siam score:  -0.7672432
printing an ep nov before normalisation:  60.59991467707524
maxi score, test score, baseline:  0.2450066666666665 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  41 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.561]
 [0.502]
 [0.502]
 [0.502]
 [0.502]
 [0.502]] [[42.13 ]
 [40.078]
 [42.13 ]
 [42.13 ]
 [42.13 ]
 [42.13 ]
 [42.13 ]] [[0.797]
 [0.832]
 [0.797]
 [0.797]
 [0.797]
 [0.797]
 [0.797]]
maxi score, test score, baseline:  0.2444466666666665 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2444466666666665 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  76.28282752269685
maxi score, test score, baseline:  0.2444466666666665 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.61534263297959
actor:  1 policy actor:  1  step number:  62 total reward:  0.206666666666666  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  0.055717710409339816
actor:  0 policy actor:  0  step number:  46 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  55.68514347076416
maxi score, test score, baseline:  0.24371333333333317 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.43358342067999
maxi score, test score, baseline:  0.24371333333333317 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.3810053010176
printing an ep nov before normalisation:  39.59888935089111
maxi score, test score, baseline:  0.24371333333333317 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.24371333333333317 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  59 total reward:  0.2799999999999998  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.24371333333333317 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.413]
 [0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]] [[52.245]
 [57.464]
 [52.245]
 [52.245]
 [52.245]
 [52.245]
 [52.245]] [[0.751]
 [0.937]
 [0.751]
 [0.751]
 [0.751]
 [0.751]
 [0.751]]
line 256 mcts: sample exp_bonus 41.214396766838455
Printing some Q and Qe and total Qs values:  [[0.868]
 [0.939]
 [0.868]
 [0.868]
 [0.868]
 [0.868]
 [0.868]] [[44.76 ]
 [40.608]
 [44.76 ]
 [44.76 ]
 [44.76 ]
 [44.76 ]
 [44.76 ]] [[0.868]
 [0.939]
 [0.868]
 [0.868]
 [0.868]
 [0.868]
 [0.868]]
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.9  ]
 [0.759]
 [0.729]
 [0.735]
 [0.731]
 [0.81 ]] [[30.699]
 [38.101]
 [37.593]
 [30.576]
 [35.175]
 [30.236]
 [39.756]] [[0.727]
 [0.9  ]
 [0.759]
 [0.729]
 [0.735]
 [0.731]
 [0.81 ]]
printing an ep nov before normalisation:  58.52042558012462
printing an ep nov before normalisation:  44.49752558919137
actor:  0 policy actor:  0  step number:  37 total reward:  0.46666666666666645  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  39.8581942014276
actor:  1 policy actor:  1  step number:  37 total reward:  0.5333333333333334  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.24325999999999984 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  39 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.24272666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24272666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24272666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24272666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.543]
 [0.324]
 [0.324]
 [0.324]
 [0.324]
 [0.324]] [[48.416]
 [43.72 ]
 [48.416]
 [48.416]
 [48.416]
 [48.416]
 [48.416]] [[0.533]
 [0.723]
 [0.533]
 [0.533]
 [0.533]
 [0.533]
 [0.533]]
maxi score, test score, baseline:  0.24272666666666654 0.6920000000000002 0.6920000000000002
maxi score, test score, baseline:  0.24272666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.47454615736446
actor:  0 policy actor:  0  step number:  47 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.24208666666666656 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24208666666666656 0.6920000000000002 0.6920000000000002
printing an ep nov before normalisation:  48.572969111840834
maxi score, test score, baseline:  0.24208666666666656 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.805]
 [0.785]
 [0.664]
 [0.763]
 [0.779]
 [0.675]
 [0.678]] [[36.943]
 [39.909]
 [21.949]
 [40.021]
 [38.391]
 [21.914]
 [22.299]] [[1.027]
 [1.032]
 [0.759]
 [1.011]
 [1.013]
 [0.769]
 [0.775]]
printing an ep nov before normalisation:  68.03975629960746
maxi score, test score, baseline:  0.24208666666666656 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.763131305505276
actor:  1 policy actor:  1  step number:  61 total reward:  0.15999999999999948  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  37.33796219585921
maxi score, test score, baseline:  0.24208666666666656 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.4133333333333329  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.24208666666666656 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.702]
 [0.682]
 [0.682]
 [0.682]
 [0.684]
 [0.682]] [[25.445]
 [34.202]
 [25.445]
 [25.445]
 [25.445]
 [25.575]
 [25.445]] [[0.682]
 [0.702]
 [0.682]
 [0.682]
 [0.682]
 [0.684]
 [0.682]]
maxi score, test score, baseline:  0.24208666666666656 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.31817343839745
actor:  1 policy actor:  1  step number:  59 total reward:  0.32000000000000006  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.24208666666666656 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.019061733398757497
maxi score, test score, baseline:  0.24208666666666656 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  47 total reward:  0.2933333333333329  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2412866666666665 0.6920000000000002 0.6920000000000002
maxi score, test score, baseline:  0.2412866666666665 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2412866666666665 0.6920000000000002 0.6920000000000002
actor:  1 policy actor:  1  step number:  46 total reward:  0.4066666666666662  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.479]
 [0.479]
 [0.479]
 [0.478]
 [0.479]
 [0.479]] [[53.433]
 [43.693]
 [43.693]
 [43.693]
 [60.212]
 [43.693]
 [43.693]] [[0.667]
 [0.651]
 [0.651]
 [0.651]
 [0.762]
 [0.651]
 [0.651]]
printing an ep nov before normalisation:  27.48156898607952
maxi score, test score, baseline:  0.2412866666666665 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2412866666666665 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.664]
 [0.593]
 [0.595]
 [0.607]
 [0.59 ]
 [0.586]] [[35.373]
 [44.98 ]
 [34.648]
 [33.711]
 [42.442]
 [32.804]
 [32.729]] [[1.175]
 [1.669]
 [1.151]
 [1.113]
 [1.502]
 [1.069]
 [1.062]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2412866666666665 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2412866666666665 0.6920000000000002 0.6920000000000002
printing an ep nov before normalisation:  46.468991527193054
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.481]
 [0.453]
 [0.453]
 [0.453]
 [0.453]
 [0.453]] [[48.39]
 [52.2 ]
 [48.39]
 [48.39]
 [48.39]
 [48.39]
 [48.39]] [[0.752]
 [0.814]
 [0.752]
 [0.752]
 [0.752]
 [0.752]
 [0.752]]
maxi score, test score, baseline:  0.2412866666666665 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2412866666666665 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.21366108997023
actor:  1 policy actor:  1  step number:  47 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2412866666666665 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2412866666666665 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.651425264292136
maxi score, test score, baseline:  0.2412866666666665 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.378163655476314
printing an ep nov before normalisation:  46.25885144001735
maxi score, test score, baseline:  0.2412866666666665 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.155879744279204
printing an ep nov before normalisation:  49.17176948462945
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.00818508674367
printing an ep nov before normalisation:  38.957672582088264
printing an ep nov before normalisation:  52.86005595494544
actor:  1 policy actor:  1  step number:  49 total reward:  0.3733333333333331  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2378999999999999 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.452]
 [0.571]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.452]
 [0.571]
 [0.452]
 [0.452]
 [0.452]
 [0.452]
 [0.452]]
maxi score, test score, baseline:  0.2378999999999999 0.6920000000000002 0.6920000000000002
maxi score, test score, baseline:  0.2378999999999999 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.321]
 [0.579]
 [0.321]
 [0.321]
 [0.321]
 [0.321]
 [0.321]] [[47.546]
 [49.843]
 [47.546]
 [47.546]
 [47.546]
 [47.546]
 [47.546]] [[0.714]
 [1.009]
 [0.714]
 [0.714]
 [0.714]
 [0.714]
 [0.714]]
maxi score, test score, baseline:  0.2378999999999999 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.733473515711204
maxi score, test score, baseline:  0.2378999999999999 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2378999999999999 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2378999999999999 0.6920000000000002 0.6920000000000002
actor:  1 policy actor:  1  step number:  55 total reward:  0.23999999999999988  reward:  1.0 rdn_beta:  1.667
siam score:  -0.7504172
maxi score, test score, baseline:  0.2378999999999999 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2378999999999999 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.585131187395675
maxi score, test score, baseline:  0.2378999999999999 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2378999999999999 0.6920000000000002 0.6920000000000002
maxi score, test score, baseline:  0.2378999999999999 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.2378999999999999 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2378999999999999 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.5151, 0.0200, 0.0889, 0.0873, 0.1077, 0.0939, 0.0871],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0050, 0.9091, 0.0121, 0.0150, 0.0011, 0.0019, 0.0558],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1097, 0.0024, 0.3935, 0.1281, 0.0985, 0.1546, 0.1131],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1113, 0.0094, 0.1230, 0.3362, 0.1017, 0.1844, 0.1341],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1437, 0.0076, 0.0956, 0.1059, 0.4572, 0.1052, 0.0848],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0736, 0.0066, 0.1256, 0.0873, 0.0756, 0.5395, 0.0918],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1083, 0.0682, 0.1101, 0.2605, 0.0853, 0.0953, 0.2724],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2378999999999999 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  72 total reward:  0.019999999999999463  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2378999999999999 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2378999999999999 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2378999999999999 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.75075362476771
actor:  1 policy actor:  1  step number:  47 total reward:  0.3599999999999993  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.4866666666666668  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  48.97973100922974
maxi score, test score, baseline:  0.2378999999999999 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2378999999999999 0.6920000000000002 0.6920000000000002
maxi score, test score, baseline:  0.2378999999999999 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2378999999999999 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2378999999999999 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.99072651288509
printing an ep nov before normalisation:  41.95985632924422
actor:  0 policy actor:  0  step number:  44 total reward:  0.39333333333333287  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  42 total reward:  0.393333333333333  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.959221194449455
maxi score, test score, baseline:  0.2366999999999999 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2366999999999999 0.6920000000000002 0.6920000000000002
line 256 mcts: sample exp_bonus 37.53656979259798
actions average: 
K:  1  action  0 :  tensor([0.4448, 0.0151, 0.1055, 0.0900, 0.1690, 0.0813, 0.0944],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0018,     0.9884,     0.0013,     0.0005,     0.0002,     0.0003,
            0.0075], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1275, 0.0049, 0.3143, 0.1244, 0.1261, 0.1756, 0.1272],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1295, 0.0503, 0.1435, 0.2597, 0.1394, 0.1232, 0.1544],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0697, 0.0019, 0.0498, 0.0742, 0.6713, 0.0406, 0.0925],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1257, 0.0105, 0.1932, 0.1311, 0.1381, 0.2555, 0.1459],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1421, 0.0148, 0.1334, 0.1497, 0.1600, 0.1468, 0.2532],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2366999999999999 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.14666666666666606  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2366999999999999 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.788]
 [0.706]
 [0.706]
 [0.677]
 [0.722]
 [0.706]
 [0.706]] [[37.713]
 [35.904]
 [35.904]
 [40.965]
 [51.669]
 [35.904]
 [35.904]] [[0.788]
 [0.706]
 [0.706]
 [0.677]
 [0.722]
 [0.706]
 [0.706]]
maxi score, test score, baseline:  0.2333133333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3266666666666663  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.345]
 [0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.294]
 [0.345]
 [0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]]
printing an ep nov before normalisation:  39.60951635353469
printing an ep nov before normalisation:  29.977692498101128
using explorer policy with actor:  1
printing an ep nov before normalisation:  66.19141454354144
printing an ep nov before normalisation:  66.93892149916758
printing an ep nov before normalisation:  61.906710961234666
printing an ep nov before normalisation:  57.88373838089843
printing an ep nov before normalisation:  28.563501029697242
maxi score, test score, baseline:  0.2333133333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2333133333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.011312185054634938
maxi score, test score, baseline:  0.22992666666666656 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7558452
maxi score, test score, baseline:  0.22992666666666656 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.808]
 [0.824]
 [0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.808]
 [0.824]
 [0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]]
Printing some Q and Qe and total Qs values:  [[0.736]
 [0.815]
 [0.756]
 [0.778]
 [0.736]
 [0.736]
 [0.736]] [[38.107]
 [40.367]
 [39.565]
 [42.779]
 [38.651]
 [38.987]
 [38.726]] [[0.736]
 [0.815]
 [0.756]
 [0.778]
 [0.736]
 [0.736]
 [0.736]]
maxi score, test score, baseline:  0.22992666666666656 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75218165
actions average: 
K:  0  action  0 :  tensor([0.4819, 0.0038, 0.0946, 0.0966, 0.1167, 0.1022, 0.1042],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0034, 0.9630, 0.0054, 0.0098, 0.0010, 0.0011, 0.0163],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1051, 0.0155, 0.3298, 0.1257, 0.1230, 0.1656, 0.1354],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0889, 0.0023, 0.1263, 0.4329, 0.1293, 0.1021, 0.1182],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0831, 0.0012, 0.0779, 0.0703, 0.6288, 0.0680, 0.0706],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([    0.0502,     0.0005,     0.0943,     0.0763,     0.0549,     0.6674,
            0.0564], grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.0845, 0.2180, 0.1030, 0.0952, 0.0933, 0.0969, 0.3091],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.22992666666666656 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.22992666666666656 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.597]
 [0.652]
 [0.589]
 [0.597]
 [0.597]
 [0.597]
 [0.593]] [[48.778]
 [44.457]
 [51.608]
 [48.778]
 [48.778]
 [48.778]
 [47.941]] [[1.91 ]
 [1.736]
 [2.053]
 [1.91 ]
 [1.91 ]
 [1.91 ]
 [1.862]]
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.596]
 [0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]] [[48.216]
 [45.525]
 [48.216]
 [48.216]
 [48.216]
 [48.216]
 [48.216]] [[0.676]
 [0.809]
 [0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]]
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.705]
 [0.769]
 [0.711]
 [0.701]
 [0.687]
 [0.69 ]] [[14.828]
 [11.313]
 [14.35 ]
 [12.7  ]
 [13.255]
 [ 8.002]
 [11.419]] [[1.23 ]
 [1.106]
 [1.283]
 [1.164]
 [1.174]
 [0.966]
 [1.095]]
actor:  1 policy actor:  1  step number:  47 total reward:  0.4933333333333336  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  64.63642765250243
Printing some Q and Qe and total Qs values:  [[0.112]
 [0.132]
 [0.112]
 [0.112]
 [0.115]
 [0.108]
 [0.111]] [[55.422]
 [46.368]
 [55.422]
 [55.422]
 [57.401]
 [54.035]
 [53.554]] [[1.632]
 [1.226]
 [1.632]
 [1.632]
 [1.729]
 [1.563]
 [1.544]]
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.45405734716033
printing an ep nov before normalisation:  50.33750577640381
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3799999999999998  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.83810793200841
siam score:  -0.7531107
printing an ep nov before normalisation:  35.855471637408456
actor:  1 policy actor:  1  step number:  54 total reward:  0.4066666666666662  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
printing an ep nov before normalisation:  65.13548014979258
printing an ep nov before normalisation:  34.33753326584405
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.38 ]
 [0.44 ]
 [0.386]
 [0.437]
 [0.379]
 [0.381]
 [0.382]] [[32.885]
 [42.657]
 [36.71 ]
 [41.597]
 [32.307]
 [32.759]
 [32.894]] [[0.524]
 [0.68 ]
 [0.568]
 [0.667]
 [0.518]
 [0.524]
 [0.526]]
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.1999999999999994  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  70 total reward:  0.08666666666666567  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.87703176774981
using explorer policy with actor:  1
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.175]
 [0.126]
 [0.055]
 [0.061]
 [0.055]
 [0.055]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.055]
 [0.175]
 [0.126]
 [0.055]
 [0.061]
 [0.055]
 [0.055]]
line 256 mcts: sample exp_bonus 53.1728039251204
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.13397853250685
line 256 mcts: sample exp_bonus 49.244919697689646
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
printing an ep nov before normalisation:  40.063830521411056
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  60 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  53 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]
 [0.476]] [[63.291]
 [63.291]
 [63.291]
 [63.291]
 [63.291]
 [63.291]
 [63.291]] [[1.446]
 [1.446]
 [1.446]
 [1.446]
 [1.446]
 [1.446]
 [1.446]]
printing an ep nov before normalisation:  56.814244685625376
printing an ep nov before normalisation:  66.9282881591393
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
line 256 mcts: sample exp_bonus 36.98749494627966
printing an ep nov before normalisation:  0.062041388047191504
actor:  1 policy actor:  1  step number:  55 total reward:  0.4  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.71138071679768
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.74890304
maxi score, test score, baseline:  0.22653999999999988 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  44 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.61420092678315
printing an ep nov before normalisation:  53.62584303897436
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.50123918614861
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.142]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]] [[27.427]
 [44.823]
 [27.427]
 [27.427]
 [27.427]
 [27.427]
 [27.427]] [[0.861]
 [1.603]
 [0.861]
 [0.861]
 [0.861]
 [0.861]
 [0.861]]
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
actor:  1 policy actor:  1  step number:  45 total reward:  0.48  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.363]
 [0.304]
 [0.304]
 [0.304]
 [0.252]
 [0.304]] [[44.241]
 [31.814]
 [44.241]
 [44.241]
 [44.241]
 [54.463]
 [44.241]] [[0.544]
 [0.504]
 [0.544]
 [0.544]
 [0.544]
 [0.574]
 [0.544]]
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.749003
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.553]
 [0.553]
 [0.542]
 [0.553]
 [0.553]
 [0.553]] [[42.623]
 [42.623]
 [42.623]
 [41.321]
 [42.623]
 [42.623]
 [42.623]] [[1.968]
 [1.968]
 [1.968]
 [1.875]
 [1.968]
 [1.968]
 [1.968]]
printing an ep nov before normalisation:  48.68001634626776
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.85705375671387
actor:  1 policy actor:  1  step number:  50 total reward:  0.41999999999999993  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
printing an ep nov before normalisation:  44.32384386400187
printing an ep nov before normalisation:  64.23650023688667
Printing some Q and Qe and total Qs values:  [[0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.699]
 [0.552]
 [0.699]] [[40.385]
 [40.385]
 [40.385]
 [40.385]
 [40.385]
 [46.605]
 [40.385]] [[1.538]
 [1.538]
 [1.538]
 [1.538]
 [1.538]
 [1.665]
 [1.538]]
printing an ep nov before normalisation:  35.06127491812378
Printing some Q and Qe and total Qs values:  [[0.25]
 [0.26]
 [0.25]
 [0.25]
 [0.25]
 [0.25]
 [0.25]] [[50.183]
 [47.555]
 [50.183]
 [50.183]
 [50.183]
 [50.183]
 [50.183]] [[0.917]
 [0.874]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]]
actor:  1 policy actor:  1  step number:  64 total reward:  0.0733333333333327  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.308]
 [0.426]
 [0.274]
 [0.296]
 [0.267]
 [0.307]
 [0.287]] [[33.926]
 [41.583]
 [33.849]
 [33.544]
 [35.155]
 [41.271]
 [34.912]] [[0.62 ]
 [0.874]
 [0.585]
 [0.602]
 [0.601]
 [0.749]
 [0.617]]
printing an ep nov before normalisation:  29.04523415126274
actor:  1 policy actor:  1  step number:  60 total reward:  0.2999999999999997  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7518862
printing an ep nov before normalisation:  50.84838735767923
printing an ep nov before normalisation:  43.59747385229021
printing an ep nov before normalisation:  38.661360156734844
printing an ep nov before normalisation:  46.450819969177246
printing an ep nov before normalisation:  58.20698266462108
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
line 256 mcts: sample exp_bonus 28.791985756139066
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  59 total reward:  0.22666666666666613  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
printing an ep nov before normalisation:  38.683860899073174
actions average: 
K:  1  action  0 :  tensor([0.4962, 0.0019, 0.0743, 0.0874, 0.1778, 0.0654, 0.0969],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0178, 0.9186, 0.0091, 0.0060, 0.0027, 0.0020, 0.0438],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0595, 0.0013, 0.4882, 0.0859, 0.0690, 0.2148, 0.0812],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1099, 0.0581, 0.1071, 0.3977, 0.1161, 0.0930, 0.1180],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1040, 0.0008, 0.1046, 0.1388, 0.4417, 0.0950, 0.1152],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1030, 0.0009, 0.1360, 0.1384, 0.1325, 0.3687, 0.1205],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1074, 0.0170, 0.2081, 0.1462, 0.1205, 0.1165, 0.2843],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  49 total reward:  0.3866666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.15400756517883
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.4066666666666665  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
actor:  1 policy actor:  1  step number:  54 total reward:  0.44666666666666666  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]
 [0.595]] [[38.339]
 [38.339]
 [38.339]
 [38.339]
 [38.339]
 [38.339]
 [38.339]] [[2.262]
 [2.262]
 [2.262]
 [2.262]
 [2.262]
 [2.262]
 [2.262]]
printing an ep nov before normalisation:  55.47874033323744
actor:  1 policy actor:  1  step number:  57 total reward:  0.15999999999999925  reward:  1.0 rdn_beta:  1.667
siam score:  -0.7540791
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
printing an ep nov before normalisation:  39.684062004089355
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.68000865008938
actor:  1 policy actor:  1  step number:  64 total reward:  0.09999999999999953  reward:  1.0 rdn_beta:  1.667
siam score:  -0.7572302
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.23333333333333284  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.22596666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.22257999999999986 0.6920000000000002 0.6920000000000002
maxi score, test score, baseline:  0.22257999999999986 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.22257999999999986 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[ 0.362]
 [ 0.487]
 [-0.093]
 [ 0.305]
 [ 0.381]
 [-0.072]
 [ 0.381]] [[46.938]
 [50.471]
 [35.994]
 [35.021]
 [42.068]
 [36.575]
 [42.068]] [[0.766]
 [0.941]
 [0.155]
 [0.538]
 [0.716]
 [0.184]
 [0.716]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.21697783455435
printing an ep nov before normalisation:  48.65805462274655
actions average: 
K:  2  action  0 :  tensor([0.4770, 0.0471, 0.0867, 0.1052, 0.0951, 0.0776, 0.1113],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0045, 0.9573, 0.0050, 0.0032, 0.0010, 0.0014, 0.0275],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0698, 0.0032, 0.4493, 0.1050, 0.0681, 0.2070, 0.0975],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1236, 0.0044, 0.1454, 0.2770, 0.1332, 0.1578, 0.1586],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1005, 0.0013, 0.0660, 0.1015, 0.5393, 0.0916, 0.0998],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0862, 0.0818, 0.1932, 0.0932, 0.0812, 0.3613, 0.1030],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1171, 0.0225, 0.2195, 0.1625, 0.1084, 0.1293, 0.2406],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  51 total reward:  0.3066666666666663  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.22257999999999986 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.733116866784975
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.232]
 [0.184]
 [0.162]
 [0.177]
 [0.16 ]
 [0.17 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.176]
 [0.232]
 [0.184]
 [0.162]
 [0.177]
 [0.16 ]
 [0.17 ]]
siam score:  -0.75756425
printing an ep nov before normalisation:  47.38819122314453
maxi score, test score, baseline:  0.22257999999999986 0.6920000000000002 0.6920000000000002
maxi score, test score, baseline:  0.22257999999999986 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.14882263026674
printing an ep nov before normalisation:  32.37384557723999
printing an ep nov before normalisation:  37.53418658224598
actor:  1 policy actor:  1  step number:  50 total reward:  0.36666666666666625  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  53.792497430484865
actor:  1 policy actor:  1  step number:  68 total reward:  0.16666666666666619  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  71.43172621349767
maxi score, test score, baseline:  0.22257999999999986 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.378]
 [0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]] [[35.606]
 [64.509]
 [35.606]
 [35.606]
 [35.606]
 [35.606]
 [35.606]] [[0.973]
 [1.934]
 [0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]]
printing an ep nov before normalisation:  39.5278643764662
printing an ep nov before normalisation:  42.34610557556152
maxi score, test score, baseline:  0.22257999999999986 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.6326208114624
maxi score, test score, baseline:  0.22257999999999986 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[-0.12 ]
 [-0.109]
 [-0.119]
 [-0.119]
 [-0.119]
 [-0.119]
 [-0.12 ]] [[21.119]
 [24.41 ]
 [22.099]
 [21.948]
 [21.624]
 [20.741]
 [20.408]] [[0.295]
 [0.37 ]
 [0.315]
 [0.311]
 [0.305]
 [0.288]
 [0.281]]
maxi score, test score, baseline:  0.22257999999999986 0.6920000000000002 0.6920000000000002
maxi score, test score, baseline:  0.22257999999999986 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.4533333333333335  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.299]
 [0.342]
 [0.263]
 [0.205]
 [0.265]
 [0.301]
 [0.302]] [[54.653]
 [35.529]
 [46.027]
 [46.234]
 [45.608]
 [44.808]
 [44.665]] [[1.557]
 [0.964]
 [1.234]
 [1.183]
 [1.222]
 [1.232]
 [1.227]]
maxi score, test score, baseline:  0.22257999999999986 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.37258319935932
maxi score, test score, baseline:  0.22257999999999986 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.37939854894457
actor:  1 policy actor:  1  step number:  66 total reward:  0.24666666666666615  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 57.82370498200987
using explorer policy with actor:  1
siam score:  -0.76145446
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.526]
 [0.475]
 [0.475]
 [0.475]
 [0.475]
 [0.474]] [[42.815]
 [43.383]
 [42.815]
 [42.815]
 [42.815]
 [42.815]
 [46.886]] [[1.853]
 [1.94 ]
 [1.853]
 [1.853]
 [1.853]
 [1.853]
 [2.107]]
printing an ep nov before normalisation:  42.841554051726
printing an ep nov before normalisation:  42.901294425120206
maxi score, test score, baseline:  0.21920666666666652 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.09166549277194
using explorer policy with actor:  1
printing an ep nov before normalisation:  23.128223419189453
printing an ep nov before normalisation:  43.8392323094032
maxi score, test score, baseline:  0.21920666666666652 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.92581806950529
maxi score, test score, baseline:  0.21920666666666652 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  49.966494192823
printing an ep nov before normalisation:  46.882702328137476
actor:  0 policy actor:  0  step number:  56 total reward:  0.12666666666666604  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2180866666666665 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2180866666666665 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2180866666666665 0.6920000000000002 0.6920000000000002
actor:  0 policy actor:  0  step number:  54 total reward:  0.21999999999999953  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.227]
 [0.264]
 [0.227]
 [0.204]
 [0.227]
 [0.227]
 [0.256]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.227]
 [0.264]
 [0.227]
 [0.204]
 [0.227]
 [0.227]
 [0.256]]
printing an ep nov before normalisation:  45.80742041718229
printing an ep nov before normalisation:  48.075579113824304
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.801]
 [0.814]
 [0.815]
 [0.815]
 [0.817]
 [0.819]] [[45.117]
 [41.304]
 [44.33 ]
 [44.924]
 [47.246]
 [44.479]
 [44.526]] [[0.816]
 [0.801]
 [0.814]
 [0.815]
 [0.815]
 [0.817]
 [0.819]]
actor:  0 policy actor:  0  step number:  46 total reward:  0.36666666666666614  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  12.186653590905934
maxi score, test score, baseline:  0.2165133333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2165133333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  3.1844945381010348
actor:  0 policy actor:  0  step number:  42 total reward:  0.44666666666666655  reward:  1.0 rdn_beta:  1.667
siam score:  -0.751973
maxi score, test score, baseline:  0.2160333333333332 0.6920000000000002 0.6920000000000002
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.573]
 [0.438]
 [0.523]
 [0.438]
 [0.438]
 [0.438]] [[35.616]
 [44.268]
 [35.616]
 [42.397]
 [35.616]
 [35.616]
 [35.616]] [[0.976]
 [1.366]
 [0.976]
 [1.261]
 [0.976]
 [0.976]
 [0.976]]
Printing some Q and Qe and total Qs values:  [[ 0.371]
 [ 0.385]
 [-0.004]
 [ 0.356]
 [ 0.365]
 [ 0.36 ]
 [ 0.358]] [[42.276]
 [49.695]
 [36.754]
 [41.24 ]
 [42.087]
 [42.128]
 [39.626]] [[1.149]
 [1.413]
 [0.587]
 [1.099]
 [1.137]
 [1.133]
 [1.046]]
printing an ep nov before normalisation:  53.135862879324236
actor:  1 policy actor:  1  step number:  41 total reward:  0.5866666666666669  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.613]
 [0.563]
 [0.565]
 [0.568]
 [0.571]
 [0.569]] [[45.03 ]
 [41.038]
 [44.288]
 [44.784]
 [44.54 ]
 [44.629]
 [44.058]] [[1.899]
 [1.723]
 [1.854]
 [1.884]
 [1.873]
 [1.881]
 [1.848]]
maxi score, test score, baseline:  0.2160333333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.651378781968333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  42 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  43 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  1.0
siam score:  -0.7606527
maxi score, test score, baseline:  0.21540666666666655 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.45896625518799
printing an ep nov before normalisation:  49.58923056096592
printing an ep nov before normalisation:  52.83528496600665
maxi score, test score, baseline:  0.21540666666666655 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.438652992248535
printing an ep nov before normalisation:  39.20879126223875
siam score:  -0.7574283
printing an ep nov before normalisation:  43.215276247917444
maxi score, test score, baseline:  0.21540666666666655 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.120671164895185
printing an ep nov before normalisation:  39.74318742752075
siam score:  -0.7558377
maxi score, test score, baseline:  0.21540666666666655 0.6920000000000002 0.6920000000000002
actor:  1 policy actor:  1  step number:  59 total reward:  0.17333333333333278  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.21540666666666655 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.35999999999999954  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  46.29143835790077
actor:  1 policy actor:  1  step number:  61 total reward:  0.2933333333333332  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21540666666666655 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.43204193931447
siam score:  -0.7578075
Printing some Q and Qe and total Qs values:  [[0.828]
 [0.896]
 [0.828]
 [0.828]
 [0.828]
 [0.604]
 [0.828]] [[55.5  ]
 [43.458]
 [55.5  ]
 [55.5  ]
 [55.5  ]
 [50.955]
 [55.5  ]] [[1.16 ]
 [1.128]
 [1.16 ]
 [1.16 ]
 [1.16 ]
 [0.899]
 [1.16 ]]
printing an ep nov before normalisation:  34.73146707351478
actor:  0 policy actor:  0  step number:  34 total reward:  0.6066666666666668  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.13 ]
 [0.197]
 [0.128]
 [0.128]
 [0.128]
 [0.125]
 [0.128]] [[29.241]
 [44.86 ]
 [26.536]
 [26.536]
 [26.536]
 [28.171]
 [26.536]] [[0.244]
 [0.421]
 [0.223]
 [0.223]
 [0.223]
 [0.232]
 [0.223]]
Printing some Q and Qe and total Qs values:  [[0.928]
 [0.928]
 [0.928]
 [0.928]
 [0.928]
 [0.928]
 [0.928]] [[58.527]
 [58.527]
 [58.527]
 [58.527]
 [58.527]
 [58.527]
 [58.527]] [[1.238]
 [1.238]
 [1.238]
 [1.238]
 [1.238]
 [1.238]
 [1.238]]
line 256 mcts: sample exp_bonus 49.47284164426511
actor:  1 policy actor:  1  step number:  54 total reward:  0.11333333333333273  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2163933333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.108]
 [0.144]
 [0.108]
 [0.108]
 [0.108]
 [0.103]
 [0.108]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.108]
 [0.144]
 [0.108]
 [0.108]
 [0.108]
 [0.103]
 [0.108]]
printing an ep nov before normalisation:  48.37548375191533
maxi score, test score, baseline:  0.2163933333333332 0.6920000000000002 0.6920000000000002
printing an ep nov before normalisation:  50.83228616522572
maxi score, test score, baseline:  0.2163933333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.2163933333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.2642389193453
maxi score, test score, baseline:  0.2163933333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2163933333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2163933333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2163933333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 44.57736468147617
actor:  1 policy actor:  1  step number:  69 total reward:  0.37333333333333274  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2163933333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.26288260493219
printing an ep nov before normalisation:  54.50448392630321
printing an ep nov before normalisation:  37.23470718440596
Printing some Q and Qe and total Qs values:  [[0.326]
 [0.418]
 [0.326]
 [0.326]
 [0.406]
 [0.326]
 [0.383]] [[51.369]
 [55.041]
 [51.369]
 [51.369]
 [63.716]
 [51.369]
 [56.314]] [[1.025]
 [1.207]
 [1.025]
 [1.025]
 [1.406]
 [1.025]
 [1.203]]
actor:  0 policy actor:  0  step number:  38 total reward:  0.5000000000000001  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  47.23316968355343
printing an ep nov before normalisation:  44.34301840961573
printing an ep nov before normalisation:  43.48421653832139
maxi score, test score, baseline:  0.21689999999999987 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.3599999999999991  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.21689999999999987 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  38 total reward:  0.5266666666666667  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.84313774108887
maxi score, test score, baseline:  0.21689999999999987 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21689999999999987 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.09534361325889
printing an ep nov before normalisation:  40.831374651134595
actor:  1 policy actor:  1  step number:  45 total reward:  0.4933333333333334  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.21689999999999987 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21689999999999987 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.2713, 0.0356, 0.1598, 0.1133, 0.1234, 0.1188, 0.1778],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0020,     0.9815,     0.0022,     0.0012,     0.0005,     0.0006,
            0.0120], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1179, 0.0170, 0.4081, 0.1004, 0.1007, 0.1182, 0.1378],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1404, 0.1169, 0.1442, 0.1521, 0.1544, 0.1448, 0.1472],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0789, 0.1059, 0.0530, 0.0460, 0.6148, 0.0443, 0.0572],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0767, 0.0096, 0.1088, 0.0768, 0.0746, 0.5726, 0.0809],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1475, 0.0540, 0.1558, 0.1408, 0.1484, 0.1387, 0.2148],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.21689999999999987 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21689999999999987 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21689999999999987 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.36415288768786
printing an ep nov before normalisation:  45.023205015428545
Printing some Q and Qe and total Qs values:  [[0.304]
 [0.328]
 [0.304]
 [0.292]
 [0.291]
 [0.304]
 [0.265]] [[35.688]
 [30.39 ]
 [35.688]
 [37.275]
 [40.603]
 [35.688]
 [38.373]] [[0.871]
 [0.732]
 [0.871]
 [0.909]
 [1.011]
 [0.871]
 [0.916]]
actor:  1 policy actor:  1  step number:  46 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.21689999999999987 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.769788355261376
maxi score, test score, baseline:  0.21689999999999987 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]
 [0.46]] [[58.734]
 [58.734]
 [58.734]
 [58.734]
 [58.734]
 [58.734]
 [58.734]] [[1.46]
 [1.46]
 [1.46]
 [1.46]
 [1.46]
 [1.46]
 [1.46]]
maxi score, test score, baseline:  0.21689999999999987 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.663902608126705
maxi score, test score, baseline:  0.21689999999999987 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.643247741276255
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.55294052075916
actor:  0 policy actor:  0  step number:  52 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.783]
 [0.805]
 [0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]] [[49.039]
 [47.285]
 [49.039]
 [49.039]
 [49.039]
 [49.039]
 [49.039]] [[0.783]
 [0.805]
 [0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]]
actor:  1 policy actor:  1  step number:  70 total reward:  0.25999999999999923  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  46.98857201470269
Printing some Q and Qe and total Qs values:  [[0.883]
 [0.946]
 [0.883]
 [0.883]
 [0.883]
 [0.883]
 [0.852]] [[36.638]
 [45.921]
 [36.638]
 [36.638]
 [36.638]
 [36.638]
 [30.461]] [[0.883]
 [0.946]
 [0.883]
 [0.883]
 [0.883]
 [0.883]
 [0.852]]
printing an ep nov before normalisation:  53.812962772022196
actor:  1 policy actor:  1  step number:  53 total reward:  0.42666666666666664  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.21952666666666656 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.7517498
Printing some Q and Qe and total Qs values:  [[0.909]
 [0.909]
 [0.909]
 [0.909]
 [0.909]
 [0.909]
 [0.909]] [[38.678]
 [38.678]
 [38.678]
 [38.678]
 [38.678]
 [38.678]
 [38.678]] [[0.909]
 [0.909]
 [0.909]
 [0.909]
 [0.909]
 [0.909]
 [0.909]]
printing an ep nov before normalisation:  49.375602421296946
actor:  0 policy actor:  0  step number:  43 total reward:  0.45333333333333314  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.40323074171414
line 256 mcts: sample exp_bonus 41.21402313481173
maxi score, test score, baseline:  0.21705999999999986 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.265 0.347 0.02  0.082 0.102 0.102 0.082]
maxi score, test score, baseline:  0.21705999999999986 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.89151299421388
actor:  1 policy actor:  1  step number:  68 total reward:  0.15333333333333266  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  61 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.21705999999999986 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21705999999999986 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  0.313333333333333  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21705999999999986 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21705999999999986 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21705999999999986 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.21705999999999986 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.992]
 [0.992]
 [0.992]
 [0.992]
 [0.992]
 [0.992]
 [0.992]] [[20.925]
 [20.925]
 [20.925]
 [20.925]
 [20.925]
 [20.925]
 [20.925]] [[1.884]
 [1.884]
 [1.884]
 [1.884]
 [1.884]
 [1.884]
 [1.884]]
maxi score, test score, baseline:  0.21705999999999986 0.6920000000000002 0.6920000000000002
maxi score, test score, baseline:  0.21705999999999986 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  0.3866666666666666  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  38 total reward:  0.47333333333333316  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  69.66458521389093
printing an ep nov before normalisation:  45.57707186590745
printing an ep nov before normalisation:  70.32183649676166
maxi score, test score, baseline:  0.21703333333333322 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]
 [0.434]] [[69.357]
 [69.357]
 [69.357]
 [69.357]
 [69.357]
 [69.357]
 [69.357]] [[1.085]
 [1.085]
 [1.085]
 [1.085]
 [1.085]
 [1.085]
 [1.085]]
actor:  1 policy actor:  1  step number:  60 total reward:  0.11333333333333284  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  60.45874139366739
printing an ep nov before normalisation:  45.7379355435967
printing an ep nov before normalisation:  50.50329141619971
printing an ep nov before normalisation:  46.038533755465785
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.65 ]
 [0.596]
 [0.616]
 [0.432]
 [0.578]
 [0.649]] [[47.821]
 [41.426]
 [45.534]
 [45.716]
 [48.731]
 [46.431]
 [42.913]] [[1.463]
 [1.353]
 [1.433]
 [1.459]
 [1.374]
 [1.445]
 [1.401]]
maxi score, test score, baseline:  0.21703333333333322 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21703333333333322 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.584]
 [0.52 ]
 [0.534]
 [0.493]
 [0.499]
 [0.521]] [[40.898]
 [48.143]
 [41.245]
 [43.969]
 [41.124]
 [40.871]
 [39.906]] [[0.675]
 [0.843]
 [0.709]
 [0.751]
 [0.68 ]
 [0.684]
 [0.696]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  0.43333333333333335  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  32.19586551350133
maxi score, test score, baseline:  0.21703333333333322 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21703333333333322 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.04337311493615
maxi score, test score, baseline:  0.21703333333333322 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  2.8048104948004493
maxi score, test score, baseline:  0.21703333333333322 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.865]
 [0.817]
 [0.817]
 [0.817]
 [0.848]
 [0.817]
 [0.553]] [[47.989]
 [42.219]
 [42.219]
 [42.219]
 [47.573]
 [42.219]
 [41.318]] [[0.865]
 [0.817]
 [0.817]
 [0.817]
 [0.848]
 [0.817]
 [0.553]]
printing an ep nov before normalisation:  29.988456844379904
printing an ep nov before normalisation:  30.364361913979845
printing an ep nov before normalisation:  40.85851367866668
actions average: 
K:  3  action  0 :  tensor([0.1329, 0.0991, 0.1408, 0.1639, 0.1134, 0.1790, 0.1710],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0474, 0.8845, 0.0123, 0.0061, 0.0043, 0.0050, 0.0404],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1083, 0.0074, 0.3695, 0.1526, 0.1111, 0.1287, 0.1223],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1203, 0.0299, 0.1317, 0.2678, 0.1202, 0.1543, 0.1758],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1176, 0.0016, 0.0513, 0.1246, 0.5265, 0.0946, 0.0838],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1133, 0.0035, 0.1605, 0.1175, 0.1455, 0.3533, 0.1064],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0717, 0.2892, 0.0442, 0.1155, 0.0411, 0.0468, 0.3915],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  48.87953464002026
printing an ep nov before normalisation:  27.518563471371586
printing an ep nov before normalisation:  40.008352701085236
maxi score, test score, baseline:  0.2170333333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21456666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21456666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21456666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.35333333333333294  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.21456666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21456666666666654 0.6920000000000002 0.6920000000000002
actions average: 
K:  1  action  0 :  tensor([0.3281, 0.0061, 0.1551, 0.1314, 0.1304, 0.1112, 0.1376],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0062, 0.8879, 0.0166, 0.0187, 0.0020, 0.0101, 0.0585],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.1046, 0.0179, 0.3971, 0.1422, 0.0907, 0.1314, 0.1161],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1302, 0.0028, 0.1469, 0.2649, 0.1378, 0.1355, 0.1820],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1020, 0.0011, 0.0962, 0.0712, 0.5670, 0.0790, 0.0836],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1206, 0.0069, 0.1978, 0.1176, 0.1246, 0.3105, 0.1220],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0762, 0.0714, 0.0687, 0.0901, 0.0736, 0.0602, 0.5597],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.21456666666666654 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.563777227823852
printing an ep nov before normalisation:  32.90949796297776
actions average: 
K:  1  action  0 :  tensor([0.5893, 0.0115, 0.1102, 0.0458, 0.1181, 0.0535, 0.0716],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0051,     0.9239,     0.0328,     0.0024,     0.0009,     0.0030,
            0.0318], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0930, 0.0497, 0.3650, 0.1044, 0.0870, 0.2080, 0.0929],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0986, 0.0622, 0.1348, 0.3044, 0.1217, 0.1421, 0.1362],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1098, 0.0112, 0.0828, 0.0918, 0.5194, 0.0740, 0.1111],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1028, 0.0022, 0.1220, 0.0924, 0.1049, 0.4818, 0.0938],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.1697, 0.0309, 0.2185, 0.1225, 0.1269, 0.1459, 0.1854],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.53202579941879
siam score:  -0.75748664
actions average: 
K:  4  action  0 :  tensor([0.3499, 0.1077, 0.1081, 0.1013, 0.1280, 0.0855, 0.1195],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0188, 0.8792, 0.0140, 0.0369, 0.0099, 0.0092, 0.0319],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0722, 0.0362, 0.5830, 0.0558, 0.0654, 0.1282, 0.0592],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1661, 0.0140, 0.1221, 0.1609, 0.2588, 0.0986, 0.1796],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1458, 0.0204, 0.1320, 0.1355, 0.3061, 0.1187, 0.1415],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.1279, 0.0161, 0.1603, 0.1125, 0.1167, 0.3485, 0.1180],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.0856, 0.1119, 0.1077, 0.1018, 0.0782, 0.1779, 0.3368],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  49 total reward:  0.41333333333333333  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.641]
 [0.614]
 [0.614]
 [0.614]
 [0.614]
 [0.614]] [[52.243]
 [56.149]
 [52.243]
 [52.243]
 [52.243]
 [52.243]
 [52.243]] [[1.65 ]
 [1.795]
 [1.65 ]
 [1.65 ]
 [1.65 ]
 [1.65 ]
 [1.65 ]]
printing an ep nov before normalisation:  65.75286545935667
printing an ep nov before normalisation:  38.8411410834366
Printing some Q and Qe and total Qs values:  [[0.722]
 [0.844]
 [0.416]
 [0.722]
 [0.722]
 [0.722]
 [0.738]] [[45.666]
 [42.952]
 [44.634]
 [45.666]
 [45.666]
 [45.666]
 [45.356]] [[2.055]
 [2.038]
 [1.696]
 [2.055]
 [2.055]
 [2.055]
 [2.055]]
printing an ep nov before normalisation:  52.32873695955398
printing an ep nov before normalisation:  52.58092589216841
printing an ep nov before normalisation:  49.14463172502663
maxi score, test score, baseline:  0.2150733333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.814]
 [0.897]
 [0.814]
 [0.814]
 [0.814]
 [0.814]
 [0.814]] [[58.317]
 [50.584]
 [58.317]
 [58.317]
 [58.317]
 [58.317]
 [58.317]] [[2.07 ]
 [1.854]
 [2.07 ]
 [2.07 ]
 [2.07 ]
 [2.07 ]
 [2.07 ]]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  34 total reward:  0.5533333333333333  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.21553999999999987 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21553999999999987 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.164]
 [0.147]
 [0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.164]
 [0.147]
 [0.164]
 [0.164]
 [0.164]
 [0.164]
 [0.164]]
printing an ep nov before normalisation:  38.198823796305554
maxi score, test score, baseline:  0.21553999999999987 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.98288657848757
maxi score, test score, baseline:  0.21553999999999987 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  70.0585488582179
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.245]
 [0.318]
 [0.286]
 [0.282]
 [0.275]
 [0.245]
 [0.277]] [[46.865]
 [47.707]
 [35.786]
 [34.929]
 [34.908]
 [46.865]
 [34.039]] [[0.939]
 [1.034]
 [0.687]
 [0.661]
 [0.653]
 [0.939]
 [0.632]]
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.337]
 [0.321]
 [0.312]
 [0.312]
 [0.325]
 [0.312]] [[45.522]
 [49.961]
 [49.04 ]
 [44.317]
 [44.317]
 [43.081]
 [44.317]] [[1.022]
 [1.168]
 [1.125]
 [0.98 ]
 [0.98 ]
 [0.958]
 [0.98 ]]
maxi score, test score, baseline:  0.21553999999999987 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.40495238493178
actor:  0 policy actor:  0  step number:  60 total reward:  0.4066666666666666  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.116]
 [0.111]
 [0.116]
 [0.097]
 [0.116]
 [0.116]
 [0.116]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.116]
 [0.111]
 [0.116]
 [0.097]
 [0.116]
 [0.116]
 [0.116]]
maxi score, test score, baseline:  0.21524666666666656 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.314]
 [0.288]
 [0.304]
 [0.297]
 [0.297]
 [0.309]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.286]
 [0.314]
 [0.288]
 [0.304]
 [0.297]
 [0.297]
 [0.309]]
actor:  1 policy actor:  1  step number:  49 total reward:  0.41333333333333333  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 25.356147619068917
actor:  1 policy actor:  1  step number:  53 total reward:  0.3066666666666662  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  41.79160097492066
maxi score, test score, baseline:  0.21524666666666656 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.768843636548304
Printing some Q and Qe and total Qs values:  [[0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]
 [0.613]] [[60.267]
 [60.267]
 [60.267]
 [60.267]
 [60.267]
 [60.267]
 [60.267]] [[2.613]
 [2.613]
 [2.613]
 [2.613]
 [2.613]
 [2.613]
 [2.613]]
printing an ep nov before normalisation:  31.460103465583988
actor:  1 policy actor:  1  step number:  40 total reward:  0.45999999999999996  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.21524666666666656 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21524666666666656 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21524666666666656 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21524666666666656 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.408]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]
 [0.33 ]] [[39.854]
 [57.561]
 [39.854]
 [39.854]
 [39.854]
 [39.854]
 [39.854]] [[1.111]
 [1.882]
 [1.111]
 [1.111]
 [1.111]
 [1.111]
 [1.111]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[ 0.254]
 [ 0.415]
 [ 0.311]
 [ 0.247]
 [ 0.311]
 [-0.106]
 [ 0.277]] [[33.315]
 [42.927]
 [40.894]
 [28.237]
 [40.894]
 [32.738]
 [28.443]] [[0.409]
 [0.646]
 [0.525]
 [0.362]
 [0.525]
 [0.044]
 [0.394]]
maxi score, test score, baseline:  0.21524666666666656 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.21524666666666656 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  53 total reward:  0.37333333333333296  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  47.14411310904692
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  41.428139168519984
printing an ep nov before normalisation:  48.125958357084066
Printing some Q and Qe and total Qs values:  [[-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.083]
 [-0.066]
 [-0.083]] [[30.318]
 [30.318]
 [30.318]
 [30.318]
 [30.318]
 [39.749]
 [30.318]] [[1.097]
 [1.097]
 [1.097]
 [1.097]
 [1.097]
 [1.482]
 [1.097]]
Printing some Q and Qe and total Qs values:  [[ 0.156]
 [ 0.156]
 [ 0.156]
 [-0.076]
 [ 0.156]
 [ 0.163]
 [ 0.156]] [[59.615]
 [59.615]
 [59.615]
 [73.926]
 [59.615]
 [60.598]
 [59.615]] [[1.272]
 [1.272]
 [1.272]
 [1.564]
 [1.272]
 [1.315]
 [1.272]]
maxi score, test score, baseline:  0.2179933333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.34185309256434
printing an ep nov before normalisation:  58.946730927416375
actor:  1 policy actor:  1  step number:  49 total reward:  0.35999999999999965  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  63.47335395182045
printing an ep nov before normalisation:  65.77674246256169
maxi score, test score, baseline:  0.2179933333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.49914360046387
Printing some Q and Qe and total Qs values:  [[-0.028]
 [-0.028]
 [ 0.263]
 [-0.028]
 [-0.028]
 [ 0.404]
 [-0.028]] [[46.774]
 [46.774]
 [41.988]
 [46.774]
 [46.774]
 [41.953]
 [46.774]] [[0.601]
 [0.601]
 [0.786]
 [0.601]
 [0.601]
 [0.927]
 [0.601]]
maxi score, test score, baseline:  0.2179933333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2179933333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.462]
 [0.531]
 [0.462]
 [0.462]
 [0.462]
 [0.462]
 [0.462]] [[43.613]
 [42.69 ]
 [43.613]
 [43.613]
 [43.613]
 [43.613]
 [43.613]] [[1.633]
 [1.654]
 [1.633]
 [1.633]
 [1.633]
 [1.633]
 [1.633]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.2237293191668
maxi score, test score, baseline:  0.2179933333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.708]
 [0.683]
 [0.683]
 [0.683]
 [0.683]
 [0.683]] [[39.231]
 [50.015]
 [39.231]
 [39.231]
 [39.231]
 [39.231]
 [39.231]] [[1.42 ]
 [1.841]
 [1.42 ]
 [1.42 ]
 [1.42 ]
 [1.42 ]
 [1.42 ]]
maxi score, test score, baseline:  0.2179933333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  0.4866666666666666  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  45.99393715478907
actor:  1 policy actor:  1  step number:  64 total reward:  0.15333333333333266  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.715]
 [0.635]
 [0.635]
 [0.635]
 [0.635]
 [0.635]] [[35.736]
 [45.61 ]
 [35.736]
 [35.736]
 [35.736]
 [35.736]
 [35.736]] [[1.01 ]
 [1.294]
 [1.01 ]
 [1.01 ]
 [1.01 ]
 [1.01 ]
 [1.01 ]]
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.415]
 [0.507]
 [0.496]
 [0.415]
 [0.415]
 [0.415]] [[30.585]
 [30.585]
 [29.517]
 [29.84 ]
 [30.585]
 [30.585]
 [30.585]] [[1.114]
 [1.114]
 [1.16 ]
 [1.163]
 [1.114]
 [1.114]
 [1.114]]
maxi score, test score, baseline:  0.2179933333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 53.66846280648402
actor:  1 policy actor:  1  step number:  71 total reward:  0.06666666666666621  reward:  1.0 rdn_beta:  0.667
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.2179933333333332 0.6920000000000002 0.6920000000000002
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]
 [0.136]]
printing an ep nov before normalisation:  35.389154100213055
printing an ep nov before normalisation:  46.413955152296104
actions average: 
K:  3  action  0 :  tensor([0.7042, 0.0310, 0.0663, 0.0395, 0.0569, 0.0373, 0.0648],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0034,     0.9736,     0.0029,     0.0021,     0.0007,     0.0006,
            0.0165], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0835, 0.0829, 0.4538, 0.0856, 0.0686, 0.1231, 0.1026],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0850, 0.0379, 0.0990, 0.4332, 0.0652, 0.1261, 0.1536],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1199, 0.0060, 0.1168, 0.1542, 0.3368, 0.1182, 0.1481],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0817, 0.0092, 0.2084, 0.0858, 0.0835, 0.4202, 0.1112],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1622, 0.1199, 0.1078, 0.1000, 0.1014, 0.0828, 0.3258],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.83066834343804
line 256 mcts: sample exp_bonus 38.087180680984105
printing an ep nov before normalisation:  45.58488564686986
printing an ep nov before normalisation:  41.911127408571915
printing an ep nov before normalisation:  40.95151476537616
printing an ep nov before normalisation:  38.08317188975437
printing an ep nov before normalisation:  37.233339026349086
printing an ep nov before normalisation:  0.07886510411196923
printing an ep nov before normalisation:  0.22341517967987556
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  25 total reward:  0.6933333333333335  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  26 total reward:  0.6866666666666668  reward:  1.0 rdn_beta:  2
actor:  0 policy actor:  0  step number:  27 total reward:  0.6666666666666667  reward:  1.0 rdn_beta:  2
actor:  1 policy actor:  1  step number:  51 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  50.590074888020894
printing an ep nov before normalisation:  47.72489544736952
maxi score, test score, baseline:  0.23563333333333322 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23563333333333322 0.6913333333333335 0.6913333333333335
actor:  1 policy actor:  1  step number:  51 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  48.64101224583703
actor:  1 policy actor:  1  step number:  53 total reward:  0.41333333333333333  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.6621150970459
maxi score, test score, baseline:  0.23563333333333322 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  0.23563333333333322 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23563333333333322 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  45.30754251514157
maxi score, test score, baseline:  0.23563333333333322 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.24367297759151
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.548]
 [0.564]
 [0.518]
 [0.564]
 [0.564]
 [0.564]] [[39.694]
 [40.726]
 [43.876]
 [40.252]
 [43.876]
 [43.876]
 [43.876]] [[2.403]
 [2.548]
 [2.866]
 [2.472]
 [2.866]
 [2.866]
 [2.866]]
maxi score, test score, baseline:  0.23563333333333322 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.315192315939914
actor:  1 policy actor:  1  step number:  67 total reward:  0.19999999999999907  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  64.1351289654344
maxi score, test score, baseline:  0.23563333333333322 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23563333333333322 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  40.893768315884095
maxi score, test score, baseline:  0.23563333333333322 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.0416245456853
actor:  1 policy actor:  1  step number:  44 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  2.0
siam score:  -0.7593908
maxi score, test score, baseline:  0.23563333333333322 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  0.23563333333333322 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23563333333333322 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.914]
 [0.983]
 [0.914]
 [0.914]
 [0.914]
 [0.914]
 [0.914]] [[46.054]
 [52.532]
 [46.054]
 [46.054]
 [46.054]
 [46.054]
 [46.054]] [[1.946]
 [2.316]
 [1.946]
 [1.946]
 [1.946]
 [1.946]
 [1.946]]
printing an ep nov before normalisation:  42.868997987204175
maxi score, test score, baseline:  0.23563333333333322 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.38  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.23563333333333322 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  0.3133333333333326  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.21126686995599
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.23563333333333322 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  45.98152111838146
printing an ep nov before normalisation:  43.625463705645224
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.19831354537246
printing an ep nov before normalisation:  58.44099871006765
actions average: 
K:  3  action  0 :  tensor([0.1994, 0.0012, 0.1074, 0.1206, 0.3113, 0.1132, 0.1469],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0071, 0.9622, 0.0064, 0.0042, 0.0030, 0.0031, 0.0140],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1099, 0.0094, 0.4399, 0.1074, 0.0858, 0.1346, 0.1131],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1865, 0.0043, 0.1624, 0.1658, 0.1478, 0.1614, 0.1718],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.1262,     0.0003,     0.0635,     0.0737,     0.5969,     0.0620,
            0.0774], grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1030, 0.0041, 0.1168, 0.1273, 0.0794, 0.4357, 0.1336],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.2166, 0.0496, 0.1074, 0.1434, 0.0753, 0.1061, 0.3015],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.23563333333333322 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  0.23563333333333322 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  50 total reward:  0.33999999999999975  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  33.95809778146518
actor:  1 policy actor:  1  step number:  45 total reward:  0.4266666666666665  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.23831333333333318 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23831333333333318 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  0.3733333333333334  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  40.24322712607121
maxi score, test score, baseline:  0.23831333333333318 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.59440234906929
Printing some Q and Qe and total Qs values:  [[0.831]
 [0.81 ]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.829]] [[27.784]
 [50.782]
 [27.784]
 [27.784]
 [27.784]
 [27.784]
 [30.355]] [[0.831]
 [0.81 ]
 [0.831]
 [0.831]
 [0.831]
 [0.831]
 [0.829]]
siam score:  -0.75395703
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.609]
 [0.623]
 [0.64 ]
 [0.569]
 [0.548]
 [0.609]] [[43.52 ]
 [45.701]
 [48.356]
 [45.885]
 [46.137]
 [46.731]
 [46.004]] [[1.753]
 [1.88 ]
 [2.037]
 [1.921]
 [1.864]
 [1.875]
 [1.896]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.32666666666666644  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  56 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  40.5904853777363
maxi score, test score, baseline:  0.2382866666666665 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2382866666666665 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  0.3733333333333332  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  46.865054176147886
printing an ep nov before normalisation:  41.41493877098725
printing an ep nov before normalisation:  39.93551710732767
actions average: 
K:  2  action  0 :  tensor([0.4031, 0.0097, 0.1192, 0.1162, 0.1148, 0.1171, 0.1200],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0143,     0.9488,     0.0076,     0.0039,     0.0014,     0.0008,
            0.0232], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.1021, 0.0451, 0.4170, 0.0951, 0.0838, 0.1416, 0.1153],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1075, 0.0101, 0.1029, 0.3687, 0.0945, 0.1593, 0.1569],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.1006,     0.0004,     0.0460,     0.0603,     0.6782,     0.0625,
            0.0520], grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0803, 0.0032, 0.1201, 0.0791, 0.0718, 0.5434, 0.1022],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.1220, 0.2301, 0.1034, 0.0849, 0.0673, 0.0662, 0.3262],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.26283932381162
printing an ep nov before normalisation:  47.275533648687066
siam score:  -0.7596021
maxi score, test score, baseline:  0.2382866666666665 0.6913333333333335 0.6913333333333335
siam score:  -0.7597703
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.547]
 [0.547]
 [0.585]
 [0.547]
 [0.522]
 [0.547]] [[47.478]
 [47.478]
 [47.478]
 [48.86 ]
 [47.478]
 [49.354]
 [47.478]] [[2.083]
 [2.083]
 [2.083]
 [2.198]
 [2.083]
 [2.162]
 [2.083]]
maxi score, test score, baseline:  0.2382866666666665 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.68332253765692
maxi score, test score, baseline:  0.2382866666666665 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5333333333333335  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  53 total reward:  0.3333333333333329  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actor:  0 policy actor:  0  step number:  51 total reward:  0.3466666666666661  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  33.65168653391632
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.97345946915415
siam score:  -0.75622594
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
actor:  1 policy actor:  1  step number:  56 total reward:  0.13999999999999946  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.473]
 [0.538]
 [0.468]
 [0.475]
 [0.478]
 [0.475]
 [0.434]] [[35.546]
 [35.782]
 [37.194]
 [35.961]
 [37.754]
 [35.961]
 [34.541]] [[0.81 ]
 [0.881]
 [0.849]
 [0.823]
 [0.875]
 [0.823]
 [0.743]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  66 total reward:  0.16666666666666563  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  38.34031105041504
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  0.33999999999999986  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.74830353
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  0.5466666666666669  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  44.806990267245794
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 43.90981057265812
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  0.15333333333333288  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  49 total reward:  0.5066666666666669  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.81752811209978
printing an ep nov before normalisation:  46.20750069719976
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.061 0.245 0.102 0.061 0.388 0.061 0.082]
printing an ep nov before normalisation:  49.941503923784424
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.41038233161186
printing an ep nov before normalisation:  16.134166026112137
actor:  1 policy actor:  1  step number:  63 total reward:  0.239999999999999  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  27.347981589380677
actor:  1 policy actor:  1  step number:  48 total reward:  0.38  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  47 total reward:  0.4133333333333332  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.196724235095665
siam score:  -0.74994814
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.23575469013756
printing an ep nov before normalisation:  43.0813477849447
Printing some Q and Qe and total Qs values:  [[0.716]
 [0.756]
 [0.721]
 [0.717]
 [0.715]
 [0.716]
 [0.714]] [[31.902]
 [46.757]
 [32.789]
 [35.366]
 [32.533]
 [32.849]
 [32.817]] [[0.716]
 [0.756]
 [0.721]
 [0.717]
 [0.715]
 [0.716]
 [0.714]]
maxi score, test score, baseline:  0.24097999999999983 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  22.96959638595581
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.456]
 [0.624]
 [0.617]
 [0.611]
 [0.636]
 [0.597]] [[13.289]
 [15.907]
 [20.204]
 [16.269]
 [15.974]
 [15.842]
 [12.706]] [[0.617]
 [0.456]
 [0.624]
 [0.617]
 [0.611]
 [0.636]
 [0.597]]
actor:  1 policy actor:  1  step number:  53 total reward:  0.34666666666666657  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  57 total reward:  0.22666666666666635  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  50.819275921661195
printing an ep nov before normalisation:  46.06065785780484
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.20115974451344
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.202274635790126
printing an ep nov before normalisation:  52.623027614980664
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  0.33999999999999964  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.508]
 [0.534]
 [0.508]
 [0.508]
 [0.508]
 [0.508]
 [0.508]] [[40.661]
 [52.779]
 [40.661]
 [40.661]
 [40.661]
 [40.661]
 [40.661]] [[1.334]
 [1.825]
 [1.334]
 [1.334]
 [1.334]
 [1.334]
 [1.334]]
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
actor:  1 policy actor:  1  step number:  49 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.15611174686439
printing an ep nov before normalisation:  48.25318115679559
printing an ep nov before normalisation:  50.63197875348629
printing an ep nov before normalisation:  0.016377678271339846
actor:  1 policy actor:  1  step number:  49 total reward:  0.38666666666666627  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  59.93326039707457
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
Printing some Q and Qe and total Qs values:  [[0.208]
 [0.213]
 [0.111]
 [0.205]
 [0.203]
 [0.206]
 [0.202]] [[33.192]
 [34.007]
 [36.404]
 [34.171]
 [33.431]
 [34.078]
 [33.425]] [[0.438]
 [0.454]
 [0.384]
 [0.449]
 [0.437]
 [0.448]
 [0.436]]
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
actor:  1 policy actor:  1  step number:  52 total reward:  0.3800000000000001  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.03307006864202
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.696]
 [0.542]
 [0.542]
 [0.542]
 [0.542]
 [0.542]] [[41.018]
 [48.215]
 [41.018]
 [41.018]
 [41.018]
 [41.018]
 [41.018]] [[1.695]
 [2.191]
 [1.695]
 [1.695]
 [1.695]
 [1.695]
 [1.695]]
Printing some Q and Qe and total Qs values:  [[0.686]
 [0.71 ]
 [0.689]
 [0.489]
 [0.489]
 [0.703]
 [0.489]] [[41.597]
 [40.834]
 [32.714]
 [35.012]
 [35.012]
 [39.649]
 [35.012]] [[2.091]
 [2.088]
 [1.794]
 [1.671]
 [1.671]
 [2.041]
 [1.671]]
actor:  1 policy actor:  1  step number:  61 total reward:  0.1999999999999995  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  1  action  0 :  tensor([0.5822, 0.0019, 0.0687, 0.0732, 0.1007, 0.0612, 0.1122],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0029,     0.9522,     0.0014,     0.0020,     0.0003,     0.0003,
            0.0409], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0633, 0.0102, 0.5142, 0.0833, 0.0617, 0.1662, 0.1012],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1432, 0.1081, 0.1218, 0.2383, 0.1008, 0.1190, 0.1688],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1384, 0.0080, 0.0913, 0.1127, 0.4138, 0.1149, 0.1208],
       grad_fn=<DivBackward0>)
K:  1  action  5 :  tensor([0.1049, 0.0181, 0.1340, 0.1124, 0.0961, 0.4053, 0.1293],
       grad_fn=<DivBackward0>)
K:  1  action  6 :  tensor([0.0989, 0.1533, 0.0968, 0.1570, 0.0743, 0.0912, 0.3285],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.2651, 0.0239, 0.1161, 0.0977, 0.1852, 0.1354, 0.1766],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0047, 0.9118, 0.0085, 0.0368, 0.0018, 0.0033, 0.0332],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0660, 0.0545, 0.3887, 0.0529, 0.0528, 0.1117, 0.2734],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1051, 0.0564, 0.1284, 0.2552, 0.1219, 0.1525, 0.1804],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0987, 0.0409, 0.0986, 0.0947, 0.4526, 0.1005, 0.1140],
       grad_fn=<DivBackward0>)
K:  4  action  5 :  tensor([0.0547, 0.0670, 0.0971, 0.0448, 0.0654, 0.5697, 0.1015],
       grad_fn=<DivBackward0>)
K:  4  action  6 :  tensor([0.1347, 0.0257, 0.1637, 0.1403, 0.1553, 0.1814, 0.1989],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.4600000000000001  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.35091377587429
printing an ep nov before normalisation:  63.73929831166654
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
actor:  1 policy actor:  1  step number:  43 total reward:  0.4266666666666664  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]
 [0.34]] [[39.186]
 [39.186]
 [39.186]
 [39.186]
 [39.186]
 [39.186]
 [39.186]] [[52.575]
 [52.575]
 [52.575]
 [52.575]
 [52.575]
 [52.575]
 [52.575]]
printing an ep nov before normalisation:  0.006814911331360918
actor:  1 policy actor:  1  step number:  56 total reward:  0.36666666666666636  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  18.91639471054077
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  83.26617529404447
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3933333333333334  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  43 total reward:  0.5066666666666668  reward:  1.0 rdn_beta:  1.667
siam score:  -0.7495079
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.87766228178625
Printing some Q and Qe and total Qs values:  [[0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]
 [0.636]] [[45.433]
 [45.433]
 [45.433]
 [45.433]
 [45.433]
 [45.433]
 [45.433]] [[1.117]
 [1.117]
 [1.117]
 [1.117]
 [1.117]
 [1.117]
 [1.117]]
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
Printing some Q and Qe and total Qs values:  [[0.31 ]
 [0.343]
 [0.31 ]
 [0.266]
 [0.26 ]
 [0.27 ]
 [0.31 ]] [[36.859]
 [44.279]
 [36.859]
 [28.567]
 [28.586]
 [28.8  ]
 [36.859]] [[1.018]
 [1.321]
 [1.018]
 [0.672]
 [0.666]
 [0.685]
 [1.018]]
actor:  1 policy actor:  1  step number:  54 total reward:  0.31333333333333324  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  39.29166793823242
printing an ep nov before normalisation:  33.37270259857178
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 36.821671494874025
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.36614179281429
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.8392621487987
printing an ep nov before normalisation:  47.569379921556944
Printing some Q and Qe and total Qs values:  [[0.661]
 [0.736]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]] [[45.913]
 [50.532]
 [45.913]
 [45.913]
 [45.913]
 [45.913]
 [45.913]] [[0.661]
 [0.736]
 [0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]]
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.06931710865652
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  48.939104080200195
siam score:  -0.7420712
maxi score, test score, baseline:  0.2434333333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.332]
 [0.332]
 [0.332]
 [0.332]
 [0.276]
 [0.332]
 [0.332]] [[49.587]
 [49.587]
 [49.587]
 [49.587]
 [66.353]
 [49.587]
 [49.587]] [[1.112]
 [1.112]
 [1.112]
 [1.112]
 [1.593]
 [1.112]
 [1.112]]
printing an ep nov before normalisation:  48.68851304699139
actor:  0 policy actor:  0  step number:  41 total reward:  0.5066666666666666  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  59.595020738343
actions average: 
K:  2  action  0 :  tensor([0.4031, 0.0628, 0.1058, 0.0934, 0.1113, 0.1009, 0.1227],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0130, 0.8813, 0.0077, 0.0187, 0.0238, 0.0040, 0.0514],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0593, 0.0052, 0.4766, 0.0689, 0.0733, 0.2396, 0.0771],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0744, 0.0090, 0.0990, 0.4874, 0.1071, 0.1030, 0.1202],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1828, 0.0284, 0.0899, 0.1195, 0.3679, 0.1073, 0.1042],
       grad_fn=<DivBackward0>)
K:  2  action  5 :  tensor([0.0751, 0.0585, 0.1120, 0.0994, 0.0833, 0.4817, 0.0901],
       grad_fn=<DivBackward0>)
K:  2  action  6 :  tensor([0.0664, 0.2339, 0.0725, 0.0931, 0.0855, 0.0758, 0.3728],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  63 total reward:  0.15999999999999936  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.24644666666666656 0.6913333333333335 0.6913333333333335
Printing some Q and Qe and total Qs values:  [[0.383]
 [0.383]
 [0.383]
 [0.383]
 [0.295]
 [0.383]
 [0.327]] [[47.7  ]
 [47.7  ]
 [47.7  ]
 [47.7  ]
 [64.984]
 [47.7  ]
 [52.684]] [[0.873]
 [0.873]
 [0.873]
 [0.873]
 [1.095]
 [0.873]
 [0.907]]
actor:  1 policy actor:  1  step number:  52 total reward:  0.3933333333333332  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.24644666666666656 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24644666666666656 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]
 [0.363]] [[59.]
 [59.]
 [59.]
 [59.]
 [59.]
 [59.]
 [59.]] [[1.27]
 [1.27]
 [1.27]
 [1.27]
 [1.27]
 [1.27]
 [1.27]]
printing an ep nov before normalisation:  51.60400390622925
printing an ep nov before normalisation:  50.77696837744589
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24644666666666656 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.364041791429756
maxi score, test score, baseline:  0.24644666666666656 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  0.5200000000000002  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  0.0004506635434609052
printing an ep nov before normalisation:  0.0001742192841902579
actor:  1 policy actor:  1  step number:  45 total reward:  0.45333333333333337  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  67.09075637012553
Printing some Q and Qe and total Qs values:  [[0.394]
 [0.428]
 [0.394]
 [0.394]
 [0.394]
 [0.394]
 [0.394]] [[34.253]
 [57.814]
 [34.253]
 [34.253]
 [34.253]
 [34.253]
 [34.253]] [[0.589]
 [0.919]
 [0.589]
 [0.589]
 [0.589]
 [0.589]
 [0.589]]
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.758]] [[43.446]
 [43.446]
 [43.446]
 [43.446]
 [43.446]
 [43.446]
 [43.446]] [[0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.758]
 [0.758]]
printing an ep nov before normalisation:  15.818336009979248
actor:  1 policy actor:  1  step number:  63 total reward:  0.1999999999999993  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.58726700310934
Printing some Q and Qe and total Qs values:  [[0.162]
 [0.287]
 [0.162]
 [0.152]
 [0.089]
 [0.162]
 [0.162]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.162]
 [0.287]
 [0.162]
 [0.152]
 [0.089]
 [0.162]
 [0.162]]
maxi score, test score, baseline:  0.24644666666666654 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24644666666666654 0.6913333333333335 0.6913333333333335
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
actor:  1 policy actor:  1  step number:  48 total reward:  0.3133333333333329  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  3  action  0 :  tensor([0.5618, 0.0019, 0.0651, 0.0657, 0.1669, 0.0725, 0.0661],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0137, 0.8851, 0.0121, 0.0216, 0.0097, 0.0112, 0.0466],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1041, 0.0057, 0.3804, 0.1097, 0.0940, 0.1791, 0.1269],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0811, 0.0465, 0.0825, 0.3922, 0.0794, 0.0898, 0.2285],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1055, 0.0038, 0.0639, 0.0781, 0.5443, 0.0899, 0.1146],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0886, 0.0071, 0.1295, 0.0956, 0.0928, 0.4880, 0.0983],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.0657, 0.3033, 0.0638, 0.0867, 0.0535, 0.0525, 0.3746],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  47.17653556765479
actions average: 
K:  3  action  0 :  tensor([0.3295, 0.0511, 0.1231, 0.1291, 0.1047, 0.1226, 0.1399],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0045, 0.9427, 0.0031, 0.0087, 0.0014, 0.0013, 0.0382],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0796, 0.0040, 0.4701, 0.0838, 0.0699, 0.1521, 0.1405],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0833, 0.1333, 0.0353, 0.6713, 0.0095, 0.0038, 0.0634],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.1016,     0.0001,     0.0484,     0.0478,     0.6857,     0.0580,
            0.0585], grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0598, 0.0078, 0.0888, 0.0943, 0.0598, 0.5944, 0.0950],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1507, 0.1876, 0.0920, 0.0811, 0.0809, 0.0821, 0.3256],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.43469888953896
maxi score, test score, baseline:  0.24416666666666653 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.057884686027414
printing an ep nov before normalisation:  38.81422519683838
printing an ep nov before normalisation:  0.14543331807814752
actor:  1 policy actor:  1  step number:  64 total reward:  0.433333333333333  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.24416666666666653 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.384]
 [0.301]
 [0.301]
 [0.251]
 [0.301]
 [0.301]] [[41.916]
 [43.063]
 [41.916]
 [41.916]
 [50.43 ]
 [41.916]
 [41.916]] [[0.502]
 [0.594]
 [0.502]
 [0.502]
 [0.526]
 [0.502]
 [0.502]]
printing an ep nov before normalisation:  29.010349231400927
actor:  1 policy actor:  1  step number:  53 total reward:  0.44000000000000017  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.24416666666666653 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  0.293333333333333  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.84802562149662
printing an ep nov before normalisation:  53.966512936936056
printing an ep nov before normalisation:  47.34875017048442
siam score:  -0.75140244
printing an ep nov before normalisation:  47.73931276548159
actor:  1 policy actor:  1  step number:  51 total reward:  0.3333333333333328  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
siam score:  -0.7509062
maxi score, test score, baseline:  0.24416666666666653 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24416666666666653 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  0.4666666666666667  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.24416666666666653 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.15]
 [0.15]
 [0.15]
 [0.15]
 [0.15]
 [0.15]
 [0.15]] [[38.943]
 [38.943]
 [38.943]
 [38.943]
 [38.943]
 [38.943]
 [38.943]] [[13.117]
 [13.117]
 [13.117]
 [13.117]
 [13.117]
 [13.117]
 [13.117]]
printing an ep nov before normalisation:  48.051866191172024
actor:  1 policy actor:  1  step number:  71 total reward:  0.10666666666666569  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  31.29908561706543
printing an ep nov before normalisation:  34.882988970091866
maxi score, test score, baseline:  0.24416666666666653 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.642667100595695
maxi score, test score, baseline:  0.24416666666666653 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24416666666666653 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24416666666666653 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24137999999999984 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.742]
 [0.689]
 [0.656]
 [0.671]
 [0.68 ]
 [0.655]] [[39.574]
 [42.355]
 [39.506]
 [39.078]
 [39.568]
 [40.973]
 [39.657]] [[1.856]
 [1.991]
 [1.855]
 [1.809]
 [1.839]
 [1.889]
 [1.826]]
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.748]
 [0.67 ]
 [0.631]
 [0.632]
 [0.408]
 [0.422]] [[35.891]
 [33.765]
 [36.069]
 [35.074]
 [35.155]
 [36.02 ]
 [30.41 ]] [[1.835]
 [1.839]
 [1.836]
 [1.765]
 [1.768]
 [1.572]
 [1.404]]
actor:  1 policy actor:  1  step number:  51 total reward:  0.42666666666666675  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.24137999999999984 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  11042
printing an ep nov before normalisation:  47.909698405465555
siam score:  -0.7514125
maxi score, test score, baseline:  0.24137999999999984 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.565]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]
 [0.49 ]] [[42.188]
 [44.762]
 [42.188]
 [42.188]
 [42.188]
 [42.188]
 [42.188]] [[1.381]
 [1.562]
 [1.381]
 [1.381]
 [1.381]
 [1.381]
 [1.381]]
maxi score, test score, baseline:  0.24137999999999984 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.597]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]
 [0.5  ]] [[46.136]
 [50.228]
 [46.136]
 [46.136]
 [46.136]
 [46.136]
 [46.136]] [[1.302]
 [1.514]
 [1.302]
 [1.302]
 [1.302]
 [1.302]
 [1.302]]
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.562]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.46 ]
 [0.462]] [[31.682]
 [43.62 ]
 [31.026]
 [30.257]
 [29.761]
 [30.478]
 [33.402]] [[0.869]
 [1.329]
 [0.842]
 [0.818]
 [0.804]
 [0.825]
 [0.916]]
maxi score, test score, baseline:  0.24137999999999984 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  0.25333333333333297  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.24137999999999984 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.08539105370766
printing an ep nov before normalisation:  53.41042876568939
maxi score, test score, baseline:  0.24137999999999984 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24137999999999984 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  0.1999999999999994  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24137999999999984 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24137999999999984 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]
 [0.249]]
printing an ep nov before normalisation:  42.969417572021484
maxi score, test score, baseline:  0.24137999999999984 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24137999999999984 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.245 0.184 0.265 0.082 0.082 0.082 0.061]
printing an ep nov before normalisation:  24.876707408316896
maxi score, test score, baseline:  0.24137999999999984 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.9556497218841
actor:  1 policy actor:  1  step number:  53 total reward:  0.2666666666666663  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.164]
 [0.346]
 [0.164]
 [0.164]
 [0.157]
 [0.164]
 [0.164]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.164]
 [0.346]
 [0.164]
 [0.164]
 [0.157]
 [0.164]
 [0.164]]
printing an ep nov before normalisation:  48.73465061187744
maxi score, test score, baseline:  0.24137999999999984 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  37.126861106221675
maxi score, test score, baseline:  0.24137999999999984 0.6913333333333335 0.6913333333333335
maxi score, test score, baseline:  0.24137999999999984 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.46787877412184
maxi score, test score, baseline:  0.24137999999999984 0.6913333333333335 0.6913333333333335
Printing some Q and Qe and total Qs values:  [[0.412]
 [0.53 ]
 [0.412]
 [0.412]
 [0.412]
 [0.412]
 [0.412]] [[43.999]
 [42.688]
 [43.999]
 [43.999]
 [43.999]
 [43.999]
 [43.999]] [[1.026]
 [1.109]
 [1.026]
 [1.026]
 [1.026]
 [1.026]
 [1.026]]
printing an ep nov before normalisation:  49.40771062126881
maxi score, test score, baseline:  0.24137999999999984 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0
printing an ep nov before normalisation:  39.21512842178345
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.52 ]
 [0.517]
 [0.488]
 [0.518]
 [0.454]
 [0.554]] [[44.632]
 [41.843]
 [47.402]
 [40.71 ]
 [41.041]
 [41.012]
 [41.901]] [[0.916]
 [0.907]
 [1.004]
 [0.856]
 [0.892]
 [0.827]
 [0.943]]
maxi score, test score, baseline:  0.24137999999999984 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.061 0.408 0.122 0.041 0.061 0.082 0.224]
maxi score, test score, baseline:  0.24137999999999984 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  46 total reward:  0.4199999999999996  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  64 total reward:  0.17999999999999883  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.24128666666666654 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.78483554707137
printing an ep nov before normalisation:  41.2919807434082
maxi score, test score, baseline:  0.24128666666666654 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.4891, 0.0808, 0.0949, 0.0862, 0.0879, 0.0687, 0.0922],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0019,     0.9695,     0.0052,     0.0068,     0.0004,     0.0004,
            0.0158], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0857, 0.0845, 0.4057, 0.1046, 0.0873, 0.1170, 0.1152],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1265, 0.0312, 0.1169, 0.3708, 0.1097, 0.0947, 0.1501],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1073, 0.0032, 0.1063, 0.1120, 0.4741, 0.0936, 0.1034],
       grad_fn=<DivBackward0>)
K:  0  action  5 :  tensor([0.0329, 0.0122, 0.1945, 0.0424, 0.0261, 0.6349, 0.0570],
       grad_fn=<DivBackward0>)
K:  0  action  6 :  tensor([0.1392, 0.0081, 0.1533, 0.1587, 0.1377, 0.1189, 0.2840],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.24128666666666654 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.28464424101856
printing an ep nov before normalisation:  48.58462333679199
maxi score, test score, baseline:  0.24128666666666654 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24128666666666654 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.859702189095024
actor:  1 policy actor:  1  step number:  54 total reward:  0.206666666666666  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.24128666666666654 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24128666666666654 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  0.25999999999999945  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  70 total reward:  0.27333333333333276  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.24128666666666654 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.07926368713379
siam score:  -0.73343694
printing an ep nov before normalisation:  0.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24128666666666654 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  0.24128666666666654 0.6913333333333335 0.6913333333333335
printing an ep nov before normalisation:  55.7189846027197
printing an ep nov before normalisation:  43.85478773863215
maxi score, test score, baseline:  0.2383133333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2383133333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.09976970613147
maxi score, test score, baseline:  0.2383133333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2383133333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2383133333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.148876251604044
maxi score, test score, baseline:  0.2383133333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2383133333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  0.38  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  53 total reward:  0.30666666666666664  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  53.32467573521517
maxi score, test score, baseline:  0.2383133333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.224]
 [0.2  ]
 [0.243]
 [0.243]
 [0.202]
 [0.243]] [[35.961]
 [38.996]
 [29.705]
 [35.961]
 [35.961]
 [29.805]
 [35.961]] [[0.783]
 [0.843]
 [0.578]
 [0.783]
 [0.783]
 [0.581]
 [0.783]]
Printing some Q and Qe and total Qs values:  [[0.872]
 [0.954]
 [0.836]
 [0.827]
 [0.872]
 [0.872]
 [0.828]] [[27.725]
 [31.99 ]
 [28.661]
 [28.851]
 [27.725]
 [27.725]
 [29.809]] [[0.872]
 [0.954]
 [0.836]
 [0.827]
 [0.872]
 [0.872]
 [0.828]]
maxi score, test score, baseline:  0.2383133333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.805]
 [0.853]
 [0.805]
 [0.805]
 [0.805]
 [0.805]
 [0.803]] [[24.51 ]
 [23.971]
 [24.51 ]
 [24.51 ]
 [24.51 ]
 [24.51 ]
 [24.36 ]] [[0.805]
 [0.853]
 [0.805]
 [0.805]
 [0.805]
 [0.805]
 [0.803]]
Printing some Q and Qe and total Qs values:  [[0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]] [[30.975]
 [30.975]
 [30.975]
 [30.975]
 [30.975]
 [30.975]
 [30.975]] [[0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2383133333333332 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]] [[29.28]
 [29.28]
 [29.28]
 [29.28]
 [29.28]
 [29.28]
 [29.28]] [[0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]
 [0.79]]
actor:  0 policy actor:  0  step number:  56 total reward:  0.2866666666666664  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.24088666666666655 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24088666666666655 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  0.3199999999999995  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  42 total reward:  0.4866666666666667  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.131]] [[44.64]
 [44.64]
 [44.64]
 [44.64]
 [44.64]
 [44.64]
 [44.64]] [[0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]
 [0.624]]
maxi score, test score, baseline:  0.2411799999999999 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.49975016010993
printing an ep nov before normalisation:  33.94587970511128
printing an ep nov before normalisation:  34.896290311111066
printing an ep nov before normalisation:  48.83036232654998
printing an ep nov before normalisation:  53.43881303298851
actions average: 
K:  3  action  0 :  tensor([0.4909, 0.0219, 0.0848, 0.0943, 0.1096, 0.0871, 0.1114],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0144,     0.9375,     0.0030,     0.0042,     0.0018,     0.0004,
            0.0387], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0890, 0.0063, 0.3668, 0.0924, 0.1004, 0.2205, 0.1245],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0618, 0.0103, 0.0769, 0.5244, 0.1189, 0.1055, 0.1022],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1125, 0.0103, 0.0699, 0.0762, 0.5422, 0.0817, 0.1072],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.0707, 0.0179, 0.2329, 0.0746, 0.0631, 0.4278, 0.1131],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1132, 0.1673, 0.0591, 0.1443, 0.0722, 0.0594, 0.3847],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  33.59637519372351
actions average: 
K:  3  action  0 :  tensor([0.4622, 0.1033, 0.0684, 0.0847, 0.0905, 0.0801, 0.1110],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0057,     0.9189,     0.0121,     0.0017,     0.0008,     0.0011,
            0.0596], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0990, 0.0045, 0.4567, 0.0920, 0.0979, 0.1318, 0.1181],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1491, 0.0184, 0.1072, 0.2784, 0.1530, 0.1415, 0.1524],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1368, 0.0010, 0.0497, 0.0478, 0.6316, 0.0762, 0.0568],
       grad_fn=<DivBackward0>)
K:  3  action  5 :  tensor([0.1218, 0.0079, 0.1069, 0.0952, 0.0996, 0.4487, 0.1199],
       grad_fn=<DivBackward0>)
K:  3  action  6 :  tensor([0.1198, 0.1362, 0.1090, 0.1204, 0.1237, 0.1241, 0.2669],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2411799999999999 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2411799999999999 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2411799999999999 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]] [[49.833]
 [49.833]
 [49.833]
 [49.833]
 [49.833]
 [49.833]
 [49.833]] [[0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]
 [0.871]]
actor:  0 policy actor:  0  step number:  45 total reward:  0.413333333333333  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.24400666666666654 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24400666666666654 0.6913333333333335 0.6913333333333335
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.221813395808866
actor:  1 policy actor:  1  step number:  56 total reward:  0.17999999999999938  reward:  1.0 rdn_beta:  0.333
